WEBVTT

00:00:19.700 --> 00:00:20.567 align:middle
大家好

00:00:20.767 --> 00:00:24.367 align:middle
我是Henry Mason
是Siri语音识别工程师

00:00:25.433 --> 00:00:28.733 align:middle
今天我们非常激动地
发布一项全新的API

00:00:28.933 --> 00:00:32.567 align:middle
它将让语音识别
也能为你的app解决问题

00:00:35.100 --> 00:00:37.767 align:middle
先快速回顾一下什么是语音识别

00:00:38.300 --> 00:00:40.467 align:middle
语音识别是自动的过程

00:00:40.533 --> 00:00:43.500 align:middle
将人类语音的音频转换成文本

00:00:44.133 --> 00:00:46.267 align:middle
它取决于语音的语言

00:00:46.467 --> 00:00:49.533 align:middle
比如 英语会和汉语的识别不同

00:00:50.233 --> 00:00:52.333 align:middle
在iOS 大多数人会想到Siri

00:00:52.433 --> 00:00:55.367 align:middle
但语音识别对许多其他任务也有用

00:00:56.367 --> 00:00:58.767 align:middle
由于Siri与iPhone 4S
一起发布

00:00:59.200 --> 00:01:01.600 align:middle
iOS也带有keyboard听写

00:00:59.200 --> 00:01:01.600 align:middle
iOS也带有keyboard听写

00:01:02.700 --> 00:01:06.267 align:middle
在iOS keyboard空格键旁
那个小小的麦克风按键

00:01:06.333 --> 00:01:10.067 align:middle
触发对任何UI kit
文本输入的语音识别

00:01:11.033 --> 00:01:13.733 align:middle
每天有成千上万个应用使用这个功能

00:01:14.200 --> 00:01:17.300 align:middle
事实上 大约三分之一的请求
来自第三方应用

00:01:18.100 --> 00:01:20.067 align:middle
它使用起来极其方便

00:01:20.133 --> 00:01:23.400 align:middle
它处理录音和录音中断

00:01:23.667 --> 00:01:25.567 align:middle
它显示用户界面

00:01:25.633 --> 00:01:27.433 align:middle
它不需要你再写任何代码

00:01:27.500 --> 00:01:29.700 align:middle
就能支持任何文本输入

00:01:30.600 --> 00:01:32.700 align:middle
而且它从iOS 5开始就可供使用

00:01:32.767 --> 00:01:35.900 align:middle
iOS keyboard听写
从2011年起便可供使用

00:01:35.967 --> 00:01:38.333 align:middle
但它的简化带来很多限制

00:01:40.033 --> 00:01:43.567 align:middle
你的用户界面
通常并不需要keyboard

00:01:44.433 --> 00:01:47.133 align:middle
当录音开始时你不能控制

00:01:47.733 --> 00:01:49.867 align:middle
不能控制使用哪一种语言

00:01:49.933 --> 00:01:52.667 align:middle
只是刚好使用系统的
keyboard语言

00:01:53.167 --> 00:01:56.433 align:middle
甚至没有办法知道
听写键是否可用

00:01:58.100 --> 00:02:01.633 align:middle
默认录音可能对你的使用案例不合理

00:01:58.100 --> 00:02:01.633 align:middle
默认录音可能对你的使用案例不合理

00:02:02.267 --> 00:02:04.733 align:middle
你可能想要更多信息 而不只是文本

00:02:07.100 --> 00:02:09.133 align:middle
那么现在在iOS 10

00:02:09.199 --> 00:02:11.567 align:middle
我们引入一种新的语音框架

00:02:11.667 --> 00:02:14.233 align:middle
语音识别API更加强大

00:02:14.300 --> 00:02:18.533 align:middle
它使用相同基本技术和Siri及
Dictation中所使用的一样

00:02:19.500 --> 00:02:21.600 align:middle
它提供快速而准确的结果

00:02:21.833 --> 00:02:24.167 align:middle
显而易见地定制给用户

00:02:24.600 --> 00:02:26.800 align:middle
而无需你收集任何用户数据

00:02:29.033 --> 00:02:31.633 align:middle
该框架也提供了识别的更多信息

00:02:33.300 --> 00:02:35.300 align:middle
而不只是文本

00:02:36.333 --> 00:02:38.900 align:middle
例如 我们也提供另外的解读

00:02:38.967 --> 00:02:42.867 align:middle
关于你的用户可能说了什么
置信水平 以及定时信息

00:02:44.433 --> 00:02:47.800 align:middle
用于API的音频可来自预录文件

00:02:47.900 --> 00:02:49.600 align:middle
或现场来源 比如麦克风

00:02:49.667 --> 00:02:52.167 align:middle
语音识别API的可用性
深远而广泛 经过许可

00:02:52.233 --> 00:02:57.133 align:middle
iOS 10支持超过50种语言和
方言 从阿拉伯语到越南语

00:02:59.000 --> 00:03:01.433 align:middle
任何运行iOS 10的设备都支持

00:02:59.000 --> 00:03:01.433 align:middle
任何运行iOS 10的设备都支持

00:03:03.233 --> 00:03:06.200 align:middle
语音识别API
通常能胜任

00:03:06.267 --> 00:03:09.367 align:middle
在需要互联网连接的大型服务器上

00:03:10.367 --> 00:03:15.800 align:middle
不过 某些新的设备确实
时刻都支持语音识别

00:03:16.400 --> 00:03:18.833 align:middle
我们提供可用性API以确定

00:03:18.900 --> 00:03:21.133 align:middle
某个既定语言当前是否可用

00:03:21.400 --> 00:03:25.233 align:middle
使用这个 而不是去寻找互联网连接

00:03:28.267 --> 00:03:29.700 align:middle
由于语音识别需要

00:03:29.767 --> 00:03:32.100 align:middle
传送用户的音频经过互联网

00:03:32.633 --> 00:03:34.667 align:middle
用户必须明确提供许可给你的应用

00:03:34.733 --> 00:03:37.767 align:middle
在可以使用语音识别之前

00:03:39.367 --> 00:03:40.767 align:middle
语音识别
解释、授权、请求

00:03:40.833 --> 00:03:44.200 align:middle
有四个主要步骤
在你的应用中采用语音识别

00:03:46.733 --> 00:03:50.267 align:middle
首先在应用的Info.plist中
提供使用描述

00:03:51.533 --> 00:03:54.967 align:middle
例如 你的相机应用Phromage

00:03:55.033 --> 00:03:59.400 align:middle
可能用了语音识别的使用描述...

00:04:00.333 --> 00:04:04.167 align:middle
这能让你只说cheese就能拍照

00:04:05.967 --> 00:04:10.367 align:middle
其次 请求授权
利用请求授权级别方法

00:04:11.467 --> 00:04:14.300 align:middle
你先前提供的解释会被呈现给用户

00:04:14.367 --> 00:04:16.533 align:middle
在一个熟悉的对话中

00:04:17.200 --> 00:04:18.800 align:middle
然后用户将能够决定

00:04:18.867 --> 00:04:21.666 align:middle
他们是否想要让你的应用语音识别

00:04:23.200 --> 00:04:25.500 align:middle
接下来 创建语音识别请求

00:04:27.133 --> 00:04:29.500 align:middle
如果你已经有录好的音频文件

00:04:29.567 --> 00:04:33.433 align:middle
使用SFSpeechURL
RecognitionRequest级别

00:04:34.000 --> 00:04:35.100 align:middle
否则 你要使用

00:04:35.167 --> 00:04:38.800 align:middle
SFSpeechAudioBuffer
RecognitionRequest

00:04:40.767 --> 00:04:42.667 align:middle
最后 提交识别请求

00:04:42.733 --> 00:04:45.433 align:middle
给SFSpeech Recognizer
开始识别

00:04:46.333 --> 00:04:49.267 align:middle
你可以选择保留返回的识别任务

00:04:49.333 --> 00:04:52.867 align:middle
这有助于监控识别过程

00:04:56.700 --> 00:04:58.567 align:middle line:1
我们来看看这个在代码中长什么样

00:04:59.300 --> 00:05:01.467 align:middle line:1
假定我们已更新info.plist

00:04:59.300 --> 00:05:01.467 align:middle line:1
假定我们已更新info.plist

00:05:01.533 --> 00:05:04.733 align:middle line:1
通过准确的描述
关于如何使用它

00:05:05.467 --> 00:05:07.500 align:middle line:1
下一步是请求授权

00:05:08.667 --> 00:05:10.467 align:middle line:1
也许最好等到

00:05:10.533 --> 00:05:12.733 align:middle line:1
用户调用你的应用的功能后再这样做

00:05:12.800 --> 00:05:14.667 align:middle line:1
这个功能要依靠语音识别

00:05:17.267 --> 00:05:20.633 align:middle line:1
请求授权级别方法
借助完成处理程序

00:05:20.700 --> 00:05:23.433 align:middle line:1
它不保证某个执行语境

00:05:24.533 --> 00:05:27.100 align:middle line:1
应用通常要发送到主队列

00:05:27.167 --> 00:05:31.000 align:middle line:1
如果它们要做点什么
比如开启或关闭用户界面按钮

00:05:33.967 --> 00:05:37.600 align:middle line:1
如果你的授权处理程序
已给出authorized状态

00:05:38.200 --> 00:05:40.100 align:middle line:1
你应该准备开始识别

00:05:41.567 --> 00:05:44.300 align:middle
否则 识别就无法对你的应用可用

00:05:45.500 --> 00:05:48.500 align:middle
重要的是采用合适的方法
禁用必要的功能

00:05:48.567 --> 00:05:50.267 align:middle
当用户作出这个决定时

00:05:50.800 --> 00:05:54.767 align:middle
或当设备受限 无法使用语音识别时

00:05:55.500 --> 00:05:59.300 align:middle
授权可稍后修改
在设备的隐私设置里

00:06:01.633 --> 00:06:04.900 align:middle line:1
我们来看看如何识别
一个预录的音频文件

00:06:05.867 --> 00:06:08.133 align:middle line:1
假设我们已有一个文件url

00:06:09.800 --> 00:06:14.867 align:middle line:1
识别需要语音识别程序
它只识别一种语言

00:06:15.467 --> 00:06:19.833 align:middle line:1
默认的SFSpeechRecognizer
启动程序可能会失败

00:06:20.533 --> 00:06:23.933 align:middle line:1
于是我返回0
如果区域不支持的话

00:06:24.833 --> 00:06:27.867 align:middle line:1
默认的启动程序使用设备的当前区域

00:06:29.800 --> 00:06:32.400 align:middle line:1
在这个功能中
我们只要返回1 在这个情况下

00:06:34.700 --> 00:06:38.633 align:middle line:1
虽然这个语音识别可能受支持
但它也许不可用

00:06:38.700 --> 00:06:41.133 align:middle line:1
可能由于没有互联网连接

00:06:41.900 --> 00:06:45.833 align:middle line:1
使用isAvailable属性
在你的识别程序中 以便监控它

00:06:48.833 --> 00:06:52.400 align:middle line:1
现在我们创建一个识别请求
用录好的文件的url

00:06:52.467 --> 00:06:57.300 align:middle line:1
然后将它给予识别程序的识别任务方法

00:07:01.967 --> 00:07:03.800 align:middle
这个方法完成处理程序

00:07:03.867 --> 00:07:06.467 align:middle
借助两种可选的参数
result和error

00:07:07.600 --> 00:07:09.233 align:middle
如果result是0

00:07:09.300 --> 00:07:11.700 align:middle
那意味着出于某种原因 识别失败

00:07:12.000 --> 00:07:14.267 align:middle
检查error的参数 寻求解释

00:07:15.667 --> 00:07:18.900 align:middle
否则 我们可以读出
我们已经识别的语音

00:07:19.233 --> 00:07:20.367 align:middle
通过查看结果

00:07:21.633 --> 00:07:25.167 align:middle
注意 完成处理程序
可能会被唤起不止一次

00:07:25.233 --> 00:07:27.267 align:middle
当语音被逐步识别

00:07:28.200 --> 00:07:30.400 align:middle
你可以确定识别已完成

00:07:30.467 --> 00:07:33.900 align:middle
通过检查结果的isFinal属性

00:07:34.667 --> 00:07:37.600 align:middle
这里我们只打印出最终识别的文本

00:07:43.833 --> 00:07:47.867 align:middle line:1
识别来自设备麦克风的
现场音频也很相似

00:07:47.933 --> 00:07:49.400 align:middle line:1
但需要一些改动

00:07:50.500 --> 00:07:53.367 align:middle line:1
我们要做出音频缓冲识别请求

00:07:53.867 --> 00:07:56.933 align:middle line:1
这能让我们提供内存音频缓冲的序列

00:07:57.000 --> 00:07:58.567 align:middle line:1
而不是硬盘上的文件

00:07:59.633 --> 00:08:03.300 align:middle line:1
我们使用AVAudioEngine
来获取音频缓冲流

00:07:59.633 --> 00:08:03.300 align:middle line:1
我们使用AVAudioEngine
来获取音频缓冲流

00:08:04.933 --> 00:08:06.700 align:middle line:1
然后将其附加到请求

00:08:07.467 --> 00:08:09.733 align:middle line:1
注意 完全可以附加音频缓冲

00:08:09.800 --> 00:08:13.533 align:middle line:1
到识别请求
在开始识别之前和之后

00:08:17.333 --> 00:08:18.233 align:middle
一个不同之处在于

00:08:18.300 --> 00:08:22.767 align:middle
我们不再忽略识别任务方法的返回值

00:08:23.400 --> 00:08:25.733 align:middle
反而 我们要将它保存在
一个变量的属性中

00:08:26.300 --> 00:08:27.533 align:middle
等会儿我们就知道为什么

00:08:28.833 --> 00:08:30.100 align:middle
当我们完成录音后

00:08:31.133 --> 00:08:34.232 align:middle
我们需要通知请求
没有更多音频了

00:08:34.299 --> 00:08:35.967 align:middle
以便它能完成识别

00:08:36.933 --> 00:08:39.200 align:middle
使用endAudio方法来实现

00:08:40.267 --> 00:08:44.100 align:middle
但要是用户取消录音
或者录音被中断呢？

00:08:44.733 --> 00:08:47.333 align:middle
在这种情况下 我们真的不关心结果

00:08:47.400 --> 00:08:51.000 align:middle
而且我们应该释放
仍在被语音识别使用的任何资源

00:08:52.700 --> 00:08:55.067 align:middle
只要取消我们开始的识别任务...

00:08:55.133 --> 00:08:57.000 align:middle
我们开始识别时保存的

00:08:57.367 --> 00:09:00.100 align:middle
这对于预录音频的识别也能做到

00:08:57.367 --> 00:09:00.100 align:middle
这对于预录音频的识别也能做到

00:09:00.967 --> 00:09:02.100 align:middle
最佳做法

00:09:02.167 --> 00:09:04.633 align:middle
简单说说一些最佳做法

00:09:05.833 --> 00:09:07.367 align:middle
资源
负责任

00:09:07.433 --> 00:09:10.667 align:middle
我们开放语音识别
给所有应用免费使用

00:09:11.233 --> 00:09:13.100 align:middle
但我们的确有设置一些合理的限制

00:09:13.167 --> 00:09:15.600 align:middle
以便这项服务一直对每个人可用

00:09:17.100 --> 00:09:20.033 align:middle
不同的设备可能受限于

00:09:20.100 --> 00:09:21.700 align:middle
每天可以识别的量

00:09:22.967 --> 00:09:26.900 align:middle
应用也会在全球范围内被节流
根据每天的请求

00:09:28.600 --> 00:09:33.133 align:middle
正如API支持的其他服务
例如CLGO Coder

00:09:33.567 --> 00:09:36.733 align:middle
要有所准备以处理
网络和速率受限的故障

00:09:38.233 --> 00:09:40.667 align:middle
如果你发现你经常达到节流的限制

00:09:40.733 --> 00:09:41.767 align:middle
请告诉我们

00:09:44.633 --> 00:09:46.833 align:middle
同样重要的是 要注意语音识别

00:09:46.900 --> 00:09:50.467 align:middle
会极大地耗费电池和网络流量

00:09:52.333 --> 00:09:56.900 align:middle line:1
对于iOS 10我们开始限制
音频长度为大约一分钟

00:09:56.967 --> 00:09:59.133 align:middle line:1
类似于keyboard听写的时长

00:10:01.100 --> 00:10:02.767 align:middle
隐私和可用性
透明度

00:10:02.833 --> 00:10:06.500 align:middle
简单说说关于透明度
以及尊重用户的隐私

00:10:07.600 --> 00:10:09.267 align:middle
如果你在录用户的语音

00:10:09.333 --> 00:10:12.833 align:middle
最好在你的用户界面中说得非常明确

00:10:13.533 --> 00:10:17.100 align:middle
播放录制的声音和/或
显示可见的录制指示

00:10:17.167 --> 00:10:20.167 align:middle
可让用户清楚知道他们正在被录音

00:10:22.133 --> 00:10:24.733 align:middle
有些语音不适合识别

00:10:25.467 --> 00:10:29.133 align:middle
密码、健康数据、财务信息
以及其他敏感语音

00:10:29.200 --> 00:10:31.333 align:middle
不应给予语音识别

00:10:33.733 --> 00:10:37.100 align:middle
显示识别的语音
像Siri和Dictation做的

00:10:37.167 --> 00:10:40.333 align:middle
也能帮助用户理解你的应用在做什么

00:10:40.900 --> 00:10:42.467 align:middle
它对用户很有帮助

00:10:42.533 --> 00:10:45.300 align:middle
以便他们可以在识别出错时及时看到

00:10:46.500 --> 00:10:47.767 align:middle
总结

00:10:47.833 --> 00:10:49.300 align:middle
那么 开发者们

00:10:49.367 --> 00:10:50.867 align:middle
你们的应用现在可以免费获得

00:10:50.933 --> 00:10:54.233 align:middle
高性能的语音识别
可识别几十种语言

00:10:54.933 --> 00:10:57.567 align:middle
但重要的是要得体地处理
当它不可用时的情况

00:10:57.633 --> 00:10:59.833 align:middle
或者用户不想让你的应用使用它

00:11:01.067 --> 00:11:03.233 align:middle
透明度是最好的政策

00:11:03.400 --> 00:11:06.500 align:middle
让用户清楚知道
什么时候语音识别正在被使用

00:11:07.900 --> 00:11:12.300 align:middle
我们很兴奋地期待
你们会为语音识别带来什么新用途

00:11:13.200 --> 00:11:14.033 align:middle
更多信息

00:11:14.100 --> 00:11:18.400 align:middle
欲了解更多信息及一些样本代码
请查看本讲的网页

00:11:18.900 --> 00:11:21.700 align:middle
你可能会对部分关于
SiriKit的会话感兴趣

00:11:21.867 --> 00:11:25.633 align:middle
周三有一场
周四有一场更高级别的

00:11:26.633 --> 00:11:29.233 align:middle
谢谢参与
祝你们在 WWDC 大有收获
