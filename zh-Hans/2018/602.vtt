WEBVTT

00:00:17.417 --> 00:00:22.122 align:middle line:0
（ARKit 2新特性
演讲602）

00:00:26.527 --> 00:00:27.961 align:middle line:-1
早上好

00:00:29.930 --> 00:00:32.698 align:middle line:-2
欢迎参加我们的演讲
“ARKit 2新特性”

00:00:34.768 --> 00:00:37.838 align:middle line:-2
我叫Arsalan
我是ARKit团队的工程师

00:00:41.074 --> 00:00:46.046 align:middle line:-1
去年 我们非常高兴能将ARKit

00:00:47.314 --> 00:00:49.149 align:middle line:-2
作为iOS 11更新
的一部分交给你

00:00:51.118 --> 00:00:55.556 align:middle line:-1
ARKit已部署到数亿台设备

00:00:56.023 --> 00:00:59.893 align:middle line:-2
使iOS成为最大
且最先进的AR平台

00:01:03.230 --> 00:01:07.534 align:middle line:-2
ARKit为你提供了强大功能集的
简单易用的接口

00:01:10.103 --> 00:01:13.740 align:middle line:-2
我们对你们到目前为止
使用ARKit所创造的东西

00:01:13.807 --> 00:01:14.975 align:middle line:-1
感到非常惊讶

00:01:15.409 --> 00:01:18.011 align:middle line:-2
让我们看看App Store中的
一些例子

00:01:20.214 --> 00:01:25.686 align:middle line:-2
Civilizations是一款AR app
它可以在你面前展示历史文物

00:01:26.253 --> 00:01:28.689 align:middle line:-1
你可以从各个角度查看它们

00:01:28.755 --> 00:01:33.861 align:middle line:-2
你还可以启用X射线模式
以进行更多的交互

00:01:35.128 --> 00:01:37.965 align:middle line:-1
你可以把它们放到你的后院

00:01:39.333 --> 00:01:41.001 align:middle line:-1
你甚至可以将它们放到现实背景中

00:01:41.068 --> 00:01:43.904 align:middle line:-1
就跟几百年前它们的样子一样

00:01:45.005 --> 00:01:48.141 align:middle line:-1
因此这是一个浏览历史文物的好工具

00:01:50.811 --> 00:01:55.916 align:middle line:-2
Boulevard AR是一款AR app
它可以让你以前所未有的方式

00:01:55.983 --> 00:01:58.318 align:middle line:-1
浏览艺术作品

00:01:58.685 --> 00:02:01.255 align:middle line:-1
你可以将它们放在地上或墙上

00:01:58.685 --> 00:02:01.255 align:middle line:-1
你可以将它们放在地上或墙上

00:02:01.321 --> 00:02:05.459 align:middle line:-1
你可以靠近它们来查看所有的细节

00:02:06.026 --> 00:02:09.795 align:middle line:-1
这是一个讲述艺术故事的好方法

00:02:16.336 --> 00:02:19.439 align:middle line:-2
ARKit也是一种
教育所有人的有趣方式

00:02:20.007 --> 00:02:24.878 align:middle line:-2
Free Reverse是一款
将沉浸式地貌展现到你面前的app

00:02:25.546 --> 00:02:29.082 align:middle line:-1
你可以跟随河流穿过整个地貌

00:02:29.149 --> 00:02:31.485 align:middle line:-1
并看到群体和野生动物

00:02:32.019 --> 00:02:34.121 align:middle line:-1
你可以看到人类活动

00:02:34.955 --> 00:02:40.360 align:middle line:-2
如何通过建筑对这些群体
和野生动物产生影响

00:02:42.296 --> 00:02:44.431 align:middle line:-1
所以这是教育每个人

00:02:44.498 --> 00:02:49.102 align:middle line:-1
关于环保和可持续发展的好方法

00:02:50.103 --> 00:02:52.139 align:middle line:-1
以上就是一些这样的例子

00:02:52.206 --> 00:02:55.108 align:middle line:-2
你可以在App Store中
查看更多示例

00:02:57.744 --> 00:03:00.280 align:middle line:-1
你们中有些人可能刚接触ARKit

00:02:57.744 --> 00:03:00.280 align:middle line:-1
你们中有些人可能刚接触ARKit

00:03:00.347 --> 00:03:03.717 align:middle line:-2
所以让我简单概述一下
ARKit是什么

00:03:08.555 --> 00:03:11.124 align:middle line:-1
跟踪是ARKit的核心组件

00:03:11.525 --> 00:03:15.562 align:middle line:-2
它为你提供设备在真实世界中的
位置和方向

00:03:18.999 --> 00:03:23.303 align:middle line:-2
它还可以跟踪对象
例如人脸

00:03:26.139 --> 00:03:27.541 align:middle line:-1
场景理解

00:03:28.442 --> 00:03:31.778 align:middle line:-2
通过获取有关环境的更多属性
来强化跟踪

00:03:33.046 --> 00:03:38.819 align:middle line:-2
我们可以检测水平面
例如地平面或桌面

00:03:39.720 --> 00:03:41.688 align:middle line:-1
我们还可以检测垂直平面

00:03:41.755 --> 00:03:44.925 align:middle line:-1
这让你可以将虚拟对象放置在场景中

00:03:51.098 --> 00:03:56.770 align:middle line:-2
场景理解还可以
了解环境中的光照条件

00:03:57.905 --> 00:04:02.576 align:middle line:-1
因此你可以使用光照在虚拟场景中

00:03:57.905 --> 00:04:02.576 align:middle line:-1
因此你可以使用光照在虚拟场景中

00:04:02.643 --> 00:04:04.044 align:middle line:-1
准确反映真实环境

00:04:04.111 --> 00:04:07.314 align:middle line:-2
从而使你的物体看起来
不会太亮或太暗

00:04:10.017 --> 00:04:13.453 align:middle line:-1
渲染是用户实际在设备上看到的内容

00:04:13.520 --> 00:04:15.222 align:middle line:-1
并与增强现实场景进行交互的过程

00:04:16.356 --> 00:04:19.358 align:middle line:-1
ARKit API让你能够轻松

00:04:19.426 --> 00:04:22.329 align:middle line:-1
集成你选择的任何渲染引擎

00:04:26.900 --> 00:04:30.871 align:middle line:-2
ARKit为SceneKit
和SpriteKit提供内置视图

00:04:32.606 --> 00:04:35.909 align:middle line:-2
在Xcode中
我们还有一个Metal模板

00:04:35.976 --> 00:04:39.947 align:middle line:-1
来让你快速开始自己的增强现实体验

00:04:41.615 --> 00:04:47.187 align:middle line:-2
注意 Unity和Unreal
将ARKit的完整功能集

00:04:47.254 --> 00:04:48.789 align:middle line:-1
集成到它们热门的游戏引擎中

00:04:49.389 --> 00:04:52.492 align:middle line:-2
因此你可以通过使用
所有这些渲染技术

00:04:52.926 --> 00:04:55.062 align:middle line:-1
来开始使用ARKit

00:04:58.999 --> 00:05:01.768 align:middle line:-2
让我们看看
今年ARKit 2新特性

00:04:58.999 --> 00:05:01.768 align:middle line:-2
让我们看看
今年ARKit 2新特性

00:05:07.875 --> 00:05:10.210 align:middle line:-1
我们现在可以保存和加载映射

00:05:10.277 --> 00:05:16.350 align:middle line:-2
它支持着持久性和多用户体验的
强大新功能

00:05:19.753 --> 00:05:22.289 align:middle line:-1
我们也为你提供环境纹理

00:05:22.356 --> 00:05:25.559 align:middle line:-1
以便你可以逼真地渲染增强现实场景

00:05:27.728 --> 00:05:30.998 align:middle line:-2
ARKit现在可以
实时跟踪2D图像

00:05:33.400 --> 00:05:37.604 align:middle line:-2
我们不仅限于2D
我们还可以检测场景中的3D对象

00:05:41.508 --> 00:05:44.978 align:middle line:-2
最后 我们为面部跟踪提供了一些
有趣的增强

00:05:48.215 --> 00:05:50.851 align:middle line:-1
让我们从保存和加载映射开始吧

00:05:56.590 --> 00:05:59.860 align:middle line:-1
保存和加载映射是世界跟踪的一部分

00:06:00.327 --> 00:06:03.931 align:middle line:-1
世界跟踪可为你提供现实世界中

00:06:04.631 --> 00:06:07.701 align:middle line:-1
设备的六自由度位置和方向

00:06:08.135 --> 00:06:11.972 align:middle line:-1
这可以让你在场景中放置对象

00:06:12.639 --> 00:06:16.376 align:middle line:-1
比如你在此视频中看到的这些桌椅

00:06:18.645 --> 00:06:22.549 align:middle line:-2
世界跟踪还可以为你提供
准确的物理比例

00:06:22.616 --> 00:06:25.652 align:middle line:-1
以便你可以正确的比例放置对象

00:06:26.119 --> 00:06:29.122 align:middle line:-1
因此你的对象看起来不会太大或太小

00:06:34.061 --> 00:06:39.233 align:middle line:-1
这也可用于实现精确测量

00:06:39.299 --> 00:06:41.335 align:middle line:-2
如昨天看到的
Measure app

00:06:44.605 --> 00:06:49.443 align:middle line:-1
世界跟踪还为你提供3D特征点

00:06:49.510 --> 00:06:51.712 align:middle line:-1
以便你了解环境的一些物理结构

00:06:52.012 --> 00:06:56.617 align:middle line:-2
这可用于在场景中放置对象时
进行碰撞测试

00:07:00.387 --> 00:07:04.024 align:middle line:-2
在iOS 11.3中
我们引入了重定位

00:07:04.725 --> 00:07:08.228 align:middle line:-1
此功能可让你在AR会话中断后

00:07:08.295 --> 00:07:11.465 align:middle line:-1
恢复跟踪状态

00:07:12.866 --> 00:07:15.469 align:middle line:-2
这种情况可能发生在
比如你的app被切换到后台时

00:07:16.003 --> 00:07:19.139 align:middle line:-2
或者你在iPad的图片模式上
使用图片时

00:07:24.044 --> 00:07:30.017 align:middle line:-2
重定位可以与
由世界跟踪持续构建的映射

00:07:30.384 --> 00:07:32.686 align:middle line:-1
一起使用

00:07:33.954 --> 00:07:37.824 align:middle line:-1
我们越是绕着环境进行移动

00:07:38.392 --> 00:07:40.561 align:middle line:-1
它就越能够扩展并学习越多

00:07:40.627 --> 00:07:43.430 align:middle line:-1
关于环境的不同特征

00:07:48.135 --> 00:07:53.774 align:middle line:-2
该映射过去只有在你的AR会话
仍存活时才可用

00:07:54.875 --> 00:07:57.945 align:middle line:-1
但现在我们为你提供此映射

00:08:00.214 --> 00:08:05.519 align:middle line:-2
ARKit API中 此映射作为
ARWorldMap对象提供给你

00:08:14.228 --> 00:08:17.865 align:middle line:-2
ARWorldMap
代表物理3D空间的映射

00:08:18.498 --> 00:08:23.637 align:middle line:-1
类似于我们在右侧视图中看到的那样

00:08:25.873 --> 00:08:29.610 align:middle line:-1
我们也知道锚是物理空间中的重要点

00:08:30.210 --> 00:08:33.947 align:middle line:-1
它们是你想要放置虚拟对象的位置

00:08:34.481 --> 00:08:39.419 align:middle line:-2
我们在ARWorldMap中
默认也包含平面锚

00:08:40.988 --> 00:08:45.092 align:middle line:-2
此外你还可以将自定义锚
添加到此列表中

00:08:45.158 --> 00:08:46.793 align:middle line:-1
因为这是一个可变列表

00:08:47.160 --> 00:08:51.431 align:middle line:-2
你可以在场景中创建自定义锚
并将其添加到WorldMap

00:08:57.504 --> 00:09:03.143 align:middle line:-2
对可视化和调试 WorldMap
还为你提供原始特征点和范围

00:08:57.504 --> 00:09:03.143 align:middle line:-2
对可视化和调试 WorldMap
还为你提供原始特征点和范围

00:09:03.210 --> 00:09:09.283 align:middle line:-1
以便你了解刚扫描的真实物理空间

00:09:12.753 --> 00:09:16.690 align:middle line:-2
更重要的是 WorldMap
是一个可序列化对象

00:09:17.291 --> 00:09:21.195 align:middle line:-2
因此它可以序列化为
你选择的任何数据流

00:09:21.261 --> 00:09:26.600 align:middle line:-2
例如本地文件系统上的文件
或共享网络位置

00:09:29.536 --> 00:09:31.972 align:middle line:-1
这个ARWorldMap对象

00:09:32.239 --> 00:09:37.344 align:middle line:-2
支持着ARKit中的
两组强大的新体验

00:09:39.546 --> 00:09:41.849 align:middle line:-1
首先是持久性

00:09:44.284 --> 00:09:46.787 align:middle line:-2
这是向你展示它是如何工作的
一个例子

00:09:48.288 --> 00:09:54.127 align:middle line:-2
我们有一个用户开始进行世界跟踪
他通过ARKit碰撞测试

00:09:54.494 --> 00:09:56.296 align:middle line:-1
将一个对象放在场景中

00:09:57.564 --> 00:10:04.104 align:middle line:-2
在离开场景之前
他在设备上保存了WorldMap

00:09:57.564 --> 00:10:04.104 align:middle line:-2
在离开场景之前
他在设备上保存了WorldMap

00:10:08.876 --> 00:10:12.346 align:middle line:-1
过了一段时间

00:10:12.913 --> 00:10:14.314 align:middle line:-1
该用户回来了

00:10:15.048 --> 00:10:18.051 align:middle line:-1
他可以加载相同的WorldMap

00:10:18.452 --> 00:10:22.222 align:middle line:-1
并找回相同的增强现实体验

00:10:22.623 --> 00:10:27.394 align:middle line:-2
他可以多次重复这种体验
并且他会发现

00:10:27.461 --> 00:10:31.198 align:middle line:-2
每次开始体验时
这些物体都会出现在桌子上

00:10:31.398 --> 00:10:34.968 align:middle line:-1
这就是世界跟踪的持久性

00:10:37.771 --> 00:10:38.605 align:middle line:-1
谢谢

00:10:44.311 --> 00:10:47.614 align:middle line:-2
ARWorldMap
还支持多用户体验

00:10:49.249 --> 00:10:51.251 align:middle line:-1
现在你的增强现实体验

00:10:51.318 --> 00:10:53.887 align:middle line:-1
不仅限于单个设备或单个用户

00:10:54.888 --> 00:10:57.524 align:middle line:-1
它可以与许多用户共享

00:11:00.294 --> 00:11:06.800 align:middle line:-2
一个用户可以创建WorldMap
并将其分享给一个或多个用户

00:11:08.335 --> 00:11:14.474 align:middle line:-2
请注意 WorldMap代表
现实世界中的单个坐标系

00:11:14.942 --> 00:11:19.646 align:middle line:-2
这意味着所有用户将共享
同一个坐标空间

00:11:20.347 --> 00:11:23.917 align:middle line:-1
他们能够从不同的角度体验到

00:11:23.984 --> 00:11:25.519 align:middle line:-1
相同的增强现实体验

00:11:27.621 --> 00:11:29.256 align:middle line:-1
这是一个很棒的特性

00:11:29.723 --> 00:11:32.593 align:middle line:-1
你可以使用WorldMap

00:11:32.659 --> 00:11:37.497 align:middle line:-2
实现多用户游戏
比如我们昨天看到的游戏

00:11:39.800 --> 00:11:45.906 align:middle line:-2
我们还可使用ARWorldMap
创建多用户共享教育体验

00:11:50.143 --> 00:11:55.382 align:middle line:-2
注意我们现将ARWorldMap
对象交到你手中

00:11:55.449 --> 00:12:00.587 align:middle line:-2
因此你可以自由选择
任何与其它用户共享的技术

00:11:55.449 --> 00:12:00.587 align:middle line:-2
因此你可以自由选择
任何与其它用户共享的技术

00:12:02.422 --> 00:12:07.227 align:middle line:-2
比如对于共享 你可以使用
AirDrop或Multipeer Connectivity

00:12:07.294 --> 00:12:10.664 align:middle line:-2
它们依赖于本地蓝牙
或Wi-Fi连接

00:12:11.999 --> 00:12:14.968 align:middle line:-1
这意味着你不需要联网

00:12:15.035 --> 00:12:16.537 align:middle line:-1
就可以使用此功能

00:12:22.643 --> 00:12:25.312 align:middle line:-2
让我们看看
ARKit API如何让你

00:12:25.379 --> 00:12:28.148 align:middle line:-2
轻松实现获取
并加载WorldMap

00:12:31.018 --> 00:12:32.653 align:middle line:-1
在你的AR会话对象上

00:12:33.320 --> 00:12:36.290 align:middle line:-1
你可以在任何时间点

00:12:36.356 --> 00:12:38.091 align:middle line:-1
调用getCurrentWorldMap

00:12:39.793 --> 00:12:42.229 align:middle line:-1
此方法带有一个完成处理程序

00:12:42.729 --> 00:12:46.166 align:middle line:-2
它将返回一个
ARWorldMap对象

00:12:48.869 --> 00:12:51.738 align:middle line:-1
另外注意它也可能返回错误

00:12:51.805 --> 00:12:53.941 align:middle line:-1
如果WorldMap不可用的话

00:12:54.174 --> 00:12:57.611 align:middle line:-2
因此在app代码中
处理此错误非常重要

00:12:59.746 --> 00:13:01.715 align:middle line:-1
一旦你得到了ARWorldMap

00:12:59.746 --> 00:13:01.715 align:middle line:-1
一旦你得到了ARWorldMap

00:13:02.916 --> 00:13:04.084 align:middle line:-1
你可以简单地

00:13:07.554 --> 00:13:11.458 align:middle line:0
在世界跟踪配置中设置
initialWorldMap属性

00:13:12.025 --> 00:13:14.127 align:middle line:0
并运行你的会话

00:13:15.996 --> 00:13:18.732 align:middle line:-1
注意这也可以动态更改

00:13:18.799 --> 00:13:23.203 align:middle line:-2
因此你始终可以通过运行新配置
来重新配置AR会话

00:13:25.539 --> 00:13:28.876 align:middle line:-2
一旦用ARWorldMap
启动了AR会话

00:13:30.143 --> 00:13:32.579 align:middle line:-2
它将遵循与iOS 11.3中
引入的重定位

00:13:32.980 --> 00:13:36.483 align:middle line:-1
完全相同的行为

00:13:44.558 --> 00:13:50.831 align:middle line:-2
重定位的可靠性
对你的app体验来说非常重要

00:13:52.466 --> 00:13:55.869 align:middle line:-2
因此获取好的WorldMap
非常重要

00:13:56.303 --> 00:13:58.572 align:middle line:-1
注意你可以随时调用

00:13:58.639 --> 00:14:00.274 align:middle line:-1
getCurrentWorldMap

00:13:58.639 --> 00:14:00.274 align:middle line:-1
getCurrentWorldMap

00:14:02.442 --> 00:14:06.213 align:middle line:-1
从多个不同视角扫描你的物理空间

00:14:06.747 --> 00:14:08.482 align:middle line:-1
非常重要

00:14:08.549 --> 00:14:11.385 align:middle line:-1
这样跟踪系统才可以真正理解

00:14:11.451 --> 00:14:13.587 align:middle line:-1
周围环境的物理结构

00:14:16.290 --> 00:14:21.428 align:middle line:-2
环境应该是静态的并且纹理清晰
以便我们可以提取它的更多特征

00:14:21.495 --> 00:14:24.464 align:middle line:-1
并了解有关环境的更多信息

00:14:27.835 --> 00:14:32.039 align:middle line:-1
而且在映射上有密集的特征点很重要

00:14:32.206 --> 00:14:35.075 align:middle line:-1
这样它就可以可靠地重定位

00:14:37.344 --> 00:14:40.480 align:middle line:-1
但你不必担心所有这些问题

00:14:41.582 --> 00:14:45.552 align:middle line:0
在ARKit中 其API通过向你提供
ARFrame上的WorldMappingStatus

00:14:45.619 --> 00:14:48.422 align:middle line:0
来大大简化你的工作

00:14:49.323 --> 00:14:54.194 align:middle line:-2
WorldMappingStatus在每个
ARFrame中进行更新

00:14:54.261 --> 00:14:56.463 align:middle line:-2
并且可以通过
WorldMappingStatus属性获取

00:14:57.064 --> 00:14:58.599 align:middle line:-1
让我们看看这是如何工作的

00:15:01.568 --> 00:15:04.938 align:middle line:-2
当我们刚开始世界跟踪时
WorldMappingStatus为不可用

00:15:05.005 --> 00:15:07.407 align:middle line:0
当我们开始扫描物理空间时

00:15:07.474 --> 00:15:08.909 align:middle line:0
它会变为受限制

00:15:10.577 --> 00:15:13.447 align:middle line:0
我们在真实世界中移动得越多

00:15:14.014 --> 00:15:16.750 align:middle line:0
世界跟踪将继续扩展该映射

00:15:19.553 --> 00:15:24.291 align:middle line:0
如果我们从当前的角度
扫描了足够多的真实世界信息

00:15:24.358 --> 00:15:26.360 align:middle line:0
WorldMappingStatus将变为已映射

00:15:34.568 --> 00:15:39.306 align:middle line:0
注意如果你从已映射的物理空间移开

00:15:39.873 --> 00:15:42.609 align:middle line:0
WorldMappingStatus可能会
回到受限制状态

00:15:43.076 --> 00:15:44.711 align:middle line:0
并将开始学习更多

00:15:44.778 --> 00:15:48.382 align:middle line:0
关于我们所看到的新环境的信息

00:15:51.285 --> 00:15:54.388 align:middle line:-2
那么如何在你的app代码中
使用WorldMappingStatus呢

00:15:56.423 --> 00:16:01.028 align:middle line:-2
假设你有一个app可以让你
与其他用户分享你的WorldMap

00:15:56.423 --> 00:16:01.028 align:middle line:-2
假设你有一个app可以让你
与其他用户分享你的WorldMap

00:16:01.762 --> 00:16:05.432 align:middle line:-2
并且你的用户界面上有一个
共享映射按钮

00:16:07.401 --> 00:16:10.637 align:middle line:-2
当WorldMappingStatus
为不可用或受限制时

00:16:10.704 --> 00:16:12.773 align:middle line:-1
禁用此按钮是一种好实践

00:16:14.474 --> 00:16:17.010 align:middle line:-1
当WorldMappingStatus为扩展时

00:16:18.212 --> 00:16:21.481 align:middle line:-1
你可能希望在UI上显示活动指示器

00:16:21.882 --> 00:16:26.486 align:middle line:-2
这会鼓励你的终端用户
继续在物理世界中移动

00:16:26.553 --> 00:16:29.857 align:middle line:-1
并继续扫描以扩展映射

00:16:30.457 --> 00:16:32.426 align:middle line:-1
因为你需要它来进行重定位

00:16:36.530 --> 00:16:39.233 align:middle line:-2
一旦WorldMappingStatus
完全映射后

00:16:40.234 --> 00:16:45.472 align:middle line:-2
你就可以启用共享映射按钮
并隐藏你的活动指示器

00:16:45.939 --> 00:16:48.575 align:middle line:-1
这将让你的用户可以分享该映射

00:16:53.614 --> 00:16:56.717 align:middle line:-2
让我们来看一个
保存和加载WorldMap的演示

00:17:06.627 --> 00:17:07.493 align:middle line:-1
好的

00:17:09.229 --> 00:17:11.164 align:middle line:-1
我们可以切换到AR 1吗？

00:17:13.200 --> 00:17:14.034 align:middle line:-1
好的

00:17:14.601 --> 00:17:17.171 align:middle line:-1
对于这个演示 我有两个app

00:17:17.570 --> 00:17:22.709 align:middle line:-2
第一个app中 我将获取
WorldMap并将其保存到本地文件

00:17:23.277 --> 00:17:28.682 align:middle line:-2
第二个app中 我将加载
这个WorldMap来恢复

00:17:28.749 --> 00:17:31.418 align:middle line:-1
同样的增强现实体验

00:17:31.485 --> 00:17:32.386 align:middle line:-1
让我们开始吧

00:17:35.856 --> 00:17:39.726 align:middle line:-2
如你所见 WorldMappingStatus
位于右上角

00:17:39.793 --> 00:17:41.261 align:middle line:-1
它为不可用

00:17:41.728 --> 00:17:44.531 align:middle line:-1
一旦我开始在环境中移动

00:17:45.032 --> 00:17:48.068 align:middle line:-1
它就开始扩展我的WorldMap

00:17:48.368 --> 00:17:52.005 align:middle line:-1
如果我继续在该环境中移动

00:17:53.207 --> 00:17:56.243 align:middle line:-2
WorldMappingStatus
将变为已映射

00:17:57.077 --> 00:18:01.815 align:middle line:-2
这意味着它已经从这个角度
看到了足够多的特征

00:17:57.077 --> 00:18:01.815 align:middle line:-2
这意味着它已经从这个角度
看到了足够多的特征

00:18:02.049 --> 00:18:03.617 align:middle line:-1
来进行重定位

00:18:03.684 --> 00:18:08.488 align:middle line:-2
现在是获取和序列化
WorldMap对象的好时机

00:18:10.190 --> 00:18:14.528 align:middle line:-1
但先让我们放置一个自定义锚

00:18:14.595 --> 00:18:16.296 align:middle line:-1
来让这个增强现实场景更有趣

00:18:17.064 --> 00:18:20.767 align:middle line:-2
通过碰撞测试
我创建了一个自定义锚

00:18:21.168 --> 00:18:25.172 align:middle line:-1
并将这个对象叠加上去

00:18:25.239 --> 00:18:27.307 align:middle line:-1
这是一台旧电视

00:18:27.808 --> 00:18:30.244 align:middle line:-1
我想你们大多数人以前可能见过它

00:18:33.413 --> 00:18:37.217 align:middle line:-1
当然 我仍然可以继续映射世界

00:18:37.784 --> 00:18:40.387 align:middle line:-1
让我们保存该WorldMap

00:18:41.221 --> 00:18:43.490 align:middle line:-1
当我保存WorldMap时

00:18:43.924 --> 00:18:47.694 align:middle line:-2
它还可以显示属于
此WorldMap的原始特征点

00:18:47.761 --> 00:18:52.466 align:middle line:-2
你看到的那些蓝点
它们都是WorldMap的一部分

00:18:55.402 --> 00:18:57.237 align:middle line:-1
另外作为一个好实践

00:18:58.639 --> 00:19:03.977 align:middle line:-2
在保存WorldMap时
我也保存了该视角的屏幕截图

00:18:58.639 --> 00:19:03.977 align:middle line:-2
在保存WorldMap时
我也保存了该视角的屏幕截图

00:19:07.347 --> 00:19:11.318 align:middle line:-2
现在我们已将WorldMap
序列化到我们的文件中

00:19:12.119 --> 00:19:15.189 align:middle line:-1
我们现在可以在另一个app中

00:19:15.255 --> 00:19:16.456 align:middle line:-1
恢复这个增强现实体验

00:19:16.924 --> 00:19:18.225 align:middle line:-1
让我们试试吧

00:19:18.859 --> 00:19:21.495 align:middle line:-1
我将从一个不同的位置启动此app

00:19:24.298 --> 00:19:28.936 align:middle line:-2
你可以看到这是我的世界原点
它定义在桌子的这一侧

00:19:29.469 --> 00:19:32.606 align:middle line:-2
并且我的世界跟踪
现在处于重定位状态

00:19:33.140 --> 00:19:35.676 align:middle line:-2
这是与我们在iOS 11.3中
引入的相同的

00:19:35.742 --> 00:19:37.611 align:middle line:-1
开放重定位行为

00:19:38.512 --> 00:19:44.852 align:middle line:-1
让我把设备指向我刚才

00:19:44.918 --> 00:19:47.054 align:middle line:-1
创建WorldMap的物理位置

00:19:48.722 --> 00:19:53.627 align:middle line:-2
当我指向该空间时
它就会立即将我的世界原点

00:19:54.428 --> 00:19:55.996 align:middle line:-1
恢复到原来的位置

00:19:56.496 --> 00:20:00.868 align:middle line:-1
同时它还恢复了我的自定义锚

00:19:56.496 --> 00:20:00.868 align:middle line:-1
同时它还恢复了我的自定义锚

00:20:00.934 --> 00:20:03.237 align:middle line:-1
因此我得到了完全相同的AR体验

00:20:09.543 --> 00:20:10.444 align:middle line:-1
谢谢

00:20:11.445 --> 00:20:15.449 align:middle line:-1
请注意我可以多次启动此app

00:20:15.682 --> 00:20:18.986 align:middle line:-2
并且每次启动时它都会
向我展示相同的体验

00:20:19.453 --> 00:20:20.888 align:middle line:-1
这就是持久性

00:20:22.389 --> 00:20:25.392 align:middle line:-1
当然 这也可以与其它设备共享

00:20:27.427 --> 00:20:28.929 align:middle line:-1
回到幻灯片

00:20:34.501 --> 00:20:37.304 align:middle line:-1
以上就是保存和加载映射

00:20:37.938 --> 00:20:40.741 align:middle line:-1
这是ARKit 2中强大的新功能

00:20:40.807 --> 00:20:45.879 align:middle line:-1
即支持持久性和多用户共享体验

00:20:49.716 --> 00:20:53.921 align:middle line:-2
在ARKit 2中 我们有更快的
初始化和平面检测

00:20:56.857 --> 00:20:59.159 align:middle line:-1
世界跟踪现在更加鲁棒

00:20:59.226 --> 00:21:02.329 align:middle line:-1
我们可以在更复杂的环境中检测平面

00:20:59.226 --> 00:21:02.329 align:middle line:-1
我们可以在更复杂的环境中检测平面

00:21:07.668 --> 00:21:12.206 align:middle line:-2
水平和垂直平面
都有更准确的范围和边界

00:21:12.673 --> 00:21:16.009 align:middle line:-2
这意味着你可以准确地将对象
放置在场景中

00:21:20.214 --> 00:21:24.551 align:middle line:-2
在iOS 11.3中
我们为你的增强现实体验

00:21:24.618 --> 00:21:26.920 align:middle line:-1
引入了连续自动对焦功能

00:21:28.589 --> 00:21:32.593 align:middle line:-1
iOS 12专门针对增强现实体验

00:21:32.659 --> 00:21:35.362 align:middle line:-1
进行了更多优化

00:21:39.066 --> 00:21:44.304 align:middle line:-2
我们还在ARKit中引入了
4:3视频格式

00:21:47.140 --> 00:21:53.013 align:middle line:-2
4:3是一种广角视频格式
它极大增强了iPad上可视化效果

00:21:53.080 --> 00:21:58.018 align:middle line:-2
因为iPad也有4:3的
显示屏宽高比

00:21:59.720 --> 00:22:04.558 align:middle line:-2
请注意4:3视频格式将是
ARKit 2中的默认视频格式

00:21:59.720 --> 00:22:04.558 align:middle line:-2
请注意4:3视频格式将是
ARKit 2中的默认视频格式

00:22:06.760 --> 00:22:11.698 align:middle line:-2
所有这些增强特性都将应用于
App Store中所有现有app

00:22:11.765 --> 00:22:15.369 align:middle line:-1
但4:3视频格式除外

00:22:15.435 --> 00:22:19.640 align:middle line:-2
为此你必须使用新的SDK
来重新构建你的app

00:22:23.310 --> 00:22:28.315 align:middle line:-1
回到改善终端用户体验的话题

00:22:30.817 --> 00:22:32.953 align:middle line:-1
我们引入了环境纹理

00:22:33.987 --> 00:22:39.026 align:middle line:-1
这极大增强了终端用户的渲染体验

00:22:41.261 --> 00:22:44.998 align:middle line:-1
假设你的设计师很努力地

00:22:45.065 --> 00:22:49.269 align:middle line:-2
为你的增强现实场景
创建了这些虚拟对象

00:22:50.904 --> 00:22:52.172 align:middle line:-1
这看起来真的很棒

00:22:52.806 --> 00:22:57.311 align:middle line:-1
但你需要为增强现实场景做更多事情

00:22:59.646 --> 00:23:06.153 align:middle line:-2
你需要在AR场景中保持
正确的位置和方向

00:22:59.646 --> 00:23:06.153 align:middle line:-2
你需要在AR场景中保持
正确的位置和方向

00:23:06.653 --> 00:23:11.391 align:middle line:-2
这样对象看起来就像是
放在真实世界中一样

00:23:13.694 --> 00:23:16.864 align:middle line:-1
保证比例正确也很重要

00:23:16.930 --> 00:23:18.765 align:middle line:-1
这样你的对象才不会太大或太小

00:23:19.166 --> 00:23:24.771 align:middle line:-2
ARKit会在世界跟踪中
通过为你提供正确的转换来帮助你

00:23:28.775 --> 00:23:30.010 align:middle line:-1
对于逼真的渲染

00:23:30.077 --> 00:23:33.614 align:middle line:-1
考虑环境中的光照也很重要

00:23:37.284 --> 00:23:41.221 align:middle line:-2
ARKit为你提供了环境光估计器
你可以在渲染时使用它

00:23:41.855 --> 00:23:45.158 align:middle line:-1
来纠正虚拟对象的亮度

00:23:45.359 --> 00:23:48.762 align:middle line:-1
以便你的物体看起来不会太亮或太暗

00:23:49.363 --> 00:23:51.298 align:middle line:-1
它们会直接融入环境中

00:23:54.968 --> 00:23:58.305 align:middle line:-1
如果要将物体放在物理表面上

00:23:58.372 --> 00:23:59.740 align:middle line:-1
如水平面上

00:24:00.541 --> 00:24:04.011 align:middle line:-1
为对象添加阴影也很重要

00:24:04.411 --> 00:24:07.915 align:middle line:-1
这极大改善了人类的视觉感知

00:24:07.981 --> 00:24:10.651 align:middle line:-1
他们会真正感知到物体就在表面上

00:24:13.420 --> 00:24:17.457 align:middle line:-1
最后 如果是反光物体

00:24:18.725 --> 00:24:20.194 align:middle line:-1
人们希望

00:24:21.028 --> 00:24:24.831 align:middle line:-1
从虚拟对象的表面看到环境的倒影

00:24:25.766 --> 00:24:29.303 align:middle line:-1
这就是环境纹理所能达到的效果

00:24:31.538 --> 00:24:35.676 align:middle line:-2
让我们看看这个对象
在增强现实场景中的样子

00:24:38.111 --> 00:24:40.514 align:middle line:-1
昨天晚上在我准备这个演讲时

00:24:40.581 --> 00:24:42.783 align:middle line:-1
我创造了这个场景

00:24:44.718 --> 00:24:49.890 align:middle line:-2
在吃这些水果的同时
我也想放置这个虚拟对象

00:24:51.792 --> 00:24:58.131 align:middle line:-2
你可以看到比例是正确的
更重要的是

00:24:58.632 --> 00:25:01.201 align:middle line:-1
你可以在对象中看到环境的倒影

00:24:58.632 --> 00:25:01.201 align:middle line:-1
你可以在对象中看到环境的倒影

00:25:02.436 --> 00:25:04.872 align:middle line:-1
在此对象的右侧

00:25:05.239 --> 00:25:10.244 align:middle line:-2
你可以在右边看到这些水果的
黄色和橙色的倒影

00:25:10.944 --> 00:25:14.915 align:middle line:-2
而在左侧你可以注意到
树叶的绿色纹理

00:25:16.049 --> 00:25:19.753 align:middle line:-1
你也可以在中间看到长凳表面的倒影

00:25:21.555 --> 00:25:25.492 align:middle line:-2
这是通过ARKit 2中的
环境纹理实现的

00:25:29.329 --> 00:25:30.163 align:middle line:-1
谢谢

00:25:34.034 --> 00:25:37.538 align:middle line:-1
环境纹理会收集场景的纹理信息

00:25:40.774 --> 00:25:43.877 align:middle line:-1
通常它表示为立方体贴图

00:25:43.944 --> 00:25:46.046 align:middle line:-1
但也有其它表示形式

00:25:49.116 --> 00:25:52.152 align:middle line:-1
环境纹理或这个立方体贴图

00:25:52.219 --> 00:25:55.589 align:middle line:-1
可用作渲染引擎中的反射探头

00:25:58.926 --> 00:26:03.163 align:middle line:-2
该反射探头可以将其应用为
虚拟对象上的纹理信息

00:25:58.926 --> 00:26:03.163 align:middle line:-2
该反射探头可以将其应用为
虚拟对象上的纹理信息

00:26:03.230 --> 00:26:06.567 align:middle line:-2
例如我们在上一张幻灯片中
看到的对象

00:26:07.768 --> 00:26:13.707 align:middle line:-2
因此它极大改善了
反射物体的视觉效果

00:26:15.042 --> 00:26:19.513 align:middle line:-2
让我们看看这个视频短片中
这是如何工作的

00:26:22.382 --> 00:26:26.553 align:middle line:-2
ARKit在运行世界跟踪
和场景理解的同时

00:26:26.920 --> 00:26:29.189 align:middle line:-1
继续了解有关环境的更多信息

00:26:30.224 --> 00:26:31.959 align:middle line:-1
通过使用计算机视觉技术

00:26:32.025 --> 00:26:37.865 align:middle line:-2
它可以提取纹理信息
并开始填充此立方体贴图

00:26:39.600 --> 00:26:42.870 align:middle line:-1
而该立方体贴图被准确放置在场景中

00:26:44.771 --> 00:26:47.541 align:middle line:-1
注意此立方体贴图只是部分填充

00:26:49.076 --> 00:26:51.545 align:middle line:-1
为了设置反射探头

00:26:51.612 --> 00:26:54.114 align:middle line:-1
我们需要一个完全立方体贴图

00:26:57.584 --> 00:26:59.286 align:middle line:-1
要获得完全立方体贴图

00:26:59.353 --> 00:27:03.490 align:middle line:-1
你需要扫描整个物理空间

00:26:59.353 --> 00:27:03.490 align:middle line:-1
你需要扫描整个物理空间

00:27:03.557 --> 00:27:07.294 align:middle line:-2
就像使用全景装置
进行360度扫描一样

00:27:08.562 --> 00:27:11.798 align:middle line:-1
但这对终端用户来说并不现实

00:27:13.600 --> 00:27:19.673 align:middle line:-2
ARKit通过使用先进的
机器学习算法自动完成此立方体贴图

00:27:19.740 --> 00:27:22.209 align:middle line:-1
来让该过程变得更简单

00:27:26.713 --> 00:27:27.548 align:middle line:-1
谢谢

00:27:31.485 --> 00:27:33.954 align:middle line:-1
另外注意所有这些处理

00:27:34.021 --> 00:27:36.890 align:middle line:-1
都实时发生在你的本地设备上

00:27:40.060 --> 00:27:43.497 align:middle line:-2
一旦我们有了立方体贴图
我们就可以设置反射探头

00:27:43.864 --> 00:27:47.301 align:middle line:-1
并且只要我们在场景中放置虚拟对象

00:27:47.367 --> 00:27:49.636 align:middle line:-1
它们就会开始反射真实环境

00:27:51.271 --> 00:27:52.573 align:middle line:-1
这是对环境纹理化过程

00:27:52.639 --> 00:27:55.409 align:middle line:-1
工作原理的快速概述

00:27:57.611 --> 00:28:02.883 align:middle line:-2
让我们看看ARKit API
如何让你轻松使用此特性

00:27:57.611 --> 00:28:02.883 align:middle line:-2
让我们看看ARKit API
如何让你轻松使用此特性

00:28:07.821 --> 00:28:11.758 align:middle line:-2
你需要在世界跟踪配置中
做的所有事情

00:28:12.392 --> 00:28:18.298 align:middle line:-2
就是将environmentTexturing属性
设置为.automatic并运行会话

00:28:19.166 --> 00:28:20.667 align:middle line:-1
就这么简单

00:28:29.643 --> 00:28:33.247 align:middle line:-2
AR会话将在后台自动运行
该环境纹理化过程

00:28:33.313 --> 00:28:37.284 align:middle line:-1
并将环境纹理作为环境探头锚

00:28:37.985 --> 00:28:39.753 align:middle line:-1
提供给你

00:28:41.188 --> 00:28:45.359 align:middle line:-2
AREnvironmentProbeAnchor
是ARAnchor的扩展

00:28:45.425 --> 00:28:49.162 align:middle line:-2
这意味着它具有六自由度的
位置和方向变换

00:28:50.998 --> 00:28:54.668 align:middle line:-2
此外 它有个MTLTexture
形式的立方体贴图

00:28:58.405 --> 00:29:02.709 align:middle line:-2
ARKit还为你提供了
立方体贴图的物理范围

00:28:58.405 --> 00:29:02.709 align:middle line:-2
ARKit还为你提供了
立方体贴图的物理范围

00:29:02.776 --> 00:29:07.347 align:middle line:-1
它是反射探头所影响的区域

00:29:07.915 --> 00:29:12.653 align:middle line:-1
并且渲染代理可以使用它来校正并行

00:29:12.753 --> 00:29:17.324 align:middle line:-2
因此如果你的对象在场景中移动
它将自动适应到新位置

00:29:17.391 --> 00:29:22.229 align:middle line:-1
并且环境中的新纹理将被反射出来

00:29:24.965 --> 00:29:28.836 align:middle line:-1
请注意这与其它锚的生命周期相同

00:29:28.902 --> 00:29:31.772 align:middle line:-2
例如ARPlaneAnchor
或ARImageAnchor

00:29:36.944 --> 00:29:40.681 align:middle line:-2
此外 它被完全集成到
ARSCNView中

00:29:40.981 --> 00:29:44.852 align:middle line:-2
因此如果你使用SceneKit
作为你的渲染技术

00:29:45.752 --> 00:29:50.557 align:middle line:-1
你只需在世界跟踪配置中启用此功能

00:29:50.991 --> 00:29:54.194 align:middle line:-2
其余部分将由
ARSCNView自动完成

00:30:00.434 --> 00:30:04.471 align:middle line:0
注意对于高级用例
你可能想要在场景中手动放置

00:30:04.538 --> 00:30:07.274 align:middle line:0
环境探测锚

00:30:11.545 --> 00:30:15.549 align:middle line:-2
为此你需要将environmentTexturing模式
设置为.manual

00:30:16.550 --> 00:30:20.587 align:middle line:-2
然后你就可以在你想要的位置和方向
创建环境探测锚

00:30:21.054 --> 00:30:25.526 align:middle line:-1
并将它们添加到AR会话对象

00:30:28.996 --> 00:30:33.000 align:middle line:-2
注意这只允许你将探测锚
放置在场景中

00:30:33.800 --> 00:30:37.804 align:middle line:-2
AR会话一旦获得
有关环境的更多信息

00:30:37.871 --> 00:30:40.107 align:middle line:-1
它就会自动更新其纹理

00:30:42.309 --> 00:30:48.115 align:middle line:-2
因此当你的增强现实场景
只有一个对象时

00:30:48.182 --> 00:30:49.149 align:middle line:-1
就可以使用此模式

00:30:49.216 --> 00:30:53.620 align:middle line:-2
你不希望使用太多环境探头锚
来使系统负载过高

00:30:57.224 --> 00:31:00.227 align:middle line:-1
让我们看一个环境纹理的快速演示

00:30:57.224 --> 00:31:00.227 align:middle line:-1
让我们看一个环境纹理的快速演示

00:31:00.294 --> 00:31:04.798 align:middle line:-2
并看看我们如何逼真地渲染
增强现实场景

00:31:16.176 --> 00:31:18.712 align:middle line:-1
我们切换到AR 1

00:31:23.550 --> 00:31:24.384 align:middle line:-1
好的

00:31:24.451 --> 00:31:28.655 align:middle line:-2
对于这个演示
我所运行的世界跟踪配置

00:31:29.456 --> 00:31:32.092 align:middle line:-1
未启用环境纹理功能

00:31:33.427 --> 00:31:38.732 align:middle line:0
正如你在底部开关控制器上
看到的那样

00:31:38.799 --> 00:31:41.168 align:middle line:0
它只使用了环境光估计

00:31:41.735 --> 00:31:45.973 align:middle line:0
让我们放置之前见过的那个对象

00:31:47.074 --> 00:31:48.308 align:middle line:0
你可以看到

00:31:50.544 --> 00:31:53.947 align:middle line:0
这看起来没问题
我的意思是 你可以在桌面上看到它

00:31:54.014 --> 00:31:58.385 align:middle line:0
你可以看到它的阴影
它看起来是一个非常好的AR场景

00:31:59.553 --> 00:32:04.725 align:middle line:0
但它不能反射桌子的木质表面

00:31:59.553 --> 00:32:04.725 align:middle line:0
但它不能反射桌子的木质表面

00:32:06.760 --> 00:32:09.763 align:middle line:0
而且如果我在场景中放置一些东西

00:32:10.998 --> 00:32:12.533 align:middle line:0
如真实的水果

00:32:16.136 --> 00:32:20.774 align:middle line:0
我们在虚拟对象中看不到它的倒影

00:32:21.942 --> 00:32:24.244 align:middle line:0
现在让我们启用环境纹理

00:32:24.311 --> 00:32:29.650 align:middle line:0
并看看它如何逼真地显示这种纹理

00:32:31.185 --> 00:32:33.887 align:middle line:-1
正如你所看到的 我一启用环境纹理

00:32:34.488 --> 00:32:38.892 align:middle line:-1
对象就开始反射桌子的木质表面

00:32:38.959 --> 00:32:42.262 align:middle line:-1
以及此香蕉的纹理

00:32:50.871 --> 00:32:51.839 align:middle line:-1
谢谢

00:32:52.573 --> 00:32:57.177 align:middle line:0
这极大改善了你的增强现实场景

00:32:57.244 --> 00:33:02.416 align:middle line:0
它看起来非常真实
好像它真的在桌面上一样

00:32:57.244 --> 00:33:02.416 align:middle line:0
它看起来非常真实
好像它真的在桌面上一样

00:33:04.751 --> 00:33:06.253 align:middle line:-1
好的 回到幻灯片

00:33:12.693 --> 00:33:14.761 align:middle line:-1
这就是环境纹理

00:33:14.828 --> 00:33:20.501 align:middle line:-2
它是ARKit 2中强大的新功能
能够让你尽可能逼真地创建

00:33:20.567 --> 00:33:23.804 align:middle line:-1
你的增强现实场景

00:33:25.506 --> 00:33:29.977 align:middle line:-1
现在 为了继续介绍其它新特性

00:33:30.043 --> 00:33:32.479 align:middle line:-1
我想邀请Reinhard上台来

00:33:38.986 --> 00:33:40.320 align:middle line:-1
Reinhard

00:33:40.387 --> 00:33:41.221 align:middle line:-1
开始了

00:33:42.222 --> 00:33:43.090 align:middle line:-1
这能用吗？

00:33:43.156 --> 00:33:43.991 align:middle line:-1
哦 好的 太好了

00:33:45.993 --> 00:33:50.397 align:middle line:-2
早上好 我叫Reinhard
我是ARKit团队的工程师

00:33:50.631 --> 00:33:53.000 align:middle line:-1
接下来 我们将讨论图像跟踪

00:33:54.101 --> 00:33:58.639 align:middle line:-2
在iOS 11.3中 我们引入了
图像检测作为世界跟踪的一部分

00:33:59.606 --> 00:34:03.177 align:middle line:-2
图像检测会搜索场景中
已知的2D图像

00:33:59.606 --> 00:34:03.177 align:middle line:-2
图像检测会搜索场景中
已知的2D图像

00:34:04.311 --> 00:34:07.714 align:middle line:-2
这里的术语“检测”
意味着这些图像是静态的

00:34:07.781 --> 00:34:09.583 align:middle line:-1
因此它们应该不会移动

00:34:10.250 --> 00:34:14.888 align:middle line:-2
这些图像的例子包括电影海报
或博物馆中的画

00:34:16.356 --> 00:34:20.293 align:middle line:-1
一旦检测到图像

00:34:20.360 --> 00:34:23.664 align:middle line:-2
ARKit将估算
其六自由度位置和方向

00:34:24.431 --> 00:34:28.068 align:middle line:-2
该姿态可用于触发
你的渲染场景中的内容

00:34:29.570 --> 00:34:30.704 align:middle line:-1
正如我前面提到的

00:34:31.103 --> 00:34:33.607 align:middle line:-1
所有这些都完全集成在世界跟踪中

00:34:34.007 --> 00:34:37.143 align:middle line:-2
你所要做的只是在你的属性中
对它进行一次设置

00:34:38.978 --> 00:34:42.114 align:middle line:-1
为了加载用于图像检测的图像

00:34:42.181 --> 00:34:45.886 align:middle line:-2
你可以从文件加载它们
或使用Xcode的素材目录

00:34:45.953 --> 00:34:49.121 align:middle line:-1
它还可以为你提供图像的检测质量

00:34:50.357 --> 00:34:54.795 align:middle line:-2
图像检测现在已经很棒了
但在iOS 12中我们能做得更好

00:34:54.862 --> 00:34:57.130 align:middle line:-1
让我们来谈谈图像跟踪

00:34:57.798 --> 00:35:02.369 align:middle line:-1
图像跟踪是图像检测的一种扩展

00:34:57.798 --> 00:35:02.369 align:middle line:-1
图像跟踪是图像检测的一种扩展

00:35:02.436 --> 00:35:05.672 align:middle line:-2
但它的优势在于图像
不再需要是静态的并且可以移动

00:35:07.574 --> 00:35:11.512 align:middle line:-1
ARKit现将以每秒60帧的速度

00:35:11.578 --> 00:35:13.080 align:middle line:-1
估算每一帧的位置和方向

00:35:13.647 --> 00:35:20.454 align:middle line:-2
这可以让你准确地增强2D图像
比如杂志 棋盘游戏

00:35:20.521 --> 00:35:23.490 align:middle line:-1
或任何具有真实图像特征的东西

00:35:25.158 --> 00:35:29.129 align:middle line:-1
且ARKit也可同时跟踪多个图像

00:35:30.831 --> 00:35:34.301 align:middle line:-2
默认情况下它只选择一个
但在某些情况下

00:35:34.368 --> 00:35:38.906 align:middle line:-2
比如杂志的封面
你可能希望将它设置为1

00:35:38.972 --> 00:35:42.609 align:middle line:-1
或者如果是杂志内的双页杂志

00:35:42.676 --> 00:35:43.844 align:middle line:-1
你可以把它设置为2

00:35:45.279 --> 00:35:50.717 align:middle line:-2
在iOS 12的ARKit 2中
我们有一个全新的配置

00:35:50.784 --> 00:35:53.120 align:middle line:-1
即ARImageTrackingConfiguration

00:35:53.587 --> 00:35:56.657 align:middle line:-1
它可让你进行独立的图像跟踪

00:35:57.157 --> 00:35:58.792 align:middle line:-1
让我们看看如何设置它

00:35:59.960 --> 00:36:02.229 align:middle line:-2
我们首先加载一组
ReferenceImage

00:35:59.960 --> 00:36:02.229 align:middle line:-2
我们首先加载一组
ReferenceImage

00:36:02.296 --> 00:36:04.565 align:middle line:-1
其可以来自文件或素材目录

00:36:05.732 --> 00:36:08.335 align:middle line:-2
加载完这组
ReferenceImage后

00:36:08.669 --> 00:36:13.507 align:middle line:-2
我通过它指定
其detectionImages属性

00:36:13.574 --> 00:36:17.277 align:middle line:-2
来将会话设为
WorldTrackingConfiguration类型

00:36:17.344 --> 00:36:19.213 align:middle line:-1
或通过指定trackingImages属性

00:36:19.279 --> 00:36:21.081 align:middle line:-2
将其设为ARImageTracking
Configuration类型

00:36:22.349 --> 00:36:27.054 align:middle line:-1
完成配置后 我用它来运行我的会话

00:36:28.355 --> 00:36:31.291 align:middle line:-1
和往常一样 一旦会话开始运行

00:36:31.358 --> 00:36:33.727 align:middle line:-2
我会在每次更新时
获得一个ARFrame

00:36:34.428 --> 00:36:39.032 align:middle line:-2
一旦检测到图像
这个ARFrame将包含一个

00:36:39.099 --> 00:36:41.034 align:middle line:-2
ARImageAnchor类型
的对象

00:36:42.436 --> 00:36:45.138 align:middle line:-2
这个ARImageAnchor
是一个可跟踪的对象

00:36:45.205 --> 00:36:48.242 align:middle line:-2
我可以通过遵守
ARTrackable协议看到它

00:36:48.709 --> 00:36:52.613 align:middle line:-2
其中有一个
布尔值变量isTracked

00:36:53.113 --> 00:36:56.550 align:middle line:-1
它可以通知你图像的跟踪状态

00:36:56.850 --> 00:36:59.419 align:middle line:-2
如果对象已跟踪则为true
否则为false

00:37:00.387 --> 00:37:04.291 align:middle line:-1
它还会通知你检测到的图像及其位置

00:37:04.591 --> 00:37:09.396 align:middle line:-1
其位置和方向以4*4矩阵表示

00:37:11.665 --> 00:37:15.802 align:middle line:-2
为了获得这样的图像锚
一切都从加载图像开始

00:37:15.869 --> 00:37:19.173 align:middle line:-2
这很好 让我们来看看
哪些图像比较适合跟踪

00:37:19.740 --> 00:37:24.344 align:middle line:-1
这张图片可以在儿童书中找到

00:37:24.411 --> 00:37:26.213 align:middle line:0
事实上 它非常适合图像跟踪

00:37:26.713 --> 00:37:28.649 align:middle line:0
它有很多不同的视觉特征

00:37:28.715 --> 00:37:31.318 align:middle line:0
它的纹理清晰
并且有非常好的对比度

00:37:32.686 --> 00:37:35.956 align:middle line:0
另一方面 在儿童书中也可以
找到这样的图像

00:37:36.023 --> 00:37:40.093 align:middle line:0
但这种图像并不合适

00:37:40.694 --> 00:37:44.064 align:middle line:0
它有很多重复的结构
一致的颜色区域

00:37:44.131 --> 00:37:47.201 align:middle line:0
并且一旦将其转换为灰度值
它将有一个非常窄的直方图

00:37:48.335 --> 00:37:50.737 align:middle line:-1
但你不必自己识别这些统计信息

00:37:50.804 --> 00:37:52.206 align:middle line:-1
因为Xcode可为你提供帮助

00:37:52.873 --> 00:37:55.242 align:middle line:-1
如果我将这两个图像导入Xcode

00:37:55.976 --> 00:37:59.112 align:middle line:-2
我们可以看到海洋生物图像
没有任何警告

00:37:59.179 --> 00:38:02.249 align:middle line:-2
这意味着它是推荐使用的
而有三个孩子阅读的那个图像

00:37:59.179 --> 00:38:02.249 align:middle line:-2
这意味着它是推荐使用的
而有三个孩子阅读的那个图像

00:38:02.316 --> 00:38:04.885 align:middle line:-1
显示警告图标以表示不建议使用

00:38:06.019 --> 00:38:10.457 align:middle line:-2
如果我点击此图标
我会得到关于为什么这个图像

00:38:10.524 --> 00:38:13.760 align:middle line:-1
不适合用于图像跟踪的精确描述

00:38:14.194 --> 00:38:16.430 align:middle line:-1
我可以得到的信息包括直方图

00:38:16.496 --> 00:38:19.499 align:middle line:-1
一致的颜色区域以及直方图

00:38:21.435 --> 00:38:23.704 align:middle line:-1
一旦我加载完图片

00:38:23.770 --> 00:38:26.273 align:middle line:-1
我有两种配置选择

00:38:26.707 --> 00:38:29.443 align:middle line:-2
首先是
ARWorldTrackingConfiguration

00:38:29.510 --> 00:38:30.511 align:middle line:-1
我们来谈谈这个

00:38:33.347 --> 00:38:35.883 align:middle line:-2
当我们使用世界跟踪
来进行图像跟踪时

00:38:35.949 --> 00:38:38.886 align:middle line:-1
图像锚在世界坐标系中表示

00:38:39.253 --> 00:38:43.190 align:middle line:-1
这意味着图像锚可选择平面锚

00:38:44.291 --> 00:38:49.363 align:middle line:-2
相机和世界原点本身
都出现在同一个坐标系中

00:38:49.730 --> 00:38:52.666 align:middle line:-1
这使它们的互动非常简单直观

00:38:54.067 --> 00:38:56.370 align:middle line:-1
而作为iOS 12中的新功能

00:38:56.436 --> 00:39:00.407 align:middle line:-2
对于以前只能检测到的图像
现在可以进行跟踪了

00:38:56.436 --> 00:39:00.407 align:middle line:-2
对于以前只能检测到的图像
现在可以进行跟踪了

00:39:02.276 --> 00:39:05.679 align:middle line:-2
我们有了一个新的配置
即ARImageTrackingConfiguration

00:39:05.746 --> 00:39:08.048 align:middle line:-1
它可以执行独立的图像跟踪

00:39:08.949 --> 00:39:12.586 align:middle line:-1
这意味着它独立于世界跟踪

00:39:12.653 --> 00:39:15.522 align:middle line:-1
而不依赖运动传感器来进行跟踪

00:39:16.123 --> 00:39:20.227 align:middle line:-2
这意味着此配置在开始识别图像之前
不会初始化

00:39:20.561 --> 00:39:25.165 align:middle line:-1
并且可以在无法进行世界跟踪的

00:39:25.232 --> 00:39:26.733 align:middle line:-1
场景中成功识别

00:39:26.800 --> 00:39:30.170 align:middle line:-1
如电梯或火车等移动平台

00:39:31.438 --> 00:39:35.843 align:middle line:-2
在这种情况下 ARKit将以
每秒60帧的速度

00:39:35.909 --> 00:39:38.178 align:middle line:-1
估算每帧的位置和方向

00:39:39.313 --> 00:39:43.116 align:middle line:-2
并且只需四行简单代码
就可以完成这项工作

00:39:43.951 --> 00:39:46.987 align:middle line:-1
你需要做的是首先创建一个

00:39:47.054 --> 00:39:50.457 align:middle line:-2
类型为ARImageTracking
Configuration的配置

00:39:50.524 --> 00:39:52.626 align:middle line:-1
并指定想要跟踪的一组图像

00:39:53.093 --> 00:39:57.297 align:middle line:-2
在这个例子中
我指定了猫、狗和鸟的照片

00:39:59.466 --> 00:40:02.569 align:middle line:-1
我告诉配置我要跟踪的图像数量

00:39:59.466 --> 00:40:02.569 align:middle line:-1
我告诉配置我要跟踪的图像数量

00:40:03.003 --> 00:40:04.972 align:middle line:-1
在这个例子中 我将其指定为2

00:40:06.106 --> 00:40:09.910 align:middle line:-2
在我的用例中
假设只有两个图像会互动

00:40:09.977 --> 00:40:11.712 align:middle line:-1
而不会同时出现三个

00:40:12.479 --> 00:40:16.917 align:middle line:-2
注意如果我正在跟踪两个图像
而第三个图像进入其视图

00:40:17.618 --> 00:40:21.855 align:middle line:-2
它将不会被跟踪
但它仍会触发检测更新

00:40:23.190 --> 00:40:26.126 align:middle line:-1
然后我使用此配置来运行我的会话

00:40:27.728 --> 00:40:31.698 align:middle line:-2
正如我之前提到的
你也可以通过换掉这两行代码

00:40:31.765 --> 00:40:33.934 align:middle line:-1
并使用世界跟踪来做到这点

00:40:35.169 --> 00:40:38.038 align:middle line:-1
图像检测和跟踪之间的唯一区别

00:40:38.105 --> 00:40:39.940 align:middle line:-1
是跟踪图像的最大数量

00:40:40.340 --> 00:40:45.412 align:middle line:-1
如果你有一个使用图像检测的app

00:40:45.746 --> 00:40:49.950 align:middle line:-2
你可以简单地添加它并重新编译
然后你的app就可以使用跟踪了

00:40:50.884 --> 00:40:55.255 align:middle line:-2
为了向你展示这是多么容易
让我们在Xcode中做一个演示

00:41:03.130 --> 00:41:06.767 align:middle line:-2
我们可以切换到AR 2吗？
好的 对于这个演示

00:41:06.834 --> 00:41:10.404 align:middle line:-1
我想创建一个AR相框

00:41:10.470 --> 00:41:13.140 align:middle line:-1
为此我带了一张我家的猫的照片

00:41:13.774 --> 00:41:15.409 align:middle line:-1
让我们使用Xcode构建它

00:41:16.610 --> 00:41:22.749 align:middle line:-2
我先用Xcode
创建一个iOS app模板

00:41:22.983 --> 00:41:25.485 align:middle line:-1
如你所见 它现在是空的

00:41:26.720 --> 00:41:29.890 align:middle line:-1
接下来我需要指定要附加的图像

00:41:30.557 --> 00:41:33.093 align:middle line:-2
为此我导入了
我的猫Daisy的照片

00:41:33.160 --> 00:41:34.394 align:middle line:-1
让我们在这里打开她

00:41:36.997 --> 00:41:37.831 align:middle line:-1
这就是我的猫

00:41:40.400 --> 00:41:41.969 align:middle line:-1
我需要指定一个名字

00:41:42.436 --> 00:41:44.838 align:middle line:-2
我将其命名为Daisy
即我的猫的名字

00:41:44.905 --> 00:41:49.710 align:middle line:-2
我在这里指定了在现实世界中
该图像的物理宽度

00:41:49.776 --> 00:41:50.911 align:middle line:-1
即我的相框宽度

00:41:52.246 --> 00:41:55.282 align:middle line:-1
我也载入了一段我的猫的视频

00:41:56.350 --> 00:41:57.851 align:middle line:-1
让我们把这一切都集中在一起

00:41:58.619 --> 00:42:01.154 align:middle line:-1
首先我将创建一个配置

00:41:58.619 --> 00:42:01.154 align:middle line:-1
首先我将创建一个配置

00:42:01.221 --> 00:42:04.691 align:middle line:0
这是一个
ARImageTrackingConfiguration

00:42:04.758 --> 00:42:06.527 align:middle line:0
类型的配置

00:42:07.528 --> 00:42:10.230 align:middle line:-1
我通过使用组名Photos

00:42:10.864 --> 00:42:13.300 align:middle line:-1
从素材目录中加载了一组跟踪图像

00:42:13.734 --> 00:42:17.838 align:middle line:-2
其中只包含一张图片
即我的猫Daisy的照片

00:42:18.939 --> 00:42:19.806 align:middle line:-1
接下来

00:42:21.408 --> 00:42:22.709 align:middle line:-1
我通过指定trackingImages属性

00:42:22.776 --> 00:42:25.379 align:middle line:-1
设置好图像跟踪配置

00:42:26.180 --> 00:42:29.183 align:middle line:-2
并将maximumNumberOfTrackedImages
设为1

00:42:30.450 --> 00:42:33.887 align:middle line:-1
此时 app已经启动了AR会话

00:42:33.954 --> 00:42:36.557 align:middle line:-1
并在检测到图像时为你提供其图像锚

00:42:36.957 --> 00:42:38.258 align:middle line:-1
但让我们添加一些内容

00:42:39.026 --> 00:42:40.894 align:middle line:-1
我将通过在资源面板中

00:42:42.029 --> 00:42:47.768 align:middle line:-2
载入视频并根据它创建一个
AVPlayer来加载它

00:42:49.102 --> 00:42:52.072 align:middle line:-1
现在让我们将它添加到真实图像之上

00:42:55.142 --> 00:42:58.579 align:middle line:-2
为此我需要检查该锚
是否是ImageAnchor类型

00:42:59.379 --> 00:43:03.317 align:middle line:-1
并创建一个与场景中的图像

00:42:59.379 --> 00:43:03.317 align:middle line:-1
并创建一个与场景中的图像

00:43:03.383 --> 00:43:05.052 align:middle line:-2
具有相同物理尺寸的
SCNPlane

00:43:06.320 --> 00:43:09.389 align:middle line:-2
我将videoPlayer
设为该平面的纹理

00:43:09.890 --> 00:43:11.792 align:middle line:-1
并开始播放videoPlayer

00:43:13.260 --> 00:43:16.230 align:middle line:-2
我通过geometry
创建一个SCNNode

00:43:16.296 --> 00:43:21.235 align:middle line:-1
并反转它以匹配锚坐标系

00:43:22.402 --> 00:43:26.306 align:middle line:-2
就是这样 我们运行它
并看看其效果

00:43:30.310 --> 00:43:33.380 align:middle line:-1
一旦我将相机对准我的猫的相框

00:43:34.114 --> 00:43:37.251 align:middle line:-2
视频将开始播放
我可以看到猫动了起来

00:43:44.491 --> 00:43:47.394 align:middle line:-1
由于ARKit实时估计位置

00:43:47.461 --> 00:43:50.764 align:middle line:-2
我可以自由移动设备
也可以移动对象

00:43:51.265 --> 00:43:53.934 align:middle line:-1
我可以看到每一帧都在更新

00:43:55.736 --> 00:43:56.770 align:middle line:-1
她跑掉了

00:43:58.539 --> 00:44:00.741 align:middle line:-2
我们的演示就到这里吧
让我们回到幻灯片

00:43:58.539 --> 00:44:00.741 align:middle line:-2
我们的演示就到这里吧
让我们回到幻灯片

00:44:08.248 --> 00:44:12.753 align:middle line:-2
如你所见 在ARKit中
使用图像跟踪非常简单

00:44:13.120 --> 00:44:15.856 align:middle line:-2
事实上 制作一个猫的视频
要比这困难得多

00:44:17.991 --> 00:44:21.828 align:middle line:-2
图像跟踪非常适合
与2D对象进行交互

00:44:22.262 --> 00:44:25.232 align:middle line:-1
但我们不仅限于平坦的2D物体

00:44:25.299 --> 00:44:28.435 align:middle line:-1
我们接下来谈谈对象检测

00:44:33.607 --> 00:44:38.779 align:middle line:-2
对象检测可用于检测场景中的
已知3D对象

00:44:39.880 --> 00:44:43.550 align:middle line:-2
就像刚才的图像检测一样
术语“检测”意味着这个对象

00:44:43.617 --> 00:44:47.454 align:middle line:-1
需要是静态的 也就是它不应该移动

00:44:48.355 --> 00:44:52.125 align:middle line:-2
此类对象的很好的例子
可能是博物馆中的展品

00:44:52.192 --> 00:44:54.027 align:middle line:-1
某些玩具或家居用品

00:44:56.496 --> 00:44:57.965 align:middle line:-1
和图像检测一样

00:44:58.031 --> 00:45:01.902 align:middle line:-2
首先需要使用运行ARKit
的iOS app扫描这个对象

00:44:58.031 --> 00:45:01.902 align:middle line:-2
首先需要使用运行ARKit
的iOS app扫描这个对象

00:45:03.437 --> 00:45:08.642 align:middle line:-2
为此我们提供了一个全功能
iOS app的完整源代码

00:45:08.709 --> 00:45:10.611 align:middle line:-1
它允许你扫描自己的3D对象

00:45:11.378 --> 00:45:16.517 align:middle line:-2
此类对象需要具有一些特性
比如它需要纹理良好

00:45:16.583 --> 00:45:18.218 align:middle line:-1
刚性且无反射

00:45:18.652 --> 00:45:21.655 align:middle line:-1
它们的大小应该与桌面摆设大致相同

00:45:23.790 --> 00:45:27.194 align:middle line:-1
ARKit可用于估计这些物体

00:45:27.261 --> 00:45:29.363 align:middle line:-1
在六自由度中的位置和方向

00:45:31.765 --> 00:45:35.202 align:middle line:-1
所有这些都完全集成到世界跟踪中

00:45:35.269 --> 00:45:37.971 align:middle line:-1
你需要做的只是设置一个属性

00:45:38.038 --> 00:45:39.806 align:middle line:-1
就可以开始进行对象检测了

00:45:40.707 --> 00:45:42.576 align:middle line:-1
让我们来看看这是如何设置的

00:45:43.944 --> 00:45:48.348 align:middle line:-2
我从文件或Xcode素材目录中
加载一组ARReferenceImage

00:45:49.416 --> 00:45:51.652 align:middle line:-1
我稍后再讨论参考对象

00:45:52.452 --> 00:45:54.288 align:middle line:-1
加载完这些参考对象后

00:45:54.354 --> 00:45:59.459 align:middle line:-2
我通过指定
detectionObjects属性

00:45:59.526 --> 00:46:01.728 align:middle line:-1
以用它们来设置我的

00:45:59.526 --> 00:46:01.728 align:middle line:-1
以用它们来设置我的

00:46:01.962 --> 00:46:04.298 align:middle line:-2
ARWorldTrackingConfiguration
类型的配置

00:46:05.699 --> 00:46:10.504 align:middle line:-1
设置好配置后 使用它运行会话

00:46:11.271 --> 00:46:14.341 align:middle line:-1
就像图像检测一样 AR会话运行后

00:46:14.408 --> 00:46:16.243 align:middle line:-2
每次更新
我都会得到一个ARFrame

00:46:16.310 --> 00:46:17.678 align:middle line:-1
在这种情况下

00:46:18.579 --> 00:46:21.148 align:middle line:-1
一旦在场景中检测到一个对象

00:46:21.215 --> 00:46:26.553 align:middle line:-2
ARFrame中就会出现
一个ARObjectAnchor

00:46:28.789 --> 00:46:32.659 align:middle line:-2
该AR对象
是ARAnchor的简单子类

00:46:33.093 --> 00:46:35.696 align:middle line:-1
所以它有个transform变量

00:46:35.762 --> 00:46:40.667 align:middle line:-2
代表其六自由度位置和方向
且它通过ARReferenceObject类型的引用

00:46:40.734 --> 00:46:44.805 align:middle line:-1
来告诉我它检测到了哪些对象

00:46:46.206 --> 00:46:52.279 align:middle line:-1
只需三行简单代码就可以实现这点

00:46:52.980 --> 00:46:53.947 align:middle line:-1
我创建了一个

00:46:54.014 --> 00:46:56.416 align:middle line:-2
ARWorldTrackingConfiguration
类型的配置

00:46:56.717 --> 00:46:59.953 align:middle line:-1
并指定一组我想要检测的对象

00:47:00.554 --> 00:47:05.492 align:middle line:-2
在这个例子中 我设想通过检测
一个古老的半身像和一个陶罐

00:47:05.559 --> 00:47:07.628 align:middle line:-1
来构建一个简单的AR博物馆app

00:47:08.962 --> 00:47:11.231 align:middle line:-1
然后我用它来运行我的会话

00:47:12.432 --> 00:47:17.037 align:middle line:-2
事实上 我们在办公室构建了这个
非常简单的AR博物馆app

00:47:17.437 --> 00:47:18.472 align:middle line:-1
所以让我们来看看

00:47:19.072 --> 00:47:22.009 align:middle line:-1
一旦这个半身像

00:47:22.075 --> 00:47:24.278 align:middle line:-1
进入我的iOS app视图

00:47:24.611 --> 00:47:28.749 align:middle line:-2
我就可以得到其六自由度姿态
并可以用它来查看

00:47:29.082 --> 00:47:32.286 align:middle line:-2
一个悬浮在雕像上方的
非常简单的信息图

00:47:32.886 --> 00:47:35.856 align:middle line:-2
在这个例子中
我们只是添加了出生日期

00:47:35.923 --> 00:47:38.258 align:middle line:-2
这个埃及女王的名字
即Nefertiti

00:47:38.692 --> 00:47:40.327 align:middle line:-1
但你可以添加你的渲染引擎

00:47:40.394 --> 00:47:42.362 align:middle line:-1
允许你使用的任何内容

00:47:43.897 --> 00:47:47.501 align:middle line:-2
为了构建这个app
我必须先扫描对象

00:47:48.001 --> 00:47:49.870 align:middle line:-1
所以我们来谈谈对象扫描

00:47:51.872 --> 00:47:55.943 align:middle line:-2
对象扫描可以从世界中
提取累积场景信息

00:47:57.144 --> 00:47:59.880 align:middle line:-1
它与平面估计关系密切

00:47:59.947 --> 00:48:02.249 align:middle line:-1
其中我们使用累积场景信息

00:47:59.947 --> 00:48:02.249 align:middle line:-1
其中我们使用累积场景信息

00:48:02.516 --> 00:48:05.285 align:middle line:-1
来估计水平或垂直平面的位置

00:48:06.086 --> 00:48:09.089 align:middle line:-1
在这种情况下 我们使用这类信息

00:48:09.489 --> 00:48:13.427 align:middle line:-1
来收集有关3D对象的信息

00:48:15.128 --> 00:48:19.600 align:middle line:-2
为了指定要查找对象的区域
我只需指定

00:48:19.666 --> 00:48:22.135 align:middle line:-1
转换 范围和中心

00:48:22.769 --> 00:48:24.137 align:middle line:-1
这实际上是一个对象周围的边界框

00:48:24.204 --> 00:48:28.141 align:middle line:-1
用来定义它在场景中的位置

00:48:29.877 --> 00:48:33.680 align:middle line:-2
Xcode素材目录
完全支持提取对象

00:48:33.981 --> 00:48:38.652 align:middle line:-1
因此将它们导入新app非常容易

00:48:38.719 --> 00:48:41.288 align:middle line:-1
并可以根据需要重复使用它们

00:48:42.556 --> 00:48:44.825 align:middle line:-1
对于扫描 我们添加了一个新配置

00:48:44.892 --> 00:48:46.927 align:middle line:-1
即ARObjectScanningConfiguration

00:48:48.161 --> 00:48:51.665 align:middle line:-1
但你无需实现自己的扫描app

00:48:52.099 --> 00:48:56.270 align:middle line:-2
因为你可以使用
全功能扫描app的完整示例代码

00:48:56.336 --> 00:48:59.072 align:middle line:-1
它被称为“扫描和检测3D对象”

00:49:00.140 --> 00:49:02.276 align:middle line:-1
我们来看看这个app是如何工作的

00:49:02.709 --> 00:49:06.446 align:middle line:-2
我首先在感兴趣的对象周围
创建一个边界框

00:49:06.513 --> 00:49:08.248 align:middle line:-2
在这个例子中
为Nefertiti的雕像

00:49:09.316 --> 00:49:11.118 align:middle line:-1
注意边界框不需要

00:49:11.185 --> 00:49:12.786 align:middle line:-1
严格包围对象

00:49:12.853 --> 00:49:13.687 align:middle line:-1
我们所关心的是

00:49:13.754 --> 00:49:17.925 align:middle line:-1
最重要的特征点均在其范围内

00:49:19.193 --> 00:49:21.061 align:middle line:-1
当我对边界框感到满意时

00:49:21.828 --> 00:49:26.733 align:middle line:-2
我可以点击扫描按钮
然后我们就可以开始扫描对象

00:49:27.134 --> 00:49:30.404 align:middle line:-2
我可以看到扫描从下往上进行
并且这种表现形式

00:49:30.804 --> 00:49:34.875 align:middle line:-1
可以以空间形式表示扫描对象的程度

00:49:35.943 --> 00:49:38.846 align:middle line:-1
注意你不必从所有方向扫描对象

00:49:39.546 --> 00:49:43.984 align:middle line:-2
例如 如果在博物馆中有一个
面向墙壁的雕像

00:49:44.351 --> 00:49:48.856 align:middle line:-1
因此你无法从一个特定的视角检测它

00:49:49.223 --> 00:49:51.091 align:middle line:-1
那么你就不需要从那边扫描它

00:49:53.360 --> 00:49:55.729 align:middle line:-1
一旦你对扫描结果感到满意

00:49:56.964 --> 00:50:01.101 align:middle line:-1
你就可以调整范围的中心

00:49:56.964 --> 00:50:01.101 align:middle line:-1
你就可以调整范围的中心

00:50:01.535 --> 00:50:03.437 align:middle line:-1
它对应于对象的原点

00:50:04.238 --> 00:50:05.272 align:middle line:-1
这里唯一的要求

00:50:05.339 --> 00:50:08.575 align:middle line:-1
是中心必须保持在对象的范围内

00:50:10.010 --> 00:50:14.348 align:middle line:-2
最后 扫描app
可让你执行检测测试

00:50:15.349 --> 00:50:18.785 align:middle line:-2
在这个例子中
从各个视角来看该检测都是成功的

00:50:18.852 --> 00:50:20.587 align:middle line:-1
这意味着这是一次很好的扫描

00:50:21.755 --> 00:50:26.627 align:middle line:-2
我们在这里的建议是
也要将对象移动到不同的位置

00:50:26.693 --> 00:50:28.862 align:middle line:-1
来测试该检测

00:50:29.463 --> 00:50:32.299 align:middle line:-2
在不同纹理和不同光照条件下
是否有效

00:50:34.434 --> 00:50:38.772 align:middle line:-2
完成扫描后 你将得到
ARReferenceObject类型的对象

00:50:39.540 --> 00:50:41.441 align:middle line:-1
我们在前面的图中见过它

00:50:42.276 --> 00:50:47.047 align:middle line:-2
此对象通常可以序列化为
.arobject文件扩展类型

00:50:47.581 --> 00:50:52.286 align:middle line:-2
它有一个名称变量
其也会显示在你的素材目录中

00:50:52.352 --> 00:50:55.422 align:middle line:-1
以及用于扫描它的中心和范围变量

00:50:56.490 --> 00:50:59.059 align:middle line:-2
此外你还将得到执行扫描时
在该区域内

00:50:59.126 --> 00:51:02.696 align:middle line:-1
找到的所有原始特征点

00:50:59.126 --> 00:51:02.696 align:middle line:-1
找到的所有原始特征点

00:51:05.332 --> 00:51:09.536 align:middle line:-2
这就是对象检测
请记住 在检测对象之前

00:51:09.770 --> 00:51:12.639 align:middle line:-2
你需要扫描它们
但这有现成的完整源代码可用

00:51:12.706 --> 00:51:14.107 align:middle line:-1
你可以立即下载它

00:51:16.310 --> 00:51:19.479 align:middle line:-1
我们接下来谈谈面部跟踪

00:51:24.318 --> 00:51:26.787 align:middle line:-1
当我们去年发布iPhone X时

00:51:26.854 --> 00:51:30.057 align:middle line:-2
我们为ARKit添加了健壮的
人脸检测和跟踪功能

00:51:30.457 --> 00:51:34.461 align:middle line:-1
ARKit以每秒60帧的速度

00:51:34.528 --> 00:51:37.297 align:middle line:-1
估计每一帧中脸的位置和方向

00:51:37.764 --> 00:51:42.870 align:middle line:-2
这个姿态可以通过添加帽子
或替换面部的全部纹理

00:51:42.936 --> 00:51:46.006 align:middle line:-1
来增强用户的面部

00:51:47.774 --> 00:51:50.143 align:middle line:-1
ARKit还为你提供拟合三角网格

00:51:50.210 --> 00:51:52.145 align:middle line:-2
来构成
ARFaceGeometry对象

00:51:54.882 --> 00:51:58.151 align:middle line:-2
此ARFaceGeometry
类型包含

00:51:58.218 --> 00:52:05.225 align:middle line:-2
渲染这个面部网格所需的所有信息
其表现形式为网格所有顶点 三角形

00:51:58.218 --> 00:52:05.225 align:middle line:-2
渲染这个面部网格所需的所有信息
其表现形式为网格所有顶点 三角形

00:52:05.292 --> 00:52:06.860 align:middle line:-1
以及检测坐标

00:52:08.795 --> 00:52:11.865 align:middle line:-2
人脸跟踪的主要锚类型
是ARFaceAnchor

00:52:11.932 --> 00:52:15.369 align:middle line:-2
其中包含执行面部跟踪
所需的所有信息

00:52:16.937 --> 00:52:19.873 align:middle line:-1
为了逼真地渲染这样的几何图形

00:52:20.407 --> 00:52:22.676 align:middle line:-1
我们添加了定向光估计

00:52:23.544 --> 00:52:27.514 align:middle line:-2
这里 ARKit使用你的光照
作为光探头

00:52:27.581 --> 00:52:33.086 align:middle line:-2
并估算此ARDirectionLightEstimate
其包括光的强度

00:52:33.153 --> 00:52:35.255 align:middle line:-1
方向以及色温

00:52:36.623 --> 00:52:42.095 align:middle line:-2
此估计值足以让大多数app
看起来很棒

00:52:42.429 --> 00:52:44.698 align:middle line:-1
但如果你的app有更复杂的需求

00:52:45.432 --> 00:52:49.369 align:middle line:-1
我们还提供二次球谐系数

00:52:49.436 --> 00:52:52.306 align:middle line:-1
来收集整个场景的光照条件

00:52:53.307 --> 00:52:55.776 align:middle line:-1
从而让你的内容看起来甚至更棒

00:52:57.878 --> 00:53:00.480 align:middle line:-1
ARKit还可以实时跟踪表情

00:52:57.878 --> 00:53:00.480 align:middle line:-1
ARKit还可以实时跟踪表情

00:53:01.315 --> 00:53:03.784 align:middle line:-1
这些表情是所谓的混合形状

00:53:03.851 --> 00:53:06.820 align:middle line:-1
它们有50多种

00:53:08.021 --> 00:53:10.657 align:middle line:-2
这种混合形状假定一个
介于0和1之间的值

00:53:11.191 --> 00:53:13.994 align:middle line:-2
1意味着完全激活
0意味着没有

00:53:14.294 --> 00:53:18.532 align:middle line:-2
例如 如果我张开嘴
则下颚打开系数值将接近1

00:53:18.599 --> 00:53:21.468 align:middle line:-1
如果我合上嘴 则值接近0

00:53:22.769 --> 00:53:25.639 align:middle line:-2
这对为你自己的虚拟角色创建动画
非常有用

00:53:26.106 --> 00:53:29.209 align:middle line:-2
在这个例子中 我使用了张开下颚
和眨左眼

00:53:29.276 --> 00:53:32.846 align:middle line:-2
以及眨右眼来为这个简单的
盒子脸角色创建动画

00:53:34.181 --> 00:53:35.382 align:middle line:-1
但它可以做得更好

00:53:36.216 --> 00:53:40.621 align:middle line:-2
事实上 在我们制作表情符号时
我们使用了更多这样的混合形状

00:53:41.121 --> 00:53:43.390 align:middle line:-1
你在这里所看到的移动蓝条

00:53:43.457 --> 00:53:48.195 align:middle line:-2
被用来将我的面部表情
映射到熊猫脸上

00:53:49.763 --> 00:53:54.234 align:middle line:-2
注意ARKit提供了为你自己的
角色制作动画所需的所有工具

00:53:54.701 --> 00:53:56.470 align:middle line:-1
就像我们制作表情符号时一样

00:53:59.673 --> 00:54:00.507 align:middle line:-1
谢谢

00:53:59.673 --> 00:54:00.507 align:middle line:-1
谢谢

00:54:04.778 --> 00:54:07.848 align:middle line:-2
让我们看看ARKit 2中
面部跟踪的新特性

00:54:08.682 --> 00:54:12.853 align:middle line:-2
我们添加了凝视跟踪
它可以在六自由度中

00:54:12.920 --> 00:54:14.621 align:middle line:-1
跟踪左眼和右眼

00:54:21.495 --> 00:54:24.464 align:middle line:-2
你会发现它们是
ARFaceAnchor成员变量

00:54:24.765 --> 00:54:26.233 align:middle line:-1
还有一lookAtPoint变量

00:54:26.300 --> 00:54:29.536 align:middle line:-1
它对应于两个凝视方向的交叉点

00:54:30.270 --> 00:54:34.174 align:middle line:-2
你可以使用此信息
为自己的角色制作动画

00:54:34.441 --> 00:54:36.944 align:middle line:-1
或你的app的任何其它形式的输入

00:54:37.778 --> 00:54:38.612 align:middle line:-1
而且不止如此

00:54:39.680 --> 00:54:41.114 align:middle line:-1
我们增加了对舌头的支持

00:54:42.149 --> 00:54:43.884 align:middle line:-1
它以新的混合形状出现

00:54:44.585 --> 00:54:47.120 align:middle line:-2
如果我伸出舌头
这个混合形状将假设值为1

00:54:47.187 --> 00:54:48.589 align:middle line:-1
否则为0

00:54:49.556 --> 00:54:53.594 align:middle line:-2
你同样可以使用它来为自己的角色
制作动画或使用它

00:54:53.660 --> 00:54:56.530 align:middle line:-1
作为你app的输入形式

00:55:00.667 --> 00:55:01.502 align:middle line:-1
谢谢

00:55:03.704 --> 00:55:08.141 align:middle line:-2
看到自己一遍又一遍地伸出舌头
这是一个总结的好时机

00:55:08.942 --> 00:55:12.813 align:middle line:-2
ARKit 2新特性
我们来看一下

00:55:13.814 --> 00:55:16.984 align:middle line:-1
我们看到了保存和加载映射

00:55:17.050 --> 00:55:20.521 align:middle line:-2
这是用于持久性和多用户协作的
强大新特性

00:55:21.822 --> 00:55:25.959 align:middle line:-2
世界跟踪增强可以进行
更好更快的平面检测

00:55:26.293 --> 00:55:28.362 align:middle line:-1
以及新的视频格式

00:55:29.496 --> 00:55:31.231 align:middle line:-1
对于环境纹理

00:55:31.665 --> 00:55:36.370 align:middle line:-2
我们可以通过收集场景纹理
并将其应用为纹理对象

00:55:36.637 --> 00:55:40.807 align:middle line:-1
来让内容看起来好像真的在场景中

00:55:41.875 --> 00:55:43.510 align:middle line:-1
通过图像跟踪…

00:55:46.413 --> 00:55:51.084 align:middle line:-2
通过图像跟踪 我们现在能够
以图像的形式跟踪2D对象

00:55:51.518 --> 00:55:53.887 align:middle line:-1
但ARKit也可以检测3D对象

00:55:54.588 --> 00:55:57.758 align:middle line:-2
对于脸部追踪
我们有凝视跟踪和舌头跟踪

00:55:59.893 --> 00:56:01.895 align:middle line:-1
所有这些功能都以ARKit的

00:55:59.893 --> 00:56:01.895 align:middle line:-1
所有这些功能都以ARKit的

00:56:01.962 --> 00:56:04.498 align:middle line:-1
构建块的形式供你使用

00:56:06.233 --> 00:56:09.203 align:middle line:-2
在iOS 12中
ARKit具有五种不同的配置

00:56:09.269 --> 00:56:10.370 align:middle line:-1
包括两个新增配置

00:56:10.704 --> 00:56:12.339 align:middle line:-1
即ARImageTrackingConfiguration

00:56:12.406 --> 00:56:14.508 align:middle line:-1
它用于独立图像跟踪

00:56:14.575 --> 00:56:16.944 align:middle line:-2
以及
ARObjectScanningConfiguration

00:56:18.078 --> 00:56:20.914 align:middle line:-1
此外还有一系列补充类型

00:56:20.981 --> 00:56:22.883 align:middle line:-1
可以用于与AR会话进行交互

00:56:23.350 --> 00:56:25.619 align:middle line:-2
比如ARFrame
和ARCamera

00:56:26.720 --> 00:56:28.021 align:middle line:-1
其中有两个新成员

00:56:28.088 --> 00:56:30.057 align:middle line:-2
用于对象检测的
ARReferenceObject

00:56:30.424 --> 00:56:33.493 align:middle line:-2
和用于持久性及多用户的
ARWorldMap

00:56:34.895 --> 00:56:36.029 align:middle line:-1
ARAnchor代表了

00:56:36.096 --> 00:56:39.466 align:middle line:-2
现实世界中的位置
锚类型也有两个新成员

00:56:39.533 --> 00:56:44.371 align:middle line:-2
ARObjectAnchor
和AREnvironmentProbeAnchor

00:56:45.639 --> 00:56:48.909 align:middle line:-2
我很期待看到你们将使用
iOS 12的ARKit中

00:56:48.976 --> 00:56:52.312 align:middle line:-1
所有这些构建块所构建的东西

00:57:00.721 --> 00:57:04.024 align:middle line:0
还有另一个很酷的演讲
关于如何在你的app中

00:57:04.091 --> 00:57:07.528 align:middle line:0
集成AR Quick Look
来让你的内容看起来更棒

00:57:08.662 --> 00:57:12.366 align:middle line:-1
非常感谢 请享受大会的其余部分
