WEBVTT

00:00:07.516 --> 00:00:16.500 A:middle
[ 音乐 ]

00:00:19.516 --> 00:00:25.500 A:middle
[ 掌声 ]

00:00:26.456 --> 00:00:27.606 A:middle
&gt;&gt; 大家好

00:00:27.986 --> 00:00:29.526 A:middle
很高兴今天能在这里

00:00:29.526 --> 00:00:31.446 A:middle
与大家探讨

00:00:31.446 --> 00:00:34.326 A:middle
ARKit 的跟踪与检测功能

00:00:34.326 --> 00:00:36.496 A:middle
将如何创造出更好

00:00:36.496 --> 00:00:39.576 A:middle
的 AR（增强现实）体验

00:00:40.596 --> 00:00:42.316 A:middle
我是 Marion

00:00:42.346 --> 00:00:43.266 A:middle
我来自 ARKit 团队

00:00:43.266 --> 00:00:44.756 A:middle
你呢

00:00:46.126 --> 00:00:49.206 A:middle
你是一位对 ARKit 的发展有着

00:00:49.206 --> 00:00:50.736 A:middle
浓厚兴趣的

00:00:50.736 --> 00:00:51.276 A:middle
资深 ARKit 开发者吗

00:00:51.896 --> 00:00:53.316 A:middle
那么这个演讲就是为你准备的

00:00:53.316 --> 00:00:56.596 A:middle
刚刚接触 ARKit

00:00:57.546 --> 00:00:59.606 A:middle
那从这次演讲中 你将会学到

00:00:59.606 --> 00:01:01.046 A:middle
不同的跟踪技巧以及

00:00:59.606 --> 00:01:01.046 A:middle
不同的跟踪技巧以及

00:01:01.206 --> 00:01:02.876 A:middle
一些 AR 常用的

00:01:02.966 --> 00:01:04.676 A:middle
基本知识与术语

00:01:04.676 --> 00:01:06.756 A:middle
这些基本知识与术语

00:01:06.756 --> 00:01:08.176 A:middle
能帮助你创造属于你

00:01:08.176 --> 00:01:10.016 A:middle
自己的第一份 AR 的体验

00:01:10.726 --> 00:01:14.586 A:middle
让我们开始吧

00:01:15.106 --> 00:01:17.926 A:middle
跟踪指的是什么

00:01:18.066 --> 00:01:19.536 A:middle
跟踪将提供你的相机

00:01:19.836 --> 00:01:22.906 A:middle
在真实世界中

00:01:23.156 --> 00:01:24.956 A:middle
观察位置与方向

00:01:25.376 --> 00:01:26.806 A:middle
有了这些位置与方向

00:01:26.806 --> 00:01:29.036 A:middle
你就可以在你的相机中

00:01:29.036 --> 00:01:30.096 A:middle
加入增强视觉的因素

00:01:31.146 --> 00:01:33.656 A:middle
比如在这个视频中

00:01:33.696 --> 00:01:35.836 A:middle
在真实的物理平台上的

00:01:35.836 --> 00:01:39.206 A:middle
桌子和椅子

00:01:39.926 --> 00:01:41.416 A:middle
是增强视觉的虚拟内容

00:01:42.756 --> 00:01:44.006 A:middle
顺便说一下 这个是宜家

00:01:45.296 --> 00:01:47.186 A:middle
需要注意的是 视觉内容

00:01:47.186 --> 00:01:48.926 A:middle
从视觉上看都是正确的

00:01:49.746 --> 00:01:52.086 A:middle
正确的放置位置 正确的大小

00:01:52.686 --> 00:01:54.516 A:middle
以及正确的透视角度

00:01:55.066 --> 00:01:56.836 A:middle
因此不同的跟踪

00:01:56.836 --> 00:01:58.906 A:middle
技术能为相机

00:01:59.076 --> 00:02:01.586 A:middle
提供不同的参考系

00:01:59.076 --> 00:02:01.586 A:middle
提供不同的参考系

00:02:01.816 --> 00:02:03.556 A:middle
这意味着相机相对于

00:02:03.556 --> 00:02:05.246 A:middle
你的世界参考系

00:02:05.246 --> 00:02:07.916 A:middle
相机相对于图像或者 3D 物品的参考

00:02:09.086 --> 00:02:11.186 A:middle
在接下来这一个小时里

00:02:11.296 --> 00:02:12.396 A:middle
我们将会讨论

00:02:12.396 --> 00:02:14.116 A:middle
不同类型的跟踪技术

00:02:14.936 --> 00:02:16.476 A:middle
这样子你就可以

00:02:16.476 --> 00:02:17.626 A:middle
针对你的特定用例

00:02:17.626 --> 00:02:18.796 A:middle
做出正确的选择

00:02:19.216 --> 00:02:22.296 A:middle
我们将会讨论

00:02:22.296 --> 00:02:24.196 A:middle
现在已有的 AR 技术

00:02:24.486 --> 00:02:26.726 A:middle
这包括 方向跟踪

00:02:26.866 --> 00:02:28.876 A:middle
世界跟踪 以及 平面检测

00:02:29.616 --> 00:02:31.376 A:middle
之后 我们才会进一步

00:02:31.496 --> 00:02:33.466 A:middle
分析 ARKit 2 上

00:02:33.466 --> 00:02:36.536 A:middle
自带的新的

00:02:36.536 --> 00:02:38.076 A:middle
跟踪和检测技术

00:02:38.696 --> 00:02:40.316 A:middle
这些技术包括 保存和加载地图

00:02:40.316 --> 00:02:43.776 A:middle
图像跟踪以及物品检测

00:02:45.266 --> 00:02:46.926 A:middle
在我们对这些

00:02:46.926 --> 00:02:48.886 A:middle
技术做进一步的探讨之前

00:02:48.946 --> 00:02:51.416 A:middle
不妨先来对 ARKit 进行一个

00:02:51.416 --> 00:02:52.646 A:middle
简单的概括性的回顾

00:02:53.176 --> 00:02:55.936 A:middle
这对刚刚接触 ARKit 的人来说会

00:02:55.936 --> 00:02:56.276 A:middle
很有趣

00:02:56.786 --> 00:03:00.556 A:middle
首先 你需要创造

00:02:56.786 --> 00:03:00.556 A:middle
首先 你需要创造

00:03:00.556 --> 00:03:01.826 A:middle
一个 ARSession

00:03:02.496 --> 00:03:04.576 A:middle
ARSession 是处理

00:03:04.576 --> 00:03:06.946 A:middle
从配置到运行 AR 技术

00:03:06.946 --> 00:03:11.226 A:middle
所有事务的对象

00:03:11.476 --> 00:03:15.696 A:middle
并且返回 AR 技术的结果

00:03:16.196 --> 00:03:19.406 A:middle
然后你需要做的是

00:03:19.496 --> 00:03:21.866 A:middle
描述你想要运行的技术类型

00:03:22.266 --> 00:03:23.256 A:middle
比如说 你想要使用

00:03:23.256 --> 00:03:25.136 A:middle
哪种跟踪技术或者

00:03:25.136 --> 00:03:26.376 A:middle
想要启用哪种功能

00:03:26.436 --> 00:03:28.096 A:middle
比如说平面检测功能

00:03:28.966 --> 00:03:32.506 A:middle
接下来 你需要使用这一

00:03:32.506 --> 00:03:35.406 A:middle
特定的 ARConfiguration

00:03:36.256 --> 00:03:39.476 A:middle
并运行你自己的

00:03:39.476 --> 00:03:40.116 A:middle
ARSession 实例

00:03:41.576 --> 00:03:43.206 A:middle
然后 ARsession 会开始内部

00:03:44.306 --> 00:03:47.336 A:middle
配置 AVCaptureSession

00:03:47.336 --> 00:03:49.596 A:middle
并开始接受图像

00:03:50.096 --> 00:03:55.096 A:middle
以及启动动作管理器

00:03:55.096 --> 00:03:57.616 A:middle
接收运动传感数据

00:03:57.616 --> 00:03:58.626 A:middle
这就是你设备

00:03:58.626 --> 00:04:02.666 A:middle
上内置的 ARKit 输入系统

00:03:58.626 --> 00:04:02.666 A:middle
上内置的 ARKit 输入系统

00:04:04.066 --> 00:04:07.066 A:middle
数据经过处理后

00:04:07.066 --> 00:04:09.306 A:middle
会返回为每秒 60 帧

00:04:09.306 --> 00:04:10.926 A:middle
的 ARFrames

00:04:12.126 --> 00:04:14.226 A:middle
ARFrame 是一种及时快照

00:04:14.226 --> 00:04:15.396 A:middle
它能为你的

00:04:15.396 --> 00:04:17.055 A:middle
AR 景象提供

00:04:17.055 --> 00:04:17.875 A:middle
所需要的一切

00:04:18.366 --> 00:04:20.776 A:middle
就好像 你拍摄的图像

00:04:20.776 --> 00:04:25.176 A:middle
会成为你的 AR 场景的背景

00:04:25.826 --> 00:04:27.136 A:middle
跟踪相机运动

00:04:27.136 --> 00:04:31.256 A:middle
适用于调整虚拟相机

00:04:31.706 --> 00:04:34.406 A:middle
使之能从

00:04:34.446 --> 00:04:36.746 A:middle
物理相机的角度

00:04:36.746 --> 00:04:37.586 A:middle
调整虚拟物品的角度

00:04:38.766 --> 00:04:40.426 A:middle
它还能提供

00:04:40.876 --> 00:04:42.086 A:middle
环境相关信息

00:04:42.086 --> 00:04:43.536 A:middle
举个例子 它可以检测到盘子

00:04:43.536 --> 00:04:46.806 A:middle
现在 让我们

00:04:46.806 --> 00:04:48.436 A:middle
从第一个跟踪技术开始

00:04:48.436 --> 00:04:49.216 A:middle
说起和构建

00:04:51.586 --> 00:04:53.176 A:middle
方向跟踪

00:04:54.386 --> 00:04:56.396 A:middle
你们觉得方向跟踪所跟踪

00:04:56.536 --> 00:04:56.976 A:middle
的是什么呢

00:04:57.176 --> 00:04:58.156 A:middle
方向

00:04:58.676 --> 00:05:01.226 A:middle
说明它只跟踪事物的旋转

00:04:58.676 --> 00:05:01.226 A:middle
说明它只跟踪事物的旋转

00:05:02.136 --> 00:05:03.206 A:middle
你可以这样子想象

00:05:03.206 --> 00:05:05.456 A:middle
你只能用你的帽子来观察

00:05:05.456 --> 00:05:06.916 A:middle
虚拟物品 而帽子本身只能

00:05:07.156 --> 00:05:09.046 A:middle
通过旋转来观察事物

00:05:09.966 --> 00:05:11.716 A:middle
也就是说你可以从

00:05:11.866 --> 00:05:13.406 A:middle
固定的位置的不同角度

00:05:13.406 --> 00:05:15.656 A:middle
来体验虚拟物体

00:05:15.656 --> 00:05:17.276 A:middle
但任何位置上的改变都

00:05:17.276 --> 00:05:17.856 A:middle
无法被跟踪到

00:05:19.596 --> 00:05:21.546 A:middle
旋转数据是分三个

00:05:21.546 --> 00:05:22.836 A:middle
轴来跟踪

00:05:22.946 --> 00:05:24.276 A:middle
这也是为什么有的人

00:05:24.316 --> 00:05:26.416 A:middle
称它为三个自由度跟踪

00:05:26.416 --> 00:05:28.926 A:middle
你可以把这个技术应用到一个

00:05:28.926 --> 00:05:30.846 A:middle
球形的虚拟环境中

00:05:30.846 --> 00:05:32.256 A:middle
像体验一个 360 度的视频

00:05:32.256 --> 00:05:35.066 A:middle
在这个视频中

00:05:35.066 --> 00:05:36.496 A:middle
体验者可以从同一个位置

00:05:36.496 --> 00:05:38.116 A:middle
对虚拟内容进行观察

00:05:39.346 --> 00:05:41.226 A:middle
也可以用于观察一个距离很远

00:05:41.226 --> 00:05:43.196 A:middle
的 AR 物品上

00:05:44.336 --> 00:05:46.296 A:middle
方向跟踪并不适用

00:05:46.396 --> 00:05:48.236 A:middle
于真实世界中的视觉增强

00:05:48.236 --> 00:05:49.746 A:middle
因为在真实世界中

00:05:49.746 --> 00:05:50.886 A:middle
你希望能从不同的位置

00:05:50.886 --> 00:05:52.666 A:middle
对事物进行观察

00:05:54.226 --> 00:05:56.416 A:middle
让我们来看一下

00:05:56.556 --> 00:05:58.016 A:middle
当方向跟踪在运行时

00:05:58.016 --> 00:05:59.486 A:middle
究竟发生了些什么

00:06:00.036 --> 00:06:02.676 A:middle
这很简单

00:06:03.106 --> 00:06:05.016 A:middle
它只是使用了核心运动

00:06:05.016 --> 00:06:07.166 A:middle
的旋转数据

00:06:07.326 --> 00:06:09.146 A:middle
其将传感器融合应用到

00:06:09.146 --> 00:06:09.986 A:middle
运动传感器数据中

00:06:11.426 --> 00:06:13.626 A:middle
与相机图像相比

00:06:13.756 --> 00:06:15.696 A:middle
运动数据的更新频率要高出许多

00:06:15.696 --> 00:06:17.936 A:middle
一旦相机图像可用

00:06:18.206 --> 00:06:20.736 A:middle
方向跟踪就会

00:06:20.736 --> 00:06:22.616 A:middle
采用这一系列数据中

00:06:22.616 --> 00:06:23.636 A:middle
最新的运动数据

00:06:23.776 --> 00:06:25.766 A:middle
然后将两个结果

00:06:26.086 --> 00:06:27.206 A:middle
返回给 ARFrame

00:06:27.526 --> 00:06:28.066 A:middle
就这样

00:06:28.126 --> 00:06:28.956 A:middle
很简单

00:06:29.686 --> 00:06:31.726 A:middle
请注意 在方向跟踪中

00:06:31.726 --> 00:06:33.266 A:middle
并不会处理到

00:06:33.266 --> 00:06:34.526 A:middle
相机回馈信息

00:06:34.856 --> 00:06:35.956 A:middle
也就是说这个过程

00:06:35.956 --> 00:06:37.036 A:middle
中并不会有电脑版本产生

00:06:38.286 --> 00:06:40.376 A:middle
如果你想要运转方向跟踪功能

00:06:40.986 --> 00:06:43.096 A:middle
你只需要在你的

00:06:43.286 --> 00:06:45.876 A:middle
ARSession 添加

00:06:45.876 --> 00:06:47.256 A:middle
AROrientation TrackingConfiguration

00:06:48.266 --> 00:06:49.686 A:middle
所产生的结果将会

00:06:49.686 --> 00:06:52.216 A:middle
通过 ARFrame 返回为

00:06:52.996 --> 00:06:55.116 A:middle
一个 ARCamera 对象

00:06:55.116 --> 00:06:57.926 A:middle
通常一个 ARCamera 对象

00:06:57.926 --> 00:06:59.826 A:middle
会包括不同的变换

00:06:59.826 --> 00:07:01.106 A:middle
在方向跟踪中

00:06:59.826 --> 00:07:01.106 A:middle
在方向跟踪中

00:07:01.106 --> 00:07:02.936 A:middle
这些变形只包括

00:07:02.936 --> 00:07:05.006 A:middle
你的现实相机反馈的

00:07:05.116 --> 00:07:06.016 A:middle
旋转数据

00:07:07.096 --> 00:07:09.616 A:middle
这些旋转数据

00:07:09.666 --> 00:07:11.596 A:middle
也可以用欧拉角来代表

00:07:12.276 --> 00:07:14.306 A:middle
你可以根据自己的喜好来选择

00:07:16.866 --> 00:07:18.686 A:middle
接下来让我们

00:07:18.686 --> 00:07:20.326 A:middle
讨论一些更高级的跟踪技术

00:07:21.156 --> 00:07:22.546 A:middle
我们先从世界跟踪开始

00:07:23.076 --> 00:07:25.406 A:middle
世界跟踪能够跟踪你的

00:07:25.676 --> 00:07:28.186 A:middle
相机角度方向

00:07:28.186 --> 00:07:30.096 A:middle
还能跟踪真实环境中

00:07:30.316 --> 00:07:32.056 A:middle
的位置变化

00:07:32.326 --> 00:07:34.196 A:middle
而且无需事先了解

00:07:34.196 --> 00:07:35.226 A:middle
你的周围环境

00:07:36.266 --> 00:07:37.656 A:middle
大家可以看到

00:07:37.656 --> 00:07:41.316 A:middle
左边是现实生活中

00:07:41.316 --> 00:07:42.766 A:middle
相机所看到的景象

00:07:42.766 --> 00:07:45.416 A:middle
右边是跟踪相机

00:07:45.546 --> 00:07:47.466 A:middle
在探索过程中的

00:07:47.466 --> 00:07:50.506 A:middle
跟踪轨迹 这轨迹是用

00:07:50.506 --> 00:07:51.756 A:middle
坐标系来表示

00:07:52.986 --> 00:07:54.406 A:middle
接下来 我们将详细

00:07:54.506 --> 00:07:55.466 A:middle
的说明当世界跟踪运作时

00:07:55.536 --> 00:07:56.376 A:middle
到底发生了什么

00:07:56.896 --> 00:08:00.406 A:middle
世界跟踪所使用

00:07:56.896 --> 00:08:00.406 A:middle
世界跟踪所使用

00:08:00.406 --> 00:08:03.236 A:middle
运动传感器

00:08:03.236 --> 00:08:05.436 A:middle
是你设备上的加速器

00:08:05.436 --> 00:08:08.376 A:middle
和陀螺仪的运动数据

00:08:08.376 --> 00:08:10.796 A:middle
由此高频计算它的方向

00:08:11.116 --> 00:08:12.436 A:middle
和旋转变化

00:08:14.706 --> 00:08:16.926 A:middle
它同时还为 Metal

00:08:17.116 --> 00:08:19.276 A:middle
提供了正确比例数据

00:08:20.656 --> 00:08:23.056 A:middle
从专业方面来说

00:08:23.106 --> 00:08:24.306 A:middle
这一部分的跟踪系统

00:08:24.306 --> 00:08:25.976 A:middle
也被称为惯性里程计

00:08:27.076 --> 00:08:29.046 A:middle
尽管这一运动数据

00:08:29.106 --> 00:08:31.026 A:middle
能为短间隔动作

00:08:31.026 --> 00:08:32.826 A:middle
提供详细的运动信息

00:08:32.826 --> 00:08:34.226 A:middle
但当出现

00:08:34.275 --> 00:08:36.566 A:middle
突然的运动时

00:08:36.645 --> 00:08:38.635 A:middle
就可能会出现较长时间的漂移

00:08:39.015 --> 00:08:40.456 A:middle
因为这些数据

00:08:40.616 --> 00:08:42.126 A:middle
并没有理想中准确

00:08:42.126 --> 00:08:43.296 A:middle
而且有累积误差的可能性

00:08:44.496 --> 00:08:45.616 A:middle
这也是为什么它不能

00:08:45.906 --> 00:08:47.446 A:middle
单独用于跟踪

00:08:48.036 --> 00:08:51.456 A:middle
为了补偿这一漂移

00:08:51.696 --> 00:08:53.416 A:middle
世界跟踪在使用

00:08:53.516 --> 00:08:55.506 A:middle
相机图像时

00:08:55.506 --> 00:08:59.606 A:middle
应用了一个电脑版本

00:09:00.706 --> 00:09:02.426 A:middle
这个技术提供了

00:09:02.506 --> 00:09:05.276 A:middle
更高的准确性 但却

00:09:05.276 --> 00:09:06.546 A:middle
以计算时间为代价

00:09:08.056 --> 00:09:10.176 A:middle
同时 它对快速相机运动

00:09:10.226 --> 00:09:12.596 A:middle
十分敏感

00:09:12.666 --> 00:09:14.396 A:middle
这就会导致

00:09:14.396 --> 00:09:15.296 A:middle
图像上的模糊

00:09:16.516 --> 00:09:18.626 A:middle
这一系统中

00:09:18.626 --> 00:09:20.926 A:middle
的版本也被称为

00:09:20.926 --> 00:09:21.666 A:middle
视觉里程计

00:09:22.016 --> 00:09:24.276 A:middle
通过对计算机视觉和

00:09:24.276 --> 00:09:26.576 A:middle
动作这两个系统的融合

00:09:26.576 --> 00:09:30.016 A:middle
ARKit 吸收了这两个系统的优势

00:09:30.646 --> 00:09:32.086 A:middle
ARKit 选用了计算机视觉中

00:09:32.086 --> 00:09:34.116 A:middle
长时间间隔的

00:09:34.116 --> 00:09:34.946 A:middle
高准确性

00:09:35.566 --> 00:09:37.576 A:middle
而从运动数据中 它吸收了

00:09:37.576 --> 00:09:39.586 A:middle
短时间间隔的准确性

00:09:39.586 --> 00:09:41.356 A:middle
和测量过程中的

00:09:41.356 --> 00:09:43.446 A:middle
高更新频率以及

00:09:43.446 --> 00:09:44.276 A:middle
以米为测量单位

00:09:44.856 --> 00:09:47.456 A:middle
通过合并这两个系统

00:09:47.456 --> 00:09:49.446 A:middle
在处理某些框架时

00:09:49.586 --> 00:09:51.036 A:middle
世界跟踪可以跳过

00:09:51.246 --> 00:09:52.886 A:middle
计算机视觉这一步骤

00:09:52.886 --> 00:09:54.736 A:middle
但仍保持高效和

00:09:54.776 --> 00:09:55.966 A:middle
高反馈的跟踪

00:09:56.956 --> 00:09:58.856 A:middle
这样子就可以减少 CPU 占用

00:09:58.856 --> 00:10:00.356 A:middle
你可以将多出的 CPU 空间用于

00:09:58.856 --> 00:10:00.356 A:middle
你可以将多出的 CPU 空间用于

00:10:00.356 --> 00:10:00.956 A:middle
你的 App 上

00:10:02.876 --> 00:10:04.376 A:middle
在专业方面

00:10:04.376 --> 00:10:06.456 A:middle
这一合并技术也被称为

00:10:06.506 --> 00:10:07.696 A:middle
视觉惯性里程计

00:10:08.916 --> 00:10:11.456 A:middle
让我们进一步了解这一部分

00:10:11.456 --> 00:10:13.306 A:middle
中的视觉部分

00:10:14.116 --> 00:10:15.926 A:middle
计算机视觉正在

00:10:15.926 --> 00:10:19.286 A:middle
处理我导出的图像

00:10:19.286 --> 00:10:21.096 A:middle
中的一个重点区域

00:10:21.356 --> 00:10:23.526 A:middle
像这里的蓝色和橙色的点

00:10:24.406 --> 00:10:26.006 A:middle
它们被提取出来

00:10:26.006 --> 00:10:27.496 A:middle
以确保在同一环境

00:10:27.946 --> 00:10:30.246 A:middle
的其他图像中

00:10:30.286 --> 00:10:31.516 A:middle
也能提取出同样的点

00:10:33.016 --> 00:10:34.096 A:middle
这些重点区域

00:10:34.096 --> 00:10:35.166 A:middle
也被称为特征

00:10:36.496 --> 00:10:37.616 A:middle
可以看到

00:10:37.616 --> 00:10:39.996 A:middle
根据它们本身的相似性与外观

00:10:40.196 --> 00:10:43.686 A:middle
可以在同一相机的不同图像中

00:10:43.686 --> 00:10:44.326 A:middle
将它们匹配起来

00:10:45.176 --> 00:10:46.716 A:middle
接下来发生的与你眼睛

00:10:46.716 --> 00:10:48.566 A:middle
看到 3D 的效果

00:10:48.566 --> 00:10:49.286 A:middle
的过程类似

00:10:50.176 --> 00:10:51.576 A:middle
这两个点之间

00:10:51.576 --> 00:10:54.226 A:middle
有细微的倾向一侧的距离

00:10:55.056 --> 00:10:56.976 A:middle
它们之间的视觉差

00:10:56.976 --> 00:10:58.796 A:middle
是十分重要的

00:10:58.796 --> 00:11:00.636 A:middle
因为它们对环境

00:10:58.796 --> 00:11:00.636 A:middle
因为它们对环境

00:11:00.636 --> 00:11:02.236 A:middle
的观察角度有细微的差别

00:11:02.606 --> 00:11:04.706 A:middle
这个差别能让画面更立体

00:11:04.816 --> 00:11:06.056 A:middle
更有深度感

00:11:07.106 --> 00:11:08.406 A:middle
这就是 ARKit 在处理

00:11:08.406 --> 00:11:10.176 A:middle
三角测量时

00:11:10.176 --> 00:11:12.076 A:middle
对于同一相机

00:11:12.076 --> 00:11:14.156 A:middle
不同角度的处理方式

00:11:14.736 --> 00:11:16.206 A:middle
只要画面中有足够的视觉差

00:11:16.256 --> 00:11:18.046 A:middle
ARKit 就能运行这一功能

00:11:18.896 --> 00:11:20.786 A:middle
它能够计算这些

00:11:20.786 --> 00:11:23.306 A:middle
匹配特征之间的深度数据

00:11:23.706 --> 00:11:26.826 A:middle
换句话说 这些图像中的 2D 特征

00:11:26.826 --> 00:11:29.316 A:middle
通过 3D 方式获得重组

00:11:30.806 --> 00:11:32.066 A:middle
但是 这个重组

00:11:32.066 --> 00:11:34.476 A:middle
成功的关键

00:11:35.706 --> 00:11:37.536 A:middle
在于相机位置的

00:11:37.676 --> 00:11:39.776 A:middle
改变以提供

00:11:39.776 --> 00:11:41.316 A:middle
足够的视觉差

00:11:42.356 --> 00:11:44.626 A:middle
比如说 往某一侧倾斜的运动

00:11:44.966 --> 00:11:47.746 A:middle
单纯的旋转并不能为重组

00:11:47.746 --> 00:11:48.856 A:middle
提供足够的信息

00:11:50.536 --> 00:11:52.606 A:middle
这就是有关你所身处环境的

00:11:52.606 --> 00:11:53.646 A:middle
第一份小地图

00:11:53.646 --> 00:11:55.826 A:middle
在 ARKit 中 我们将它称为

00:11:55.906 --> 00:11:56.136 A:middle
世界地图

00:11:57.396 --> 00:11:59.826 A:middle
与此同时 你的镜头的

00:11:59.826 --> 00:12:01.626 A:middle
相机位置以及

00:11:59.826 --> 00:12:01.626 A:middle
相机位置以及

00:12:01.626 --> 00:12:04.226 A:middle
方向都经过了计算

00:12:04.226 --> 00:12:06.766 A:middle
在用字母 C 这里标志出来

00:12:07.476 --> 00:12:08.546 A:middle
这代表着你的世界跟踪

00:12:08.546 --> 00:12:09.396 A:middle
刚刚完成初始化

00:12:09.396 --> 00:12:12.496 A:middle
这是跟踪系统的

00:12:12.496 --> 00:12:12.746 A:middle
初始化阶段

00:12:12.746 --> 00:12:15.886 A:middle
当世界地图

00:12:15.886 --> 00:12:17.326 A:middle
经过初始重组时

00:12:17.326 --> 00:12:19.216 A:middle
也定义了

00:12:19.406 --> 00:12:21.246 A:middle
世界原点

00:12:21.986 --> 00:12:23.676 A:middle
它被设置为

00:12:23.886 --> 00:12:27.326 A:middle
第一个相机的三角框架帧的原点

00:12:28.046 --> 00:12:30.296 A:middle
同时也被设置为与重力对齐

00:12:31.056 --> 00:12:34.286 A:middle
我们在幻灯片中用 W 来代表

00:12:34.906 --> 00:12:35.786 A:middle
因此 你现在可以在

00:12:35.786 --> 00:12:37.246 A:middle
它的世界坐标系

00:12:37.246 --> 00:12:39.366 A:middle
重建作为世界地图的

00:12:39.366 --> 00:12:40.896 A:middle
真实环境

00:12:40.896 --> 00:12:42.046 A:middle
的小型代表

00:12:42.836 --> 00:12:44.446 A:middle
你所使用的相机

00:12:44.596 --> 00:12:46.646 A:middle
也被用同样的

00:12:46.646 --> 00:12:48.646 A:middle
世界坐标来跟踪

00:12:50.896 --> 00:12:53.186 A:middle
现在 你可以往

00:12:53.186 --> 00:12:56.436 A:middle
你的相机视角中加入视觉

00:12:56.436 --> 00:12:57.056 A:middle
内容来增强它们

00:12:58.656 --> 00:13:01.016 A:middle
为了将视觉内容

00:12:58.656 --> 00:13:01.016 A:middle
为了将视觉内容

00:13:01.066 --> 00:13:03.416 A:middle
正确的加入到一个 ARSession 中

00:13:03.416 --> 00:13:06.256 A:middle
你需要运用到 ARkit 中的

00:13:06.636 --> 00:13:07.826 A:middle
ARAnchors 这里我们用

00:13:07.826 --> 00:13:08.096 A:middle
A 来表示

00:13:09.536 --> 00:13:12.326 A:middle
ARAnchors 在这个世界地图

00:13:12.536 --> 00:13:14.076 A:middle
也就是这个世界坐标体系

00:13:14.076 --> 00:13:15.816 A:middle
中是一个参考点

00:13:16.486 --> 00:13:18.386 A:middle
ARAnchor 是不可或缺的

00:13:18.386 --> 00:13:20.686 A:middle
因为世界跟踪在跟踪

00:13:20.686 --> 00:13:22.206 A:middle
时可能会更新

00:13:22.206 --> 00:13:23.636 A:middle
也就是说

00:13:23.636 --> 00:13:25.336 A:middle
所有分配给它的视觉内容都

00:13:25.336 --> 00:13:27.776 A:middle
并会被更新并正确

00:13:27.886 --> 00:13:32.746 A:middle
的增强到相机的景象中

00:13:32.746 --> 00:13:34.446 A:middle
既然你已经应用了

00:13:34.446 --> 00:13:36.436 A:middle
ARAnchors 你可以将视觉内容

00:13:36.466 --> 00:13:38.356 A:middle
添加到锚点中

00:13:38.386 --> 00:13:40.496 A:middle
这些内容稍后会

00:13:40.886 --> 00:13:44.116 A:middle
以正确的增强方式添加到相机的镜头中

00:13:45.576 --> 00:13:48.676 A:middle
从现在开始 这个基于你环境的

00:13:48.676 --> 00:13:51.026 A:middle
3D 世界地图将会成为

00:13:51.026 --> 00:13:53.236 A:middle
世界跟踪的参考系统

00:13:54.046 --> 00:13:56.456 A:middle
它也是新图像的参考依据

00:13:57.076 --> 00:13:58.796 A:middle
不同图像中的特征

00:13:58.796 --> 00:14:01.806 A:middle
互相匹配且进行三角化

00:13:58.796 --> 00:14:01.806 A:middle
互相匹配且进行三角化

00:14:02.666 --> 00:14:04.216 A:middle
与此同时 新的稳定的

00:14:04.216 --> 00:14:05.776 A:middle
特征被提取出来

00:14:06.106 --> 00:14:08.246 A:middle
经过匹配 三角化

00:14:08.246 --> 00:14:10.496 A:middle
最终能帮助你扩展

00:14:10.556 --> 00:14:10.786 A:middle
你的世界地图

00:14:11.286 --> 00:14:14.036 A:middle
换句话说 ARKit 正在学习你的环境

00:14:15.926 --> 00:14:17.006 A:middle
这个学习过程会产生

00:14:17.006 --> 00:14:18.686 A:middle
对当前相机的

00:14:18.946 --> 00:14:20.936 A:middle
位置和方向的

00:14:20.936 --> 00:14:21.956 A:middle
跟踪计算的更新

00:14:23.086 --> 00:14:24.986 A:middle
最后就能向

00:14:24.986 --> 00:14:26.796 A:middle
当前相机视角输出

00:14:26.796 --> 00:14:27.426 A:middle
正确的增强元素

00:14:27.946 --> 00:14:31.966 A:middle
当你持续不断的探索世界时

00:14:31.966 --> 00:14:33.786 A:middle
世界跟踪可以

00:14:33.786 --> 00:14:35.706 A:middle
持续的跟踪你的相机

00:14:35.706 --> 00:14:37.826 A:middle
并持续学习你

00:14:38.076 --> 00:14:39.426 A:middle
所处的环境

00:14:40.506 --> 00:14:42.866 A:middle
但是随着时间的推移

00:14:42.866 --> 00:14:45.506 A:middle
增强效果可能会出现细微的漂移

00:14:45.506 --> 00:14:47.366 A:middle
就像你可以在左边

00:14:47.366 --> 00:14:49.096 A:middle
的图像中看到

00:14:49.096 --> 00:14:51.086 A:middle
对增强效果的细微补偿

00:14:52.366 --> 00:14:54.736 A:middle
这是因为就算很细微的补偿

00:14:55.216 --> 00:14:58.016 A:middle
或者细微的错误

00:14:58.016 --> 00:14:59.786 A:middle
在经过累积后都

00:14:59.786 --> 00:15:01.496 A:middle
会变得明显

00:14:59.786 --> 00:15:01.496 A:middle
会变得明显

00:15:03.486 --> 00:15:05.286 A:middle
现在这个设备回到了

00:15:05.426 --> 00:15:07.216 A:middle
一个熟悉的视角

00:15:07.216 --> 00:15:09.036 A:middle
它之前曾经探索过这个视角

00:15:09.086 --> 00:15:10.956 A:middle
比如说是我们

00:15:10.956 --> 00:15:11.756 A:middle
探索的出发点

00:15:11.756 --> 00:15:14.116 A:middle
ARKit 会进行另外

00:15:14.116 --> 00:15:15.846 A:middle
一种优化步骤

00:15:16.546 --> 00:15:18.086 A:middle
这一步骤会让

00:15:18.086 --> 00:15:19.496 A:middle
视觉惯性里程计系统

00:15:19.496 --> 00:15:21.986 A:middle
这个 ARKit 支持的系统

00:15:21.986 --> 00:15:24.176 A:middle
转换为一个

00:15:24.286 --> 00:15:26.206 A:middle
视觉惯性 SLAM 系统

00:15:27.376 --> 00:15:28.826 A:middle
现在让我们回到

00:15:28.916 --> 00:15:30.726 A:middle
世界跟踪开始探索

00:15:30.726 --> 00:15:32.476 A:middle
的第一张图像

00:15:33.956 --> 00:15:35.136 A:middle
世界跟踪现在要做的事情是

00:15:35.136 --> 00:15:37.166 A:middle
检查当前的跟踪信息

00:15:37.166 --> 00:15:39.246 A:middle
与世界地图对当前

00:15:39.446 --> 00:15:41.076 A:middle
景象的渲染是否

00:15:41.076 --> 00:15:43.666 A:middle
与过去的吻合

00:15:43.926 --> 00:15:45.276 A:middle
也就是一开始的图像

00:15:45.276 --> 00:15:48.586 A:middle
接着 ARKit 就会进行

00:15:48.586 --> 00:15:51.886 A:middle
优化步骤

00:15:52.066 --> 00:15:54.316 A:middle
将当前的信息以及世界地图

00:15:54.316 --> 00:15:56.306 A:middle
与你的真实

00:15:56.306 --> 00:15:57.666 A:middle
物理环境匹配起来

00:15:58.816 --> 00:16:00.256 A:middle
你们是否有注意到

00:15:58.816 --> 00:16:00.256 A:middle
你们是否有注意到

00:16:00.256 --> 00:16:01.946 A:middle
在这个过程中

00:16:01.946 --> 00:16:02.536 A:middle
ARAnchor 也被更新了

00:16:03.006 --> 00:16:04.476 A:middle
这也是为什么你在

00:16:04.476 --> 00:16:07.036 A:middle
往你眼前的景象中加入

00:16:07.036 --> 00:16:09.436 A:middle
视觉内容时要使用 ARAnchor

00:16:09.956 --> 00:16:14.396 A:middle
在这个视频中 你可以看到

00:16:14.396 --> 00:16:16.956 A:middle
这个修正步骤在真实相机反馈

00:16:16.956 --> 00:16:17.716 A:middle
中重复了一次

00:16:18.116 --> 00:16:20.736 A:middle
在左边 我们可以从相机角度看

00:16:20.736 --> 00:16:22.596 A:middle
这个环境 还能看到

00:16:22.596 --> 00:16:25.026 A:middle
这个图像中所跟踪的特征

00:16:25.406 --> 00:16:26.986 A:middle
在右边则是

00:16:26.986 --> 00:16:28.356 A:middle
从俯视角度观察该景象的画面

00:16:28.356 --> 00:16:30.596 A:middle
这代表着 ARKit

00:16:30.596 --> 00:16:33.166 A:middle
了解这个环境并

00:16:34.016 --> 00:16:36.386 A:middle
正在展示这个环境的 3D 重建

00:16:36.996 --> 00:16:39.506 A:middle
这些点的颜色

00:16:39.506 --> 00:16:41.346 A:middle
是根据重建点的高度

00:16:41.346 --> 00:16:43.306 A:middle
进行编译

00:16:43.306 --> 00:16:45.036 A:middle
蓝色代表地面高度

00:16:45.096 --> 00:16:46.746 A:middle
红色代表桌子和椅子

00:16:47.376 --> 00:16:51.216 A:middle
一旦相机回到

00:16:51.216 --> 00:16:52.576 A:middle
它曾经拍摄过的视角

00:16:52.576 --> 00:16:54.546 A:middle
就好像这里的开始的视角

00:16:54.546 --> 00:16:56.636 A:middle
ARKit 就会应用

00:16:56.636 --> 00:16:57.896 A:middle
这一优化步骤

00:16:57.996 --> 00:16:59.436 A:middle
你需要留意的是

00:16:59.436 --> 00:17:01.416 A:middle
点云以及相机轨迹

00:16:59.436 --> 00:17:01.416 A:middle
点云以及相机轨迹

00:17:02.826 --> 00:17:04.106 A:middle
不知道你是否有留意到这一更新

00:17:04.556 --> 00:17:05.506 A:middle
让我再展示一次

00:17:05.996 --> 00:17:10.866 A:middle
这个更新将 ARKit

00:17:10.866 --> 00:17:12.175 A:middle
的知识与你的现实世界

00:17:12.286 --> 00:17:15.016 A:middle
以及相机运动

00:17:15.016 --> 00:17:17.536 A:middle
匹配起来

00:17:17.536 --> 00:17:19.425 A:middle
为接下来的相机帧数

00:17:19.425 --> 00:17:20.626 A:middle
提供更好的增强效果

00:17:21.955 --> 00:17:23.306 A:middle
顺便说一下

00:17:23.306 --> 00:17:24.935 A:middle
有关世界跟踪的所有计算

00:17:25.435 --> 00:17:28.886 A:middle
以及有关你所处环境的信息

00:17:29.346 --> 00:17:31.076 A:middle
这些事情都只需要在你的设备

00:17:31.076 --> 00:17:31.996 A:middle
上就可以完成

00:17:32.136 --> 00:17:33.496 A:middle
这些数据也只存在于

00:17:33.496 --> 00:17:35.096 A:middle
你的设备上

00:17:35.096 --> 00:17:37.836 A:middle
如何才能将这一复杂的

00:17:37.836 --> 00:17:40.566 A:middle
技术应用到你的 App 中呢

00:17:41.926 --> 00:17:45.606 A:middle
很简单

00:17:45.716 --> 00:17:47.486 A:middle
如果你想要运行世界跟踪功能

00:17:47.526 --> 00:17:49.766 A:middle
你只需要在你的 ARSession 中

00:17:49.816 --> 00:17:51.896 A:middle
添加 ARWorldTrackingConfiguration 类

00:17:52.976 --> 00:17:55.376 A:middle
它将会以 ARCamera

00:17:55.376 --> 00:17:57.956 A:middle
的对象 ARFrame 形式返回结果

00:18:00.216 --> 00:18:03.636 A:middle
ARCamera 的对象

00:18:03.636 --> 00:18:05.626 A:middle
包括了转换

00:18:05.626 --> 00:18:06.946 A:middle
在世界跟踪中

00:18:07.496 --> 00:18:08.806 A:middle
转换还包括旋转

00:18:08.806 --> 00:18:12.116 A:middle
以及对跟踪相机的平移

00:18:13.286 --> 00:18:15.106 A:middle
除此之外 ARCamera 还包括

00:18:15.106 --> 00:18:16.886 A:middle
有关跟踪状态

00:18:16.886 --> 00:18:18.556 A:middle
以及 trackingStateReason

00:18:18.556 --> 00:18:19.746 A:middle
的信息

00:18:20.166 --> 00:18:22.366 A:middle
这将会为当前

00:18:22.366 --> 00:18:24.206 A:middle
的跟踪质量提供

00:18:24.306 --> 00:18:25.216 A:middle
一些信息

00:18:26.736 --> 00:18:27.976 A:middle
接下来 要讨论的是跟踪质量

00:18:28.516 --> 00:18:29.906 A:middle
你是否曾经使用过一个

00:18:30.016 --> 00:18:32.376 A:middle
跟踪十分差劲

00:18:32.376 --> 00:18:34.476 A:middle
或者根本不能跟踪

00:18:34.476 --> 00:18:35.646 A:middle
的 AR App 呢

00:18:36.276 --> 00:18:37.346 A:middle
那时你的感觉是怎么样的

00:18:38.446 --> 00:18:39.846 A:middle
是不是很崩溃

00:18:39.846 --> 00:18:40.796 A:middle
可能你再也不会用

00:18:40.796 --> 00:18:41.166 A:middle
这个 App 了

00:18:42.026 --> 00:18:43.566 A:middle
那么 怎样才在你的 App 中

00:18:43.566 --> 00:18:45.326 A:middle
实现更好的跟踪质量呢

00:18:46.726 --> 00:18:48.626 A:middle
要实现更好的跟踪质量

00:18:48.626 --> 00:18:49.846 A:middle
我们需要明白什么主要因素

00:18:49.846 --> 00:18:51.646 A:middle
会影响到跟踪质量

00:18:52.156 --> 00:18:53.856 A:middle
在这里 我想要强调其中三点

00:18:55.056 --> 00:18:56.756 A:middle
首先 世界跟踪依赖于

00:18:56.756 --> 00:18:58.996 A:middle
持续不断的

00:18:59.056 --> 00:19:01.136 A:middle
相机图像以及传感器数据

00:18:59.056 --> 00:19:01.136 A:middle
相机图像以及传感器数据

00:19:01.556 --> 00:19:02.876 A:middle
如果这些信息中断太久

00:19:02.876 --> 00:19:05.456 A:middle
就可能会限制跟踪

00:19:06.926 --> 00:19:08.676 A:middle
第二 世界跟踪在

00:19:08.676 --> 00:19:10.326 A:middle
纹理明显 光线充足的环境下

00:19:10.326 --> 00:19:12.306 A:middle
才最好的工作

00:19:12.646 --> 00:19:14.656 A:middle
因为世界跟踪需要用到

00:19:14.656 --> 00:19:16.726 A:middle
这些特征点来作为参考

00:19:16.726 --> 00:19:18.916 A:middle
并最终将其位置三角化

00:19:18.916 --> 00:19:20.976 A:middle
所以环境需要有

00:19:20.976 --> 00:19:23.136 A:middle
足够的视觉复杂性这一点

00:19:23.136 --> 00:19:23.686 A:middle
是十分重要的

00:19:24.706 --> 00:19:26.216 A:middle
如果环境达不到这一要求

00:19:26.466 --> 00:19:28.046 A:middle
比如说光线过于昏暗

00:19:28.046 --> 00:19:29.666 A:middle
或者你正对着一面白墙

00:19:29.666 --> 00:19:31.606 A:middle
那跟踪的表现

00:19:31.606 --> 00:19:32.656 A:middle
就会很糟糕

00:19:33.166 --> 00:19:36.936 A:middle
第三点是世界跟踪的

00:19:36.936 --> 00:19:38.646 A:middle
最佳运作环境

00:19:38.646 --> 00:19:39.366 A:middle
是静态环境

00:19:40.326 --> 00:19:42.026 A:middle
如果你的取景框中

00:19:42.106 --> 00:19:45.166 A:middle
大部分事物都是移动的

00:19:45.166 --> 00:19:47.036 A:middle
那就会导致视觉数据与

00:19:47.036 --> 00:19:50.526 A:middle
运动数据不吻合

00:19:50.526 --> 00:19:51.776 A:middle
最终可能会导致漂移

00:19:52.846 --> 00:19:54.706 A:middle
同时 设备不能处于

00:19:54.706 --> 00:19:55.936 A:middle
一个移动平台上

00:19:55.936 --> 00:19:57.456 A:middle
比如说公交车或者电梯

00:19:58.326 --> 00:19:59.726 A:middle
假设设备被放置在电梯之中

00:19:59.726 --> 00:20:01.176 A:middle
那运动传感器

00:19:59.726 --> 00:20:01.176 A:middle
那运动传感器

00:20:01.176 --> 00:20:02.706 A:middle
会检测到

00:20:02.706 --> 00:20:04.476 A:middle
向上或向下的运动趋势

00:20:04.476 --> 00:20:06.766 A:middle
但视觉上 它所处的环境

00:20:06.816 --> 00:20:07.566 A:middle
并没有变化

00:20:08.066 --> 00:20:11.846 A:middle
那你要如何获得

00:20:11.846 --> 00:20:13.966 A:middle
用户在使用

00:20:14.036 --> 00:20:16.926 A:middle
你的 App 时的体验的反馈呢

00:20:18.236 --> 00:20:20.586 A:middle
ARKit 会监控它自己的跟踪表现

00:20:21.186 --> 00:20:22.776 A:middle
我们在 ARKit 中应用了机器学习

00:20:23.076 --> 00:20:24.536 A:middle
ARKit 在成千上万组

00:20:24.536 --> 00:20:26.546 A:middle
的数据学习中获得提升

00:20:26.546 --> 00:20:28.706 A:middle
这些数据中包括在不同

00:20:28.706 --> 00:20:30.266 A:middle
情况下跟踪的表现

00:20:31.776 --> 00:20:33.276 A:middle
为了训练出一个能够

00:20:33.276 --> 00:20:35.036 A:middle
告诉你跟踪表现的分类器

00:20:35.036 --> 00:20:36.846 A:middle
我们使用了一些注释

00:20:36.846 --> 00:20:39.486 A:middle
比如视觉内容的数量

00:20:39.566 --> 00:20:41.136 A:middle
在图像中跟踪到的可视特征

00:20:41.936 --> 00:20:44.346 A:middle
以及设备当时的运动速率

00:20:45.496 --> 00:20:47.866 A:middle
在运作时 跟踪的表现

00:20:47.866 --> 00:20:50.636 A:middle
是由这些参数

00:20:50.636 --> 00:20:51.746 A:middle
所决定的

00:20:52.616 --> 00:20:55.116 A:middle
在这段视频中

00:20:55.116 --> 00:20:57.026 A:middle
我们将镜头遮住

00:20:57.026 --> 00:20:58.406 A:middle
但保持活动和

00:20:58.406 --> 00:21:00.846 A:middle
对环境的探索

00:20:58.406 --> 00:21:00.846 A:middle
对环境的探索

00:21:00.846 --> 00:21:03.926 A:middle
可以看到左下角的健康预估

00:21:03.926 --> 00:21:06.296 A:middle
指数在下降

00:21:07.796 --> 00:21:09.596 A:middle
而当我们把遮住镜头

00:21:09.596 --> 00:21:11.336 A:middle
的东西移走时

00:21:11.336 --> 00:21:12.496 A:middle
这一数据就回归正常

00:21:14.036 --> 00:21:16.066 A:middle
ARKit 通过给用户

00:21:16.066 --> 00:21:18.586 A:middle
提供一个跟踪状态将

00:21:18.586 --> 00:21:19.596 A:middle
数据进行简化

00:21:20.466 --> 00:21:22.186 A:middle
跟踪状态由三个

00:21:22.316 --> 00:21:23.426 A:middle
不同的值表示

00:21:23.426 --> 00:21:26.776 A:middle
正常也就是

00:21:26.856 --> 00:21:29.676 A:middle
健康状态 大部分情况下

00:21:29.676 --> 00:21:30.546 A:middle
都是这种状态

00:21:30.546 --> 00:21:31.956 A:middle
没错 在大部分情况下都会是这种状态

00:21:32.506 --> 00:21:34.096 A:middle
另一种状态是限制状态

00:21:34.236 --> 00:21:35.396 A:middle
这种状态会出现在跟踪

00:21:35.396 --> 00:21:36.426 A:middle
表现糟糕时

00:21:37.486 --> 00:21:40.116 A:middle
如果出现了这种状态

00:21:40.116 --> 00:21:42.056 A:middle
那 ARKit 还会告诉你限制

00:21:42.056 --> 00:21:43.786 A:middle
的原因 比如说没有足够的

00:21:43.946 --> 00:21:45.406 A:middle
特征或者

00:21:45.406 --> 00:21:47.766 A:middle
移动太快或者

00:21:47.806 --> 00:21:49.966 A:middle
当前正处于初始化阶段

00:21:50.716 --> 00:21:53.496 A:middle
还有另一种状态便是不可用状态

00:21:53.496 --> 00:21:55.256 A:middle
这意味着当前还未

00:21:55.256 --> 00:21:56.086 A:middle
完成初始化

00:21:57.116 --> 00:21:58.826 A:middle
这样子 不管跟踪状态何时改变

00:21:58.916 --> 00:22:01.146 A:middle
你都能够知道

00:21:58.916 --> 00:22:01.146 A:middle
你都能够知道

00:22:01.416 --> 00:22:03.636 A:middle
相机在跟踪过程中确实

00:22:03.636 --> 00:22:04.076 A:middle
会发生变化

00:22:05.216 --> 00:22:06.116 A:middle
这个功能让你能够

00:22:06.116 --> 00:22:08.666 A:middle
在限制状态出现时

00:22:08.896 --> 00:22:10.346 A:middle
及时地通知

00:22:10.346 --> 00:22:11.096 A:middle
你的用户

00:22:12.116 --> 00:22:13.226 A:middle
你还需要为你的

00:22:13.426 --> 00:22:15.426 A:middle
用户提供有关

00:22:15.776 --> 00:22:17.936 A:middle
改善他们跟踪情况

00:22:17.936 --> 00:22:19.626 A:middle
的高实用性且可操作的建议

00:22:20.136 --> 00:22:22.566 A:middle
因为改进跟踪环境的主动权

00:22:22.566 --> 00:22:23.396 A:middle
掌握在用户手上

00:22:23.776 --> 00:22:25.706 A:middle
这些建议可以是

00:22:25.706 --> 00:22:27.996 A:middle
我们之前讨论过的 往某侧移动以

00:22:28.096 --> 00:22:30.656 A:middle
运行设备进行初始化

00:22:31.206 --> 00:22:32.236 A:middle
或者确保有足够的

00:22:32.236 --> 00:22:34.316 A:middle
光线来保证

00:22:34.416 --> 00:22:35.646 A:middle
足够的复杂性

00:22:36.236 --> 00:22:39.646 A:middle
接下来 我将会总结一下

00:22:39.646 --> 00:22:40.446 A:middle
世界跟踪

00:22:42.136 --> 00:22:46.156 A:middle
世界跟踪为你的相机

00:22:46.156 --> 00:22:48.046 A:middle
提供方向和位置的 6

00:22:48.276 --> 00:22:51.556 A:middle
自由度跟踪

00:22:51.556 --> 00:22:53.356 A:middle
这个跟踪基于你所处的环境

00:22:53.356 --> 00:22:55.286 A:middle
但它本身对你所处

00:22:55.286 --> 00:22:56.916 A:middle
对环境并没有

00:22:56.916 --> 00:22:59.066 A:middle
事先的了解

00:22:59.606 --> 00:23:01.866 A:middle
这个跟踪让在真实

00:22:59.606 --> 00:23:01.866 A:middle
这个跟踪让在真实

00:23:01.866 --> 00:23:02.836 A:middle
世界中的增强内容

00:23:02.836 --> 00:23:05.166 A:middle
可以从任何角度进行察看

00:23:06.636 --> 00:23:08.676 A:middle
世界跟踪也创造了一个

00:23:08.676 --> 00:23:11.196 A:middle
世界地图

00:23:11.196 --> 00:23:13.206 A:middle
这个世界地图成为

00:23:13.206 --> 00:23:15.416 A:middle
新的图像定位的参考系统

00:23:17.266 --> 00:23:18.386 A:middle
为了提供更好的用户体验

00:23:18.386 --> 00:23:20.466 A:middle
跟踪质量需要被监控

00:23:20.466 --> 00:23:22.696 A:middle
并能为用户提供

00:23:22.786 --> 00:23:25.196 A:middle
反馈或者指引

00:23:25.736 --> 00:23:28.676 A:middle
世界跟踪只在你的

00:23:28.676 --> 00:23:29.606 A:middle
设备上运行

00:23:30.056 --> 00:23:31.516 A:middle
所有的结果都只保存

00:23:31.516 --> 00:23:31.926 A:middle
于你的设备中

00:23:33.446 --> 00:23:34.716 A:middle
如果你还没有尝试过这个

00:23:35.576 --> 00:23:37.206 A:middle
不妨在我们的开发者

00:23:37.206 --> 00:23:37.836 A:middle
模板中试一下

00:23:37.936 --> 00:23:39.116 A:middle
比如说你可以创造你的

00:23:39.116 --> 00:23:41.546 A:middle
AR 初体验

00:23:41.546 --> 00:23:43.726 A:middle
你可以做出一些探索

00:23:43.726 --> 00:23:44.866 A:middle
花上 15 分钟时间了解不同情况

00:23:44.866 --> 00:23:46.536 A:middle
下的跟踪质量 比如不同灯光效果

00:23:46.616 --> 00:23:48.036 A:middle
或者运动频率

00:23:48.536 --> 00:23:50.476 A:middle
一直记住 在用户遇到

00:23:50.476 --> 00:23:53.596 A:middle
限制跟踪状态时

00:23:53.826 --> 00:23:56.696 A:middle
要及时给他提供指导

00:23:56.696 --> 00:23:58.646 A:middle
以保证他能有一个

00:23:58.886 --> 00:24:00.036 A:middle
好的跟踪体验

00:23:58.886 --> 00:24:00.036 A:middle
好的跟踪体验

00:24:01.456 --> 00:24:04.586 A:middle
世界跟踪与相机相关

00:24:04.586 --> 00:24:06.566 A:middle
它与你的相机位置

00:24:06.566 --> 00:24:09.186 A:middle
以及你所处的环境相关

00:24:10.156 --> 00:24:13.026 A:middle
接下来 我们要讨论

00:24:13.026 --> 00:24:14.536 A:middle
视觉内容是如何

00:24:14.836 --> 00:24:16.776 A:middle
与现实环境互动的

00:24:17.136 --> 00:24:19.156 A:middle
这一效果时通过平面检测

00:24:19.156 --> 00:24:19.726 A:middle
来实现

00:24:22.916 --> 00:24:24.876 A:middle
接下来这段视频

00:24:24.876 --> 00:24:26.576 A:middle
没错这段视频也来自于 Ikea App

00:24:26.576 --> 00:24:28.146 A:middle
是一个很棒的平面检测例子

00:24:28.466 --> 00:24:30.626 A:middle
将虚拟物品放置到

00:24:30.626 --> 00:24:32.566 A:middle
你的真实环境中

00:24:32.566 --> 00:24:34.426 A:middle
并与它互动

00:24:35.296 --> 00:24:37.786 A:middle
首先 我们要留意的是

00:24:37.786 --> 00:24:39.256 A:middle
在 Ikea App 中 设计者是如何

00:24:39.256 --> 00:24:41.156 A:middle
指引用户进行运动的

00:24:42.296 --> 00:24:44.326 A:middle
接着 一旦检测到水平面后

00:24:44.376 --> 00:24:46.796 A:middle
一个虚拟桌子出现

00:24:46.796 --> 00:24:49.916 A:middle
并等待着你去放置

00:24:51.256 --> 00:24:52.856 A:middle
你为它选好位置 并按照你的

00:24:52.856 --> 00:24:54.646 A:middle
想法旋转后

00:24:54.646 --> 00:24:55.886 A:middle
你可以将它固定到环境中

00:24:56.126 --> 00:24:56.976 A:middle
你是否有留意到

00:24:56.976 --> 00:24:59.206 A:middle
当你将这个桌子固定住时

00:24:59.256 --> 00:25:02.196 A:middle
平面与桌子之间的互动

00:24:59.256 --> 00:25:02.196 A:middle
平面与桌子之间的互动

00:25:02.196 --> 00:25:04.916 A:middle
就是地面有轻微的抖动

00:25:05.276 --> 00:25:06.986 A:middle
当我们知道地面在哪时

00:25:06.986 --> 00:25:08.416 A:middle
我们就可以实现这个效果

00:25:09.556 --> 00:25:10.996 A:middle
让我们进一步的讨论

00:25:10.996 --> 00:25:12.696 A:middle
这个过程中究竟发生了什么

00:25:14.016 --> 00:25:16.086 A:middle
平面检测所运用的

00:25:16.166 --> 00:25:18.726 A:middle
是我刚刚所提到的

00:25:18.726 --> 00:25:20.656 A:middle
世界地图

00:25:20.656 --> 00:25:22.726 A:middle
在这里我们用

00:25:22.726 --> 00:25:24.716 A:middle
黄色的点来代表它

00:25:25.176 --> 00:25:28.446 A:middle
平面检测利用这些点

00:25:28.446 --> 00:25:30.966 A:middle
来检测水平或者垂直的平面

00:25:30.966 --> 00:25:32.936 A:middle
比如说地面

00:25:32.936 --> 00:25:34.586 A:middle
长椅以及小墙面

00:25:35.386 --> 00:25:36.946 A:middle
它通过积累多个

00:25:36.946 --> 00:25:38.746 A:middle
ARFrame 的信息

00:25:38.746 --> 00:25:39.266 A:middle
来实现这个效果

00:25:40.096 --> 00:25:42.496 A:middle
随着用户对场景的

00:25:42.596 --> 00:25:44.316 A:middle
进一步探索 它能获得

00:25:44.316 --> 00:25:46.026 A:middle
更多有关真实平面的信息

00:25:46.896 --> 00:25:48.086 A:middle
这些信息让平面检测

00:25:48.086 --> 00:25:52.056 A:middle
可以提供和扩张平面

00:25:52.056 --> 00:25:52.916 A:middle
就像一个凸壳

00:25:53.386 --> 00:25:58.006 A:middle
如果在同一物理表面上

00:25:58.006 --> 00:26:00.676 A:middle
检测到多于一个的平面

00:25:58.006 --> 00:26:00.676 A:middle
检测到多于一个的平面

00:26:00.676 --> 00:26:02.876 A:middle
就像我们现在看的这一部分

00:26:02.876 --> 00:26:04.636 A:middle
绿色和紫色的平面

00:26:05.226 --> 00:26:06.726 A:middle
一旦他们重叠到一起

00:26:06.726 --> 00:26:07.926 A:middle
就会被合并

00:26:08.596 --> 00:26:11.356 A:middle
如果水平和垂直的平面

00:26:11.356 --> 00:26:13.026 A:middle
出现相交的情况

00:26:13.026 --> 00:26:15.666 A:middle
它们相交的地方会被剪掉

00:26:15.666 --> 00:26:18.156 A:middle
这也是 ARKit 2 中的新功能

00:26:19.876 --> 00:26:21.776 A:middle
平面检测被设定为

00:26:21.776 --> 00:26:24.686 A:middle
有很少的误差

00:26:24.766 --> 00:26:27.166 A:middle
因为它重新使用了世界跟踪

00:26:27.166 --> 00:26:28.296 A:middle
中的 3D 点

00:26:29.086 --> 00:26:31.166 A:middle
然后它将平面放置到

00:26:31.166 --> 00:26:33.386 A:middle
这些点云中

00:26:33.696 --> 00:26:36.346 A:middle
并不断的集合越来越多的点

00:26:36.346 --> 00:26:38.326 A:middle
将已重叠的平面

00:26:38.606 --> 00:26:40.286 A:middle
合并起来

00:26:40.936 --> 00:26:42.516 A:middle
所以 检测第一个平面

00:26:42.516 --> 00:26:44.806 A:middle
需要耗费一定的时间

00:26:46.096 --> 00:26:47.276 A:middle
这对你有什么影响呢

00:26:48.856 --> 00:26:50.726 A:middle
当你的 App 刚启动时

00:26:50.726 --> 00:26:53.036 A:middle
可能并不会立刻有可以

00:26:53.036 --> 00:26:55.846 A:middle
放置物品或与物品互动的平面

00:26:57.266 --> 00:26:58.786 A:middle
如果你的体验一定需要

00:26:58.876 --> 00:27:01.496 A:middle
检测到平面的话

00:26:58.876 --> 00:27:01.496 A:middle
检测到平面的话

00:27:01.786 --> 00:27:04.006 A:middle
你需要指引你的用户

00:27:04.426 --> 00:27:06.006 A:middle
移动相机以获得足够

00:27:06.106 --> 00:27:08.956 A:middle
的平移来提供

00:27:08.956 --> 00:27:11.616 A:middle
足够的视觉差以保证密集的重建

00:27:11.616 --> 00:27:13.396 A:middle
以及在场景中

00:27:13.396 --> 00:27:15.086 A:middle
有足够的视觉复杂性

00:27:15.976 --> 00:27:18.096 A:middle
对了 单有旋转数据是

00:27:18.096 --> 00:27:20.416 A:middle
不足以重建的

00:27:20.946 --> 00:27:23.856 A:middle
怎么样才能启用

00:27:23.856 --> 00:27:24.796 A:middle
平面检测呢

00:27:25.786 --> 00:27:26.966 A:middle
这个也很简单

00:27:27.116 --> 00:27:28.676 A:middle
因为平面检测使用的是

00:27:28.676 --> 00:27:30.286 A:middle
世界跟踪的 3D 地图

00:27:30.286 --> 00:27:32.486 A:middle
它可以通过使用

00:27:32.646 --> 00:27:33.316 A:middle
ARWorldTrackingConfiguration

00:27:33.316 --> 00:27:35.126 A:middle
这一参数进行配置

00:27:35.646 --> 00:27:38.346 A:middle
planeDetection 的特征

00:27:38.346 --> 00:27:40.706 A:middle
可以设置为

00:27:40.706 --> 00:27:41.986 A:middle
水平 垂直

00:27:41.986 --> 00:27:43.626 A:middle
或者像这种情况下

00:27:43.706 --> 00:27:43.926 A:middle
两者皆可

00:27:45.116 --> 00:27:47.446 A:middle
接下来你便可以

00:27:47.446 --> 00:27:49.046 A:middle
用这个配置

00:27:49.106 --> 00:27:50.226 A:middle
来运行你的 ARSession

00:27:50.476 --> 00:27:52.146 A:middle
ARKit 就开始对

00:27:52.146 --> 00:27:52.856 A:middle
平面的检测了

00:27:53.526 --> 00:27:56.086 A:middle
那这些检测到的平面结果

00:27:56.086 --> 00:27:58.576 A:middle
是如何返回给你的呢

00:28:01.496 --> 00:28:03.216 A:middle
检测的平面会以

00:28:03.216 --> 00:28:05.006 A:middle
ARPlaneAnchor 的形式返回给你

00:28:05.936 --> 00:28:07.996 A:middle
ARPlaneAnchor 是 ARAnchor

00:28:07.996 --> 00:28:08.946 A:middle
的一个子类

00:28:10.106 --> 00:28:14.476 A:middle
每个 ARAnchor 都会提供一个变换

00:28:14.476 --> 00:28:16.326 A:middle
这个变换包含锚点在你

00:28:16.606 --> 00:28:17.566 A:middle
的世界地图中的位置信息

00:28:18.126 --> 00:28:20.216 A:middle
一个平面锚点

00:28:20.216 --> 00:28:25.886 A:middle
也有着有关平面表面的几何信息

00:28:26.816 --> 00:28:27.856 A:middle
这些信息有两种

00:28:27.856 --> 00:28:28.976 A:middle
可选的代表方式

00:28:29.316 --> 00:28:31.256 A:middle
一种是像一个有着中心点

00:28:31.256 --> 00:28:35.096 A:middle
和拓展的密封箱子

00:28:35.096 --> 00:28:37.306 A:middle
另一种则是 3D 网格

00:28:37.356 --> 00:28:39.336 A:middle
这些网格描述着

00:28:39.386 --> 00:28:41.306 A:middle
所检测到的平面的凸壳以及其几何特征

00:28:42.636 --> 00:28:44.646 A:middle
当平面出现增加 更新

00:28:44.736 --> 00:28:48.266 A:middle
或者移除的情况时

00:28:48.266 --> 00:28:51.266 A:middle
需要有提示

00:28:51.266 --> 00:28:53.676 A:middle
来通知我们

00:28:54.656 --> 00:28:57.086 A:middle
这能让你及时

00:28:57.086 --> 00:28:59.566 A:middle
利用这些平面并对更新

00:28:59.686 --> 00:29:01.486 A:middle
做出反应

00:28:59.686 --> 00:29:01.486 A:middle
做出反应

00:29:01.666 --> 00:29:03.366 A:middle
你可以对平面做些什么呢

00:29:04.866 --> 00:29:05.956 A:middle
我们刚刚在 Ikea App 中

00:29:05.956 --> 00:29:07.796 A:middle
看到的就是很好的例子

00:29:08.026 --> 00:29:09.496 A:middle
比如说你可以通过冲击测试

00:29:09.496 --> 00:29:10.916 A:middle
来放置虚拟物品

00:29:12.046 --> 00:29:13.606 A:middle
你也可以与一些

00:29:13.606 --> 00:29:15.186 A:middle
虚拟物品进行真实互动

00:29:15.286 --> 00:29:17.846 A:middle
就好像我们刚刚也看到了

00:29:17.846 --> 00:29:18.706 A:middle
抖动是可以实现的

00:29:19.816 --> 00:29:21.966 A:middle
你也可以在检测到的平面

00:29:21.966 --> 00:29:23.226 A:middle
上添加一个遮挡平面

00:29:23.226 --> 00:29:25.346 A:middle
所有虚拟物品

00:29:25.496 --> 00:29:27.286 A:middle
就会被隐藏到

00:29:27.336 --> 00:29:30.916 A:middle
这个遮挡屏幕之下或者之后

00:29:32.616 --> 00:29:34.436 A:middle
所以 让我来总结一下我们现在

00:29:34.436 --> 00:29:35.386 A:middle
所学习了的东西

00:29:36.536 --> 00:29:37.756 A:middle
我们了解了方向跟踪

00:29:37.816 --> 00:29:41.176 A:middle
世界跟踪

00:29:41.286 --> 00:29:44.386 A:middle
以及平面检测

00:29:45.306 --> 00:29:47.666 A:middle
接下里 Michele 将会

00:29:47.666 --> 00:29:48.986 A:middle
进一步的介绍我们的新的

00:29:48.986 --> 00:29:50.316 A:middle
跟踪技术 这些技术

00:29:50.386 --> 00:29:52.976 A:middle
将会应用在 ARKit 2 中

00:29:53.046 --> 00:29:54.246 A:middle
让我们欢迎 Michele

00:29:55.176 --> 00:29:57.500 A:middle
[ 掌声 ]

00:29:58.396 --> 00:29:59.186 A:middle
&gt;&gt; 谢谢 Marion

00:30:00.506 --> 00:30:01.766 A:middle
大家好 我是 Michele

00:30:01.766 --> 00:30:02.766 A:middle
很高兴能负责为大家

00:30:02.766 --> 00:30:03.636 A:middle
讲解此次演示

00:30:03.636 --> 00:30:03.886 A:middle
余下的话题

00:30:05.136 --> 00:30:07.886 A:middle
接下来要讨论的是保存和

00:30:07.886 --> 00:30:08.326 A:middle
加载地图

00:30:08.936 --> 00:30:10.486 A:middle
这个功能可以

00:30:10.486 --> 00:30:11.816 A:middle
将一个阶段中所需要的

00:30:11.816 --> 00:30:12.906 A:middle
所有数据存储起来

00:30:13.336 --> 00:30:14.256 A:middle
这样子 在稍后的另一个阶段中

00:30:14.256 --> 00:30:16.216 A:middle
这些数据就可以

00:30:16.216 --> 00:30:18.116 A:middle
被重新运用并用于

00:30:18.336 --> 00:30:19.706 A:middle
创造与某个特定地区

00:30:19.706 --> 00:30:21.486 A:middle
体验一致的 AR 体验

00:30:22.406 --> 00:30:23.566 A:middle
或者它也可以存储在

00:30:23.566 --> 00:30:25.416 A:middle
另一个设备上来创造

00:30:25.606 --> 00:30:27.906 A:middle
多用户的 AR 体验

00:30:28.646 --> 00:30:30.000 A:middle
举个例子

00:30:37.076 --> 00:30:38.876 A:middle
这里有个男人

00:30:38.876 --> 00:30:40.906 A:middle
让我们叫他 Andre

00:30:40.906 --> 00:30:41.956 A:middle
他手里拿着设备

00:30:41.956 --> 00:30:43.506 A:middle
绕着桌子走

00:30:43.506 --> 00:30:44.416 A:middle
正在体验 AR

00:30:45.366 --> 00:30:47.596 A:middle
你可以看到他现在

00:30:47.986 --> 00:30:48.856 A:middle
通过手上的设备往桌上

00:30:48.856 --> 00:30:50.766 A:middle
添加了一个虚拟花瓶

00:30:50.766 --> 00:30:52.506 A:middle
让这一切看起来更有趣了

00:30:54.556 --> 00:30:56.746 A:middle
几分钟后

00:30:56.986 --> 00:30:58.126 A:middle
他的朋友也来了

00:30:58.366 --> 00:31:00.576 A:middle
现在他们都看着这个场景

00:30:58.366 --> 00:31:00.576 A:middle
现在他们都看着这个场景

00:31:00.656 --> 00:31:01.926 A:middle
Andre 的设备在左边

00:31:02.446 --> 00:31:04.196 A:middle
而他朋友的设备

00:31:04.196 --> 00:31:05.126 A:middle
在右边

00:31:06.546 --> 00:31:07.446 A:middle
你可以看到他们

00:31:07.706 --> 00:31:08.836 A:middle
正在看同一个空间

00:31:08.926 --> 00:31:09.886 A:middle
他们能看到彼此

00:31:10.266 --> 00:31:11.806 A:middle
但最重要的是

00:31:11.806 --> 00:31:13.386 A:middle
他们可以看到同样的虚拟内容

00:31:14.286 --> 00:31:15.446 A:middle
他们正在体验的是一个

00:31:15.446 --> 00:31:19.246 A:middle
共享的 AR 体验

00:31:19.246 --> 00:31:21.456 A:middle
上面这个例子

00:31:21.456 --> 00:31:23.916 A:middle
可以被分为三个阶段

00:31:24.266 --> 00:31:25.816 A:middle
第一个阶段 Andre 围着

00:31:25.816 --> 00:31:27.616 A:middle
桌子走动并获得了世界地图

00:31:28.886 --> 00:31:30.246 A:middle
第二个阶段 世界地图在设备之间

00:31:30.536 --> 00:31:31.506 A:middle
进行了分享

00:31:32.276 --> 00:31:34.346 A:middle
第三个阶段 他的朋友的设备

00:31:34.496 --> 00:31:36.056 A:middle
重新定位了世界地图

00:31:37.496 --> 00:31:38.986 A:middle
这意味着这个对象

00:31:38.986 --> 00:31:40.536 A:middle
在新设备上也能识别

00:31:40.536 --> 00:31:42.936 A:middle
这个场景与另一个设备上的是相同的

00:31:43.586 --> 00:31:45.406 A:middle
然后根据地图来

00:31:45.406 --> 00:31:46.556 A:middle
计算设备的准确位置

00:31:46.556 --> 00:31:48.356 A:middle
之后就开始跟踪

00:31:48.356 --> 00:31:50.106 A:middle
就好像新加入的设备

00:31:50.106 --> 00:31:51.746 A:middle
自己获取了这个世界地图一样

00:31:52.376 --> 00:31:54.956 A:middle
接下来 我们将会对

00:31:54.956 --> 00:31:56.196 A:middle
这三个阶段进行更细节的了解

00:31:56.866 --> 00:31:59.246 A:middle
但首先 让我们复习一下

00:31:59.246 --> 00:32:00.426 A:middle
什么是世界地图

00:31:59.246 --> 00:32:00.426 A:middle
什么是世界地图

00:32:01.156 --> 00:32:02.866 A:middle
世界地图包括

00:32:02.866 --> 00:32:04.966 A:middle
系统定位所需要的

00:32:04.966 --> 00:32:06.476 A:middle
所有跟踪数据

00:32:06.986 --> 00:32:08.546 A:middle
包括那些特征点

00:32:08.546 --> 00:32:09.866 A:middle
就像 Marion 刚刚已经

00:32:09.866 --> 00:32:10.506 A:middle
详细解释的一样

00:32:10.876 --> 00:32:12.596 A:middle
也包括了这个点的

00:32:12.596 --> 00:32:13.000 A:middle
周围外观

00:32:17.046 --> 00:32:18.496 A:middle
世界地图还包括了

00:32:18.496 --> 00:32:19.826 A:middle
被添加到环节中的

00:32:19.826 --> 00:32:21.806 A:middle
所有锚点 不管这些锚点是由用户

00:32:21.916 --> 00:32:23.636 A:middle
添加的 比如说像平面

00:32:24.896 --> 00:32:26.126 A:middle
我的意思是像由系统

00:32:26.126 --> 00:32:26.456 A:middle
添加的平面

00:32:26.456 --> 00:32:28.126 A:middle
或者由用户添加的锚点

00:32:28.446 --> 00:32:29.646 A:middle
就像我们刚刚在示范中看到的花瓶

00:32:30.746 --> 00:32:33.786 A:middle
这个数据是可串性化

00:32:33.786 --> 00:32:35.536 A:middle
可获取的 你可以利用这些数据

00:32:35.536 --> 00:32:37.346 A:middle
来创造一些可持续的

00:32:37.826 --> 00:32:39.966 A:middle
或者多用户的 AR 体验

00:32:40.326 --> 00:32:41.486 A:middle
现在让我们来看看第一个阶段

00:32:41.486 --> 00:32:43.526 A:middle
也就是获得

00:32:43.526 --> 00:32:44.006 A:middle
世界地图的阶段

00:32:44.756 --> 00:32:47.576 A:middle
我们可以回放第一段视频

00:32:48.056 --> 00:32:49.396 A:middle
Andre 绕着桌子走到时候

00:32:49.396 --> 00:32:51.026 A:middle
在左边 你可以

00:32:51.026 --> 00:32:52.596 A:middle
看到他的设备 就在这里

00:32:53.256 --> 00:32:57.216 A:middle
在右边 你可以以鸟瞰角度

00:32:57.216 --> 00:32:59.126 A:middle
看到通过跟踪系统获得

00:32:59.126 --> 00:33:00.426 A:middle
的世界地图

00:32:59.126 --> 00:33:00.426 A:middle
的世界地图

00:33:00.776 --> 00:33:02.516 A:middle
你可以围绕着

00:33:03.096 --> 00:33:04.566 A:middle
它的桌子和椅子

00:33:05.136 --> 00:33:06.956 A:middle
在这个获取过程中

00:33:06.956 --> 00:33:09.276 A:middle
有几点需要我们注意

00:33:10.076 --> 00:33:11.986 A:middle
首先 Marion 在跟踪部分

00:33:11.986 --> 00:33:13.946 A:middle
所说到的所有技术 都在这里得到应用

00:33:14.456 --> 00:33:15.606 A:middle
场景需要有足够的

00:33:15.606 --> 00:33:17.356 A:middle
视觉复杂性

00:33:17.446 --> 00:33:18.626 A:middle
地图才能获得足够的深度特征点

00:33:19.546 --> 00:33:21.276 A:middle
场景需要是静止的 当然

00:33:22.086 --> 00:33:22.856 A:middle
如果有细微的变化

00:33:22.856 --> 00:33:24.276 A:middle
也是没有关系的

00:33:24.276 --> 00:33:25.716 A:middle
大家也可以看到桌布被风吹动

00:33:26.026 --> 00:33:27.216 A:middle
但场景的主要部分

00:33:27.566 --> 00:33:27.956 A:middle
需要是静止的

00:33:29.036 --> 00:33:30.606 A:middle
另外 在我们

00:33:30.606 --> 00:33:31.936 A:middle
获取世界地图进行分享时

00:33:31.936 --> 00:33:33.856 A:middle
我们需要从多个

00:33:33.856 --> 00:33:35.316 A:middle
不同的角度来

00:33:35.316 --> 00:33:36.636 A:middle
探索这个环境

00:33:37.516 --> 00:33:38.746 A:middle
特别是我们想要

00:33:38.746 --> 00:33:41.046 A:middle
覆盖所有我们想要

00:33:41.106 --> 00:33:42.786 A:middle
被定位的方向

00:33:45.366 --> 00:33:46.926 A:middle
为了让这个过程变得更简单

00:33:47.356 --> 00:33:50.296 A:middle
我们增加了世界地图状态提示

00:33:50.296 --> 00:33:51.736 A:middle
让你能了解有关

00:33:51.736 --> 00:33:52.566 A:middle
世界地图的信息

00:33:53.446 --> 00:33:54.816 A:middle
如果你们有参加过

00:33:54.816 --> 00:33:56.066 A:middle
What's New in ARKit 这一演讲

00:33:56.636 --> 00:33:57.506 A:middle
那么 Arsalan 会极大的扩展这一点

00:33:57.546 --> 00:33:58.936 A:middle
以便快速回顾一下

00:33:59.576 --> 00:34:00.766 A:middle
当你开始会话时

00:33:59.576 --> 00:34:00.766 A:middle
当你开始会话时

00:34:00.766 --> 00:34:02.776 A:middle
世界地图的状态 会从限制状态开始

00:34:02.776 --> 00:34:03.936 A:middle
随着设备了解了

00:34:04.056 --> 00:34:06.746 A:middle
更多场景的信息

00:34:06.746 --> 00:34:07.616 A:middle
就会变成初始化的状态

00:34:08.036 --> 00:34:09.176 A:middle
最后当系统

00:34:09.176 --> 00:34:10.856 A:middle
确定你保持在同一位置时

00:34:10.856 --> 00:34:12.156 A:middle
我们终于可以开始

00:34:12.156 --> 00:34:12.766 A:middle
绘制地图了

00:34:13.726 --> 00:34:15.206 A:middle
这是在制图阶段

00:34:15.206 --> 00:34:16.976 A:middle
你想要保存的地图

00:34:17.626 --> 00:34:20.315 A:middle
所以 这是好的信息

00:34:20.315 --> 00:34:22.056 A:middle
但是这主要是

00:34:22.056 --> 00:34:24.746 A:middle
用户方面的适用操作

00:34:24.966 --> 00:34:26.076 A:middle
所以 这对开发者而言

00:34:26.156 --> 00:34:26.926 A:middle
代表着什么呢

00:34:27.505 --> 00:34:29.085 A:middle
意味着你需要给用户提供指引

00:34:30.275 --> 00:34:32.255 A:middle
我们可以告知用户制图时

00:34:32.326 --> 00:34:33.985 A:middle
的不同状态

00:34:33.985 --> 00:34:35.835 A:middle
甚至可以设置不允许保存

00:34:35.835 --> 00:34:37.886 A:middle
或者分享世界地图

00:34:37.926 --> 00:34:39.476 A:middle
直到制图状态显示绘制状态

00:34:39.996 --> 00:34:44.255 A:middle
我们也可以监控在

00:34:44.326 --> 00:34:45.576 A:middle
获取环节中的

00:34:45.576 --> 00:34:48.696 A:middle
跟踪质量

00:34:48.926 --> 00:34:50.045 A:middle
如果跟踪状态出现一段时间的

00:34:50.045 --> 00:34:51.356 A:middle
限制状态 比如说持续几秒

00:34:51.356 --> 00:34:52.146 A:middle
就可以反馈给用户

00:34:53.056 --> 00:34:54.356 A:middle
或许甚至可以提供

00:34:54.356 --> 00:34:56.036 A:middle
重新开始获取环节的选项

00:34:56.476 --> 00:34:58.496 A:middle
在设备的接受方面

00:34:58.496 --> 00:35:00.726 A:middle
我们也可以指引用户

00:34:58.496 --> 00:35:00.726 A:middle
我们也可以指引用户

00:35:00.726 --> 00:35:02.346 A:middle
以获得更好的定位流程

00:35:03.116 --> 00:35:04.856 A:middle
我们又回到了获取设备这里

00:35:04.856 --> 00:35:06.776 A:middle
当我们在地图阶段时

00:35:06.776 --> 00:35:08.196 A:middle
我们可以对场景进行拍照

00:35:08.196 --> 00:35:10.296 A:middle
然后将它与

00:35:10.396 --> 00:35:11.946 A:middle
世界地图一起发出

00:35:11.986 --> 00:35:13.596 A:middle
在接收端

00:35:13.596 --> 00:35:16.026 A:middle
我们可以让用户找到这个视觉

00:35:16.026 --> 00:35:17.766 A:middle
来开始你的分享体验

00:35:18.386 --> 00:35:20.946 A:middle
这就是获得

00:35:20.946 --> 00:35:21.336 A:middle
世界地图的流程

00:35:21.506 --> 00:35:22.876 A:middle
现在 让我们了解一下如何

00:35:22.876 --> 00:35:24.666 A:middle
才能分享世界地图

00:35:24.966 --> 00:35:26.366 A:middle
首先 你可以简单地通过

00:35:26.366 --> 00:35:27.386 A:middle
在 ARSession 中运行

00:35:27.496 --> 00:35:29.926 A:middle
getCurrentWorldMap 方式来

00:35:29.926 --> 00:35:30.536 A:middle
获得世界地图

00:35:30.896 --> 00:35:33.286 A:middle
这是获得世界地图的一种方式

00:35:34.376 --> 00:35:37.476 A:middle
世界地图是一个可串行化的类

00:35:37.566 --> 00:35:38.856 A:middle
所以我们可以通过

00:35:38.956 --> 00:35:40.576 A:middle
NSKeyedArchiver 公式来

00:35:40.576 --> 00:35:42.106 A:middle
将它串行为一串二进制的数据

00:35:42.106 --> 00:35:44.446 A:middle
你可以选择将这串数据

00:35:44.446 --> 00:35:46.276 A:middle
保存在硬盘中

00:35:46.276 --> 00:35:48.946 A:middle
供单机持续性的应用

00:35:49.706 --> 00:35:52.636 A:middle
你也可以选择 将它在设备之间分享

00:35:52.906 --> 00:35:54.626 A:middle
为了实现设备间分享

00:35:54.626 --> 00:35:56.176 A:middle
你可以使用 MultiPeerConnectivity 框架

00:35:56.936 --> 00:35:58.196 A:middle
这个框架有着很棒的功能

00:35:58.196 --> 00:36:00.186 A:middle
像自动装置 发现附近设备等

00:35:58.196 --> 00:36:00.186 A:middle
像自动装置 发现附近设备等

00:36:00.186 --> 00:36:01.856 A:middle
这使设备之间的数据

00:36:01.856 --> 00:36:04.876 A:middle
可以高效沟通

00:36:05.626 --> 00:36:06.996 A:middle
我们在 ARKit 中也有如何运用

00:36:06.996 --> 00:36:09.396 A:middle
这个技巧的例子

00:36:09.396 --> 00:36:11.036 A:middle
被称为 创造一个多用户的

00:36:11.036 --> 00:36:12.126 A:middle
AR 体验

00:36:12.126 --> 00:36:13.516 A:middle
你可以在我们的开发者网站上找到这个例子

00:36:14.076 --> 00:36:16.836 A:middle
让我们看一下

00:36:16.836 --> 00:36:18.086 A:middle
接收端设备一旦收到世界地图之后

00:36:18.086 --> 00:36:20.096 A:middle
需要如何设置

00:36:20.096 --> 00:36:21.046 A:middle
世界跟踪参数

00:36:21.046 --> 00:36:22.126 A:middle
来使用这个地图

00:36:22.426 --> 00:36:22.986 A:middle
非常简单

00:36:22.986 --> 00:36:24.916 A:middle
你只需要赋予世界地图

00:36:24.916 --> 00:36:27.366 A:middle
原始数据地图的特征

00:36:28.266 --> 00:36:29.986 A:middle
当你运行这个会话时

00:36:29.986 --> 00:36:31.426 A:middle
系统会尝试去寻找

00:36:31.526 --> 00:36:32.376 A:middle
之前的世界地图

00:36:33.636 --> 00:36:35.336 A:middle
但这会耗费一定时间

00:36:35.336 --> 00:36:36.366 A:middle
因为用户可能并没有

00:36:36.366 --> 00:36:37.546 A:middle
从以前的位置拍摄

00:36:37.546 --> 00:36:37.906 A:middle
这一的场景

00:36:38.756 --> 00:36:39.656 A:middle
那我们怎样才能知道

00:36:39.656 --> 00:36:40.606 A:middle
定位正在运行呢

00:36:41.686 --> 00:36:44.666 A:middle
这个信息可以在跟踪阶段获得

00:36:44.666 --> 00:36:45.896 A:middle
只要你用原始世界地图

00:36:45.896 --> 00:36:47.596 A:middle
开始一个会话

00:36:47.596 --> 00:36:49.256 A:middle
那跟踪阶段就会被

00:36:49.256 --> 00:36:51.366 A:middle
限制为重新定位

00:36:51.906 --> 00:36:54.226 A:middle
但在这里你还是可以获得

00:36:54.226 --> 00:36:55.816 A:middle
跟踪数据

00:36:56.176 --> 00:36:58.726 A:middle
但原始世界将会是

00:36:58.726 --> 00:37:00.826 A:middle
第一个相机 就像在一个新的会话中一样

00:36:58.726 --> 00:37:00.826 A:middle
第一个相机 就像在一个新的会话中一样

00:37:01.336 --> 00:37:04.156 A:middle
只要用户将设备

00:37:04.156 --> 00:37:05.606 A:middle
对着同样的场景时

00:37:05.606 --> 00:37:06.616 A:middle
系统就会开始定位

00:37:07.076 --> 00:37:08.136 A:middle
跟踪阶段会重归正常

00:37:08.136 --> 00:37:09.866 A:middle
同时原始世界

00:37:09.866 --> 00:37:12.036 A:middle
将会与记录的世界地图一致

00:37:13.046 --> 00:37:15.496 A:middle
在这里 你之前的所有

00:37:15.496 --> 00:37:16.856 A:middle
锚点在你的会话中

00:37:16.856 --> 00:37:17.916 A:middle
都是可用的 所以你可以将以前

00:37:17.976 --> 00:37:19.226 A:middle
的虚拟内容放回来

00:37:21.816 --> 00:37:23.506 A:middle
在这里需要留意的是

00:37:23.506 --> 00:37:24.986 A:middle
这些幕后操作

00:37:25.206 --> 00:37:26.806 A:middle
是我们

00:37:26.806 --> 00:37:28.206 A:middle
正在匹配那些特征点

00:37:28.616 --> 00:37:29.996 A:middle
你获得世界地图的场景

00:37:29.996 --> 00:37:32.306 A:middle
与你想要重新定位的

00:37:32.306 --> 00:37:33.326 A:middle
场景之间需要有

00:37:33.536 --> 00:37:35.146 A:middle
足够的视觉相似性

00:37:36.116 --> 00:37:37.406 A:middle
所以 如果你想在晚上的时候

00:37:37.406 --> 00:37:38.696 A:middle
回到这个桌子旁进行 AR 体验

00:37:38.696 --> 00:37:41.946 A:middle
那效果很可能会不好

00:37:42.206 --> 00:37:44.116 A:middle
这就是你如何通过

00:37:44.506 --> 00:37:46.516 A:middle
保存或者加载地图

00:37:46.516 --> 00:37:47.686 A:middle
来实现多用户体验

00:37:47.686 --> 00:37:49.526 A:middle
或者持续体验的方式

00:37:50.536 --> 00:37:54.846 A:middle
接下来要讲的是图像跟踪

00:37:54.846 --> 00:37:56.306 A:middle
AR 就是

00:37:56.306 --> 00:37:59.186 A:middle
在物理世界上添加

00:37:59.186 --> 00:38:00.306 A:middle
一些虚拟内容

00:37:59.186 --> 00:38:00.306 A:middle
一些虚拟内容

00:38:00.536 --> 00:38:01.686 A:middle
在物理世界中

00:38:01.686 --> 00:38:02.866 A:middle
到处都可以发现图像

00:38:02.866 --> 00:38:05.226 A:middle
想想

00:38:05.226 --> 00:38:07.086 A:middle
杂志封面

00:38:07.426 --> 00:38:08.336 A:middle
广告

00:38:08.816 --> 00:38:10.136 A:middle
图像跟踪是一个工具

00:38:10.136 --> 00:38:11.506 A:middle
它让你能识别这些真实图像

00:38:11.506 --> 00:38:13.976 A:middle
并能在他们的周围

00:38:14.216 --> 00:38:15.876 A:middle
建立 AR 体验

00:38:17.876 --> 00:38:18.746 A:middle
让我们来看一个例子

00:38:20.006 --> 00:38:21.896 A:middle
大家可以看到这里

00:38:21.896 --> 00:38:23.356 A:middle
有两张图像被同时跟踪

00:38:24.426 --> 00:38:26.766 A:middle
在左边的图像中

00:38:27.206 --> 00:38:29.376 A:middle
一只美丽的大象被

00:38:29.376 --> 00:38:30.716 A:middle
放在真实图像中的大象之上

00:38:31.606 --> 00:38:32.896 A:middle
而在右边 一个真实的图像

00:38:32.896 --> 00:38:35.116 A:middle
被转化为一个虚拟的屏幕

00:38:36.186 --> 00:38:37.626 A:middle
还有一点需要留意的

00:38:37.626 --> 00:38:39.016 A:middle
图像可以在环境中随意移动

00:38:39.016 --> 00:38:40.966 A:middle
因为相机正在以每秒 60 帧的

00:38:40.966 --> 00:38:42.146 A:middle
速度进行跟踪

00:38:43.306 --> 00:38:45.146 A:middle
接下来让我们

00:38:45.146 --> 00:38:46.866 A:middle
讨论在这景象

00:38:47.156 --> 00:38:47.476 A:middle
背后发生了些什么

00:38:47.476 --> 00:38:48.996 A:middle
假设说你有一张

00:38:48.996 --> 00:38:50.526 A:middle
和这个大象一样的图像

00:38:50.896 --> 00:38:52.186 A:middle
你想要在一个类似这样的

00:38:52.236 --> 00:38:52.876 A:middle
景象中找到它

00:38:54.266 --> 00:38:55.556 A:middle
我们将会用灰度来实现这个效果

00:38:55.556 --> 00:38:56.956 A:middle
第一种方式

00:38:56.956 --> 00:38:58.426 A:middle
和我们在跟踪时用到的

00:38:58.426 --> 00:38:58.836 A:middle
有点类似

00:38:58.936 --> 00:38:59.956 A:middle
我们会跟踪

00:39:00.046 --> 00:39:01.526 A:middle
参考图像与

00:39:01.526 --> 00:39:03.566 A:middle
当前环境中的特征点

00:39:04.676 --> 00:39:05.996 A:middle
然后 我们会试图

00:39:05.996 --> 00:39:07.356 A:middle
在当前的场景中匹配

00:39:07.356 --> 00:39:09.346 A:middle
那些在参考图像中特征点

00:39:10.436 --> 00:39:11.696 A:middle
通过运用一些投影几何

00:39:11.696 --> 00:39:12.866 A:middle
以及线性代数

00:39:13.186 --> 00:39:14.366 A:middle
这就可以根据

00:39:14.456 --> 00:39:16.126 A:middle
当前的场景

00:39:16.126 --> 00:39:17.286 A:middle
对图像的位置方向

00:39:17.286 --> 00:39:19.156 A:middle
进行初步的估算

00:39:20.516 --> 00:39:21.386 A:middle
但我们并不满足于此

00:39:22.566 --> 00:39:23.486 A:middle
为了让你能获得一个

00:39:23.486 --> 00:39:25.836 A:middle
真正准确的姿势并做到

00:39:25.836 --> 00:39:27.846 A:middle
每秒 60 帧的跟踪

00:39:27.846 --> 00:39:29.276 A:middle
我们做了一个高密度跟踪阶段

00:39:29.986 --> 00:39:31.316 A:middle
在初步的预估之后

00:39:31.976 --> 00:39:33.136 A:middle
所以我们将现有场景的像素

00:39:33.136 --> 00:39:36.486 A:middle
回溯到一个

00:39:36.566 --> 00:39:38.266 A:middle
长方形的形状

00:39:38.266 --> 00:39:40.916 A:middle
也就是你在右边 右上角所能看到的

00:39:41.306 --> 00:39:42.776 A:middle
所以这是将

00:39:43.536 --> 00:39:45.296 A:middle
现有图像的像素回溯成

00:39:45.296 --> 00:39:46.986 A:middle
一个长方形后重组的图像

00:39:47.916 --> 00:39:48.766 A:middle
我们将重组后的图像

00:39:48.766 --> 00:39:50.456 A:middle
与我们所拥有的

00:39:50.506 --> 00:39:51.726 A:middle
参考图像进行对比

00:39:51.726 --> 00:39:54.056 A:middle
来创造你们所看到的

00:39:54.056 --> 00:39:55.436 A:middle
下面这个误差图像

00:39:56.636 --> 00:39:59.836 A:middle
接下来 我们会优化图像的位置方向

00:39:59.906 --> 00:40:00.976 A:middle
这样能使误差最小化

00:39:59.906 --> 00:40:00.976 A:middle
这样能使误差最小化

00:40:03.366 --> 00:40:04.786 A:middle
这也意味着

00:40:04.786 --> 00:40:06.796 A:middle
对你来说这个结果将会

00:40:06.796 --> 00:40:07.266 A:middle
十分准确

00:40:08.146 --> 00:40:09.706 A:middle
谢谢

00:40:10.526 --> 00:40:12.076 A:middle
而且还能做到每秒

00:40:12.076 --> 00:40:12.826 A:middle
跟踪 60 帧

00:40:15.296 --> 00:40:16.906 A:middle
让我们看看我们在

00:40:16.906 --> 00:40:18.126 A:middle
ARKit 中时如何实现这些

00:40:18.916 --> 00:40:21.856 A:middle
照常 ARKit 的 API

00:40:21.856 --> 00:40:22.506 A:middle
十分简单

00:40:22.566 --> 00:40:24.566 A:middle
我们只需要做三个简单的步骤

00:40:24.606 --> 00:40:26.686 A:middle
首先 我们需要收集

00:40:26.686 --> 00:40:27.596 A:middle
所有参考图像

00:40:28.486 --> 00:40:31.346 A:middle
接着 我们要设置 AR 会话的参数

00:40:31.606 --> 00:40:32.646 A:middle
我们有两种选择

00:40:33.166 --> 00:40:34.306 A:middle
一个是世界跟踪配置

00:40:34.306 --> 00:40:37.426 A:middle
它也提供设备位置

00:40:37.426 --> 00:40:39.296 A:middle
这个是我们目前所讨论的方式

00:40:39.856 --> 00:40:42.576 A:middle
在 iOS 12 中 将会有一个新的配置

00:40:42.576 --> 00:40:44.476 A:middle
这是一个独立的跟踪配置

00:40:44.936 --> 00:40:47.906 A:middle
一旦你开始了会话

00:40:47.906 --> 00:40:49.896 A:middle
你就会开始通过 ARImageAnchor

00:40:49.896 --> 00:40:52.326 A:middle
的形式接收结果

00:40:53.296 --> 00:40:54.306 A:middle
下面 我们将会

00:40:54.306 --> 00:40:55.626 A:middle
进一步研究这三个步骤

00:40:56.036 --> 00:40:57.886 A:middle
让我们从参考图像开始

00:40:58.426 --> 00:41:01.286 A:middle
往你的 App 中加入

00:40:58.426 --> 00:41:01.286 A:middle
往你的 App 中加入

00:41:01.286 --> 00:41:02.836 A:middle
参考图像的最便捷方式

00:41:02.896 --> 00:41:05.106 A:middle
就是通过资产目录

00:41:06.146 --> 00:41:08.066 A:middle
你可以创造一个 AR 资源组

00:41:08.066 --> 00:41:10.386 A:middle
然后将你的图像拖拽到组里

00:41:11.566 --> 00:41:13.026 A:middle
接下来 你需要设置

00:41:13.026 --> 00:41:14.446 A:middle
图像的物理尺寸

00:41:14.556 --> 00:41:16.196 A:middle
你可以通过右上角

00:41:16.196 --> 00:41:17.436 A:middle
的特征窗口进行设置

00:41:17.946 --> 00:41:20.656 A:middle
设置物理尺寸是硬性要求

00:41:21.096 --> 00:41:23.126 A:middle
这里有几个原因

00:41:24.466 --> 00:41:26.126 A:middle
第一 设置物理尺寸能让

00:41:26.126 --> 00:41:27.816 A:middle
图像以物理尺寸展示

00:41:28.386 --> 00:41:29.776 A:middle
也就是说 你的内容

00:41:29.816 --> 00:41:31.076 A:middle
将会是物理大小

00:41:31.186 --> 00:41:32.666 A:middle
在 ARKit 中 所有事物都是

00:41:32.666 --> 00:41:33.986 A:middle
以米为测量单位 所以你的虚拟

00:41:33.986 --> 00:41:36.226 A:middle
物品将会以米为长度

00:41:37.056 --> 00:41:38.626 A:middle
第二 设置正确的

00:41:38.626 --> 00:41:40.266 A:middle
图像物理尺寸是十分重要的

00:41:40.266 --> 00:41:41.586 A:middle
因为我们

00:41:41.946 --> 00:41:43.166 A:middle
有可能会将图像跟踪

00:41:43.216 --> 00:41:44.476 A:middle
与世界跟踪合并到一起

00:41:44.936 --> 00:41:46.206 A:middle
这会立刻给予图像

00:41:46.526 --> 00:41:48.616 A:middle
与世界之间

00:41:48.616 --> 00:41:51.196 A:middle
一致的姿态

00:41:51.196 --> 00:41:52.826 A:middle
让我们看看有关参考图像的

00:41:53.256 --> 00:41:54.166 A:middle
更多例子

00:41:54.816 --> 00:41:57.806 A:middle
这里有两幅很漂亮的图像

00:41:58.476 --> 00:41:59.776 A:middle
这些图像十分适合

00:41:59.776 --> 00:42:01.066 A:middle
图像跟踪

00:41:59.776 --> 00:42:01.066 A:middle
图像跟踪

00:42:01.156 --> 00:42:03.426 A:middle
它们有着高饱和度

00:42:03.426 --> 00:42:04.956 A:middle
高对比度

00:42:04.956 --> 00:42:06.376 A:middle
合理分配的柱形图

00:42:06.376 --> 00:42:08.416 A:middle
并且没有重复的结构

00:42:08.536 --> 00:42:10.486 A:middle
这里也有一些

00:42:10.486 --> 00:42:12.936 A:middle
和系统适配度不高的图像

00:42:13.436 --> 00:42:14.726 A:middle
右边这个图像就是不合适类型

00:42:15.456 --> 00:42:16.176 A:middle
的一个示范

00:42:17.116 --> 00:42:19.606 A:middle
如果我们仔细观察上面

00:42:19.696 --> 00:42:21.596 A:middle
这两个例子 我们会发现

00:42:21.596 --> 00:42:23.466 A:middle
好的图像都有着

00:42:23.466 --> 00:42:25.036 A:middle
许多关键点

00:42:25.536 --> 00:42:26.306 A:middle
图像中的

00:42:26.306 --> 00:42:27.716 A:middle
柱形图在整个

00:42:27.716 --> 00:42:28.686 A:middle
范围内平均分布

00:42:29.316 --> 00:42:30.126 A:middle
而右边的图像

00:42:30.126 --> 00:42:33.096 A:middle
则只有很少的关键点

00:42:33.096 --> 00:42:34.286 A:middle
它们的柱形图也

00:42:34.286 --> 00:42:36.536 A:middle
倾向白色的部分

00:42:37.236 --> 00:42:40.536 A:middle
在 Xcode 中

00:42:40.536 --> 00:42:42.066 A:middle
你可以直观的了解到一张图像

00:42:42.066 --> 00:42:42.916 A:middle
是否适合用于跟踪

00:42:44.076 --> 00:42:46.066 A:middle
你只需要将图像拖放到

00:42:46.066 --> 00:42:48.396 A:middle
Xcode 中 Xcode 就会自动

00:42:48.576 --> 00:42:50.316 A:middle
对图像进行分析 然后界面会以

00:42:50.316 --> 00:42:52.206 A:middle
警告的方式给你及早的反馈

00:42:52.276 --> 00:42:54.516 A:middle
这个反馈甚至还要早于你 运行你的 App

00:42:55.646 --> 00:42:56.666 A:middle
比如说 如果你点击

00:42:56.666 --> 00:42:59.086 A:middle
底部的这张图像

00:42:59.086 --> 00:43:01.836 A:middle
这张图像可能是杂志的某一页

00:42:59.086 --> 00:43:01.836 A:middle
这张图像可能是杂志的某一页

00:43:01.836 --> 00:43:04.396 A:middle
我们可以看到 Xcode 反馈说

00:43:04.556 --> 00:43:06.436 A:middle
这张图像的柱形图 分布不均匀

00:43:06.676 --> 00:43:07.626 A:middle
你可以看到图像中

00:43:07.626 --> 00:43:09.326 A:middle
有大量的白色

00:43:09.476 --> 00:43:11.716 A:middle
这个图像也包含了许多重复结构

00:43:11.716 --> 00:43:15.636 A:middle
这些重复结构主要是文字

00:43:15.776 --> 00:43:18.076 A:middle
另一个例子就是 如果你有两张

00:43:18.076 --> 00:43:20.126 A:middle
极其类似且

00:43:20.126 --> 00:43:22.176 A:middle
十分容易在检测时混淆的图像

00:43:22.506 --> 00:43:24.366 A:middle
Xcode 也会

00:43:24.366 --> 00:43:25.166 A:middle
做出相应的警告

00:43:25.906 --> 00:43:26.986 A:middle
这两张有关

00:43:26.986 --> 00:43:28.906 A:middle
同一齿状山脉的图像

00:43:28.906 --> 00:43:29.756 A:middle
就是一个很好的例子

00:43:30.156 --> 00:43:32.446 A:middle
如果这个警告出现时

00:43:32.446 --> 00:43:33.506 A:middle
我们并不能做什么来改正

00:43:33.936 --> 00:43:34.956 A:middle
例如 让我们回到这张

00:43:34.956 --> 00:43:38.236 A:middle
有着重复结构

00:43:38.236 --> 00:43:40.746 A:middle
且分布不平均

00:43:40.746 --> 00:43:41.746 A:middle
的柱形图图像

00:43:42.516 --> 00:43:44.186 A:middle
你可以尝试去定位

00:43:44.186 --> 00:43:45.126 A:middle
这张图像中区别足够

00:43:45.126 --> 00:43:46.596 A:middle
明显的地方 在这里

00:43:46.596 --> 00:43:48.736 A:middle
就是页面中真正的

00:43:48.736 --> 00:43:49.536 A:middle
图像部分

00:43:50.106 --> 00:43:51.276 A:middle
接下来 你就可以对图像

00:43:51.276 --> 00:43:52.756 A:middle
进行裁剪 将裁剪后的图像

00:43:52.756 --> 00:43:53.556 A:middle
作为参考图像

00:43:53.876 --> 00:43:55.126 A:middle
这样子 Xcode 就不会发出

00:43:55.476 --> 00:43:56.756 A:middle
任何警告

00:43:56.756 --> 00:43:58.296 A:middle
你也可以获得更好的

00:43:58.946 --> 00:43:59.716 A:middle
跟踪质量

00:44:00.026 --> 00:44:03.106 A:middle
我们还可以

00:44:03.436 --> 00:44:06.376 A:middle
使用多个 AR 资源群组

00:44:07.496 --> 00:44:09.176 A:middle
这个功能可以同时检测

00:44:09.396 --> 00:44:10.146 A:middle
多张图像

00:44:10.416 --> 00:44:11.726 A:middle
为了保持体验的高效性

00:44:11.726 --> 00:44:13.816 A:middle
与反应性

00:44:14.236 --> 00:44:15.296 A:middle
你可以设置每个组最多

00:44:15.296 --> 00:44:16.736 A:middle
只能处理 25 张图像

00:44:17.936 --> 00:44:19.016 A:middle
但群组的数量

00:44:19.246 --> 00:44:19.826 A:middle
是没有限制的

00:44:20.036 --> 00:44:21.896 A:middle
接着 你就可以通过编程

00:44:21.896 --> 00:44:23.156 A:middle
的方式切换不同的群组

00:44:23.376 --> 00:44:26.076 A:middle
举个例子 如果你想要创造

00:44:26.076 --> 00:44:27.076 A:middle
一个博物馆中的 AR 体验

00:44:27.076 --> 00:44:28.846 A:middle
这个博物馆可能有着

00:44:28.846 --> 00:44:30.646 A:middle
成千上百的图像

00:44:31.816 --> 00:44:33.396 A:middle
通常 这些图像

00:44:33.396 --> 00:44:34.586 A:middle
分布在博物馆的

00:44:34.586 --> 00:44:35.336 A:middle
不同房间中

00:44:35.646 --> 00:44:37.646 A:middle
那你将要做的是

00:44:38.246 --> 00:44:39.756 A:middle
将在同一个房间中的图像

00:44:39.756 --> 00:44:41.476 A:middle
划分到一个群组中

00:44:41.686 --> 00:44:43.036 A:middle
在另外一个房间的图像则属于

00:44:43.036 --> 00:44:43.596 A:middle
另一个群组

00:44:44.246 --> 00:44:45.806 A:middle
随后 你可以用中心定点

00:44:45.806 --> 00:44:48.686 A:middle
来切换不同的房间

00:44:49.186 --> 00:44:52.116 A:middle
在这种情况下

00:44:52.446 --> 00:44:53.906 A:middle
只要在不同的群组中就可以

00:44:53.946 --> 00:44:55.126 A:middle
存在类似的图像

00:44:55.896 --> 00:44:57.716 A:middle
好的 以上就是有关

00:44:57.746 --> 00:44:58.266 A:middle
参考图像的信息

00:44:58.266 --> 00:45:01.916 A:middle
现在 让我们来看看我们的两个配置吧

00:44:58.266 --> 00:45:01.916 A:middle
现在 让我们来看看我们的两个配置吧

00:45:03.006 --> 00:45:05.286 A:middle
ARImageTrackingConfiguration

00:45:05.416 --> 00:45:06.876 A:middle
是一个全新的 独立的

00:45:06.876 --> 00:45:07.836 A:middle
图像跟踪配置

00:45:07.866 --> 00:45:09.646 A:middle
这意味着它不会运转世界跟踪

00:45:10.636 --> 00:45:11.846 A:middle
也就是说并没有

00:45:12.136 --> 00:45:12.916 A:middle
它并没有世界原点

00:45:13.216 --> 00:45:14.676 A:middle
所以 每一张反馈给你的图像

00:45:14.676 --> 00:45:16.526 A:middle
都是与当前相机的视角有关

00:45:18.246 --> 00:45:19.346 A:middle
你可以将图像跟踪

00:45:19.406 --> 00:45:21.646 A:middle
与世界跟踪的配置结合起来

00:45:22.466 --> 00:45:24.536 A:middle
在这种情况下

00:45:24.536 --> 00:45:25.636 A:middle
你可以获得所有与

00:45:25.636 --> 00:45:27.506 A:middle
场景理解有关的东西

00:45:27.506 --> 00:45:29.356 A:middle
像平面检测 灯光预估

00:45:29.356 --> 00:45:30.026 A:middle
以及其他信息

00:45:31.066 --> 00:45:32.796 A:middle
那这两种配置中

00:45:32.796 --> 00:45:34.376 A:middle
哪一种更适合我们使用呢

00:45:35.066 --> 00:45:35.536 A:middle
让我们看看

00:45:35.626 --> 00:45:37.106 A:middle
ARImageTrackingConfigurations

00:45:37.516 --> 00:45:39.916 A:middle
是专门为围绕

00:45:39.916 --> 00:45:41.226 A:middle
图像发生的 AR 体验

00:45:41.226 --> 00:45:42.906 A:middle
所量身定制的

00:45:43.196 --> 00:45:44.826 A:middle
在屏幕的左侧 我们可以看到相应的例子

00:45:46.606 --> 00:45:48.516 A:middle
图像可以是

00:45:48.516 --> 00:45:49.666 A:middle
课本的某一页

00:45:50.506 --> 00:45:51.576 A:middle
为了让这次体验更

00:45:51.576 --> 00:45:53.976 A:middle
吸引人 我们

00:45:54.286 --> 00:45:54.746 A:middle
动态地将图表重叠

00:45:54.746 --> 00:45:55.846 A:middle
在这个例子里就是

00:45:55.846 --> 00:45:56.796 A:middle
如何画一个等边三角形

00:45:57.866 --> 00:45:58.746 A:middle
你可以看到

00:45:58.746 --> 00:46:00.966 A:middle
这个体验真的是为图像量身定制的

00:45:58.746 --> 00:46:00.966 A:middle
这个体验真的是为图像量身定制的

00:46:01.116 --> 00:46:03.826 A:middle
让我们来看另一个例子

00:46:04.546 --> 00:46:05.516 A:middle
图像跟踪用于

00:46:05.606 --> 00:46:07.266 A:middle
触发一些超出

00:46:07.266 --> 00:46:09.496 A:middle
图像范围的内容

00:46:09.496 --> 00:46:12.086 A:middle
在这种情况下 你需要使用

00:46:12.086 --> 00:46:13.726 A:middle
ARWorldTrackingConfiguration

00:46:13.806 --> 00:46:14.906 A:middle
因为你需要设备的位置信息

00:46:14.906 --> 00:46:16.506 A:middle
来保持对图像外的

00:46:16.506 --> 00:46:19.856 A:middle
内容的跟踪

00:46:20.076 --> 00:46:21.146 A:middle
因为图像跟踪并不使用

00:46:21.246 --> 00:46:23.576 A:middle
运动数据

00:46:23.576 --> 00:46:24.676 A:middle
所以它适用于像

00:46:24.676 --> 00:46:26.506 A:middle
公交车或者电梯这类

00:46:27.076 --> 00:46:28.146 A:middle
运动数据与视觉数据

00:46:28.146 --> 00:46:29.896 A:middle
不匹配的场景下

00:46:30.916 --> 00:46:33.036 A:middle
下面要讲的是

00:46:33.036 --> 00:46:33.756 A:middle
如何在代码中实现这些功能

00:46:35.236 --> 00:46:36.916 A:middle
你可以轻易地分辨出

00:46:36.946 --> 00:46:37.716 A:middle
这里的三个步骤

00:46:37.936 --> 00:46:40.476 A:middle
第一个是收集所有图像

00:46:40.876 --> 00:46:41.796 A:middle
这里有个很方便的功能

00:46:41.796 --> 00:46:43.396 A:middle
ARReferenceImage 这个功能

00:46:43.566 --> 00:46:45.436 A:middle
可以收集在

00:46:45.436 --> 00:46:47.436 A:middle
特定群组中的

00:46:47.436 --> 00:46:48.366 A:middle
所有图像

00:46:48.556 --> 00:46:50.226 A:middle
这个群组被命名为 Room1

00:46:51.896 --> 00:46:53.256 A:middle
我们可以简单的赋予

00:46:53.256 --> 00:46:55.626 A:middle
ARImageTrackingConfigurations

00:46:55.626 --> 00:46:56.306 A:middle
中这些图像

00:46:56.306 --> 00:46:58.156 A:middle
trackingImages 的特征

00:46:58.586 --> 00:46:59.516 A:middle
然后运行这一会话

00:47:00.866 --> 00:47:02.766 A:middle
你就能开始接收

00:47:02.766 --> 00:47:04.306 A:middle
反馈 比如说在这个会话中

00:47:04.306 --> 00:47:06.346 A:middle
didUpdate 这个锚点指定了方式

00:47:06.466 --> 00:47:08.266 A:middle
你可以检查这个

00:47:08.266 --> 00:47:10.096 A:middle
锚点是否是属于

00:47:10.146 --> 00:47:11.156 A:middle
ARImageAnchor

00:47:12.606 --> 00:47:14.186 A:middle
在这个锚点中

00:47:14.246 --> 00:47:15.386 A:middle
你可以找到图像的

00:47:15.386 --> 00:47:17.336 A:middle
位置与方向以及

00:47:17.336 --> 00:47:18.656 A:middle
参考图像本身

00:47:18.716 --> 00:47:19.856 A:middle
你可能会问

00:47:19.856 --> 00:47:21.836 A:middle
那要怎样才能找到以

00:47:21.836 --> 00:47:23.156 A:middle
你式命名的图像名称

00:47:23.156 --> 00:47:24.126 A:middle
然后确认哪些图像被

00:47:24.126 --> 00:47:24.746 A:middle
检测到了呢

00:47:25.986 --> 00:47:26.966 A:middle
这里有一个叫 Boolean 的特性

00:47:26.966 --> 00:47:28.936 A:middle
它能告诉你这个

00:47:28.936 --> 00:47:30.086 A:middle
图像当前是否

00:47:30.086 --> 00:47:33.666 A:middle
正在被跟踪

00:47:33.866 --> 00:47:35.306 A:middle
除了我们现在

00:47:35.306 --> 00:47:36.496 A:middle
所了解到的使用例子外

00:47:36.496 --> 00:47:38.576 A:middle
当你围绕着

00:47:38.576 --> 00:47:40.546 A:middle
图像进行建设

00:47:40.546 --> 00:47:43.196 A:middle
图像检测以及跟踪还有

00:47:43.976 --> 00:47:44.126 A:middle.
其他的功能

00:47:45.226 --> 00:47:47.496 A:middle
就好像 如果两个设备都对着

00:47:47.496 --> 00:47:48.866 A:middle
同一张真实图像

00:47:48.866 --> 00:47:51.446 A:middle
你可以在两个设备中

00:47:51.446 --> 00:47:52.416 A:middle
同时检测到这个图像

00:47:52.846 --> 00:47:54.586 A:middle
这可以为你提供一个

00:47:54.586 --> 00:47:56.066 A:middle
分享坐标体系

00:47:56.066 --> 00:47:57.626 A:middle
这个体系可以为分享体验

00:47:58.106 --> 00:47:59.386 A:middle
提供另一种选择

00:48:01.476 --> 00:48:03.846 A:middle
另一个例子是 如果你刚好知道

00:48:03.846 --> 00:48:05.226 A:middle
某张图像在世界上的

00:48:05.346 --> 00:48:06.916 A:middle
真实位置

00:48:08.296 --> 00:48:09.826 A:middle
比如说 你知道这幅公园地图

00:48:09.826 --> 00:48:12.076 A:middle
在现实生活中的位置

00:48:12.456 --> 00:48:14.446 A:middle
你可以通过图像跟踪来

00:48:14.446 --> 00:48:15.956 A:middle
获得拍摄图像

00:48:15.956 --> 00:48:17.576 A:middle
设备的位置

00:48:17.576 --> 00:48:19.636 A:middle
还能获得这个设备

00:48:19.636 --> 00:48:20.656 A:middle
在这个世界上的位置

00:48:20.656 --> 00:48:21.946 A:middle
你可以利用这些数据

00:48:21.946 --> 00:48:23.956 A:middle
来找到真实世界

00:48:24.196 --> 00:48:27.086 A:middle
中真正的方向

00:48:27.646 --> 00:48:31.376 A:middle
有关图像跟踪的讨论

00:48:31.436 --> 00:48:31.836 A:middle
就到此为止

00:48:31.836 --> 00:48:34.006 A:middle
接下来我们要讨论的是

00:48:34.066 --> 00:48:35.266 A:middle
物体检测

00:48:38.016 --> 00:48:39.546 A:middle
通过图像跟踪 我们了解了

00:48:39.546 --> 00:48:41.716 A:middle
我们是如何检测图像的

00:48:41.716 --> 00:48:43.676 A:middle
而图像在真实生活中

00:48:43.676 --> 00:48:44.406 A:middle
是一个平面物体

00:48:45.376 --> 00:48:46.786 A:middle
物品检测将这个概念

00:48:46.786 --> 00:48:48.256 A:middle
扩展到三维世界

00:48:48.346 --> 00:48:50.586 A:middle
让检测常用物品成为可能

00:48:51.046 --> 00:48:53.626 A:middle
但是这个物品

00:48:53.816 --> 00:48:55.826 A:middle
在场景中需要是静止的

00:48:55.826 --> 00:48:57.086 A:middle
它与图像不同

00:48:57.086 --> 00:48:57.676 A:middle
检测图像时是可以移动的

00:48:58.866 --> 00:49:00.046 A:middle
我们可以看看这个例子

00:48:58.866 --> 00:49:00.046 A:middle
我们可以看看这个例子

00:49:00.046 --> 00:49:02.566 A:middle
这是娜芙蒂蒂胸像

00:49:02.716 --> 00:49:04.266 A:middle
这是一个可以在

00:49:04.266 --> 00:49:05.326 A:middle
博物馆中展览的雕塑

00:49:05.636 --> 00:49:07.576 A:middle
现在 你可以用 ARKit 来检测它

00:49:08.216 --> 00:49:10.376 A:middle
然后可以在真实物品的

00:49:10.376 --> 00:49:14.476 A:middle
上方显示一些信息

00:49:15.466 --> 00:49:17.536 A:middle
ARKit 中的物品检测

00:49:17.736 --> 00:49:18.936 A:middle
指的是有关物品的

00:49:18.936 --> 00:49:21.116 A:middle
真实实例的检测

00:49:21.646 --> 00:49:22.506 A:middle
所以我们正在讨论的

00:49:22.506 --> 00:49:23.986 A:middle
不是检测所有的雕像

00:49:24.346 --> 00:49:26.406 A:middle
而是检测这一特定的

00:49:26.406 --> 00:49:27.456 A:middle
娜芙蒂蒂雕像

00:49:28.836 --> 00:49:29.986 A:middle
我们如何在 ARKit

00:49:29.986 --> 00:49:31.006 A:middle
中重现这些物体呢

00:49:31.626 --> 00:49:33.736 A:middle
首先你需要扫描这个物体

00:49:33.806 --> 00:49:35.086 A:middle
事实上 只需要两步就可以

00:49:35.086 --> 00:49:35.196 A:middle
完成这个检测

00:49:35.316 --> 00:49:36.936 A:middle
首先 你对这个对象进行扫描

00:49:36.936 --> 00:49:38.336 A:middle
然后你就可以检测它

00:49:39.096 --> 00:49:40.546 A:middle
让我们讨论一下扫描这一部分

00:49:40.546 --> 00:49:42.446 A:middle
对开发者来说

00:49:42.446 --> 00:49:44.166 A:middle
扫描这一部分主要是你们的责任

00:49:44.886 --> 00:49:46.016 A:middle
你们需要负责

00:49:46.016 --> 00:49:47.296 A:middle
创造可用于

00:49:47.406 --> 00:49:49.176 A:middle
检测的物体重现

00:49:51.276 --> 00:49:53.716 A:middle
从内在来看 这个物体

00:49:53.716 --> 00:49:55.616 A:middle
的重现方式与

00:49:55.616 --> 00:49:56.196 A:middle
世界地图类似

00:49:56.776 --> 00:49:58.576 A:middle
在左边 你可以看到

00:49:58.916 --> 00:50:00.456 A:middle
娜芙蒂蒂胸像上的

00:49:58.916 --> 00:50:00.456 A:middle
娜芙蒂蒂胸像上的

00:50:00.456 --> 00:50:01.886 A:middle
3D 特征点

00:50:03.056 --> 00:50:04.876 A:middle
你可以使用扫描与检测 3D 物体

00:50:05.006 --> 00:50:06.516 A:middle
的开发者样本来

00:50:06.516 --> 00:50:08.496 A:middle
扫描物体

00:50:08.496 --> 00:50:10.416 A:middle
这个样本可在网站上获得

00:50:11.706 --> 00:50:13.136 A:middle
需要注意的是

00:50:13.136 --> 00:50:14.766 A:middle
你在运行时的

00:50:14.766 --> 00:50:17.306 A:middle
检测质量很大程度

00:50:17.306 --> 00:50:18.986 A:middle
上受扫描质量的影响

00:50:19.776 --> 00:50:21.826 A:middle
所以 不妨花上一些时间来

00:50:21.826 --> 00:50:23.266 A:middle
研究在扫描过程中

00:50:23.336 --> 00:50:27.386 A:middle
如何才能获得最好的质量

00:50:27.546 --> 00:50:29.066 A:middle
一旦你建立并运行了

00:50:29.066 --> 00:50:30.666 A:middle
这个开发者样本

00:50:30.786 --> 00:50:32.406 A:middle
你会在你的设备上 看到与这个类似的东西

00:50:33.286 --> 00:50:35.976 A:middle
第一步是要找到你

00:50:35.976 --> 00:50:37.926 A:middle
物体周围的空间

00:50:39.036 --> 00:50:40.066 A:middle
App 会试图

00:50:40.066 --> 00:50:41.556 A:middle
自动估算这个

00:50:41.586 --> 00:50:42.946 A:middle
密封盒子的大小

00:50:42.946 --> 00:50:43.946 A:middle
探索不同的特征点

00:50:44.896 --> 00:50:46.206 A:middle
但你可以随时通过

00:50:46.256 --> 00:50:48.856 A:middle
拖拽这个盒子的边框来

00:50:49.066 --> 00:50:50.726 A:middle
调整它 使它变大或缩小

00:50:52.876 --> 00:50:55.766 A:middle
有一点需要特别留意

00:50:55.766 --> 00:50:57.266 A:middle
当你围着这个物体扫描时

00:50:57.266 --> 00:50:58.796 A:middle
不能漏掉这个物体

00:50:58.796 --> 00:51:01.166 A:middle
的任何特征点

00:50:58.796 --> 00:51:01.166 A:middle
的任何特征点

00:51:01.856 --> 00:51:03.606 A:middle
你也可以从上方用两指手势

00:51:03.606 --> 00:51:05.146 A:middle
对盒子进行旋转操作

00:51:05.856 --> 00:51:08.406 A:middle
你需要确保这个盒子

00:51:08.606 --> 00:51:09.666 A:middle
围绕着物体

00:51:09.696 --> 00:51:11.286 A:middle
而且包括了这一物体的

00:51:11.916 --> 00:51:11.986 A:middle
所有特征点

00:51:13.016 --> 00:51:15.246 A:middle
接下来就是真正的扫描部分

00:51:16.376 --> 00:51:19.356 A:middle
在这个阶段

00:51:19.356 --> 00:51:21.426 A:middle
我们希望能真正的从各个

00:51:21.736 --> 00:51:23.486 A:middle
角度来观察这个对象

00:51:23.486 --> 00:51:24.846 A:middle
任何你觉得你的用户可能会想要

00:51:24.846 --> 00:51:25.806 A:middle
检测的角度都需要被考虑到

00:51:27.056 --> 00:51:28.386 A:middle
为了让你能更容易的

00:51:28.386 --> 00:51:30.096 A:middle
统计物体哪部分

00:51:30.096 --> 00:51:31.656 A:middle
已经被获取了

00:51:31.656 --> 00:51:33.376 A:middle
就像对这个美丽的

00:51:33.376 --> 00:51:34.356 A:middle
地板的重现

00:51:34.726 --> 00:51:36.046 A:middle
你可以看到在顶部

00:51:36.456 --> 00:51:37.676 A:middle
有一个百分比

00:51:37.676 --> 00:51:38.786 A:middle
它能告诉你有多少块地板

00:51:38.786 --> 00:51:39.366 A:middle
已经被获取了

00:51:40.406 --> 00:51:41.786 A:middle
在这个阶段中

00:51:41.786 --> 00:51:43.716 A:middle
你需要在物体有着

00:51:43.716 --> 00:51:45.126 A:middle
许多特征点的 或者

00:51:45.126 --> 00:51:46.546 A:middle
有明显区别性的区域花上足够的时间

00:51:46.726 --> 00:51:47.726 A:middle
这是十分重要的

00:51:47.726 --> 00:51:49.446 A:middle
你需要走近对象去

00:51:49.446 --> 00:51:50.346 A:middle
捕获所有细节

00:51:50.686 --> 00:51:52.046 A:middle
你还需要从所有

00:51:52.046 --> 00:51:56.246 A:middle
角度对对象进行扫描

00:51:56.436 --> 00:51:59.456 A:middle
就好像你在这里看到的

00:51:59.686 --> 00:52:01.186 A:middle
如果你对你已经获取的

00:51:59.686 --> 00:52:01.186 A:middle
如果你对你已经获取的

00:52:01.186 --> 00:52:02.996 A:middle
信息感到满意了

00:52:02.996 --> 00:52:04.476 A:middle
你可以进入下一步

00:52:04.476 --> 00:52:06.656 A:middle
你可以通过简单的拖拽

00:52:06.656 --> 00:52:09.166 A:middle
在颜色系统中

00:52:09.166 --> 00:52:10.026 A:middle
调整原点

00:52:10.536 --> 00:52:12.496 A:middle
这个系统将会

00:52:12.496 --> 00:52:13.886 A:middle
是锚点在检测阶段时

00:52:13.886 --> 00:52:16.506 A:middle
反馈给你的系统

00:52:16.506 --> 00:52:17.766 A:middle
所以你要确保你将它

00:52:17.766 --> 00:52:19.316 A:middle
放置在一个适合你的

00:52:19.316 --> 00:52:20.606 A:middle
虚拟内容的位置上

00:52:20.676 --> 00:52:25.526 A:middle
现在 你已经有了

00:52:25.676 --> 00:52:27.286 A:middle
有关物体的完整重现

00:52:27.286 --> 00:52:30.686 A:middle
这个重现可以用来检测

00:52:30.686 --> 00:52:33.106 A:middle
现在 App 会转换到

00:52:33.106 --> 00:52:34.976 A:middle
检测模式

00:52:36.106 --> 00:52:37.386 A:middle
我们强力建议你使用这一模式

00:52:37.386 --> 00:52:39.966 A:middle
以尽早获得有关

00:52:39.966 --> 00:52:41.046 A:middle
检测质量的反馈

00:52:41.866 --> 00:52:44.776 A:middle
你可能会想要

00:52:44.776 --> 00:52:46.126 A:middle
从不同的角度来观察这个物体

00:52:46.126 --> 00:52:47.796 A:middle
以确认这个物体

00:52:47.796 --> 00:52:49.776 A:middle
已经从所有不同角度

00:52:50.336 --> 00:52:51.696 A:middle
检测过了

00:52:51.696 --> 00:52:53.816 A:middle
你可以将你的设备移开

00:52:53.816 --> 00:52:55.086 A:middle
然后从另一个角度拿回来

00:52:55.836 --> 00:52:58.386 A:middle
你需要确保扫描质量好到可以

00:52:58.386 --> 00:53:00.006 A:middle
用于检测物体

00:52:58.386 --> 00:53:00.006 A:middle
用于检测物体

00:53:00.526 --> 00:53:03.106 A:middle
你也可以移动这些物体

00:53:03.276 --> 00:53:05.866 A:middle
这样 光照条件就会不同

00:53:06.836 --> 00:53:08.216 A:middle
你需要确保在这些情况下

00:53:08.216 --> 00:53:09.196 A:middle
物体还能被检测到

00:53:09.196 --> 00:53:10.316 A:middle
对像玩具这样的物体而言

00:53:10.366 --> 00:53:12.536 A:middle
这是十分重要的

00:53:12.536 --> 00:53:13.646 A:middle
因为你并不知道

00:53:13.646 --> 00:53:15.546 A:middle
它们的真实位置

00:53:17.096 --> 00:53:18.776 A:middle
我们也建议你将物体

00:53:18.776 --> 00:53:21.556 A:middle
放在一个完全不同的环境中

00:53:22.016 --> 00:53:24.576 A:middle
但还能确保它能被检测到

00:53:25.666 --> 00:53:27.756 A:middle
如果这个物体不能被检测到

00:53:27.756 --> 00:53:28.806 A:middle
那你可能需要回到

00:53:28.806 --> 00:53:31.406 A:middle
扫描环节重新扫描

00:53:31.406 --> 00:53:32.506 A:middle
并确保你所处的环境光线充足

00:53:33.786 --> 00:53:35.956 A:middle
光线充足的环境

00:53:35.956 --> 00:53:37.916 A:middle
在扫描时是至关重要的

00:53:38.506 --> 00:53:39.776 A:middle
如果你使用的是 Verilux 灯泡

00:53:39.776 --> 00:53:41.916 A:middle
那 500 lux 是最佳的亮度

00:53:43.046 --> 00:53:45.006 A:middle
如果这个亮度也不够

00:53:45.166 --> 00:53:46.556 A:middle
你可能需要保存不同的

00:53:46.556 --> 00:53:50.446 A:middle
扫描版本

00:53:50.576 --> 00:53:51.826 A:middle
当你对检测质量

00:53:51.826 --> 00:53:53.266 A:middle
感到满意时

00:53:53.266 --> 00:53:55.136 A:middle
你只需要将这个模型

00:53:55.136 --> 00:53:57.846 A:middle
存入你的 Mac 里

00:53:57.846 --> 00:53:59.656 A:middle
并将它添加到 AR 资源群组中

00:53:59.746 --> 00:54:03.016 A:middle
就像你对图像所做的一样

00:53:59.746 --> 00:54:03.016 A:middle
就像你对图像所做的一样

00:54:03.016 --> 00:54:04.526 A:middle
有一些物体

00:54:04.526 --> 00:54:07.366 A:middle
十分适合这个系统

00:54:07.586 --> 00:54:08.706 A:middle
这些物体就像

00:54:08.706 --> 00:54:08.986 A:middle
屏幕左侧展示的一样

00:54:09.586 --> 00:54:10.956 A:middle
首先 它们都是刚性物体

00:54:10.956 --> 00:54:13.206 A:middle
纹理丰富

00:54:13.206 --> 00:54:14.696 A:middle
区分明显

00:54:15.436 --> 00:54:16.416 A:middle
但也有着一些

00:54:16.416 --> 00:54:18.506 A:middle
物体不适用于这一系统

00:54:19.066 --> 00:54:22.136 A:middle
可以参考屏幕右侧的物体

00:54:22.686 --> 00:54:24.756 A:middle
像金属的

00:54:24.826 --> 00:54:26.606 A:middle
透明的 或者金属的

00:54:26.606 --> 00:54:27.796 A:middle
反光的物体不适合用于

00:54:27.856 --> 00:54:28.096 A:middle
这个系统

00:54:29.206 --> 00:54:31.496 A:middle
透明的物体

00:54:31.576 --> 00:54:32.796 A:middle
比如说玻璃材质的物体

00:54:32.796 --> 00:54:34.376 A:middle
也不适用于这个系统

00:54:34.376 --> 00:54:35.236 A:middle
因为这些物体的外观

00:54:35.236 --> 00:54:37.766 A:middle
受它们所处的场景影响

00:54:38.816 --> 00:54:38.936 A:middle
太大

00:54:39.756 --> 00:54:41.506 A:middle
这就是如何扫描物体

00:54:41.706 --> 00:54:43.126 A:middle
让我再强调一次

00:54:43.126 --> 00:54:44.026 A:middle
确保你的环境光线充足

00:54:44.996 --> 00:54:46.596 A:middle
接下来要了解的是

00:54:46.646 --> 00:54:48.056 A:middle
我们如何在 ARKit 中检测到这一操作

00:54:50.046 --> 00:54:51.956 A:middle
如果你觉得这个很眼熟

00:54:51.956 --> 00:54:53.606 A:middle
那时因为这个 API

00:54:53.606 --> 00:54:55.006 A:middle
与其中之一的图像十分相似

00:54:55.586 --> 00:54:56.716 A:middle
我们会有很方便的特征

00:54:56.816 --> 00:54:58.586 A:middle
来将所有对象集中到

00:54:58.586 --> 00:54:58.896 A:middle
一个群组中

00:54:59.506 --> 00:55:00.656 A:middle
在这里 指的是

00:54:59.506 --> 00:55:00.656 A:middle
在这里 指的是

00:55:00.656 --> 00:55:01.916 A:middle
ARReferenceObjects 这一类

00:55:02.806 --> 00:55:05.136 A:middle
当你配置你的

00:55:05.136 --> 00:55:07.056 A:middle
ARWorldTracking 参数时

00:55:07.056 --> 00:55:08.516 A:middle
你只需要将这个对象

00:55:08.516 --> 00:55:10.876 A:middle
传输到 the detectionObjects 的特征下

00:55:13.206 --> 00:55:15.566 A:middle
一旦你开始运行这个会话

00:55:15.566 --> 00:55:17.236 A:middle
你会收到结果反馈

00:55:18.306 --> 00:55:19.356 A:middle
在这里 你会需要

00:55:19.356 --> 00:55:21.656 A:middle
检查 ARObjectAnchor

00:55:22.386 --> 00:55:23.656 A:middle
它能为你提供现实生活中

00:55:23.786 --> 00:55:25.436 A:middle
物体的位置

00:55:25.436 --> 00:55:27.866 A:middle
以及方向

00:55:28.716 --> 00:55:30.346 A:middle
物体的名字也会

00:55:30.346 --> 00:55:35.246 A:middle
在资产目录中得到展示

00:55:35.246 --> 00:55:36.516 A:middle
你可能已经注意到

00:55:36.706 --> 00:55:38.316 A:middle
物体检测与

00:55:38.316 --> 00:55:40.506 A:middle
世界匹配重新定位之间

00:55:40.506 --> 00:55:42.726 A:middle
有着某些类似之处

00:55:43.426 --> 00:55:44.806 A:middle
但它们也有着一些不同

00:55:44.856 --> 00:55:46.316 A:middle
在物体检测时

00:55:46.316 --> 00:55:48.406 A:middle
我们是根据

00:55:48.746 --> 00:55:50.946 A:middle
现实世界 赋予物体位置

00:55:51.506 --> 00:55:52.426 A:middle
而在世界地图的重新定位时

00:55:52.426 --> 00:55:53.966 A:middle
是相机本身

00:55:53.966 --> 00:55:56.066 A:middle
调整了之前的

00:55:56.066 --> 00:55:56.816 A:middle
世界地图

00:55:58.286 --> 00:56:01.336 A:middle
而且 你还可以检测多个物体

00:55:58.286 --> 00:56:01.336 A:middle
而且 你还可以检测多个物体

00:56:01.966 --> 00:56:03.626 A:middle
物体检测十分适合

00:56:03.736 --> 00:56:05.306 A:middle
那些在放置在桌面上

00:56:05.496 --> 00:56:06.376 A:middle
家具大小的物体

00:56:07.146 --> 00:56:08.446 A:middle
而世界地图

00:56:08.446 --> 00:56:10.266 A:middle
获取的则是整个场景

00:56:10.776 --> 00:56:14.576 A:middle
以上便是有关物品检测的信息

00:56:14.686 --> 00:56:16.266 A:middle
让我们来总结一下我们

00:56:17.796 --> 00:56:19.336 A:middle
今天所讲过的东西

00:56:19.516 --> 00:56:21.876 A:middle
方向跟踪只能跟踪

00:56:21.876 --> 00:56:23.696 A:middle
设备的旋转

00:56:23.696 --> 00:56:25.756 A:middle
可以用于探索

00:56:25.756 --> 00:56:26.416 A:middle
静止的环境

00:56:27.946 --> 00:56:29.266 A:middle
世界跟踪是

00:56:29.266 --> 00:56:30.776 A:middle
全特征的位置和

00:56:30.776 --> 00:56:32.246 A:middle
方向跟踪

00:56:32.246 --> 00:56:33.926 A:middle
它能根据世界原点为你

00:56:33.926 --> 00:56:36.286 A:middle
提供设备的位置

00:56:37.016 --> 00:56:38.776 A:middle
并启用有关理解场景的所有

00:56:38.776 --> 00:56:40.486 A:middle
相关的功能

00:56:41.166 --> 00:56:43.366 A:middle
比如平面检测

00:56:44.476 --> 00:56:46.256 A:middle
平面检测让你可以与真实的

00:56:46.256 --> 00:56:50.226 A:middle
水平或垂直的平面互动

00:56:50.226 --> 00:56:51.636 A:middle
你可以在这些平面上放置虚拟物体

00:56:51.996 --> 00:56:55.306 A:middle
我们也了解了你可以通过

00:56:55.656 --> 00:56:57.036 A:middle
框架中的保存和上传

00:56:57.036 --> 00:56:58.926 A:middle
地图特征的功能创造

00:56:58.926 --> 00:57:00.566 A:middle
一个持续的或者多用户的体验

00:56:58.926 --> 00:57:00.566 A:middle
一个持续的或者多用户的体验

00:57:01.516 --> 00:57:03.036 A:middle
还有你如何才能用图像跟踪来

00:57:03.036 --> 00:57:04.816 A:middle
检测物理图像并以

00:57:04.816 --> 00:57:06.096 A:middle
每秒 60 帧的速度来跟踪它们

00:57:06.096 --> 00:57:08.596 A:middle
以及你是如何通过物体跟踪

00:57:08.596 --> 00:57:11.176 A:middle
来检测更多常用物体

00:57:12.576 --> 00:57:15.216 A:middle
我希望通过这个演讲

00:57:15.216 --> 00:57:17.496 A:middle
你们现在对 ARkit 中的所有

00:57:17.496 --> 00:57:19.126 A:middle
不同的跟踪技术以及

00:57:19.586 --> 00:57:20.816 A:middle
它们是如何运作的

00:57:20.816 --> 00:57:22.016 A:middle
有更清晰的理解

00:57:23.006 --> 00:57:24.666 A:middle
以及更了解你如何才能

00:57:24.806 --> 00:57:25.976 A:middle
获得最佳的跟踪质量

00:57:26.436 --> 00:57:28.106 A:middle
我们非常期待

00:57:28.106 --> 00:57:29.226 A:middle
你即将利用

00:57:29.226 --> 00:57:29.656 A:middle
ARKit 所创造出来的作品

00:57:30.976 --> 00:57:32.416 A:middle
更多的信息可以在

00:57:32.486 --> 00:57:33.636 A:middle
开发者网站上的

00:57:33.636 --> 00:57:34.496 A:middle
演讲链接中获得

00:57:34.496 --> 00:57:36.226 A:middle
明天早上 9 点

00:57:36.536 --> 00:57:37.586 A:middle
将会有 ARKit 实验室的活动

00:57:38.356 --> 00:57:39.806 A:middle
我和 Marion 都会

00:57:39.806 --> 00:57:41.786 A:middle
在现场回答有关

00:57:41.786 --> 00:57:42.666 A:middle
ARKit 的任何问题

00:57:43.756 --> 00:57:44.976 A:middle
最后 十分感谢大家

00:57:44.976 --> 00:57:46.976 A:middle
希望大家能享受这次知识冲击

00:57:47.516 --> 00:57:53.506 A:middle
[ 掌声 ]
