WEBVTT

00:00:06.516 --> 00:00:16.500 A:middle
[ Crowd Sounds ]

00:00:23.516 --> 00:00:27.046 A:middle
[ Applause ]

00:00:27.546 --> 00:00:30.546 A:middle
&gt;&gt; Wow. It's very exciting to be

00:00:30.546 --> 00:00:30.676 A:middle
here.

00:00:31.606 --> 00:00:32.786 A:middle
I'm Gaurav.

00:00:32.786 --> 00:00:35.046 A:middle
And today we are going to

00:00:35.776 --> 00:00:37.466 A:middle
discuss machine learning.

00:00:37.466 --> 00:00:38.686 A:middle
Machine learning is a very

00:00:38.686 --> 00:00:39.866 A:middle
powerful technology.

00:00:40.516 --> 00:00:42.066 A:middle
And together with the power of

00:00:42.066 --> 00:00:44.656 A:middle
our devices, you can create some

00:00:44.656 --> 00:00:46.556 A:middle
really amazing experiences.

00:00:47.896 --> 00:00:49.946 A:middle
For example, you can do things

00:00:49.946 --> 00:00:51.746 A:middle
like real-time image recognition

00:00:52.346 --> 00:00:53.846 A:middle
and content creation.

00:00:54.926 --> 00:00:56.366 A:middle
In the next 40 minutes, we are

00:00:56.366 --> 00:00:57.576 A:middle
going to tell you all about

00:00:57.576 --> 00:01:00.126 A:middle
those experiences and how Apple

00:01:00.126 --> 00:01:01.866 A:middle
is making it easy for you to

00:01:02.076 --> 00:01:03.846 A:middle
integrate machine learning in

00:01:03.846 --> 00:01:04.966 A:middle
your app.

00:01:06.156 --> 00:01:07.916 A:middle
In particular, we will be

00:01:07.916 --> 00:01:10.366 A:middle
focusing on Core ML, our machine

00:01:10.366 --> 00:01:11.066 A:middle
learning framework.

00:01:14.946 --> 00:01:17.026 A:middle
At Apple, we are using machine

00:01:17.026 --> 00:01:18.786 A:middle
learning extensively.

00:01:19.796 --> 00:01:22.586 A:middle
In our photos app, we use it for

00:01:22.586 --> 00:01:24.096 A:middle
people recognition, scene

00:01:24.096 --> 00:01:24.886 A:middle
recognition.

00:01:28.796 --> 00:01:31.106 A:middle
In our keyboard app, we use it

00:01:31.106 --> 00:01:32.496 A:middle
for the next word prediction,

00:01:33.006 --> 00:01:34.146 A:middle
smart responses.

00:01:35.236 --> 00:01:37.006 A:middle
We use it even in our watch to

00:01:37.006 --> 00:01:38.366 A:middle
do smart responses and

00:01:38.366 --> 00:01:39.736 A:middle
handwriting recognition.

00:01:40.926 --> 00:01:43.466 A:middle
And I'm sure you guys also want

00:01:43.466 --> 00:01:45.966 A:middle
to create similar experiences in

00:01:45.966 --> 00:01:47.306 A:middle
your app.

00:01:48.206 --> 00:01:51.346 A:middle
For example, some of you might

00:01:51.346 --> 00:01:52.646 A:middle
like to do real-time image

00:01:52.646 --> 00:01:53.446 A:middle
recognition.

00:01:53.996 --> 00:01:56.856 A:middle
In this way, you can tell your

00:01:56.856 --> 00:01:59.046 A:middle
users, provide them more

00:01:59.046 --> 00:02:00.636 A:middle
contextual information about

00:02:00.636 --> 00:02:02.246 A:middle
they are seeing, for example

00:02:02.246 --> 00:02:03.536 A:middle
whether they are eating a hotdog

00:02:03.536 --> 00:02:03.956 A:middle
or not.

00:02:04.516 --> 00:02:07.816 A:middle
[ Laughter ]

00:02:08.316 --> 00:02:11.656 A:middle
We want you to enable all of

00:02:11.656 --> 00:02:13.956 A:middle
these great facility in your

00:02:13.956 --> 00:02:14.236 A:middle
app.

00:02:15.076 --> 00:02:17.176 A:middle
And, hence, we are introducing

00:02:17.266 --> 00:02:18.996 A:middle
machine learning frameworks just

00:02:18.996 --> 00:02:20.476 A:middle
for you guys to enable all the

00:02:20.476 --> 00:02:21.116 A:middle
awesome things.

00:02:21.536 --> 00:02:22.586 A:middle
You can clap.

00:02:23.516 --> 00:02:27.916 A:middle
[ Applause ]

00:02:28.416 --> 00:02:30.276 A:middle
But before we clap, we should

00:02:30.276 --> 00:02:31.686 A:middle
understand why do we even need

00:02:31.686 --> 00:02:32.816 A:middle
machine learning in the first

00:02:32.816 --> 00:02:33.096 A:middle
place?

00:02:33.616 --> 00:02:38.826 A:middle
And when do you need machine

00:02:38.886 --> 00:02:39.036 A:middle
learning?

00:02:39.036 --> 00:02:42.486 A:middle
So let's just say I'm a florist.

00:02:42.986 --> 00:02:44.926 A:middle
And all I want to show is all

00:02:44.926 --> 00:02:46.416 A:middle
the images of roses in user's

00:02:46.416 --> 00:02:47.886 A:middle
photo library.

00:02:48.596 --> 00:02:50.386 A:middle
This seems like a simple task,

00:02:50.386 --> 00:02:51.876 A:middle
so I can do some programming.

00:02:53.196 --> 00:02:56.006 A:middle
Perhaps, I'll start with the

00:02:56.006 --> 00:02:56.756 A:middle
color.

00:02:57.346 --> 00:02:59.406 A:middle
I say if the dominant color in

00:02:59.406 --> 00:03:00.766 A:middle
the picture is red maybe it's a

00:03:00.766 --> 00:03:01.026 A:middle
rose.

00:03:02.576 --> 00:03:03.966 A:middle
The problem is there are many

00:03:03.966 --> 00:03:05.386 A:middle
rose that are white in nature,

00:03:05.386 --> 00:03:07.876 A:middle
white in color, or yellow in

00:03:09.076 --> 00:03:09.216 A:middle
color.

00:03:09.456 --> 00:03:11.916 A:middle
So I will actually go forward

00:03:11.916 --> 00:03:13.996 A:middle
and start describing the shape.

00:03:13.996 --> 00:03:17.716 A:middle
And soon I will realize that

00:03:17.716 --> 00:03:20.016 A:middle
it's very difficult to write

00:03:20.356 --> 00:03:23.866 A:middle
even such a simple program

00:03:25.656 --> 00:03:27.016 A:middle
programmatically.

00:03:27.016 --> 00:03:28.276 A:middle
Hence, [inaudible] to machine

00:03:28.276 --> 00:03:29.216 A:middle
learning for our help.

00:03:30.546 --> 00:03:32.116 A:middle
Rather than describing how a

00:03:32.116 --> 00:03:33.336 A:middle
rose looks like

00:03:33.376 --> 00:03:34.966 A:middle
programmatically, we will

00:03:34.966 --> 00:03:37.816 A:middle
describe a rose empirically.

00:03:38.176 --> 00:03:39.026 A:middle
We will learn that

00:03:39.026 --> 00:03:40.466 A:middle
representation of rose

00:03:40.726 --> 00:03:41.386 A:middle
empirically.

00:03:42.036 --> 00:03:43.896 A:middle
Machine learning actually has

00:03:43.896 --> 00:03:44.416 A:middle
two steps.

00:03:45.146 --> 00:03:46.406 A:middle
One, training.

00:03:48.116 --> 00:03:50.096 A:middle
So what you will do in this

00:03:50.096 --> 00:03:51.376 A:middle
case, off-line you will collect

00:03:51.376 --> 00:03:53.056 A:middle
images of roses, lilies,

00:03:53.666 --> 00:03:56.086 A:middle
sunflowers and label them.

00:03:56.086 --> 00:03:59.036 A:middle
You will run it past through a

00:03:59.096 --> 00:04:01.156 A:middle
learning algorithm.

00:04:01.156 --> 00:04:05.506 A:middle
And you will get what we call as

00:04:05.546 --> 00:04:06.036 A:middle
a model.

00:04:07.416 --> 00:04:10.396 A:middle
This model is an empirical

00:04:10.396 --> 00:04:12.616 A:middle
representation of how a rose

00:04:13.286 --> 00:04:14.896 A:middle
look like.

00:04:15.116 --> 00:04:17.056 A:middle
Almost all of this stuff happens

00:04:17.156 --> 00:04:17.656 A:middle
off-line.

00:04:18.446 --> 00:04:21.196 A:middle
And there is a big [inaudible]

00:04:21.196 --> 00:04:23.426 A:middle
on the Internet around training.

00:04:23.906 --> 00:04:25.346 A:middle
Training is a complex subject.

00:04:27.236 --> 00:04:28.846 A:middle
Now once you have the model, you

00:04:28.846 --> 00:04:30.026 A:middle
want to use this model.

00:04:30.026 --> 00:04:33.976 A:middle
So what you will do, you will

00:04:33.976 --> 00:04:36.236 A:middle
take a picture of a rose, embed

00:04:36.286 --> 00:04:38.056 A:middle
this model in your app, you run

00:04:38.136 --> 00:04:41.706 A:middle
it past through your model, and

00:04:42.376 --> 00:04:44.546 A:middle
you will get the label and the

00:04:44.546 --> 00:04:44.996 A:middle
confidence.

00:04:45.806 --> 00:04:48.436 A:middle
This step is known as inference.

00:04:49.726 --> 00:04:52.036 A:middle
Training is very complex, but

00:04:52.336 --> 00:04:53.886 A:middle
inference is also getting very,

00:04:53.886 --> 00:04:55.636 A:middle
very challenging nowadays.

00:04:58.686 --> 00:05:00.526 A:middle
For example, if you want to

00:05:00.526 --> 00:05:02.516 A:middle
describe a new, deep neural

00:05:02.516 --> 00:05:05.336 A:middle
network programmatically in your

00:05:05.336 --> 00:05:06.336 A:middle
code, you have to write

00:05:06.336 --> 00:05:07.726 A:middle
thousands of lines of code just

00:05:07.726 --> 00:05:08.686 A:middle
to describe the network.

00:05:09.336 --> 00:05:12.146 A:middle
It can be [inaudible] tedious

00:05:12.146 --> 00:05:15.866 A:middle
and perhaps you can face your

00:05:15.866 --> 00:05:17.946 A:middle
biggest challenge in proving the

00:05:17.946 --> 00:05:21.436 A:middle
correctness of such a program.

00:05:21.436 --> 00:05:23.826 A:middle
Once you make sure that the

00:05:23.826 --> 00:05:26.636 A:middle
program is correct, you have to

00:05:26.636 --> 00:05:28.656 A:middle
make sure that you're getting

00:05:28.656 --> 00:05:30.176 A:middle
the best performance possible.

00:05:31.996 --> 00:05:33.886 A:middle
Finally, you also have to make

00:05:33.886 --> 00:05:35.496 A:middle
sure that there is energy

00:05:35.496 --> 00:05:36.686 A:middle
efficiency involved here.

00:05:37.216 --> 00:05:40.116 A:middle
These tasks can be very, very

00:05:40.116 --> 00:05:43.216 A:middle
challenging and can bog you down

00:05:43.596 --> 00:05:44.246 A:middle
completely.

00:05:44.816 --> 00:05:48.046 A:middle
At Apple, we are using on-device

00:05:48.046 --> 00:05:49.306 A:middle
machine learning for several

00:05:49.306 --> 00:05:49.996 A:middle
years now.

00:05:50.536 --> 00:05:53.536 A:middle
And we have solved these

00:05:53.536 --> 00:05:54.046 A:middle
challenges.

00:05:54.126 --> 00:05:55.386 A:middle
We have faced these challenges

00:05:56.016 --> 00:05:56.716 A:middle
and solved them.

00:05:57.286 --> 00:05:58.466 A:middle
And we really don't want you to

00:05:58.466 --> 00:05:59.096 A:middle
solve them.

00:05:59.096 --> 00:06:00.066 A:middle
We will solve them for you.

00:06:00.306 --> 00:06:01.746 A:middle
We want you to focus only on the

00:06:01.746 --> 00:06:02.596 A:middle
experience.

00:06:02.596 --> 00:06:03.166 A:middle
We will give you the

00:06:03.166 --> 00:06:04.476 A:middle
implementation, the best

00:06:04.476 --> 00:06:05.776 A:middle
implementation possible.

00:06:06.536 --> 00:06:09.136 A:middle
And, hence, we are introducing

00:06:09.536 --> 00:06:11.216 A:middle
machine learning frameworks for

00:06:11.216 --> 00:06:13.706 A:middle
you in which case we take care

00:06:13.706 --> 00:06:15.516 A:middle
of all the challenges, and you

00:06:15.586 --> 00:06:17.016 A:middle
take care of all the experience

00:06:17.296 --> 00:06:17.656 A:middle
in your app.

00:06:18.786 --> 00:06:21.426 A:middle
By doing so, we are going to

00:06:21.426 --> 00:06:22.686 A:middle
follow a layered approach.

00:06:23.376 --> 00:06:26.966 A:middle
At the top is your app.

00:06:29.256 --> 00:06:31.016 A:middle
We are going to introduce new

00:06:31.016 --> 00:06:32.746 A:middle
domain specific framework, in

00:06:32.746 --> 00:06:33.686 A:middle
particular, Vision.

00:06:34.876 --> 00:06:36.496 A:middle
The Vision framework will

00:06:36.496 --> 00:06:38.846 A:middle
actually allow you to do all the

00:06:38.846 --> 00:06:40.266 A:middle
tasks related to computer

00:06:40.266 --> 00:06:40.716 A:middle
vision.

00:06:41.506 --> 00:06:42.976 A:middle
The best part about computer

00:06:42.976 --> 00:06:44.626 A:middle
vision of Vision framework is

00:06:44.626 --> 00:06:46.406 A:middle
that you don't have to be a

00:06:46.406 --> 00:06:49.616 A:middle
configuration expert for using

00:06:49.616 --> 00:06:50.406 A:middle
Vision framework.

00:06:50.906 --> 00:06:53.816 A:middle
We are also enhancing our NLP

00:06:53.816 --> 00:06:56.966 A:middle
APIs to include more languages

00:06:57.466 --> 00:06:58.606 A:middle
and more functionality.

00:06:59.196 --> 00:07:02.486 A:middle
We are introducing a brand-new

00:07:02.486 --> 00:07:04.106 A:middle
framework called Core ML.

00:07:05.076 --> 00:07:06.676 A:middle
This framework has an exhaustive

00:07:06.756 --> 00:07:08.366 A:middle
support of machine learning

00:07:08.366 --> 00:07:10.286 A:middle
algorithms, more deep learning

00:07:10.586 --> 00:07:11.656 A:middle
as well as a standard.

00:07:12.276 --> 00:07:16.266 A:middle
And finally, all of these

00:07:16.266 --> 00:07:18.496 A:middle
frameworks are built on top of

00:07:18.496 --> 00:07:20.596 A:middle
Accelerate and Metal Performance

00:07:20.596 --> 00:07:20.826 A:middle
Shaders.

00:07:21.396 --> 00:07:28.086 A:middle
To give you a sneak peak, we

00:07:28.306 --> 00:07:29.506 A:middle
have Vision framework.

00:07:31.206 --> 00:07:33.526 A:middle
Vision is our one-stop shop to

00:07:33.526 --> 00:07:34.656 A:middle
do all things related to

00:07:34.656 --> 00:07:36.006 A:middle
computer vision and images.

00:07:37.056 --> 00:07:38.966 A:middle
You can use it to do things like

00:07:39.026 --> 00:07:43.316 A:middle
object tracking or more modern

00:07:43.316 --> 00:07:44.506 A:middle
deep learning-based face

00:07:44.506 --> 00:07:45.006 A:middle
detection.

00:07:45.006 --> 00:07:47.266 A:middle
There is a lot more to Vision

00:07:47.266 --> 00:07:47.736 A:middle
framework.

00:07:47.736 --> 00:07:50.306 A:middle
We are going to discuss Vision

00:07:50.576 --> 00:07:52.066 A:middle
in detail tomorrow afternoon.

00:07:52.626 --> 00:07:59.566 A:middle
NLP, this is our one-stop shop

00:07:59.566 --> 00:08:00.836 A:middle
to do the text processing.

00:08:01.366 --> 00:08:04.686 A:middle
You can use it to do things like

00:08:04.686 --> 00:08:05.916 A:middle
language identification.

00:08:07.126 --> 00:08:09.416 A:middle
And we are also releasing Named

00:08:09.416 --> 00:08:10.886 A:middle
Entity Recognition APIs.

00:08:11.956 --> 00:08:14.986 A:middle
These APIs can actually tell you

00:08:14.986 --> 00:08:17.116 A:middle
in your text if it is a name of

00:08:17.116 --> 00:08:18.916 A:middle
the location, people, and

00:08:18.916 --> 00:08:19.626 A:middle
organization.

00:08:20.146 --> 00:08:21.246 A:middle
We believe this will be very

00:08:21.246 --> 00:08:22.856 A:middle
powerful for many app developers

00:08:23.256 --> 00:08:24.276 A:middle
so please check this out

00:08:24.656 --> 00:08:26.086 A:middle
tomorrow more in the morning

00:08:26.156 --> 00:08:27.946 A:middle
session on NLP.

00:08:29.876 --> 00:08:33.606 A:middle
Core ML, until now, we were

00:08:33.606 --> 00:08:35.476 A:middle
discussing domain specific

00:08:35.566 --> 00:08:35.936 A:middle
framework.

00:08:36.396 --> 00:08:37.916 A:middle
Core ML is our machine learning

00:08:37.916 --> 00:08:39.806 A:middle
framework and is domain

00:08:39.806 --> 00:08:40.526 A:middle
agnostic.

00:08:41.756 --> 00:08:44.316 A:middle
It supports multiple kind of

00:08:44.316 --> 00:08:47.136 A:middle
inputs such as images, text,

00:08:48.256 --> 00:08:50.636 A:middle
dictionaries, raw numbers.

00:08:51.566 --> 00:08:53.156 A:middle
So you can do things like music

00:08:53.196 --> 00:08:53.566 A:middle
tagging.

00:08:55.876 --> 00:08:57.346 A:middle
There's another interesting part

00:08:57.346 --> 00:08:59.006 A:middle
of our Core ML that it has the

00:08:59.006 --> 00:09:01.116 A:middle
ability to deal with mixed

00:09:01.116 --> 00:09:02.136 A:middle
input/output type.

00:09:02.446 --> 00:09:04.396 A:middle
So you can take an image and you

00:09:04.396 --> 00:09:06.286 A:middle
can get a text out.

00:09:09.116 --> 00:09:10.916 A:middle
The best part about all these

00:09:11.026 --> 00:09:13.386 A:middle
frameworks is that they can work

00:09:13.386 --> 00:09:13.776 A:middle
together.

00:09:15.086 --> 00:09:17.316 A:middle
So you take a text, you pass it

00:09:17.316 --> 00:09:18.866 A:middle
through your NLP, you get the

00:09:18.866 --> 00:09:20.476 A:middle
output, and you pass it through

00:09:20.476 --> 00:09:22.356 A:middle
Core ML and do things like

00:09:22.436 --> 00:09:23.376 A:middle
sentiment analysis.

00:09:23.376 --> 00:09:25.336 A:middle
We are going to discuss it in

00:09:25.336 --> 00:09:27.666 A:middle
detail in our in-depth Core ML

00:09:27.746 --> 00:09:29.706 A:middle
session, how you do that exactly

00:09:30.166 --> 00:09:32.876 A:middle
on Thursday.

00:09:33.026 --> 00:09:34.556 A:middle
All of these frameworks are

00:09:34.556 --> 00:09:36.546 A:middle
being powered by Accelerate and

00:09:36.856 --> 00:09:37.106 A:middle
MPS.

00:09:38.316 --> 00:09:39.276 A:middle
These are our engine.

00:09:40.746 --> 00:09:42.946 A:middle
Now you can use Accelerate and

00:09:43.566 --> 00:09:45.246 A:middle
MPS whenever you need some kind

00:09:45.246 --> 00:09:46.566 A:middle
of math functionality.

00:09:46.566 --> 00:09:47.876 A:middle
So it doesn't have to be

00:09:48.056 --> 00:09:49.436 A:middle
necessarily related to machine

00:09:49.436 --> 00:09:49.706 A:middle
learning.

00:09:50.116 --> 00:09:53.576 A:middle
You can also use these

00:09:53.956 --> 00:09:57.766 A:middle
Accelerate and MPS when you are

00:09:57.766 --> 00:10:00.876 A:middle
doing custom ML models.

00:10:00.876 --> 00:10:02.156 A:middle
So if you are using a machine

00:10:02.156 --> 00:10:03.746 A:middle
learning model, you might like

00:10:03.746 --> 00:10:05.776 A:middle
to use Accelerate and MPS.

00:10:08.076 --> 00:10:10.836 A:middle
All these APIs run on user's

00:10:10.836 --> 00:10:11.196 A:middle
device.

00:10:11.266 --> 00:10:12.486 A:middle
And they're very, very highly

00:10:12.486 --> 00:10:13.036 A:middle
performing.

00:10:13.146 --> 00:10:14.656 A:middle
And we work very hard to make

00:10:14.656 --> 00:10:15.846 A:middle
sure you get the best

00:10:15.846 --> 00:10:16.856 A:middle
performance possible.

00:10:18.346 --> 00:10:19.576 A:middle
Now because they learn on user's

00:10:19.576 --> 00:10:21.786 A:middle
device, there are advantages.

00:10:22.366 --> 00:10:24.456 A:middle
First, user privacy.

00:10:24.626 --> 00:10:27.676 A:middle
Your users will be happy to know

00:10:27.676 --> 00:10:28.716 A:middle
that you're not sending their

00:10:28.716 --> 00:10:30.896 A:middle
personal messages, texts, and

00:10:30.896 --> 00:10:34.456 A:middle
images to the server.

00:10:34.456 --> 00:10:35.866 A:middle
Your users don't have to pay

00:10:35.866 --> 00:10:38.956 A:middle
extra data cost just to receive

00:10:38.956 --> 00:10:40.556 A:middle
these -- just to send this data

00:10:40.556 --> 00:10:42.686 A:middle
to the server to get a

00:10:44.176 --> 00:10:44.566 A:middle
prediction.

00:10:44.566 --> 00:10:46.496 A:middle
You also don't have to actually

00:10:46.706 --> 00:10:48.136 A:middle
give huge amount of money to

00:10:48.136 --> 00:10:49.516 A:middle
server companies just to set up

00:10:49.516 --> 00:10:50.586 A:middle
the servers and get a

00:10:50.586 --> 00:10:51.176 A:middle
prediction.

00:10:51.726 --> 00:10:53.136 A:middle
Our devices are very, very

00:10:53.136 --> 00:10:53.486 A:middle
powerful.

00:10:53.486 --> 00:10:54.696 A:middle
You can do a lot here.

00:10:55.306 --> 00:11:00.206 A:middle
And finally, your app is always

00:11:00.206 --> 00:11:00.646 A:middle
available.

00:11:01.866 --> 00:11:03.856 A:middle
Let's just say your user goes to

00:11:03.936 --> 00:11:06.896 A:middle
Yosemite in the wilderness where

00:11:06.896 --> 00:11:08.246 A:middle
there is no network

00:11:08.246 --> 00:11:08.966 A:middle
connectivity.

00:11:10.076 --> 00:11:11.186 A:middle
They can still use your app.

00:11:11.186 --> 00:11:13.186 A:middle
And this is very powerful.

00:11:14.476 --> 00:11:17.366 A:middle
However, perhaps one of the best

00:11:17.436 --> 00:11:19.686 A:middle
things you can do is real-time

00:11:19.686 --> 00:11:20.396 A:middle
machine learning.

00:11:21.706 --> 00:11:23.396 A:middle
Our devices are very powerful.

00:11:23.396 --> 00:11:25.136 A:middle
They can run modern deep neural

00:11:25.136 --> 00:11:28.356 A:middle
networks such as ResNet 15

00:11:28.356 --> 00:11:29.656 A:middle
[inaudible].

00:11:29.656 --> 00:11:32.966 A:middle
You can use our devices to do

00:11:32.966 --> 00:11:34.426 A:middle
real-time image recognition.

00:11:34.816 --> 00:11:37.176 A:middle
In these cases, you really don't

00:11:37.226 --> 00:11:39.306 A:middle
have [inaudible] to actually get

00:11:40.076 --> 00:11:42.146 A:middle
the application.

00:11:43.516 --> 00:11:51.296 A:middle
[ Applause ]

00:11:51.796 --> 00:11:53.336 A:middle
So for any latency sensitive

00:11:53.396 --> 00:11:54.716 A:middle
things, you might like to do

00:11:54.896 --> 00:11:56.326 A:middle
real-time image recognition.

00:11:56.326 --> 00:12:01.276 A:middle
So just to recap here, we are

00:12:01.276 --> 00:12:02.556 A:middle
releasing a series of machine

00:12:02.556 --> 00:12:03.286 A:middle
learning frameworks.

00:12:03.816 --> 00:12:05.606 A:middle
If your app is a vision-based

00:12:05.786 --> 00:12:07.066 A:middle
app, please use Vision

00:12:07.066 --> 00:12:07.516 A:middle
framework.

00:12:08.166 --> 00:12:10.326 A:middle
If your app is a text-based app,

00:12:10.386 --> 00:12:11.486 A:middle
please use NLP.

00:12:12.226 --> 00:12:14.426 A:middle
Now if you think that NLP and

00:12:14.426 --> 00:12:16.286 A:middle
the Vision framework are not

00:12:16.776 --> 00:12:18.896 A:middle
providing the APIs you need, go

00:12:18.896 --> 00:12:19.956 A:middle
down on Core ML.

00:12:20.266 --> 00:12:21.996 A:middle
Core ML provides exhaustive

00:12:22.666 --> 00:12:24.366 A:middle
support of standard machine

00:12:24.366 --> 00:12:26.076 A:middle
learning and deep learning

00:12:27.046 --> 00:12:27.586 A:middle
algorithms.

00:12:27.586 --> 00:12:28.816 A:middle
And let's just say you are

00:12:28.956 --> 00:12:30.736 A:middle
[inaudible] your APIs machine

00:12:30.736 --> 00:12:31.906 A:middle
learning algorithms then you

00:12:31.906 --> 00:12:34.346 A:middle
should use Accelerate and MPS.

00:12:34.996 --> 00:12:36.806 A:middle
We are going to discuss all

00:12:36.806 --> 00:12:39.116 A:middle
these frameworks in detail in

00:12:39.116 --> 00:12:41.366 A:middle
the next couple of days so you

00:12:41.896 --> 00:12:43.796 A:middle
are [inaudible], but today let's

00:12:43.796 --> 00:12:44.956 A:middle
just focus on Core ML.

00:12:44.956 --> 00:12:46.806 A:middle
And to do that, I'll invite my

00:12:46.866 --> 00:12:50.586 A:middle
friend and colleague, Michael.

00:12:51.696 --> 00:12:53.356 A:middle
&gt;&gt; Thanks Gaurav.

00:12:53.486 --> 00:12:54.186 A:middle
Hi everyone.

00:12:54.186 --> 00:12:56.056 A:middle
I'm so excited to talk about

00:12:56.056 --> 00:12:58.066 A:middle
Core ML and its role in helping

00:12:58.066 --> 00:13:00.066 A:middle
you make a great app.

00:13:00.996 --> 00:13:02.246 A:middle
We're going to start out with a

00:13:02.246 --> 00:13:04.146 A:middle
high level overview of Core MLs

00:13:04.146 --> 00:13:04.976 A:middle
main goals.

00:13:06.056 --> 00:13:07.106 A:middle
We'll then talk a little bit

00:13:07.106 --> 00:13:08.286 A:middle
about models and how they're

00:13:08.286 --> 00:13:08.976 A:middle
represented.

00:13:09.016 --> 00:13:11.666 A:middle
And then we'll walk you through

00:13:11.666 --> 00:13:13.196 A:middle
the typical development process

00:13:13.196 --> 00:13:16.766 A:middle
using Core ML.

00:13:17.006 --> 00:13:18.766 A:middle
So the Core ML framework is

00:13:18.766 --> 00:13:20.956 A:middle
available on macOS, iOS,

00:13:22.186 --> 00:13:23.716 A:middle
watchOS, and tvOS.

00:13:25.046 --> 00:13:26.676 A:middle
But Core ML is more than just a

00:13:26.676 --> 00:13:28.946 A:middle
framework and a set of APIs.

00:13:29.436 --> 00:13:30.946 A:middle
It's really a set of development

00:13:30.946 --> 00:13:33.016 A:middle
tools all designed around making

00:13:33.016 --> 00:13:35.046 A:middle
it as easy as possible to take a

00:13:35.046 --> 00:13:36.506 A:middle
machine learned model and

00:13:36.506 --> 00:13:38.826 A:middle
integrate it into your app.

00:13:39.556 --> 00:13:41.366 A:middle
This will let you focus on the

00:13:41.366 --> 00:13:42.826 A:middle
user experience you're trying to

00:13:42.826 --> 00:13:44.216 A:middle
enable rather than the

00:13:44.216 --> 00:13:45.546 A:middle
implementation details.

00:13:47.876 --> 00:13:51.446 A:middle
Core ML is simple to use, will

00:13:51.446 --> 00:13:52.476 A:middle
give you the performance you

00:13:52.476 --> 00:13:55.166 A:middle
need while being compatible with

00:13:55.166 --> 00:13:56.316 A:middle
a wide variety of machine

00:13:56.316 --> 00:14:00.066 A:middle
learning tools out there.

00:14:00.266 --> 00:14:02.516 A:middle
Its simplicity comes from having

00:14:02.516 --> 00:14:04.196 A:middle
a Unified Inference API across

00:14:04.196 --> 00:14:05.226 A:middle
all model types.

00:14:06.276 --> 00:14:08.066 A:middle
Xcode integration will let you

00:14:08.066 --> 00:14:09.166 A:middle
interact with machine learned

00:14:09.166 --> 00:14:10.626 A:middle
models using the same software

00:14:10.626 --> 00:14:12.046 A:middle
development practices you're all

00:14:12.166 --> 00:14:13.556 A:middle
already experts in.

00:14:15.626 --> 00:14:18.616 A:middle
It uses the inference engines

00:14:18.746 --> 00:14:20.066 A:middle
Apple is already shipping to

00:14:20.066 --> 00:14:21.526 A:middle
millions of customers in its

00:14:21.526 --> 00:14:23.136 A:middle
apps and systems services now

00:14:23.136 --> 00:14:24.766 A:middle
being made available to you via

00:14:24.766 --> 00:14:25.286 A:middle
Core ML.

00:14:25.946 --> 00:14:27.316 A:middle
And as mentioned, these are

00:14:27.316 --> 00:14:28.466 A:middle
built on top of Metal and

00:14:28.466 --> 00:14:29.836 A:middle
Accelerate to make the best use

00:14:29.836 --> 00:14:31.076 A:middle
of hardware that your apps are

00:14:31.076 --> 00:14:32.396 A:middle
being deployed on.

00:14:35.096 --> 00:14:36.986 A:middle
Core ML is also designed to work

00:14:36.986 --> 00:14:39.266 A:middle
in this rapidly evolving machine

00:14:39.266 --> 00:14:40.126 A:middle
learning ecosystem.

00:14:40.746 --> 00:14:43.086 A:middle
It defines a new public format

00:14:43.086 --> 00:14:45.196 A:middle
for describing models as well as

00:14:45.196 --> 00:14:46.856 A:middle
a set of tools that will allow

00:14:46.856 --> 00:14:48.056 A:middle
you to convert the output of

00:14:48.056 --> 00:14:50.316 A:middle
popular training libraries into

00:14:50.316 --> 00:14:51.506 A:middle
this format.

00:14:53.156 --> 00:14:55.056 A:middle
So that's Core ML in a nutshell.

00:14:55.476 --> 00:14:56.656 A:middle
It's all about making it super

00:14:56.656 --> 00:14:58.836 A:middle
easy to get a learned model

00:14:59.056 --> 00:15:00.566 A:middle
integrated into your app.

00:15:00.756 --> 00:15:02.416 A:middle
So let's talk a little bit more

00:15:02.416 --> 00:15:03.316 A:middle
about these models.

00:15:03.816 --> 00:15:07.726 A:middle
Since Core ML, a model is simply

00:15:07.726 --> 00:15:08.256 A:middle
a function.

00:15:08.976 --> 00:15:10.836 A:middle
Now the logic for this function

00:15:10.966 --> 00:15:12.386 A:middle
happens to be learned from data

00:15:12.696 --> 00:15:13.666 A:middle
but just like any other

00:15:13.666 --> 00:15:15.086 A:middle
function, it takes in a set of

00:15:15.086 --> 00:15:16.756 A:middle
inputs and produces a set of

00:15:16.756 --> 00:15:17.356 A:middle
outputs.

00:15:17.956 --> 00:15:19.046 A:middle
In this case, shown on the

00:15:19.046 --> 00:15:20.756 A:middle
slide, we have a single input of

00:15:20.756 --> 00:15:22.766 A:middle
an image and an output, perhaps

00:15:22.766 --> 00:15:24.006 A:middle
telling you what type of flower

00:15:24.116 --> 00:15:26.196 A:middle
is present in it.

00:15:26.456 --> 00:15:28.966 A:middle
Now many of the use cases in

00:15:28.966 --> 00:15:30.066 A:middle
which you may want to apply a

00:15:30.066 --> 00:15:31.976 A:middle
machine learning model have some

00:15:32.206 --> 00:15:33.806 A:middle
key function at its core.

00:15:34.996 --> 00:15:36.876 A:middle
A common type of function is

00:15:36.876 --> 00:15:37.766 A:middle
performing some sort of

00:15:37.766 --> 00:15:38.636 A:middle
classification.

00:15:39.576 --> 00:15:41.636 A:middle
It's taking a set of inputs and

00:15:41.636 --> 00:15:42.946 A:middle
assigning some categorical

00:15:42.946 --> 00:15:43.316 A:middle
label.

00:15:43.846 --> 00:15:46.466 A:middle
Take, for example, sentiment

00:15:46.466 --> 00:15:47.066 A:middle
analysis.

00:15:47.706 --> 00:15:49.206 A:middle
Here, you may take in an English

00:15:49.206 --> 00:15:51.196 A:middle
sentence and output whether the

00:15:51.196 --> 00:15:52.566 A:middle
sentiment was positive or

00:15:52.566 --> 00:15:55.506 A:middle
negative, here represented by an

00:15:56.836 --> 00:15:57.026 A:middle
emoji.

00:15:57.116 --> 00:15:58.736 A:middle
Now, in order to encode

00:15:59.106 --> 00:16:01.606 A:middle
functions of this type, Core ML

00:16:01.606 --> 00:16:02.836 A:middle
supports a wide variety of

00:16:02.836 --> 00:16:03.366 A:middle
models.

00:16:03.906 --> 00:16:08.866 A:middle
It has extensive support for

00:16:08.866 --> 00:16:10.886 A:middle
neural networks both feedforward

00:16:10.886 --> 00:16:11.946 A:middle
and recurrent neural networks

00:16:11.946 --> 00:16:13.246 A:middle
with over 30 different layer

00:16:13.246 --> 00:16:13.706 A:middle
types.

00:16:14.956 --> 00:16:17.076 A:middle
It also supports tree ensembles,

00:16:17.076 --> 00:16:18.546 A:middle
support vector machines, and

00:16:18.546 --> 00:16:19.886 A:middle
generalized linear models.

00:16:21.176 --> 00:16:22.516 A:middle
Now each of these model types

00:16:22.516 --> 00:16:23.996 A:middle
are actually a large family of

00:16:23.996 --> 00:16:24.536 A:middle
models.

00:16:24.766 --> 00:16:26.046 A:middle
And we could spend the rest of

00:16:26.046 --> 00:16:27.696 A:middle
this session and the rest of

00:16:27.696 --> 00:16:28.956 A:middle
this conference just talking

00:16:28.956 --> 00:16:31.696 A:middle
about any one of them, but don't

00:16:31.696 --> 00:16:32.826 A:middle
let that intimidate you.

00:16:33.106 --> 00:16:34.536 A:middle
You do not need to be a machine

00:16:34.536 --> 00:16:36.036 A:middle
learning expert to make use of

00:16:36.036 --> 00:16:36.796 A:middle
these models.

00:16:37.296 --> 00:16:39.446 A:middle
You continue to focus on the use

00:16:39.446 --> 00:16:41.246 A:middle
case you're trying to enable and

00:16:41.246 --> 00:16:42.936 A:middle
let Core ML handle those

00:16:42.936 --> 00:16:43.966 A:middle
low-level details.

00:16:44.546 --> 00:16:45.896 A:middle
Core ML represents a model in a

00:16:45.896 --> 00:16:46.776 A:middle
single document.

00:16:47.316 --> 00:16:48.946 A:middle
That is, sharing a model is just

00:16:48.946 --> 00:16:50.196 A:middle
like sharing a file.

00:16:50.806 --> 00:16:52.686 A:middle
This document has the high level

00:16:52.686 --> 00:16:53.976 A:middle
information you, as a developer,

00:16:53.976 --> 00:16:54.886 A:middle
need to program against.

00:16:54.886 --> 00:16:55.656 A:middle
That is the functional

00:16:55.656 --> 00:16:57.276 A:middle
description, its inputs, types,

00:16:57.276 --> 00:16:59.566 A:middle
and outputs as well as those

00:16:59.566 --> 00:17:00.846 A:middle
details that Core ML needs to

00:17:00.846 --> 00:17:02.106 A:middle
actually execute the function.

00:17:02.446 --> 00:17:03.656 A:middle
For a neural network, this may

00:17:03.656 --> 00:17:04.686 A:middle
be the structure of the neural

00:17:04.686 --> 00:17:06.486 A:middle
network along with its trained

00:17:06.486 --> 00:17:06.916 A:middle
parameter.

00:17:07.006 --> 00:17:08.616 A:middle
We encourage you to come to our

00:17:08.616 --> 00:17:10.036 A:middle
Thursday session to learn more

00:17:10.036 --> 00:17:11.666 A:middle
about this format and the set of

00:17:11.666 --> 00:17:15.696 A:middle
models it can encode.

00:17:15.696 --> 00:17:16.856 A:middle
But maybe now in the back of

00:17:16.856 --> 00:17:17.716 A:middle
your head you're starting to

00:17:17.716 --> 00:17:20.066 A:middle
think well, where do I get these

00:17:20.066 --> 00:17:20.616 A:middle
models?

00:17:21.116 --> 00:17:24.176 A:middle
Where do models come from?

00:17:24.176 --> 00:17:25.966 A:middle
Well, a great place to start is

00:17:25.966 --> 00:17:27.326 A:middle
to visit our machine learning

00:17:27.326 --> 00:17:28.156 A:middle
landing page on

00:17:28.156 --> 00:17:29.386 A:middle
developer.apple.com.

00:17:30.816 --> 00:17:32.286 A:middle
There you'll find a small

00:17:32.286 --> 00:17:34.276 A:middle
collection of task specific,

00:17:34.766 --> 00:17:36.966 A:middle
ready to use models already in

00:17:36.966 --> 00:17:37.916 A:middle
the Core ML format.

00:17:38.976 --> 00:17:40.186 A:middle
We'll be expanding the set

00:17:40.186 --> 00:17:40.746 A:middle
overtime.

00:17:40.746 --> 00:17:42.586 A:middle
And we encourage you to explore.

00:17:42.826 --> 00:17:43.556 A:middle
Give it a try.

00:17:43.556 --> 00:17:44.586 A:middle
This is a great way to get

00:17:44.586 --> 00:17:45.656 A:middle
introduced to machine learning

00:17:45.656 --> 00:17:47.406 A:middle
in general and Core ML and how

00:17:47.406 --> 00:17:49.126 A:middle
it can be used in your app.

00:17:51.076 --> 00:17:52.586 A:middle
But we may not have all the

00:17:52.586 --> 00:17:53.366 A:middle
models you need.

00:17:53.516 --> 00:17:54.886 A:middle
And for your application, you

00:17:54.886 --> 00:17:55.586 A:middle
may need to make some

00:17:55.586 --> 00:17:56.956 A:middle
customization to an existing

00:17:56.956 --> 00:17:58.376 A:middle
model or make a whole new model

00:17:58.376 --> 00:17:59.086 A:middle
from scratch.

00:18:00.016 --> 00:18:01.856 A:middle
And for that, we encourage you

00:18:01.856 --> 00:18:03.986 A:middle
and your colleagues to tap into

00:18:03.986 --> 00:18:05.256 A:middle
the machine learning community.

00:18:05.776 --> 00:18:07.526 A:middle
There is a thriving community

00:18:07.526 --> 00:18:08.016 A:middle
out there.

00:18:09.396 --> 00:18:11.026 A:middle
There are tons of libraries and

00:18:11.026 --> 00:18:12.486 A:middle
models already existing for you

00:18:12.486 --> 00:18:13.326 A:middle
to start playing with.

00:18:14.096 --> 00:18:15.686 A:middle
In addition, there are wonderful

00:18:15.686 --> 00:18:17.426 A:middle
online courses and new tutorials

00:18:17.426 --> 00:18:18.806 A:middle
are popping up every week.

00:18:19.966 --> 00:18:20.586 A:middle
You can do it.

00:18:20.796 --> 00:18:22.106 A:middle
There's great resources to

00:18:22.106 --> 00:18:22.606 A:middle
learn.

00:18:22.606 --> 00:18:24.036 A:middle
And I'm confident that all of

00:18:24.036 --> 00:18:24.906 A:middle
you can get started.

00:18:24.906 --> 00:18:26.776 A:middle
It's a wonderful time.

00:18:28.316 --> 00:18:30.066 A:middle
But you'll see that most of

00:18:30.066 --> 00:18:32.096 A:middle
these libraries are focused

00:18:32.096 --> 00:18:33.266 A:middle
around training.

00:18:33.986 --> 00:18:35.266 A:middle
That is, it's a very important

00:18:35.266 --> 00:18:36.986 A:middle
step but they'll spend a lot of

00:18:36.986 --> 00:18:38.146 A:middle
time making sure that you can

00:18:38.146 --> 00:18:39.046 A:middle
train a great model.

00:18:39.046 --> 00:18:40.866 A:middle
What we want you to do is let

00:18:40.926 --> 00:18:42.776 A:middle
Core ML take you that last mile

00:18:42.776 --> 00:18:44.686 A:middle
from taking that machine learned

00:18:44.686 --> 00:18:45.916 A:middle
model and actually deploying it

00:18:45.916 --> 00:18:46.986 A:middle
in your app.

00:18:47.736 --> 00:18:49.416 A:middle
So we're doing this by

00:18:49.416 --> 00:18:51.206 A:middle
introducing the Core ML Tools

00:18:51.206 --> 00:18:52.166 A:middle
Python package.

00:18:52.696 --> 00:18:54.546 A:middle
This is a Python package that is

00:18:54.546 --> 00:18:55.686 A:middle
centered completely around

00:18:55.686 --> 00:18:57.826 A:middle
taking the output of these

00:18:57.826 --> 00:18:59.536 A:middle
machine learning libraries and

00:18:59.536 --> 00:19:01.126 A:middle
getting them into the Core ML

00:19:01.126 --> 00:19:01.576 A:middle
format.

00:19:02.646 --> 00:19:04.096 A:middle
We'll be expanding these tools

00:19:04.096 --> 00:19:06.606 A:middle
over time, but they're also

00:19:06.606 --> 00:19:07.976 A:middle
being made Open Source.

00:19:08.516 --> 00:19:15.046 A:middle
[ Applause ]

00:19:15.546 --> 00:19:16.706 A:middle
So what this means, if you don't

00:19:16.706 --> 00:19:17.626 A:middle
find the converter you need

00:19:17.626 --> 00:19:19.466 A:middle
there, we're super confident you

00:19:19.466 --> 00:19:20.606 A:middle
can already write your own.

00:19:21.136 --> 00:19:22.576 A:middle
All of the primitives are there

00:19:22.576 --> 00:19:23.606 A:middle
ready for you to use.

00:19:24.086 --> 00:19:25.316 A:middle
Now I encourage you, again, to

00:19:25.316 --> 00:19:27.306 A:middle
visit us on Thursday to learn

00:19:27.306 --> 00:19:28.836 A:middle
more about this Python package

00:19:28.836 --> 00:19:30.116 A:middle
and how you can easily convert

00:19:30.116 --> 00:19:30.516 A:middle
models.

00:19:30.516 --> 00:19:34.946 A:middle
OK. Now we have this model.

00:19:35.396 --> 00:19:36.626 A:middle
Let's talk about how we use it

00:19:36.626 --> 00:19:37.156 A:middle
in our apps.

00:19:37.156 --> 00:19:40.256 A:middle
So you're going to start with

00:19:40.256 --> 00:19:41.626 A:middle
the machine learned model in

00:19:41.626 --> 00:19:42.616 A:middle
this Core ML format.

00:19:43.986 --> 00:19:45.666 A:middle
You're going to simply add it to

00:19:45.666 --> 00:19:46.656 A:middle
your Xcode project.

00:19:47.416 --> 00:19:49.036 A:middle
And Xcode will automatically

00:19:49.176 --> 00:19:50.636 A:middle
recognize this model as a

00:19:50.636 --> 00:19:51.626 A:middle
machine learned model, and it

00:19:51.626 --> 00:19:53.366 A:middle
will generate an interface for

00:19:53.366 --> 00:19:53.566 A:middle
you.

00:19:54.116 --> 00:19:55.036 A:middle
That will give you a

00:19:55.036 --> 00:19:56.326 A:middle
programmatic interface to this

00:19:56.326 --> 00:19:58.746 A:middle
model using data types and

00:19:58.746 --> 00:19:59.646 A:middle
structures you're already

00:19:59.646 --> 00:20:00.906 A:middle
familiar programming against.

00:20:01.386 --> 00:20:03.536 A:middle
You'll program using this

00:20:03.686 --> 00:20:05.346 A:middle
interface and compile it into

00:20:05.346 --> 00:20:06.446 A:middle
your app.

00:20:07.376 --> 00:20:09.086 A:middle
But in addition, the model

00:20:09.086 --> 00:20:10.866 A:middle
itself will also get compiled

00:20:10.936 --> 00:20:12.496 A:middle
and bundled into your app.

00:20:12.856 --> 00:20:14.636 A:middle
That is, we'll take this format

00:20:14.636 --> 00:20:15.636 A:middle
that's been optimized for

00:20:15.636 --> 00:20:17.856 A:middle
openness and compatibility and

00:20:17.856 --> 00:20:19.176 A:middle
turn it into one that is

00:20:19.176 --> 00:20:20.886 A:middle
optimized for run time on the

00:20:20.886 --> 00:20:21.456 A:middle
device.

00:20:22.996 --> 00:20:24.556 A:middle
To show you this in action, I'm

00:20:24.656 --> 00:20:25.866 A:middle
going to invite my friend and

00:20:25.866 --> 00:20:27.156 A:middle
colleague, Lizi, to the stage.

00:20:28.516 --> 00:20:34.406 A:middle
[ Applause ]

00:20:34.906 --> 00:20:36.436 A:middle
And I'm an engineer on the Core

00:20:36.436 --> 00:20:36.946 A:middle
ML team.

00:20:37.926 --> 00:20:40.316 A:middle
Today, let's take a look at how

00:20:40.316 --> 00:20:42.216 A:middle
you can use Core ML to integrate

00:20:42.216 --> 00:20:43.576 A:middle
machine learning into your

00:20:43.576 --> 00:20:44.106 A:middle
applications.

00:20:45.156 --> 00:20:46.976 A:middle
Now first some context.

00:20:47.796 --> 00:20:49.016 A:middle
As much as I love machine

00:20:49.016 --> 00:20:51.076 A:middle
learning, I also love to garden.

00:20:51.076 --> 00:20:53.096 A:middle
And if you're anything like me,

00:20:53.546 --> 00:20:54.796 A:middle
you may get really excited

00:20:54.796 --> 00:20:56.436 A:middle
whenever you come across a new

00:20:56.436 --> 00:20:58.336 A:middle
kind of flower and instantly

00:20:58.336 --> 00:20:59.276 A:middle
want to identify it.

00:21:00.366 --> 00:21:01.706 A:middle
So we sought out to build an app

00:21:02.296 --> 00:21:03.866 A:middle
that would take images of

00:21:03.866 --> 00:21:05.476 A:middle
different types of flowers and

00:21:05.516 --> 00:21:07.426 A:middle
classify them by type.

00:21:08.346 --> 00:21:09.886 A:middle
But first, in order to do that,

00:21:10.466 --> 00:21:11.296 A:middle
we needed a model.

00:21:12.866 --> 00:21:14.166 A:middle
So we went ahead and we

00:21:14.166 --> 00:21:15.766 A:middle
collected many different images

00:21:15.896 --> 00:21:17.116 A:middle
of different types of flowers,

00:21:17.486 --> 00:21:18.686 A:middle
and we labeled each one.

00:21:20.286 --> 00:21:21.656 A:middle
I then trained a neural network

00:21:21.756 --> 00:21:23.686 A:middle
classifier using an Open Source

00:21:23.756 --> 00:21:25.296 A:middle
machine learning tool and

00:21:25.296 --> 00:21:27.326 A:middle
converted it into our Core ML

00:21:27.416 --> 00:21:27.666 A:middle
format.

00:21:29.256 --> 00:21:31.896 A:middle
So with this trained model and

00:21:31.896 --> 00:21:33.086 A:middle
with this app shell of this

00:21:33.086 --> 00:21:35.006 A:middle
flower classifier, let's take a

00:21:35.006 --> 00:21:36.906 A:middle
look at how we can put the two

00:21:36.906 --> 00:21:37.276 A:middle
together.

00:21:38.946 --> 00:21:40.746 A:middle
So over in Xcode, you can see

00:21:40.746 --> 00:21:42.736 A:middle
I'm currently running the flower

00:21:42.736 --> 00:21:44.616 A:middle
classifier app and nearing the

00:21:44.616 --> 00:21:45.626 A:middle
display on my device.

00:21:46.686 --> 00:21:48.606 A:middle
And what I can do is I can go

00:21:48.606 --> 00:21:50.346 A:middle
into the photos library and

00:21:50.346 --> 00:21:52.626 A:middle
choose an image, but right now

00:21:52.736 --> 00:21:53.906 A:middle
it doesn't currently predict

00:21:53.906 --> 00:21:54.906 A:middle
anything for the output.

00:21:55.396 --> 00:21:57.246 A:middle
It just displays this string.

00:21:58.796 --> 00:22:00.086 A:middle
So if I move over to the

00:22:00.086 --> 00:22:02.606 A:middle
ViewController, what we see is

00:22:02.606 --> 00:22:04.836 A:middle
it has this caption method that

00:22:05.046 --> 00:22:06.226 A:middle
currently just returns that

00:22:06.256 --> 00:22:06.856 A:middle
blank string.

00:22:07.426 --> 00:22:10.746 A:middle
It doesn't actually do anything.

00:22:10.836 --> 00:22:12.636 A:middle
Over here, I have this flower

00:22:12.636 --> 00:22:14.176 A:middle
classifier in the ML model

00:22:14.236 --> 00:22:14.506 A:middle
format.

00:22:15.186 --> 00:22:16.656 A:middle
And all I need to do is take it

00:22:17.306 --> 00:22:18.696 A:middle
and drag it over into the

00:22:18.696 --> 00:22:19.066 A:middle
project.

00:22:19.786 --> 00:22:22.936 A:middle
And we can see, as soon as I add

00:22:23.066 --> 00:22:24.796 A:middle
it, Xcode automatically

00:22:24.796 --> 00:22:26.546 A:middle
recognizes this file format.

00:22:27.406 --> 00:22:28.716 A:middle
And it populates the name of the

00:22:28.716 --> 00:22:30.816 A:middle
model as well as what type it is

00:22:30.816 --> 00:22:31.346 A:middle
underneath.

00:22:31.926 --> 00:22:33.666 A:middle
We can see that the size is 41

00:22:33.666 --> 00:22:35.226 A:middle
megabytes and also other

00:22:35.226 --> 00:22:36.616 A:middle
information like the author,

00:22:36.866 --> 00:22:38.296 A:middle
license, or description

00:22:38.296 --> 00:22:39.226 A:middle
associated with it.

00:22:40.446 --> 00:22:41.876 A:middle
In the bottom, we can see the

00:22:41.916 --> 00:22:43.266 A:middle
inputs and outputs that this

00:22:43.266 --> 00:22:45.736 A:middle
model takes which, first is

00:22:46.126 --> 00:22:48.136 A:middle
flower image, a type of image

00:22:48.476 --> 00:22:49.966 A:middle
that's ordered red, green, blue,

00:22:50.086 --> 00:22:51.096 A:middle
the following width and height.

00:22:52.406 --> 00:22:53.816 A:middle
We can also see that it outputs

00:22:53.816 --> 00:22:55.506 A:middle
a flower type as a string.

00:22:56.166 --> 00:22:57.756 A:middle
So if give it an image of a

00:22:57.756 --> 00:22:59.576 A:middle
rose, I would expect flower type

00:22:59.646 --> 00:23:00.526 A:middle
to be rose.

00:23:01.896 --> 00:23:03.466 A:middle
There's also a dictionary full

00:23:03.466 --> 00:23:05.016 A:middle
of probabilities for different

00:23:05.016 --> 00:23:05.736 A:middle
types of flowers.

00:23:06.306 --> 00:23:08.796 A:middle
Now the first thing I want to do

00:23:10.206 --> 00:23:11.236 A:middle
is I want to make sure that I

00:23:11.436 --> 00:23:12.966 A:middle
add this to my app.

00:23:13.586 --> 00:23:15.876 A:middle
And you'll see that afterwards

00:23:16.396 --> 00:23:18.406 A:middle
this middle pane will show that

00:23:18.406 --> 00:23:20.086 A:middle
the Swift generated source has

00:23:20.366 --> 00:23:21.896 A:middle
arrived in the application.

00:23:22.896 --> 00:23:23.966 A:middle
What that means is we can

00:23:23.966 --> 00:23:25.866 A:middle
actually use this generated code

00:23:26.176 --> 00:23:27.536 A:middle
to load the model and predict

00:23:27.536 --> 00:23:28.046 A:middle
against it.

00:23:28.846 --> 00:23:30.576 A:middle
To show you, let's go to the

00:23:30.576 --> 00:23:31.206 A:middle
ViewController.

00:23:31.816 --> 00:23:36.156 A:middle
The first thing I want to do is

00:23:36.156 --> 00:23:37.526 A:middle
define this flower model.

00:23:38.766 --> 00:23:40.216 A:middle
And to instantiate it, all I

00:23:40.216 --> 00:23:42.086 A:middle
need to do is use the name of a

00:23:42.206 --> 00:23:45.656 A:middle
model class itself.

00:23:45.806 --> 00:23:47.346 A:middle
We'll see that it's highlighted

00:23:47.506 --> 00:23:48.456 A:middle
because it exists in the

00:23:48.456 --> 00:23:48.866 A:middle
project.

00:23:49.906 --> 00:23:51.516 A:middle
Now next in the caption method,

00:23:51.766 --> 00:23:52.866 A:middle
let's define a prediction.

00:23:53.446 --> 00:23:56.886 A:middle
And we'll actually use the

00:23:56.936 --> 00:24:00.206 A:middle
flower model and look inside.

00:24:01.076 --> 00:24:02.546 A:middle
We'll see using auto complete

00:24:02.696 --> 00:24:03.896 A:middle
that this prediction method is

00:24:03.896 --> 00:24:05.156 A:middle
available that takes a

00:24:05.156 --> 00:24:07.056 A:middle
CVPixelBuffer as input.

00:24:07.906 --> 00:24:09.666 A:middle
We'll use that and pass the

00:24:09.666 --> 00:24:12.786 A:middle
image directly to it.

00:24:13.286 --> 00:24:14.726 A:middle
Now, instead of returning this

00:24:14.866 --> 00:24:16.776 A:middle
preset string, we can use the

00:24:16.776 --> 00:24:18.716 A:middle
prediction object and instead

00:24:19.236 --> 00:24:20.966 A:middle
return the flower type.

00:24:21.676 --> 00:24:24.166 A:middle
Now if I save, let's build and

00:24:24.166 --> 00:24:25.266 A:middle
run the app.

00:24:26.026 --> 00:24:27.586 A:middle
And what's happening while I do

00:24:27.586 --> 00:24:29.916 A:middle
this is the model itself is

00:24:29.916 --> 00:24:31.306 A:middle
actually getting compiled and

00:24:31.306 --> 00:24:32.566 A:middle
bundled with the application.

00:24:33.706 --> 00:24:34.936 A:middle
So if we go back to the device,

00:24:35.476 --> 00:24:37.196 A:middle
if I go to my photos library, I

00:24:37.506 --> 00:24:39.136 A:middle
can choose a rose on the second

00:24:39.136 --> 00:24:39.366 A:middle
row.

00:24:40.216 --> 00:24:41.536 A:middle
And we can see it actually is

00:24:41.536 --> 00:24:42.696 A:middle
able to display rose.

00:24:43.516 --> 00:24:49.356 A:middle
[ Applause ]

00:24:49.856 --> 00:24:51.106 A:middle
We give it another one, maybe

00:24:51.106 --> 00:24:52.356 A:middle
the sunflower on the third.

00:24:53.406 --> 00:24:54.746 A:middle
It's also able to get this one.

00:24:55.736 --> 00:24:56.846 A:middle
Or even something a little bit

00:24:56.846 --> 00:24:58.216 A:middle
more difficult like the passion

00:24:58.216 --> 00:24:59.646 A:middle
flower on the bottom, it can

00:24:59.646 --> 00:25:00.196 A:middle
still do.

00:25:01.296 --> 00:25:04.346 A:middle
So to recap, we were just able

00:25:04.576 --> 00:25:06.876 A:middle
to drag a trained ML model into

00:25:07.146 --> 00:25:09.236 A:middle
Xcode and easily add it into our

00:25:09.236 --> 00:25:10.906 A:middle
application with three lines of

00:25:10.946 --> 00:25:11.346 A:middle
code.

00:25:12.006 --> 00:25:13.696 A:middle
And now we have a full neural

00:25:13.696 --> 00:25:15.186 A:middle
network classifier running on

00:25:15.186 --> 00:25:15.616 A:middle
the device.

00:25:16.576 --> 00:25:19.096 A:middle
But I have some other models

00:25:19.096 --> 00:25:20.576 A:middle
that we might want to try so

00:25:21.006 --> 00:25:21.896 A:middle
let's go through this again.

00:25:22.376 --> 00:25:25.016 A:middle
You may have noticed there's

00:25:25.016 --> 00:25:26.506 A:middle
also this FlowerSqueeze.

00:25:27.356 --> 00:25:28.996 A:middle
And the first thing I'll do is

00:25:28.996 --> 00:25:31.346 A:middle
try to drag it in again.

00:25:32.066 --> 00:25:35.476 A:middle
And then I'll also add this one

00:25:35.596 --> 00:25:37.246 A:middle
to the flower classifier, the

00:25:37.386 --> 00:25:39.416 A:middle
hello flowers target so that it

00:25:39.566 --> 00:25:40.686 A:middle
starts generating the Swift

00:25:40.816 --> 00:25:41.786 A:middle
source in the background.

00:25:42.566 --> 00:25:43.956 A:middle
And we'll see, again if we zoom

00:25:44.036 --> 00:25:46.016 A:middle
in, that the name of this model

00:25:46.056 --> 00:25:46.456 A:middle
is different.

00:25:46.606 --> 00:25:48.606 A:middle
It's called FlowerSqueeze but in

00:25:48.606 --> 00:25:50.186 A:middle
the bottom the inputs and

00:25:50.186 --> 00:25:51.706 A:middle
outputs are exactly the same.

00:25:53.086 --> 00:25:54.626 A:middle
What this means is that if I go

00:25:54.626 --> 00:25:55.886 A:middle
back into the ViewController,

00:25:57.786 --> 00:25:59.016 A:middle
instead of flower -- actually,

00:25:59.476 --> 00:26:00.906 A:middle
let's delete flower classifier.

00:26:01.076 --> 00:26:02.006 A:middle
We don't want that model

00:26:02.006 --> 00:26:02.336 A:middle
anymore.

00:26:02.926 --> 00:26:05.656 A:middle
And in the ViewController,

00:26:06.216 --> 00:26:07.436 A:middle
instead of instantiating this

00:26:07.436 --> 00:26:10.146 A:middle
one, we can use FlowerSqueeze.

00:26:10.786 --> 00:26:13.336 A:middle
And the really neat thing about

00:26:13.386 --> 00:26:15.276 A:middle
this model is its size.

00:26:16.136 --> 00:26:19.106 A:middle
So if we go back, this one is

00:26:19.106 --> 00:26:21.146 A:middle
only 5.4 megabytes, which is a

00:26:21.146 --> 00:26:21.806 A:middle
huge difference.

00:26:22.266 --> 00:26:27.086 A:middle
Back in the device, we can go to

00:26:27.086 --> 00:26:28.756 A:middle
the photos library and choose

00:26:28.756 --> 00:26:29.906 A:middle
another one like love in the

00:26:29.906 --> 00:26:32.256 A:middle
mist and see that this app is

00:26:32.256 --> 00:26:34.966 A:middle
also able to classify this as

00:26:35.556 --> 00:26:35.676 A:middle
well.

00:26:35.886 --> 00:26:38.856 A:middle
Try another passion flower or

00:26:38.856 --> 00:26:40.376 A:middle
even a more difficult photo of a

00:26:40.376 --> 00:26:42.696 A:middle
rose on the side and it's still

00:26:42.696 --> 00:26:43.966 A:middle
able to predict correctly.

00:26:44.586 --> 00:26:48.716 A:middle
So to recap, we just saw that

00:26:48.856 --> 00:26:50.126 A:middle
we're able to run this neural

00:26:50.126 --> 00:26:51.676 A:middle
network classifier on device

00:26:51.896 --> 00:26:54.086 A:middle
using Core ML, all with only a

00:26:54.176 --> 00:26:55.886 A:middle
couple lines of code.

00:26:56.516 --> 00:27:04.086 A:middle
[ Applause ]

00:27:04.586 --> 00:27:06.396 A:middle
Now let's go back and recap what

00:27:06.396 --> 00:27:08.616 A:middle
we just saw.

00:27:08.856 --> 00:27:11.556 A:middle
We saw that in Xcode, as soon as

00:27:11.746 --> 00:27:13.586 A:middle
you drag an ML model that's been

00:27:13.586 --> 00:27:15.106 A:middle
trained into your application,

00:27:15.376 --> 00:27:16.446 A:middle
you get this view that's

00:27:16.446 --> 00:27:17.956 A:middle
populated that gives you

00:27:17.956 --> 00:27:19.616 A:middle
information such as the name of

00:27:19.616 --> 00:27:21.836 A:middle
the model, the type that it is

00:27:21.836 --> 00:27:23.316 A:middle
underneath, and also other

00:27:23.316 --> 00:27:25.806 A:middle
information like the size or any

00:27:25.806 --> 00:27:27.526 A:middle
other information the author put

00:27:27.526 --> 00:27:27.676 A:middle
in.

00:27:28.276 --> 00:27:29.996 A:middle
You can also see in the bottom

00:27:30.086 --> 00:27:31.816 A:middle
that you get information such as

00:27:31.816 --> 00:27:33.096 A:middle
the input and output of the

00:27:33.096 --> 00:27:34.946 A:middle
model which is very helpful if

00:27:34.946 --> 00:27:35.946 A:middle
you're actually trying to use

00:27:35.946 --> 00:27:36.106 A:middle
it.

00:27:36.106 --> 00:27:38.276 A:middle
And once you add it to your app

00:27:38.276 --> 00:27:40.166 A:middle
target, you get generated code

00:27:40.166 --> 00:27:42.106 A:middle
that you can use to load and use

00:27:42.106 --> 00:27:45.366 A:middle
the model to predict.

00:27:45.436 --> 00:27:47.076 A:middle
We also saw in our application

00:27:47.076 --> 00:27:48.706 A:middle
how simple it was to actually

00:27:48.706 --> 00:27:49.106 A:middle
use it.

00:27:49.956 --> 00:27:51.826 A:middle
Again, even though the model was

00:27:51.826 --> 00:27:53.746 A:middle
a neural network classifier, all

00:27:53.746 --> 00:27:55.266 A:middle
we needed to do to instantiate

00:27:55.266 --> 00:27:56.626 A:middle
it was call the name of the

00:27:56.626 --> 00:27:57.066 A:middle
file.

00:27:57.736 --> 00:27:58.856 A:middle
This means that the model type

00:27:58.856 --> 00:28:00.686 A:middle
was completely abstracted so

00:28:00.686 --> 00:28:01.856 A:middle
whether it's a support vector

00:28:01.856 --> 00:28:03.536 A:middle
machine, a linear model, or a

00:28:03.536 --> 00:28:05.686 A:middle
neural network, you all load

00:28:05.916 --> 00:28:07.906 A:middle
them the exact same way.

00:28:07.906 --> 00:28:09.206 A:middle
We also noticed that the

00:28:09.206 --> 00:28:11.036 A:middle
prediction method took a

00:28:11.136 --> 00:28:12.106 A:middle
CVPixelBuffer.

00:28:12.776 --> 00:28:14.346 A:middle
It was strongly typed so you

00:28:14.346 --> 00:28:15.406 A:middle
know you'll be able to check

00:28:15.536 --> 00:28:17.496 A:middle
input errors at compile time

00:28:17.646 --> 00:28:18.616 A:middle
rather than at runtime.

00:28:20.026 --> 00:28:22.766 A:middle
But now let's take a deeper look

00:28:23.126 --> 00:28:24.486 A:middle
at the generated source that's

00:28:24.536 --> 00:28:27.416 A:middle
being created by Xcode.

00:28:27.476 --> 00:28:29.706 A:middle
Here, we see it defines three

00:28:29.706 --> 00:28:31.696 A:middle
classes, first of which is the

00:28:31.696 --> 00:28:33.966 A:middle
input that defines the flower

00:28:33.966 --> 00:28:35.456 A:middle
image as a CVPixelBuffer.

00:28:36.456 --> 00:28:38.306 A:middle
We then see the output with the

00:28:38.306 --> 00:28:40.836 A:middle
same two types that were listed

00:28:40.836 --> 00:28:41.976 A:middle
in the generated interface.

00:28:43.176 --> 00:28:44.936 A:middle
And next, in the model class

00:28:44.936 --> 00:28:46.586 A:middle
itself, we see the convenience

00:28:46.586 --> 00:28:48.116 A:middle
initializer that we use to

00:28:48.116 --> 00:28:50.346 A:middle
actually create the model and

00:28:50.346 --> 00:28:53.336 A:middle
the predict method as well.

00:28:53.536 --> 00:28:55.486 A:middle
But also inside this generated

00:28:55.486 --> 00:28:57.326 A:middle
source, you have access to the

00:28:57.386 --> 00:28:58.376 A:middle
underlying model.

00:28:58.986 --> 00:29:00.676 A:middle
So for more advanced use cases,

00:29:01.006 --> 00:29:02.656 A:middle
you can use it as well to

00:29:02.656 --> 00:29:04.096 A:middle
programmatically interact with

00:29:04.096 --> 00:29:04.226 A:middle
it.

00:29:05.556 --> 00:29:06.816 A:middle
We take a look inside the

00:29:06.886 --> 00:29:08.966 A:middle
MLModel class, we can see that

00:29:08.966 --> 00:29:10.186 A:middle
there's a model description

00:29:10.896 --> 00:29:12.626 A:middle
which gives you access to

00:29:12.626 --> 00:29:14.946 A:middle
anything you saw in the metadata

00:29:15.136 --> 00:29:16.066 A:middle
in Xcode.

00:29:16.636 --> 00:29:18.196 A:middle
There is also an additional

00:29:18.196 --> 00:29:19.726 A:middle
prediction method that takes

00:29:19.816 --> 00:29:21.726 A:middle
protocol conforming input and

00:29:21.726 --> 00:29:23.206 A:middle
returns protocol conforming

00:29:23.206 --> 00:29:24.606 A:middle
output giving you more

00:29:24.676 --> 00:29:26.456 A:middle
flexibility in how the input is

00:29:26.456 --> 00:29:26.956 A:middle
provided.

00:29:27.836 --> 00:29:29.366 A:middle
So we walked through the top

00:29:29.366 --> 00:29:31.016 A:middle
half of this development flow

00:29:31.446 --> 00:29:32.946 A:middle
and saw what happens when you

00:29:32.946 --> 00:29:34.826 A:middle
drag a trained model into Xcode.

00:29:35.486 --> 00:29:36.766 A:middle
It's able to generate Swift

00:29:36.856 --> 00:29:38.806 A:middle
source that you can then use in

00:29:39.486 --> 00:29:41.436 A:middle
your -- to use the model in your

00:29:41.526 --> 00:29:42.006 A:middle
application.

00:29:44.096 --> 00:29:46.156 A:middle
The model itself, as we saw, is

00:29:46.156 --> 00:29:47.926 A:middle
also compiled and bundled in

00:29:47.926 --> 00:29:49.066 A:middle
your app.

00:29:51.816 --> 00:29:53.506 A:middle
To elaborate on this step a bit

00:29:53.506 --> 00:29:55.986 A:middle
more, what we do, we first take

00:29:55.986 --> 00:29:58.646 A:middle
the model from its JSON form,

00:29:58.836 --> 00:29:59.756 A:middle
which is optimized for

00:29:59.846 --> 00:30:01.056 A:middle
portability and ease of

00:30:01.056 --> 00:30:02.886 A:middle
conversion, and compile it so

00:30:03.166 --> 00:30:04.606 A:middle
that it loads quickly on your

00:30:04.606 --> 00:30:06.826 A:middle
device when you initialize it.

00:30:08.036 --> 00:30:09.886 A:middle
We also make sure that the model

00:30:09.886 --> 00:30:11.376 A:middle
is matched to the underlying

00:30:11.376 --> 00:30:12.716 A:middle
inference engines that are

00:30:12.716 --> 00:30:14.276 A:middle
ultimately evaluating it on your

00:30:14.276 --> 00:30:15.916 A:middle
device giving you optimized

00:30:16.026 --> 00:30:17.426 A:middle
prediction when you need it.

00:30:17.426 --> 00:30:20.576 A:middle
And the last thing we saw in the

00:30:20.576 --> 00:30:22.806 A:middle
demo was we swapped out models.

00:30:23.646 --> 00:30:25.586 A:middle
Mostly just to reduce the size

00:30:26.426 --> 00:30:28.196 A:middle
but there are other reasons why

00:30:28.196 --> 00:30:29.226 A:middle
you might want to play around

00:30:29.226 --> 00:30:29.936 A:middle
with different ones.

00:30:30.346 --> 00:30:32.406 A:middle
For example, accuracy or just to

00:30:32.406 --> 00:30:34.266 A:middle
test out different types to see

00:30:34.266 --> 00:30:35.096 A:middle
how they perform.

00:30:35.696 --> 00:30:40.636 A:middle
In summary, today we saw an

00:30:40.636 --> 00:30:41.966 A:middle
overview of some of the machine

00:30:41.966 --> 00:30:43.206 A:middle
learning frameworks that we have

00:30:43.206 --> 00:30:43.746 A:middle
available.

00:30:44.686 --> 00:30:46.736 A:middle
We also introduced Core ML, a

00:30:46.736 --> 00:30:48.166 A:middle
new framework for on-device

00:30:48.276 --> 00:30:48.746 A:middle
inference.

00:30:49.956 --> 00:30:51.176 A:middle
And also we showed you the

00:30:51.176 --> 00:30:52.456 A:middle
development flow in action.

00:30:54.536 --> 00:30:55.946 A:middle
For more information, make sure

00:30:55.946 --> 00:30:56.786 A:middle
you check us out at

00:30:56.786 --> 00:30:58.376 A:middle
developer.apple.com with our

00:30:58.376 --> 00:30:59.816 A:middle
session number 703.

00:31:00.696 --> 00:31:02.096 A:middle
And also, come visit some of the

00:31:02.096 --> 00:31:03.976 A:middle
related sessions such as Vision

00:31:03.976 --> 00:31:06.536 A:middle
and NLP and Core ML in depth on

00:31:06.596 --> 00:31:08.076 A:middle
Thursday if you want to see some

00:31:08.076 --> 00:31:09.606 A:middle
more use cases of how you can

00:31:09.606 --> 00:31:11.366 A:middle
use Core ML or how to actually

00:31:11.366 --> 00:31:12.946 A:middle
convert models into the Core ML

00:31:13.006 --> 00:31:13.266 A:middle
format.

00:31:14.276 --> 00:31:15.666 A:middle
Thank you and enjoy the rest of

00:31:15.666 --> 00:31:16.206 A:middle
the conference.

00:31:17.016 --> 00:31:18.000 A:middle
[ Applause ]