WEBVTT

00:00:06.516 --> 00:00:15.500 A:middle
[ Music ]

00:00:21.516 --> 00:00:24.396 A:middle
[ Applause ]

00:00:24.896 --> 00:00:25.196 A:middle
&gt;&gt; Hello!

00:00:26.016 --> 00:00:28.000 A:middle
[ Applause ]

00:00:31.236 --> 00:00:33.126 A:middle
So welcome to the second session

00:00:33.286 --> 00:00:34.696 A:middle
of Core ML.

00:00:34.696 --> 00:00:36.246 A:middle
My name is Aseem, and I'm an

00:00:36.246 --> 00:00:39.806 A:middle
engineer in the Core ML team.

00:00:40.656 --> 00:00:42.526 A:middle
As you all know, Core ML is

00:00:42.526 --> 00:00:44.486 A:middle
Apple's machine learning

00:00:44.486 --> 00:00:45.976 A:middle
framework for on-device

00:00:45.976 --> 00:00:46.406 A:middle
inference.

00:00:48.406 --> 00:00:49.906 A:middle
And the one thing I really like

00:00:49.906 --> 00:00:51.416 A:middle
about Core ML is that it's

00:00:51.546 --> 00:00:53.846 A:middle
optimized on all Apple hardware.

00:00:55.176 --> 00:00:58.116 A:middle
Over the last year, we have seen

00:00:58.406 --> 00:01:01.516 A:middle
lots of amazing apps across all


00:00:58.406 --> 00:01:01.516 A:middle
lots of amazing apps across all

00:01:01.516 --> 00:01:02.426 A:middle
Apple platforms.

00:01:03.096 --> 00:01:04.366 A:middle
So that's really exciting.

00:01:05.286 --> 00:01:07.026 A:middle
And we are even more excited

00:01:07.586 --> 00:01:08.986 A:middle
with the new features that we

00:01:08.986 --> 00:01:09.736 A:middle
have this year.

00:01:10.656 --> 00:01:13.716 A:middle
Now you can reduce the size of

00:01:13.796 --> 00:01:14.936 A:middle
your app by a lot.

00:01:16.606 --> 00:01:18.376 A:middle
You can make your app much

00:01:18.376 --> 00:01:20.246 A:middle
faster by using the new

00:01:20.246 --> 00:01:21.286 A:middle
batch-predict API.

00:01:23.066 --> 00:01:25.356 A:middle
And you can really easily

00:01:25.356 --> 00:01:27.596 A:middle
include cutting-edge research

00:01:27.796 --> 00:01:29.246 A:middle
right in your app using

00:01:29.246 --> 00:01:29.966 A:middle
customization.

00:01:31.376 --> 00:01:33.546 A:middle
So that was a recap of the first

00:01:33.546 --> 00:01:33.926 A:middle
session.

00:01:34.426 --> 00:01:36.146 A:middle
And in case you missed it, I

00:01:36.146 --> 00:01:37.466 A:middle
would highly encourage you to go

00:01:37.466 --> 00:01:38.936 A:middle
back and check the slides.

00:01:39.716 --> 00:01:43.436 A:middle
In this session, we are going to

00:01:43.436 --> 00:01:46.296 A:middle
see how to actually make use of

00:01:46.296 --> 00:01:47.046 A:middle
these features.

00:01:47.996 --> 00:01:50.046 A:middle
More specifically, we'll walk

00:01:50.046 --> 00:01:52.166 A:middle
through a few examples and show

00:01:52.166 --> 00:01:54.606 A:middle
you that how in a few simple

00:01:54.606 --> 00:01:57.466 A:middle
steps using Core ML Tools.

00:01:58.046 --> 00:01:59.486 A:middle
You can reduce the size of the

00:01:59.486 --> 00:02:01.426 A:middle
model, and you can include a


00:01:59.486 --> 00:02:01.426 A:middle
model, and you can include a

00:02:01.426 --> 00:02:02.846 A:middle
custom feature in your model.

00:02:04.586 --> 00:02:05.566 A:middle
Here's the agenda of the

00:02:05.566 --> 00:02:05.966 A:middle
session.

00:02:07.076 --> 00:02:08.556 A:middle
We'll start by a really quick

00:02:08.556 --> 00:02:10.166 A:middle
update on the Core ML Tools

00:02:10.226 --> 00:02:10.816 A:middle
ecosystem.

00:02:11.736 --> 00:02:13.586 A:middle
And then we'll dive into a demo

00:02:13.876 --> 00:02:16.866 A:middle
of our quantization and custom

00:02:16.866 --> 00:02:17.346 A:middle
conversion.

00:02:18.026 --> 00:02:18.846 A:middle
So let me start with the

00:02:18.846 --> 00:02:19.356 A:middle
ecosystem.

00:02:23.276 --> 00:02:25.306 A:middle
So how do you get an ML model?

00:02:25.746 --> 00:02:27.846 A:middle
Well, the best thing is that if

00:02:27.846 --> 00:02:32.986 A:middle
you, if you can, if you find it

00:02:32.986 --> 00:02:35.036 A:middle
online, you just download it,

00:02:36.086 --> 00:02:36.406 A:middle
right?

00:02:36.406 --> 00:02:38.616 A:middle
Very good place to download your

00:02:38.676 --> 00:02:41.176 A:middle
ML models is the Apple Machine

00:02:41.176 --> 00:02:42.366 A:middle
Learning landing page.

00:02:42.726 --> 00:02:44.206 A:middle
We have a few models there.

00:02:45.126 --> 00:02:46.836 A:middle
Now let's say you want to train

00:02:46.836 --> 00:02:48.456 A:middle
a model on your data set.

00:02:49.046 --> 00:02:51.746 A:middle
In that case, you can use Create

00:02:52.476 --> 00:02:52.546 A:middle
ML.

00:02:52.806 --> 00:02:55.336 A:middle
This is a new framework that we

00:02:55.336 --> 00:02:56.976 A:middle
have just launched this year,

00:02:57.436 --> 00:02:59.206 A:middle
and you do not have to be a

00:02:59.206 --> 00:03:00.646 A:middle
machine learning expert to use


00:02:59.206 --> 00:03:00.646 A:middle
machine learning expert to use

00:03:00.646 --> 00:03:00.816 A:middle
it.

00:03:01.326 --> 00:03:02.496 A:middle
It's really easy to use.

00:03:02.556 --> 00:03:03.786 A:middle
It's right there in Xcode.

00:03:04.446 --> 00:03:06.936 A:middle
So go and give it a try.

00:03:08.476 --> 00:03:10.316 A:middle
Now some of you are already

00:03:10.316 --> 00:03:12.326 A:middle
familiar with the amazing

00:03:12.326 --> 00:03:13.416 A:middle
machine learning tools that we

00:03:13.416 --> 00:03:15.516 A:middle
have outside in the community.

00:03:16.026 --> 00:03:19.036 A:middle
And for that, last year we had

00:03:19.106 --> 00:03:21.406 A:middle
released Core ML Tools, a Python

00:03:21.406 --> 00:03:21.956 A:middle
package.

00:03:22.546 --> 00:03:24.306 A:middle
And along with that, we had

00:03:24.306 --> 00:03:26.786 A:middle
released a few converters.

00:03:28.366 --> 00:03:30.036 A:middle
Now there has been a lot of

00:03:30.036 --> 00:03:31.606 A:middle
activity in this area over the

00:03:31.606 --> 00:03:32.236 A:middle
last year.

00:03:32.956 --> 00:03:34.436 A:middle
And this is how the picture

00:03:34.436 --> 00:03:34.976 A:middle
looks now.

00:03:35.686 --> 00:03:38.936 A:middle
So as you can see, there are

00:03:38.936 --> 00:03:42.416 A:middle
many more converters out there.

00:03:42.906 --> 00:03:44.526 A:middle
And you really do have a lot of

00:03:44.706 --> 00:03:46.306 A:middle
choice to choose your training

00:03:46.306 --> 00:03:47.016 A:middle
framework now.

00:03:48.336 --> 00:03:50.346 A:middle
And all of these converters are

00:03:50.346 --> 00:03:52.546 A:middle
built on top of Core ML Tools.

00:03:52.856 --> 00:03:57.326 A:middle
Now, I do want to highlight a

00:03:57.326 --> 00:03:58.816 A:middle
couple of different converters

00:03:58.816 --> 00:03:59.056 A:middle
here.


00:04:01.186 --> 00:04:03.566 A:middle
Last year, we collaborated with

00:04:03.566 --> 00:04:05.286 A:middle
Google and released the

00:04:05.286 --> 00:04:06.096 A:middle
TensorFlow converter.

00:04:06.096 --> 00:04:08.686 A:middle
So that was exciting.

00:04:10.396 --> 00:04:12.416 A:middle
As you know, TensorFlow is quite

00:04:12.416 --> 00:04:14.216 A:middle
popular with researchers who try

00:04:14.216 --> 00:04:16.586 A:middle
out new layers so we recently

00:04:16.586 --> 00:04:18.336 A:middle
added support for custom layers

00:04:18.555 --> 00:04:19.396 A:middle
into the converter.

00:04:20.776 --> 00:04:22.976 A:middle
And TensorFlow recently released

00:04:22.976 --> 00:04:25.236 A:middle
support for quantization during

00:04:25.376 --> 00:04:27.506 A:middle
training and that's Core ML 2

00:04:27.506 --> 00:04:28.676 A:middle
supports quantization.

00:04:29.086 --> 00:04:30.586 A:middle
This feature will be added soon

00:04:30.586 --> 00:04:31.196 A:middle
to the converter.

00:04:33.186 --> 00:04:34.616 A:middle
Another exciting partnership we

00:04:34.616 --> 00:04:37.286 A:middle
had was with Facebook and

00:04:37.326 --> 00:04:37.836 A:middle
Prisma.

00:04:38.336 --> 00:04:40.406 A:middle
And this resulted in the ONNX

00:04:40.406 --> 00:04:40.866 A:middle
converter.

00:04:42.216 --> 00:04:43.646 A:middle
The nice thing about ONNX is

00:04:43.646 --> 00:04:48.056 A:middle
that now you have access to a

00:04:48.056 --> 00:04:49.376 A:middle
bunch of different training

00:04:49.376 --> 00:04:50.946 A:middle
libraries that can all be

00:04:50.946 --> 00:04:53.106 A:middle
converted to Core ML using the

00:04:53.106 --> 00:04:54.136 A:middle
new ONNX converter.

00:04:54.136 --> 00:04:58.956 A:middle
So that was a quick wrap-up of

00:04:58.956 --> 00:05:00.446 A:middle
Core ML Tools ecosystem.


00:04:58.956 --> 00:05:00.446 A:middle
Core ML Tools ecosystem.

00:05:00.866 --> 00:05:02.496 A:middle
Now to talk about quantization,

00:05:02.656 --> 00:05:04.446 A:middle
I would like to invite my friend

00:05:04.446 --> 00:05:05.476 A:middle
Sohaib on stage.

00:05:06.516 --> 00:05:13.596 A:middle
[ Applause ]

00:05:14.096 --> 00:05:14.876 A:middle
&gt;&gt; Good morning, everyone.

00:05:15.186 --> 00:05:15.976 A:middle
My name is Sohaib.

00:05:16.206 --> 00:05:17.426 A:middle
I'm an engineer in the Core ML

00:05:17.426 --> 00:05:17.716 A:middle
team.

00:05:18.106 --> 00:05:19.156 A:middle
And today we're going to be

00:05:19.156 --> 00:05:20.256 A:middle
taking a look at new

00:05:20.256 --> 00:05:21.766 A:middle
quantization utilities in Core

00:05:21.766 --> 00:05:23.026 A:middle
ML Tools 2.0.

00:05:28.136 --> 00:05:29.826 A:middle
Core ML Tools 2.0 has support

00:05:29.826 --> 00:05:31.206 A:middle
for the latest Core ML model

00:05:31.206 --> 00:05:32.366 A:middle
format specification.

00:05:32.796 --> 00:05:34.306 A:middle
It also has utilities which make

00:05:34.306 --> 00:05:36.166 A:middle
it really easy for you to add

00:05:36.226 --> 00:05:38.016 A:middle
flexible shapes and quantize in

00:05:38.016 --> 00:05:38.876 A:middle
your own network machine

00:05:38.876 --> 00:05:39.486 A:middle
learning models.

00:05:40.296 --> 00:05:41.416 A:middle
Using these great new features

00:05:41.416 --> 00:05:43.046 A:middle
in Core ML, you can not only

00:05:43.046 --> 00:05:44.266 A:middle
reduce the size of your models.

00:05:44.576 --> 00:05:46.366 A:middle
But also reduce the number of

00:05:46.366 --> 00:05:48.126 A:middle
models in your app, reducing the

00:05:48.126 --> 00:05:49.536 A:middle
footprint of your app.

00:05:50.356 --> 00:05:51.896 A:middle
Now let's start off by taking a

00:05:51.896 --> 00:05:53.316 A:middle
look at quantization.

00:05:54.846 --> 00:05:55.876 A:middle
Core ML Tools supports

00:05:55.906 --> 00:05:57.166 A:middle
post-training quantization.

00:05:57.666 --> 00:05:59.186 A:middle
We start off with a Core ML

00:05:59.286 --> 00:06:00.976 A:middle
neural network model which has


00:05:59.286 --> 00:06:00.976 A:middle
neural network model which has

00:06:01.106 --> 00:06:03.336 A:middle
32-bit float weight parameters.

00:06:03.766 --> 00:06:05.276 A:middle
And we use Core ML Tools to

00:06:05.276 --> 00:06:07.136 A:middle
quantize the weights for this

00:06:07.136 --> 00:06:07.466 A:middle
model.

00:06:08.126 --> 00:06:09.556 A:middle
The resulting model is smaller

00:06:09.556 --> 00:06:10.026 A:middle
in size.

00:06:10.836 --> 00:06:12.346 A:middle
Now size reduction of the model

00:06:12.346 --> 00:06:13.606 A:middle
is directly dependent on the

00:06:13.606 --> 00:06:15.296 A:middle
number of bits we quantize our

00:06:15.296 --> 00:06:15.886 A:middle
model to.

00:06:17.836 --> 00:06:19.316 A:middle
Now, many of us may be wondering

00:06:19.466 --> 00:06:21.046 A:middle
what exactly is quantization?

00:06:21.336 --> 00:06:22.616 A:middle
And how can it reduce the size

00:06:22.616 --> 00:06:23.116 A:middle
of my models?

00:06:24.036 --> 00:06:25.786 A:middle
Let's step back and take a peek

00:06:25.786 --> 00:06:26.246 A:middle
under the hood.

00:06:30.386 --> 00:06:31.986 A:middle
Neural networks are composed of

00:06:31.986 --> 00:06:32.336 A:middle
layers.

00:06:32.786 --> 00:06:33.906 A:middle
And these layers can be thought

00:06:33.906 --> 00:06:35.336 A:middle
of as mathematical functions.

00:06:35.776 --> 00:06:37.356 A:middle
And these mathematical functions

00:06:37.466 --> 00:06:38.876 A:middle
have parameters called weights.

00:06:39.076 --> 00:06:41.116 A:middle
And these weights are usually

00:06:41.116 --> 00:06:42.716 A:middle
stored as 32-bit floats.

00:06:43.426 --> 00:06:45.886 A:middle
Now in our previous session, we

00:06:45.886 --> 00:06:47.206 A:middle
took a look at ResNet50.

00:06:47.636 --> 00:06:49.036 A:middle
A popular machine-learning model

00:06:49.036 --> 00:06:50.116 A:middle
which is used for image

00:06:50.116 --> 00:06:51.536 A:middle
classification amongst other

00:06:51.536 --> 00:06:51.816 A:middle
things.

00:06:52.476 --> 00:06:53.576 A:middle
Now this particular model has

00:06:53.576 --> 00:06:55.696 A:middle
over 25 million weight

00:06:55.696 --> 00:06:56.286 A:middle
parameters.

00:06:56.636 --> 00:06:58.286 A:middle
So you can imagine, if you could

00:06:58.286 --> 00:07:00.206 A:middle
somehow represent these param --


00:06:58.286 --> 00:07:00.206 A:middle
somehow represent these param --

00:07:00.516 --> 00:07:02.026 A:middle
these parameters using a fewer

00:07:02.026 --> 00:07:03.456 A:middle
number of bits, we can

00:07:03.456 --> 00:07:04.676 A:middle
drastically reduce the size of

00:07:04.676 --> 00:07:05.096 A:middle
this model.

00:07:05.776 --> 00:07:08.626 A:middle
In fact, this process is called

00:07:08.906 --> 00:07:09.676 A:middle
quantization.

00:07:10.476 --> 00:07:12.116 A:middle
In quantization, we take the

00:07:12.116 --> 00:07:13.846 A:middle
weights for our layers which

00:07:13.846 --> 00:07:15.086 A:middle
[inaudible] to minimum and to

00:07:15.086 --> 00:07:18.816 A:middle
maximum value and we map them to

00:07:18.816 --> 00:07:19.736 A:middle
unsigned integers.

00:07:20.936 --> 00:07:22.706 A:middle
Now for APIC quantization, we

00:07:22.706 --> 00:07:24.526 A:middle
map these values from a range of

00:07:24.586 --> 00:07:26.686 A:middle
0 to 55.

00:07:27.106 --> 00:07:28.876 A:middle
For 7-bit quantization, we map

00:07:28.926 --> 00:07:31.826 A:middle
them from 0 to 127, all the way

00:07:31.826 --> 00:07:32.556 A:middle
down to 1 bit.

00:07:32.786 --> 00:07:34.076 A:middle
Where we map these weights as

00:07:34.076 --> 00:07:35.486 A:middle
either zeros or ones.

00:07:36.106 --> 00:07:37.606 A:middle
Since we're using fewer bits to

00:07:37.606 --> 00:07:38.856 A:middle
represent the same information,

00:07:39.176 --> 00:07:40.476 A:middle
we reduce the size of our model.

00:07:42.406 --> 00:07:44.786 A:middle
Great. Now many of you may have

00:07:44.786 --> 00:07:46.276 A:middle
noticed that we're mapping

00:07:46.276 --> 00:07:47.246 A:middle
floats to integers.

00:07:47.646 --> 00:07:48.926 A:middle
And you may have come to the

00:07:48.926 --> 00:07:50.046 A:middle
conclusion that maybe there's

00:07:50.046 --> 00:07:52.366 A:middle
some accuracy loss in this

00:07:52.366 --> 00:07:52.786 A:middle
mapping.

00:07:53.326 --> 00:07:53.836 A:middle
That's true.

00:07:54.736 --> 00:07:56.316 A:middle
The rule of thumb is the lower

00:07:56.316 --> 00:07:57.416 A:middle
the number of bits you quantize

00:07:57.416 --> 00:07:59.226 A:middle
your model to, the more of a hit

00:07:59.226 --> 00:08:00.346 A:middle
our model takes in terms of


00:07:59.226 --> 00:08:00.346 A:middle
our model takes in terms of

00:08:00.346 --> 00:08:00.786 A:middle
accuracy.

00:08:00.786 --> 00:08:02.206 A:middle
And we'll get back to that in a

00:08:02.206 --> 00:08:02.436 A:middle
bit.

00:08:03.556 --> 00:08:04.716 A:middle
So that's an overview of

00:08:04.716 --> 00:08:05.326 A:middle
quantization.

00:08:06.016 --> 00:08:06.936 A:middle
But the question remains.

00:08:07.246 --> 00:08:08.716 A:middle
How do we obtain this mapping?

00:08:09.506 --> 00:08:10.796 A:middle
Well, there are many popular

00:08:10.796 --> 00:08:12.386 A:middle
algorithms and techniques out

00:08:12.386 --> 00:08:13.346 A:middle
there which help you to do this.

00:08:13.716 --> 00:08:15.536 A:middle
And Core ML supports two of the

00:08:15.536 --> 00:08:17.456 A:middle
most popular ones: linear

00:08:17.456 --> 00:08:19.026 A:middle
quantization and lookup table

00:08:19.026 --> 00:08:19.706 A:middle
quantization.

00:08:20.626 --> 00:08:21.986 A:middle
Let's have a brief overview.

00:08:26.276 --> 00:08:27.566 A:middle
Linear quantization is an

00:08:27.566 --> 00:08:29.116 A:middle
algorithm in which you map these

00:08:29.156 --> 00:08:31.596 A:middle
full parameters equally.

00:08:32.946 --> 00:08:34.736 A:middle
The quantization is parametrized

00:08:34.736 --> 00:08:37.106 A:middle
by a scale and by values.

00:08:37.106 --> 00:08:38.775 A:middle
And these values are calculated

00:08:38.885 --> 00:08:40.416 A:middle
based on the parameters of the

00:08:40.416 --> 00:08:42.405 A:middle
layers that we're quantizing.

00:08:43.086 --> 00:08:44.866 A:middle
Now and a really intuitive way

00:08:44.926 --> 00:08:46.946 A:middle
to see how this mapping works is

00:08:46.946 --> 00:08:47.896 A:middle
if we take a step back.

00:08:47.896 --> 00:08:49.356 A:middle
And see how we would go back

00:08:49.386 --> 00:08:50.756 A:middle
from our quantized weights which

00:08:50.756 --> 00:08:52.236 A:middle
are at the bottom back to our

00:08:52.236 --> 00:08:53.106 A:middle
original float weights.

00:08:53.876 --> 00:08:55.416 A:middle
In linear quantization, we would

00:08:55.416 --> 00:08:57.276 A:middle
simply multiply our quantized

00:08:57.276 --> 00:08:58.846 A:middle
weights with the scale parameter

00:08:59.196 --> 00:08:59.886 A:middle
and add the bias.


00:09:02.186 --> 00:09:03.346 A:middle
The second quantization

00:09:03.346 --> 00:09:04.726 A:middle
technique that Core ML supports

00:09:05.046 --> 00:09:06.356 A:middle
is lookup table quantization.

00:09:07.226 --> 00:09:08.456 A:middle
And this technique is exactly

00:09:08.456 --> 00:09:09.166 A:middle
what it sounds like.

00:09:09.716 --> 00:09:10.816 A:middle
We construct a lookup table.

00:09:11.986 --> 00:09:13.396 A:middle
Now again it's helpful if we

00:09:13.396 --> 00:09:14.596 A:middle
imagine how we would go back

00:09:14.596 --> 00:09:15.946 A:middle
from our quantized weights back

00:09:15.946 --> 00:09:16.676 A:middle
to our original weights.

00:09:17.016 --> 00:09:19.056 A:middle
And in this case, the quantized

00:09:19.056 --> 00:09:21.006 A:middle
weights are simply indices back

00:09:21.006 --> 00:09:22.746 A:middle
into our lookup table.

00:09:24.036 --> 00:09:25.326 A:middle
Now, if you notice, unlike

00:09:25.326 --> 00:09:26.546 A:middle
linear quantization, we have the

00:09:26.546 --> 00:09:28.986 A:middle
ability to move our quantized

00:09:28.986 --> 00:09:29.536 A:middle
weights around.

00:09:29.696 --> 00:09:30.956 A:middle
They don't have to be spaced out

00:09:30.956 --> 00:09:31.866 A:middle
in a linear fashion.

00:09:33.906 --> 00:09:35.806 A:middle
So to recap, Core ML Tools

00:09:35.806 --> 00:09:38.746 A:middle
supports linear quantization and

00:09:38.746 --> 00:09:40.126 A:middle
lookup table quantization where

00:09:40.126 --> 00:09:41.266 A:middle
we start off with a full

00:09:41.266 --> 00:09:42.576 A:middle
precision neural network model.

00:09:42.936 --> 00:09:43.906 A:middle
And quantize the weights for

00:09:43.906 --> 00:09:45.396 A:middle
that model using the utilities.

00:09:46.226 --> 00:09:48.276 A:middle
Now you may be wondering well

00:09:48.276 --> 00:09:49.646 A:middle
great, I can reduce the size of

00:09:49.646 --> 00:09:50.106 A:middle
my model.

00:09:50.836 --> 00:09:52.326 A:middle
But how do I figure out the

00:09:52.466 --> 00:09:53.956 A:middle
parameters for my quantization?

00:09:54.436 --> 00:09:55.296 A:middle
If I'm doing linear

00:09:55.296 --> 00:09:56.706 A:middle
quantization, how do I figure

00:09:56.706 --> 00:09:57.676 A:middle
out my scale and bias?

00:09:58.366 --> 00:09:59.336 A:middle
If I'm doing lookup table

00:09:59.336 --> 00:10:01.326 A:middle
quantization, how do I construct


00:09:59.336 --> 00:10:01.326 A:middle
quantization, how do I construct

00:10:01.326 --> 00:10:02.036 A:middle
my lookup table?

00:10:03.076 --> 00:10:04.426 A:middle
I'm here to tell you that you

00:10:04.426 --> 00:10:05.946 A:middle
don't have to worry about any of

00:10:05.986 --> 00:10:06.236 A:middle
that.

00:10:06.986 --> 00:10:08.536 A:middle
All you do is decide on the

00:10:08.536 --> 00:10:09.546 A:middle
number of bits you want to

00:10:09.546 --> 00:10:10.596 A:middle
quantize your model to.

00:10:10.876 --> 00:10:11.926 A:middle
And decide on the algorithm you

00:10:11.926 --> 00:10:13.706 A:middle
want to use, and let Core ML

00:10:13.706 --> 00:10:15.166 A:middle
Tools do the rest.

00:10:16.106 --> 00:10:16.576 A:middle
In fact --

00:10:17.516 --> 00:10:22.216 A:middle
[ Applause ]

00:10:22.716 --> 00:10:24.756 A:middle
In fact, it's so simple to take

00:10:24.756 --> 00:10:26.186 A:middle
a Core ML neural network model.

00:10:26.286 --> 00:10:27.036 A:middle
And quantize it.

00:10:27.446 --> 00:10:29.116 A:middle
Then we can do it in a few lines

00:10:29.116 --> 00:10:29.806 A:middle
of Python code.

00:10:29.996 --> 00:10:31.286 A:middle
But why stand here and talk

00:10:31.286 --> 00:10:32.396 A:middle
about it when we can show you a

00:10:32.396 --> 00:10:32.676 A:middle
demo?

00:10:40.406 --> 00:10:42.296 A:middle
So for the purposes of this

00:10:42.296 --> 00:10:44.346 A:middle
demo, I'm going to need a neural

00:10:44.346 --> 00:10:45.596 A:middle
network in the Core ML model

00:10:45.596 --> 00:10:46.026 A:middle
format.

00:10:46.706 --> 00:10:47.846 A:middle
Now, as my colleague Aseem

00:10:47.846 --> 00:10:49.376 A:middle
mentioned, a great place to find

00:10:49.376 --> 00:10:50.686 A:middle
these models is on the Core ML

00:10:50.686 --> 00:10:51.716 A:middle
machine learning home page.

00:10:52.056 --> 00:10:52.896 A:middle
And I've gone ahead and

00:10:52.896 --> 00:10:54.016 A:middle
downloaded one of the models

00:10:54.106 --> 00:10:54.846 A:middle
from that page.

00:10:55.306 --> 00:10:56.706 A:middle
So this model's called

00:10:56.706 --> 00:10:57.176 A:middle
SqueezeNet.

00:10:57.176 --> 00:10:58.146 A:middle
And let's go ahead and open it

00:10:58.146 --> 00:10:58.296 A:middle
up.

00:10:59.736 --> 00:11:02.356 A:middle
As we can see, this model is 5


00:10:59.736 --> 00:11:02.356 A:middle
As we can see, this model is 5

00:11:02.356 --> 00:11:03.226 A:middle
megabytes in size.

00:11:03.616 --> 00:11:05.986 A:middle
It has a input which is an image

00:11:06.036 --> 00:11:08.456 A:middle
of 227 by 227 pixels.

00:11:08.676 --> 00:11:09.796 A:middle
And it has two outputs.

00:11:10.476 --> 00:11:11.776 A:middle
One of the outputs is the class

00:11:11.906 --> 00:11:13.696 A:middle
label which is a string, and

00:11:13.696 --> 00:11:15.996 A:middle
this is the most likely label

00:11:15.996 --> 00:11:17.416 A:middle
for the, for the input image.

00:11:17.736 --> 00:11:19.386 A:middle
And the second output is a

00:11:19.386 --> 00:11:20.526 A:middle
mapping of strings to

00:11:20.526 --> 00:11:23.386 A:middle
probabilities given that if we

00:11:23.386 --> 00:11:24.756 A:middle
pass an image, it's going to be

00:11:24.936 --> 00:11:25.976 A:middle
a list of probabilities of what

00:11:25.976 --> 00:11:26.666 A:middle
that image may be.

00:11:28.826 --> 00:11:30.126 A:middle
Now let's start quantizing this

00:11:30.126 --> 00:11:30.486 A:middle
model.

00:11:30.736 --> 00:11:32.696 A:middle
So the first thing I want to do

00:11:32.696 --> 00:11:34.066 A:middle
is I want to get into a Python

00:11:34.066 --> 00:11:34.556 A:middle
environment.

00:11:34.986 --> 00:11:36.406 A:middle
Now a Jupyter Notebook is one

00:11:36.406 --> 00:11:37.206 A:middle
such environment that I'm

00:11:37.206 --> 00:11:37.776 A:middle
comfortable with.

00:11:38.156 --> 00:11:39.086 A:middle
So I'm going to go ahead and

00:11:39.086 --> 00:11:39.526 A:middle
open that up.

00:11:46.696 --> 00:11:49.346 A:middle
Let's open up a new notebook and

00:11:49.346 --> 00:11:50.496 A:middle
zoom in on that.

00:11:51.346 --> 00:11:53.476 A:middle
Alright. So let's start off by

00:11:53.476 --> 00:11:54.926 A:middle
importing Core ML Tools.

00:11:58.476 --> 00:11:59.536 A:middle
Let's run that.

00:11:59.536 --> 00:12:01.186 A:middle
Now the second thing I want to


00:11:59.536 --> 00:12:01.186 A:middle
Now the second thing I want to

00:12:01.186 --> 00:12:02.576 A:middle
do is I want to import all the

00:12:02.576 --> 00:12:03.906 A:middle
new quantization utilities that

00:12:03.906 --> 00:12:04.966 A:middle
we have in Core ML Tools.

00:12:05.026 --> 00:12:06.976 A:middle
And we do that by running this.

00:12:16.206 --> 00:12:17.516 A:middle
And now we need to load up the

00:12:17.516 --> 00:12:18.866 A:middle
model which we want to quantize.

00:12:19.186 --> 00:12:20.376 A:middle
And we just saw the SqueezeNet

00:12:20.376 --> 00:12:21.126 A:middle
model a minute okay.

00:12:21.126 --> 00:12:22.106 A:middle
We're going to go ahead and get

00:12:22.106 --> 00:12:22.976 A:middle
an instance of that model.

00:12:32.056 --> 00:12:37.716 A:middle
Send this to my desktop.

00:12:38.086 --> 00:12:40.416 A:middle
Great. Now to quantize this

00:12:40.416 --> 00:12:41.996 A:middle
model, we just need to make one

00:12:41.996 --> 00:12:43.056 A:middle
simple API call.

00:12:43.516 --> 00:12:44.666 A:middle
And let's try a linear,

00:12:44.666 --> 00:12:46.166 A:middle
quantizing this model using

00:12:46.166 --> 00:12:47.256 A:middle
linear quantization.

00:12:51.176 --> 00:12:53.166 A:middle
And its API is simply called

00:12:53.456 --> 00:12:54.346 A:middle
quantize weights.

00:12:54.816 --> 00:12:56.216 A:middle
And the first parameter we pass

00:12:56.216 --> 00:12:58.236 A:middle
in is the original model which

00:12:58.236 --> 00:12:59.396 A:middle
you just loaded up.

00:12:59.576 --> 00:13:00.996 A:middle
The number of bits we want to


00:12:59.576 --> 00:13:00.996 A:middle
The number of bits we want to

00:13:00.996 --> 00:13:02.066 A:middle
quantize our model to.

00:13:02.226 --> 00:13:03.536 A:middle
In this case, it's 8 bits.

00:13:04.716 --> 00:13:05.996 A:middle
And the quantization algorithm

00:13:05.996 --> 00:13:06.666 A:middle
we want to use.

00:13:07.066 --> 00:13:08.366 A:middle
Let's try linear quantization.

00:13:09.866 --> 00:13:11.436 A:middle
Now what's happening is that the

00:13:11.436 --> 00:13:12.996 A:middle
utility is iterating over all of

00:13:12.996 --> 00:13:14.586 A:middle
the layers of the linear

00:13:14.586 --> 00:13:15.126 A:middle
networks.

00:13:15.126 --> 00:13:16.876 A:middle
And is quantizing all the

00:13:16.876 --> 00:13:17.726 A:middle
weights in those layers.

00:13:18.186 --> 00:13:18.756 A:middle
And we're finished.

00:13:20.496 --> 00:13:23.246 A:middle
Now, if you recall a few moments

00:13:23.246 --> 00:13:24.806 A:middle
ago I mentioned that quantizing

00:13:24.806 --> 00:13:26.676 A:middle
our model had an associated loss

00:13:26.676 --> 00:13:27.276 A:middle
in accuracy.

00:13:27.586 --> 00:13:28.966 A:middle
So we want to know how our

00:13:28.966 --> 00:13:30.746 A:middle
quantized model stacks up to the

00:13:30.746 --> 00:13:31.286 A:middle
original model.

00:13:31.476 --> 00:13:33.236 A:middle
And the easiest way of doing

00:13:33.236 --> 00:13:34.706 A:middle
this is taking some data,

00:13:35.236 --> 00:13:37.936 A:middle
passing and getting inference on

00:13:37.936 --> 00:13:39.556 A:middle
that data using our original

00:13:39.556 --> 00:13:39.916 A:middle
model.

00:13:40.596 --> 00:13:42.296 A:middle
And doing the same inference on

00:13:42.296 --> 00:13:43.406 A:middle
the same data using our

00:13:43.406 --> 00:13:45.206 A:middle
quantized model and comparing

00:13:45.206 --> 00:13:46.416 A:middle
the predictions from that model.

00:13:47.146 --> 00:13:48.436 A:middle
And seeing how well they agree.

00:13:49.066 --> 00:13:50.296 A:middle
Core ML Tools has utilities

00:13:50.296 --> 00:13:51.226 A:middle
which help you to do that.

00:13:51.486 --> 00:13:53.096 A:middle
And we can do that by making

00:13:53.096 --> 00:13:54.176 A:middle
this call which is called

00:13:54.176 --> 00:13:55.596 A:middle
compare models.

00:13:56.236 --> 00:13:57.616 A:middle
We pass in our full precision

00:13:57.616 --> 00:13:59.796 A:middle
model, and we pass in our model

00:13:59.796 --> 00:14:01.136 A:middle
which we had just quantized.


00:13:59.796 --> 00:14:01.136 A:middle
which we had just quantized.

00:14:01.956 --> 00:14:03.126 A:middle
And because this model is a

00:14:03.126 --> 00:14:05.976 A:middle
simple image classifier which it

00:14:05.976 --> 00:14:07.386 A:middle
only has one image inputs.

00:14:08.136 --> 00:14:09.026 A:middle
We, we have a convenience

00:14:09.026 --> 00:14:09.426 A:middle
utility.

00:14:09.426 --> 00:14:11.296 A:middle
So we can just pass in a folder

00:14:11.296 --> 00:14:12.766 A:middle
containing sample data images.

00:14:13.146 --> 00:14:14.746 A:middle
Now on my desktop here, I have a

00:14:14.746 --> 00:14:16.216 A:middle
folder with a set of images

00:14:16.216 --> 00:14:17.156 A:middle
which are relevant for my

00:14:17.156 --> 00:14:17.816 A:middle
application.

00:14:18.146 --> 00:14:18.966 A:middle
So I'm going to go ahead and

00:14:18.966 --> 00:14:22.186 A:middle
pass a path to this folder as my

00:14:22.186 --> 00:14:22.816 A:middle
[inaudible] parameter.

00:14:28.066 --> 00:14:29.776 A:middle
Great. So now we see we're

00:14:29.776 --> 00:14:31.306 A:middle
analyzing all the images in that

00:14:31.306 --> 00:14:31.736 A:middle
folder.

00:14:31.766 --> 00:14:33.796 A:middle
We're running inference on the,

00:14:33.796 --> 00:14:35.546 A:middle
we're using full prediction or

00:14:35.546 --> 00:14:36.486 A:middle
full precision model.

00:14:36.576 --> 00:14:37.676 A:middle
And we're running inference on

00:14:37.676 --> 00:14:38.606 A:middle
our quantized model.

00:14:38.636 --> 00:14:39.616 A:middle
And we're comparing our two

00:14:39.616 --> 00:14:40.176 A:middle
predictions.

00:14:41.436 --> 00:14:42.376 A:middle
So we seem to have finished

00:14:42.376 --> 00:14:42.616 A:middle
that.

00:14:42.616 --> 00:14:45.066 A:middle
And you can see our Top 1

00:14:45.066 --> 00:14:46.946 A:middle
Agreement is 94.8%.

00:14:47.766 --> 00:14:50.186 A:middle
Not bad. Now what does this Top

00:14:50.186 --> 00:14:50.996 A:middle
1 Agreement mean?

00:14:51.586 --> 00:14:52.996 A:middle
This means that when I pass in

00:14:52.996 --> 00:14:55.896 A:middle
my original model, that image of

00:14:55.896 --> 00:14:57.546 A:middle
a dog for example, and it

00:14:57.546 --> 00:14:58.576 A:middle
predicted that this image was a

00:14:58.576 --> 00:14:58.916 A:middle
dog.

00:14:59.226 --> 00:15:00.766 A:middle
My quantized model did the same.


00:14:59.226 --> 00:15:00.766 A:middle
My quantized model did the same.

00:15:00.766 --> 00:15:03.476 A:middle
And that happened over 98, 94.8%

00:15:03.476 --> 00:15:04.126 A:middle
of the data set.

00:15:05.846 --> 00:15:07.576 A:middle
So I can go ahead and use this

00:15:07.576 --> 00:15:08.276 A:middle
model in my app.

00:15:08.746 --> 00:15:10.416 A:middle
But I want to see if other

00:15:10.416 --> 00:15:11.576 A:middle
quantization techniques work

00:15:11.576 --> 00:15:12.546 A:middle
better on this model.

00:15:13.526 --> 00:15:15.146 A:middle
As I mentioned, Core ML supports

00:15:15.176 --> 00:15:16.496 A:middle
two types of quantization

00:15:16.496 --> 00:15:16.936 A:middle
techniques.

00:15:17.096 --> 00:15:18.686 A:middle
Linear quantization and lookup

00:15:18.686 --> 00:15:19.546 A:middle
table quantization.

00:15:19.956 --> 00:15:21.636 A:middle
So let's go ahead and try and

00:15:21.636 --> 00:15:23.326 A:middle
quantize this model using lookup

00:15:23.326 --> 00:15:24.136 A:middle
table quantization.

00:15:30.936 --> 00:15:31.996 A:middle
Again, we pass in an original

00:15:31.996 --> 00:15:33.496 A:middle
model, the number of bits we

00:15:33.496 --> 00:15:34.726 A:middle
want to quantize our model to.

00:15:35.666 --> 00:15:37.306 A:middle
And our quantization techniques.

00:15:37.896 --> 00:15:39.976 A:middle
Oops, made a typo there.

00:15:48.156 --> 00:15:49.816 A:middle
Let's go ahead and run this.

00:15:50.656 --> 00:15:52.746 A:middle
Now, k-means is a simple

00:15:52.746 --> 00:15:54.136 A:middle
clustering algorithm which

00:15:54.196 --> 00:15:55.856 A:middle
approximates the distribution of

00:15:55.856 --> 00:15:56.346 A:middle
our weights.

00:15:56.606 --> 00:15:58.566 A:middle
And using this distribution, we

00:15:58.566 --> 00:16:00.606 A:middle
can construct the lookup table


00:15:58.566 --> 00:16:00.606 A:middle
can construct the lookup table

00:16:00.806 --> 00:16:02.046 A:middle
for our weights.

00:16:02.546 --> 00:16:04.116 A:middle
And what we're doing over here

00:16:04.116 --> 00:16:05.866 A:middle
is that we're iterating over all

00:16:05.866 --> 00:16:06.556 A:middle
the layers in the neural

00:16:06.556 --> 00:16:06.916 A:middle
network.

00:16:07.306 --> 00:16:08.496 A:middle
And we're quantizing and we're

00:16:08.496 --> 00:16:09.586 A:middle
figuring out the lookup table

00:16:09.936 --> 00:16:11.426 A:middle
for that particular layer.

00:16:12.226 --> 00:16:14.626 A:middle
Now, if you're an expert and you

00:16:14.626 --> 00:16:15.836 A:middle
know that your model, you know

00:16:15.836 --> 00:16:17.586 A:middle
your model architecture and you

00:16:17.586 --> 00:16:18.826 A:middle
know that k-means is not the

00:16:18.826 --> 00:16:20.426 A:middle
algorithm for you, you have the

00:16:20.426 --> 00:16:22.456 A:middle
flexibility of passing in your

00:16:22.456 --> 00:16:24.996 A:middle
own custom function instead of

00:16:24.996 --> 00:16:26.456 A:middle
this algorithm and the utility

00:16:26.456 --> 00:16:28.076 A:middle
will use your custom function to

00:16:28.076 --> 00:16:29.126 A:middle
actually construct the lookup

00:16:29.126 --> 00:16:29.436 A:middle
table.

00:16:30.866 --> 00:16:32.686 A:middle
So we finished quantizing this

00:16:32.686 --> 00:16:34.036 A:middle
model again using the lookup

00:16:34.036 --> 00:16:34.646 A:middle
table approach.

00:16:35.066 --> 00:16:36.596 A:middle
And now let's see how well this

00:16:36.596 --> 00:16:38.006 A:middle
model compares with our original

00:16:38.006 --> 00:16:38.306 A:middle
model.

00:16:38.626 --> 00:16:40.016 A:middle
So once again we call our

00:16:40.016 --> 00:16:41.286 A:middle
compare model's API.

00:16:42.276 --> 00:16:43.766 A:middle
We pass in our original model

00:16:43.766 --> 00:16:46.036 A:middle
and we pass in our lookup table

00:16:46.036 --> 00:16:46.366 A:middle
model.

00:16:47.036 --> 00:16:51.036 A:middle
And again we pass in our sample

00:16:51.036 --> 00:16:51.656 A:middle
data folder.

00:16:52.266 --> 00:16:56.406 A:middle
Again, we run inference over all

00:16:56.406 --> 00:16:58.616 A:middle
the images using both the

00:16:59.596 --> 00:17:00.866 A:middle
original model and the quantized


00:16:59.596 --> 00:17:00.866 A:middle
original model and the quantized

00:17:00.866 --> 00:17:01.166 A:middle
model.

00:17:01.686 --> 00:17:02.936 A:middle
And we see this time we're

00:17:02.936 --> 00:17:04.286 A:middle
getting a much better, little

00:17:04.286 --> 00:17:05.726 A:middle
bit better Top 1 Agreement.

00:17:06.256 --> 00:17:08.425 A:middle
Now for this model, we see that

00:17:08.425 --> 00:17:09.526 A:middle
lookup table was the right way

00:17:09.526 --> 00:17:09.856 A:middle
to go.

00:17:10.386 --> 00:17:11.016 A:middle
But again, this is

00:17:11.016 --> 00:17:12.226 A:middle
model-dependent and for other

00:17:12.226 --> 00:17:13.886 A:middle
models, linear may be the way.

00:17:14.136 --> 00:17:15.705 A:middle
So now that we're happy with

00:17:15.705 --> 00:17:17.536 A:middle
this and we see that this is

00:17:17.685 --> 00:17:18.566 A:middle
good enough for at least my

00:17:18.566 --> 00:17:19.836 A:middle
application, let's go ahead and

00:17:19.836 --> 00:17:21.406 A:middle
save this model out.

00:17:22.695 --> 00:17:24.925 A:middle
We do that by causing or calling

00:17:24.925 --> 00:17:25.276 A:middle
save.

00:17:31.076 --> 00:17:31.616 A:middle
I'm going to give it the

00:17:31.616 --> 00:17:33.576 A:middle
creative name of Quantized

00:17:33.656 --> 00:17:34.186 A:middle
SqueezeNet.

00:17:41.076 --> 00:17:41.746 A:middle
And there we go.

00:17:42.236 --> 00:17:43.256 A:middle
We have a quantized model.

00:17:43.816 --> 00:17:45.616 A:middle
So this was an original model.

00:17:45.616 --> 00:17:46.806 A:middle
And we saw that it was 5

00:17:46.806 --> 00:17:47.626 A:middle
megabytes in size.

00:17:48.226 --> 00:17:49.726 A:middle
Let's open up our quantized

00:17:49.726 --> 00:17:50.036 A:middle
model.

00:17:50.036 --> 00:17:53.646 A:middle
And the first thing we notice

00:17:53.646 --> 00:17:54.826 A:middle
right off the bat is that this

00:17:54.826 --> 00:17:57.206 A:middle
model is only 1.3 megabytes in

00:17:57.206 --> 00:17:57.556 A:middle
size.

00:17:58.516 --> 00:18:03.636 A:middle
[ Applause ]


00:17:58.516 --> 00:18:03.636 A:middle
[ Applause ]

00:18:04.136 --> 00:18:06.306 A:middle
So if you notice, all the

00:18:06.306 --> 00:18:07.696 A:middle
details about, about our

00:18:07.696 --> 00:18:10.196 A:middle
quantized model are the same as

00:18:10.196 --> 00:18:10.866 A:middle
the original model.

00:18:11.226 --> 00:18:12.306 A:middle
It still takes in an image

00:18:12.306 --> 00:18:14.066 A:middle
input, and it still has two

00:18:14.066 --> 00:18:14.566 A:middle
outputs.

00:18:15.216 --> 00:18:17.046 A:middle
Now, if I had an app using this

00:18:17.046 --> 00:18:18.616 A:middle
model, what I could do as we saw

00:18:18.616 --> 00:18:19.476 A:middle
in the previous demo.

00:18:20.056 --> 00:18:21.256 A:middle
Is we could just drag this

00:18:21.256 --> 00:18:22.766 A:middle
quantized model into our app and

00:18:22.766 --> 00:18:23.876 A:middle
start using that instead.

00:18:23.876 --> 00:18:25.456 A:middle
And just like that, we reduce

00:18:25.456 --> 00:18:25.956 A:middle
the size of our app.

00:18:32.386 --> 00:18:34.036 A:middle
So that was quantization using

00:18:34.036 --> 00:18:34.906 A:middle
Core ML Tools.

00:18:38.696 --> 00:18:40.996 A:middle
To recap, we saw how easy it was

00:18:41.096 --> 00:18:43.246 A:middle
to use Core ML Tools to quantize

00:18:43.246 --> 00:18:43.676 A:middle
our model.

00:18:44.356 --> 00:18:46.466 A:middle
Using a simple API, we provided

00:18:47.406 --> 00:18:49.176 A:middle
our original model, the number

00:18:49.176 --> 00:18:50.446 A:middle
of bits we wanted to quantize

00:18:50.446 --> 00:18:51.746 A:middle
our model to, and the

00:18:51.746 --> 00:18:53.016 A:middle
quantization algorithm we wanted

00:18:53.016 --> 00:18:53.396 A:middle
to use.

00:18:54.236 --> 00:18:55.736 A:middle
We also saw that Core ML Tools

00:18:55.736 --> 00:18:57.736 A:middle
has utilities which help us to

00:18:57.736 --> 00:18:59.476 A:middle
compare our quantized model to

00:18:59.476 --> 00:19:01.126 A:middle
see how it performs against our


00:18:59.476 --> 00:19:01.126 A:middle
see how it performs against our

00:19:01.126 --> 00:19:01.696 A:middle
original model.

00:19:03.516 --> 00:19:05.986 A:middle
Now as we saw in the demo, there

00:19:05.986 --> 00:19:07.766 A:middle
is a loss of accuracy associated

00:19:07.766 --> 00:19:08.876 A:middle
with quantizing our model.

00:19:09.846 --> 00:19:12.056 A:middle
And this loss of accuracy is

00:19:12.056 --> 00:19:13.756 A:middle
highly model and data dependent.

00:19:14.636 --> 00:19:17.236 A:middle
Some models work well or perform

00:19:17.236 --> 00:19:18.076 A:middle
better than others after

00:19:18.076 --> 00:19:18.746 A:middle
quantization.

00:19:19.006 --> 00:19:20.446 A:middle
As a general rule of thumb

00:19:20.566 --> 00:19:22.006 A:middle
again, the lower the number of

00:19:22.006 --> 00:19:23.696 A:middle
bits we quantize our model to

00:19:24.376 --> 00:19:25.426 A:middle
the more of a precision hit we

00:19:25.426 --> 00:19:25.676 A:middle
take.

00:19:26.596 --> 00:19:28.226 A:middle
Now in the demo we saw that we

00:19:28.436 --> 00:19:30.256 A:middle
were able to use Core ML Tools

00:19:30.446 --> 00:19:31.806 A:middle
to compare our quantized model

00:19:31.806 --> 00:19:33.326 A:middle
and the original model using our

00:19:33.326 --> 00:19:34.586 A:middle
Top 1 Agreement metric.

00:19:35.466 --> 00:19:37.596 A:middle
But you have to figure out what

00:19:37.596 --> 00:19:38.756 A:middle
the relevant metric for your

00:19:38.756 --> 00:19:40.386 A:middle
model and your use case is and

00:19:40.386 --> 00:19:41.796 A:middle
validate that your quantize

00:19:41.796 --> 00:19:42.866 A:middle
model is acceptable.

00:19:44.026 --> 00:19:45.796 A:middle
Now in a previous session, we

00:19:45.796 --> 00:19:47.136 A:middle
took a look at a style transfer

00:19:47.136 --> 00:19:47.426 A:middle
demo.

00:19:48.226 --> 00:19:50.096 A:middle
And this network took in an

00:19:50.126 --> 00:19:52.566 A:middle
input image, and the output for

00:19:52.566 --> 00:19:54.106 A:middle
this network was a stylized

00:19:54.106 --> 00:19:54.496 A:middle
image.

00:19:55.506 --> 00:19:56.566 A:middle
Let's take a look at how this

00:19:56.736 --> 00:19:57.826 A:middle
model performs at different

00:19:57.826 --> 00:19:58.886 A:middle
levels of quantization.


00:20:00.126 --> 00:20:03.686 A:middle
So on the top, top left here,

00:20:03.836 --> 00:20:04.276 A:middle
your left.

00:20:04.766 --> 00:20:06.536 A:middle
We see that original model is 30

00:20:06.686 --> 00:20:09.306 A:middle
-- is 32 bits and it's 6.7

00:20:09.306 --> 00:20:10.186 A:middle
megabytes in size.

00:20:10.326 --> 00:20:12.106 A:middle
And our 8-bit linearly quantized

00:20:12.106 --> 00:20:14.146 A:middle
model is only 1.7 megabits in

00:20:14.146 --> 00:20:14.446 A:middle
size.

00:20:14.776 --> 00:20:16.016 A:middle
And we see that the performance

00:20:16.486 --> 00:20:18.256 A:middle
by visual inspection it's good

00:20:18.256 --> 00:20:19.406 A:middle
enough for my style transfer

00:20:19.406 --> 00:20:19.746 A:middle
demo.

00:20:20.756 --> 00:20:23.176 A:middle
Now we can see that even down to

00:20:23.176 --> 00:20:25.016 A:middle
4 bits, we don't lose out much

00:20:25.016 --> 00:20:25.896 A:middle
in the way of performance.

00:20:26.636 --> 00:20:27.736 A:middle
I would even argue that for my

00:20:27.736 --> 00:20:29.256 A:middle
app at least, the 3 bit will

00:20:29.256 --> 00:20:30.066 A:middle
work fine as well.

00:20:30.066 --> 00:20:32.946 A:middle
And we see at 2 bit, we start to

00:20:32.946 --> 00:20:34.886 A:middle
see a lot of artifacts and this

00:20:34.886 --> 00:20:36.406 A:middle
may not be the right model for

00:20:37.236 --> 00:20:37.303 A:middle
us.

00:20:38.906 --> 00:20:40.406 A:middle
And that was quantization using

00:20:40.406 --> 00:20:41.186 A:middle
Core ML Tools.

00:20:42.146 --> 00:20:43.056 A:middle
Now I'm going to hand it back to

00:20:43.056 --> 00:20:44.836 A:middle
Aseem who's going to talk about

00:20:44.836 --> 00:20:45.596 A:middle
custom conversion.

00:20:45.596 --> 00:20:45.936 A:middle
Thank you.

00:20:46.516 --> 00:20:51.846 A:middle
[ Applause ]

00:20:52.346 --> 00:20:52.976 A:middle
&gt;&gt; Thank you, Sohaib.

00:20:53.646 --> 00:20:56.136 A:middle
So I want to talk about a

00:20:56.266 --> 00:20:58.396 A:middle
feature that is essential to

00:20:58.396 --> 00:20:59.706 A:middle
keep pace with the machine

00:20:59.706 --> 00:21:00.546 A:middle
learning research that's


00:20:59.706 --> 00:21:00.546 A:middle
learning research that's

00:21:00.646 --> 00:21:02.616 A:middle
happening around us.

00:21:02.616 --> 00:21:04.746 A:middle
As you all know, the field of

00:21:04.746 --> 00:21:06.336 A:middle
machine learning is expanding

00:21:06.336 --> 00:21:07.146 A:middle
very rapidly.

00:21:07.766 --> 00:21:09.576 A:middle
So it's very critical for us at

00:21:09.696 --> 00:21:11.836 A:middle
Core ML to provide you with the

00:21:11.836 --> 00:21:14.046 A:middle
necessary software tools to help

00:21:14.046 --> 00:21:15.616 A:middle
with that.

00:21:15.886 --> 00:21:16.866 A:middle
Now let's take an example.

00:21:17.836 --> 00:21:19.986 A:middle
Let's say you are experimenting

00:21:19.986 --> 00:21:21.036 A:middle
with a new model that that is

00:21:21.036 --> 00:21:22.096 A:middle
not supported on Core ML.

00:21:22.426 --> 00:21:24.536 A:middle
Or let's say you have a neural

00:21:24.536 --> 00:21:27.636 A:middle
network that runs on Core ML but

00:21:27.636 --> 00:21:29.056 A:middle
maybe there's a layer or two

00:21:29.436 --> 00:21:31.606 A:middle
that Core ML does not have yet.

00:21:32.886 --> 00:21:34.606 A:middle
In that case, you should still

00:21:34.606 --> 00:21:37.466 A:middle
be able to use the power of Core

00:21:37.466 --> 00:21:38.446 A:middle
ML, right?

00:21:38.736 --> 00:21:40.386 A:middle
And the answer to that question

00:21:40.386 --> 00:21:40.766 A:middle
is yes.

00:21:41.666 --> 00:21:43.726 A:middle
And the feature of customization

00:21:43.986 --> 00:21:44.786 A:middle
will help you there.

00:21:45.756 --> 00:21:47.906 A:middle
In the next few minutes, I want

00:21:47.906 --> 00:21:50.346 A:middle
to really focus on the specific

00:21:50.346 --> 00:21:52.866 A:middle
use case of having a new neural

00:21:52.866 --> 00:21:53.506 A:middle
network layer.

00:21:53.916 --> 00:21:55.546 A:middle
And show you how you would

00:21:55.616 --> 00:21:57.496 A:middle
convert it to Core ML and then

00:21:57.666 --> 00:21:58.796 A:middle
how you would implement it in

00:21:58.826 --> 00:22:00.596 A:middle
your app.


00:21:58.826 --> 00:22:00.596 A:middle
your app.

00:22:00.866 --> 00:22:02.496 A:middle
So let's take a look at model

00:22:02.496 --> 00:22:02.956 A:middle
conversion.

00:22:03.756 --> 00:22:05.396 A:middle
So if you have used one of our

00:22:05.396 --> 00:22:06.916 A:middle
converters, or even if you have

00:22:06.916 --> 00:22:08.996 A:middle
not, it's a really simple API.

00:22:08.996 --> 00:22:10.276 A:middle
It's just a call to one

00:22:10.276 --> 00:22:10.706 A:middle
function.

00:22:11.146 --> 00:22:14.096 A:middle
This is how it looks for the

00:22:14.166 --> 00:22:15.076 A:middle
Keras converter.

00:22:15.466 --> 00:22:17.536 A:middle
And it's very similar for say

00:22:17.656 --> 00:22:19.396 A:middle
the ONNX converter or the

00:22:19.396 --> 00:22:21.416 A:middle
TensorFlow converter.

00:22:21.776 --> 00:22:22.976 A:middle
Now when you call this function,

00:22:23.406 --> 00:22:25.626 A:middle
mostly everything goes right.

00:22:26.006 --> 00:22:27.656 A:middle
But sometimes you might get an

00:22:27.776 --> 00:22:29.156 A:middle
error message like this.

00:22:30.496 --> 00:22:32.826 A:middle
It might say, "Hey, unsupported

00:22:32.826 --> 00:22:34.146 A:middle
operation of such-and-such

00:22:34.146 --> 00:22:34.366 A:middle
kind."

00:22:35.086 --> 00:22:37.546 A:middle
Now if that happens to you, you

00:22:37.586 --> 00:22:39.286 A:middle
only need to do a little bit

00:22:39.286 --> 00:22:40.886 A:middle
more to get past this error.

00:22:41.666 --> 00:22:43.576 A:middle
More specifically, such an error

00:22:43.576 --> 00:22:44.896 A:middle
message is an indication that

00:22:44.896 --> 00:22:47.166 A:middle
you should be using a custom

00:22:47.166 --> 00:22:47.346 A:middle
layer.

00:22:48.976 --> 00:22:50.556 A:middle
And before I show you what is

00:22:50.556 --> 00:22:51.946 A:middle
the little bit of extra effort

00:22:51.946 --> 00:22:53.016 A:middle
that you need to do to convert,

00:22:53.016 --> 00:22:55.816 A:middle
let's look at a few examples

00:22:55.816 --> 00:22:57.156 A:middle
where you would need to use a

00:22:57.156 --> 00:22:57.656 A:middle
custom layer.


00:23:01.426 --> 00:23:02.776 A:middle
So let's say you have an image

00:23:02.816 --> 00:23:03.426 A:middle
classifier.

00:23:03.786 --> 00:23:05.846 A:middle
This is how it looks in Xcode.

00:23:06.246 --> 00:23:07.096 A:middle
So it will be high-level

00:23:07.096 --> 00:23:08.316 A:middle
description of the model.

00:23:08.886 --> 00:23:11.136 A:middle
If you look inside, it's very

00:23:11.136 --> 00:23:12.166 A:middle
likely that it's a neural

00:23:12.166 --> 00:23:12.606 A:middle
network.

00:23:13.046 --> 00:23:14.456 A:middle
And it's very likely that it's a

00:23:14.456 --> 00:23:16.116 A:middle
convolutional neural network.

00:23:16.116 --> 00:23:17.416 A:middle
So it has a lot of layers,

00:23:17.656 --> 00:23:19.396 A:middle
convolution, activation.

00:23:20.346 --> 00:23:22.386 A:middle
Now it might happen that there's

00:23:22.386 --> 00:23:23.526 A:middle
a new activation layer that

00:23:23.556 --> 00:23:24.856 A:middle
comes up that Core ML does not

00:23:24.856 --> 00:23:25.206 A:middle
support.

00:23:25.846 --> 00:23:29.066 A:middle
And it's like at every machine

00:23:29.066 --> 00:23:30.916 A:middle
learning conference, researchers

00:23:30.916 --> 00:23:32.046 A:middle
are coming up with new layers

00:23:32.046 --> 00:23:32.516 A:middle
all the time.

00:23:32.596 --> 00:23:33.476 A:middle
So this is a very common

00:23:33.476 --> 00:23:33.906 A:middle
scenario.

00:23:33.906 --> 00:23:37.356 A:middle
Now if this happens, you only

00:23:37.356 --> 00:23:39.306 A:middle
need to use a custom

00:23:39.486 --> 00:23:40.566 A:middle
implementation of this new

00:23:40.566 --> 00:23:40.886 A:middle
layer.

00:23:41.276 --> 00:23:42.256 A:middle
And then you are good to go.

00:23:42.716 --> 00:23:43.796 A:middle
So this is how the model will

00:23:43.796 --> 00:23:44.246 A:middle
look like.

00:23:44.246 --> 00:23:46.096 A:middle
The only difference is this

00:23:46.096 --> 00:23:48.016 A:middle
dependency section at the

00:23:48.636 --> 00:23:49.446 A:middle
bottom.

00:23:49.446 --> 00:23:52.416 A:middle
Which would say that this model

00:23:52.416 --> 00:23:53.976 A:middle
contains a description of this

00:23:54.026 --> 00:23:54.476 A:middle
custom layer.

00:23:54.856 --> 00:23:56.246 A:middle
Let's take a look at another

00:23:56.246 --> 00:23:56.746 A:middle
example.

00:23:57.056 --> 00:23:58.576 A:middle
Let's say we have a very simple

00:23:58.906 --> 00:23:59.946 A:middle
digit classifier.


00:24:01.196 --> 00:24:03.526 A:middle
Now I came across this research

00:24:03.526 --> 00:24:04.946 A:middle
paper recently.

00:24:05.056 --> 00:24:06.486 A:middle
It's called Spatial Transformer

00:24:06.486 --> 00:24:06.976 A:middle
Network.

00:24:07.576 --> 00:24:09.236 A:middle
And what it does is this.

00:24:09.886 --> 00:24:12.746 A:middle
So it inserts a neural network

00:24:12.946 --> 00:24:15.036 A:middle
after the digit that tries to

00:24:15.036 --> 00:24:16.516 A:middle
localize the digit.

00:24:17.326 --> 00:24:18.856 A:middle
And then it feeds it through a

00:24:18.856 --> 00:24:21.156 A:middle
grid sampler layer which renders

00:24:21.156 --> 00:24:22.786 A:middle
the digit again, but this time

00:24:23.066 --> 00:24:25.026 A:middle
it has already focused on the

00:24:25.026 --> 00:24:25.276 A:middle
digit.

00:24:25.596 --> 00:24:26.686 A:middle
And then you pass it through

00:24:26.686 --> 00:24:29.516 A:middle
your old classify method.

00:24:29.516 --> 00:24:31.626 A:middle
Now we don't need to worry about

00:24:31.626 --> 00:24:32.416 A:middle
the details here.

00:24:32.416 --> 00:24:33.716 A:middle
But the point to note is that

00:24:33.756 --> 00:24:35.946 A:middle
the portion in green is what

00:24:35.946 --> 00:24:36.986 A:middle
Core ML supports.

00:24:37.136 --> 00:24:38.906 A:middle
And the portion in red, which is

00:24:38.906 --> 00:24:41.456 A:middle
this new grid sampler layer, is

00:24:41.456 --> 00:24:42.846 A:middle
this new experimental layer that

00:24:42.846 --> 00:24:43.746 A:middle
Core ML does not support.

00:24:44.136 --> 00:24:45.596 A:middle
So I want to take an example of

00:24:45.716 --> 00:24:47.826 A:middle
this particular model and show

00:24:47.826 --> 00:24:49.896 A:middle
you how you would convert it

00:24:50.116 --> 00:24:51.316 A:middle
using Core ML Tools.

00:24:51.566 --> 00:24:53.336 A:middle
So let's go to demo.


00:25:00.076 --> 00:25:01.566 A:middle
I hope it works on the first

00:25:01.566 --> 00:25:01.826 A:middle
try.

00:25:03.236 --> 00:25:04.836 A:middle
Back, oh yes.

00:25:05.996 --> 00:25:09.476 A:middle
Okay. So let me close off these

00:25:09.476 --> 00:25:09.966 A:middle
windows.

00:25:14.636 --> 00:25:16.766 A:middle
Let me get, clear this.

00:25:16.926 --> 00:25:17.596 A:middle
Clear the ML.

00:25:18.106 --> 00:25:20.246 A:middle
Okay, so I'm also going to use

00:25:20.446 --> 00:25:22.526 A:middle
Jupyter Notebook to show the

00:25:22.526 --> 00:25:22.846 A:middle
demo.

00:25:30.046 --> 00:25:31.696 A:middle
So I just navigate to the folder

00:25:31.696 --> 00:25:32.956 A:middle
where I have my pre-trained

00:25:33.026 --> 00:25:33.356 A:middle
network.

00:25:34.346 --> 00:25:35.976 A:middle
So what you see here is that I

00:25:35.976 --> 00:25:38.386 A:middle
have this spatial transformer

00:25:38.386 --> 00:25:39.056 A:middle
dot [inaudible] file.

00:25:39.156 --> 00:25:41.136 A:middle
This is a pre-trained Keras

00:25:41.136 --> 00:25:41.436 A:middle
model.

00:25:42.346 --> 00:25:44.056 A:middle
And if you are wondering if I

00:25:44.056 --> 00:25:45.526 A:middle
did something special to get

00:25:45.526 --> 00:25:46.756 A:middle
this model.

00:25:47.956 --> 00:25:50.536 A:middle
Basically what I did was I could

00:25:50.646 --> 00:25:52.076 A:middle
easily find an open source

00:25:52.076 --> 00:25:53.126 A:middle
implementation of spatial

00:25:53.126 --> 00:25:53.676 A:middle
transformer.

00:25:54.086 --> 00:25:55.616 A:middle
I just exhibited that script in

00:25:55.616 --> 00:25:57.046 A:middle
Keras, and I got this model.

00:25:57.646 --> 00:25:59.196 A:middle
And along with this model, I

00:25:59.196 --> 00:26:01.476 A:middle
also got this grid sampler layer


00:25:59.196 --> 00:26:01.476 A:middle
also got this grid sampler layer

00:26:01.646 --> 00:26:02.436 A:middle
Python script.

00:26:03.206 --> 00:26:04.936 A:middle
Now this grid sampler layer that

00:26:04.936 --> 00:26:07.086 A:middle
I'm talking about, it's also not

00:26:07.086 --> 00:26:08.676 A:middle
supported on Keras natively.

00:26:09.326 --> 00:26:11.376 A:middle
So the implementation that I got

00:26:11.376 --> 00:26:13.666 A:middle
online used that Keras custom

00:26:13.666 --> 00:26:15.666 A:middle
layer to implement the layer.

00:26:16.376 --> 00:26:18.496 A:middle
So as you can see, the concept

00:26:18.496 --> 00:26:20.046 A:middle
of customization is not unique

00:26:20.046 --> 00:26:20.606 A:middle
to Core ML.

00:26:20.806 --> 00:26:22.986 A:middle
In fact, it's very common in

00:26:22.986 --> 00:26:23.816 A:middle
most machine learning

00:26:23.816 --> 00:26:24.416 A:middle
frameworks.

00:26:24.756 --> 00:26:26.556 A:middle
This is how people experiment in

00:26:26.556 --> 00:26:27.086 A:middle
new layers.

00:26:27.966 --> 00:26:29.956 A:middle
Okay, so so far, I just have a

00:26:30.006 --> 00:26:30.636 A:middle
Keras model.

00:26:30.826 --> 00:26:32.306 A:middle
And now I want to focus on how

00:26:32.306 --> 00:26:33.666 A:middle
can I get a Core ML model?

00:26:34.406 --> 00:26:40.716 A:middle
So I'll open -- there, let me

00:26:40.716 --> 00:26:42.506 A:middle
launch a new Python notebook.

00:26:43.766 --> 00:26:46.176 A:middle
So I'll start by importing this

00:26:46.226 --> 00:26:47.526 A:middle
Keras model into my Python

00:26:47.526 --> 00:26:48.086 A:middle
environment.

00:26:49.396 --> 00:26:53.576 A:middle
Okay? So I import Keras, I

00:26:53.576 --> 00:26:56.676 A:middle
import the, the custom layer

00:26:56.676 --> 00:26:57.976 A:middle
that we have in Keras.

00:26:58.316 --> 00:27:00.366 A:middle
And now I will load the model in


00:26:58.316 --> 00:27:00.366 A:middle
And now I will load the model in

00:27:00.406 --> 00:27:00.926 A:middle
Keras.

00:27:02.676 --> 00:27:04.656 A:middle
Okay? So this is how you load

00:27:05.086 --> 00:27:06.196 A:middle
model, Keras models.

00:27:06.196 --> 00:27:07.506 A:middle
You give the part to the model

00:27:07.506 --> 00:27:08.536 A:middle
and if there's a custom layer,

00:27:08.536 --> 00:27:09.796 A:middle
you give a part to that.

00:27:11.016 --> 00:27:12.886 A:middle
Okay. So we have the model now.

00:27:13.066 --> 00:27:14.276 A:middle
Now let's convert this to Core

00:27:14.276 --> 00:27:14.466 A:middle
ML.

00:27:15.656 --> 00:27:16.986 A:middle
So I'm going to import Core ML

00:27:16.986 --> 00:27:17.626 A:middle
Tools.

00:27:18.356 --> 00:27:19.076 A:middle
Execute that.

00:27:19.896 --> 00:27:22.106 A:middle
And now as I, as I showed you

00:27:22.106 --> 00:27:24.156 A:middle
before that this is just a call

00:27:24.156 --> 00:27:25.626 A:middle
to one function to convert it.

00:27:26.086 --> 00:27:26.966 A:middle
So let me do that.

00:27:28.476 --> 00:27:29.196 A:middle
That's my call.

00:27:29.776 --> 00:27:32.086 A:middle
And I get an error as expected.

00:27:32.626 --> 00:27:35.096 A:middle
Python likes to throw these huge

00:27:35.096 --> 00:27:35.696 A:middle
error messages.

00:27:36.136 --> 00:27:37.946 A:middle
But really what we're focused on

00:27:38.666 --> 00:27:40.076 A:middle
is this last line.

00:27:40.336 --> 00:27:40.896 A:middle
Let me --

00:27:46.626 --> 00:27:47.906 A:middle
So as we can see in this last

00:27:47.906 --> 00:27:49.676 A:middle
line it says that hey, the layer

00:27:49.676 --> 00:27:51.326 A:middle
or sampler is not supported.

00:27:51.836 --> 00:27:53.146 A:middle
So now let's see what we need to

00:27:53.146 --> 00:27:55.126 A:middle
do to get rid of that.

00:27:55.756 --> 00:27:57.066 A:middle
Maybe I clear this all so that

00:27:57.066 --> 00:27:57.466 A:middle
you can see.

00:27:57.786 --> 00:27:59.616 A:middle
Okay. So now I change my

00:27:59.616 --> 00:28:00.956 A:middle
converter call just a little bit


00:27:59.616 --> 00:28:00.956 A:middle
converter call just a little bit

00:28:01.176 --> 00:28:03.046 A:middle
so I have my Core ML model.

00:28:03.646 --> 00:28:08.096 A:middle
And now I'm going to pass one

00:28:08.096 --> 00:28:08.946 A:middle
additional argument.

00:28:09.176 --> 00:28:14.176 A:middle
It's called custom conversion

00:28:14.576 --> 00:28:14.976 A:middle
functions.

00:28:19.396 --> 00:28:20.676 A:middle
And this will be a dictionary

00:28:21.396 --> 00:28:23.796 A:middle
from the name of the layer to a

00:28:23.796 --> 00:28:25.496 A:middle
function that I will define in a

00:28:25.496 --> 00:28:25.826 A:middle
minute.

00:28:26.186 --> 00:28:27.066 A:middle
And that I'm calling a good

00:28:27.066 --> 00:28:27.386 A:middle
sampler.

00:28:27.926 --> 00:28:29.006 A:middle
So let me take a step back and

00:28:29.006 --> 00:28:30.026 A:middle
explain what is happening here.

00:28:30.576 --> 00:28:31.796 A:middle
So as we know the way converter

00:28:31.796 --> 00:28:34.126 A:middle
works is that it goes through

00:28:34.346 --> 00:28:35.666 A:middle
each and every Keras layer.

00:28:36.016 --> 00:28:38.386 A:middle
It will, if you look at the

00:28:38.476 --> 00:28:38.986 A:middle
first layer.

00:28:38.986 --> 00:28:40.046 A:middle
Then [inaudible] its parameters

00:28:40.046 --> 00:28:40.666 A:middle
to Core ML.

00:28:40.666 --> 00:28:41.816 A:middle
If you go to the second layer,

00:28:41.816 --> 00:28:43.146 A:middle
then translate its parameters

00:28:43.146 --> 00:28:44.306 A:middle
and so on.

00:28:44.676 --> 00:28:45.936 A:middle
Now when it hits this custom

00:28:45.936 --> 00:28:46.886 A:middle
layer, it doesn't know what to

00:28:46.886 --> 00:28:47.146 A:middle
do.

00:28:47.756 --> 00:28:49.446 A:middle
So this function that I'm

00:28:49.446 --> 00:28:50.886 A:middle
passing here that convert this

00:28:50.936 --> 00:28:53.246 A:middle
sampler is going to help my

00:28:53.246 --> 00:28:54.256 A:middle
converter in doing that.

00:28:54.876 --> 00:28:56.316 A:middle
And let me show you what this

00:28:56.316 --> 00:28:56.976 A:middle
function looks like.


00:29:01.046 --> 00:29:01.806 A:middle
So this is a function.

00:29:01.806 --> 00:29:03.976 A:middle
There are a few lines of code,

00:29:03.976 --> 00:29:05.866 A:middle
but all that it's doing is three

00:29:05.866 --> 00:29:06.276 A:middle
things.

00:29:07.226 --> 00:29:10.966 A:middle
First, it's giving a name of a

00:29:11.026 --> 00:29:11.306 A:middle
class.

00:29:11.806 --> 00:29:13.566 A:middle
So as we might have noticed, the

00:29:13.616 --> 00:29:15.126 A:middle
implementation of the layer is

00:29:15.126 --> 00:29:15.546 A:middle
not here.

00:29:15.906 --> 00:29:17.096 A:middle
The implementation will come

00:29:17.096 --> 00:29:19.506 A:middle
later in the app and it will be

00:29:19.506 --> 00:29:20.866 A:middle
encapsulated in a class.

00:29:21.286 --> 00:29:22.136 A:middle
And this is the name of the

00:29:22.136 --> 00:29:23.836 A:middle
class that we'll later

00:29:23.836 --> 00:29:24.286 A:middle
implement.

00:29:24.666 --> 00:29:26.436 A:middle
So during conversion, we just

00:29:26.436 --> 00:29:27.756 A:middle
need to specify this class name.

00:29:28.156 --> 00:29:28.456 A:middle
That's it.

00:29:29.156 --> 00:29:30.436 A:middle
And then there's the description

00:29:30.816 --> 00:29:32.206 A:middle
which is a, which you should

00:29:32.206 --> 00:29:34.536 A:middle
provide so that if anybody is,

00:29:34.746 --> 00:29:37.086 A:middle
if somebody is looking at your

00:29:37.086 --> 00:29:38.266 A:middle
model, they know what it has.

00:29:39.276 --> 00:29:40.926 A:middle
And the third thing is basically

00:29:40.996 --> 00:29:42.766 A:middle
translating any parameters that

00:29:42.766 --> 00:29:44.466 A:middle
the Keras layer had to Core ML.

00:29:45.046 --> 00:29:46.206 A:middle
For this particular layer, it

00:29:46.206 --> 00:29:47.146 A:middle
has two parameters.

00:29:47.276 --> 00:29:48.756 A:middle
The output height, and output

00:29:48.816 --> 00:29:48.946 A:middle
weight.

00:29:49.336 --> 00:29:50.716 A:middle
And I'm just translating it to

00:29:50.716 --> 00:29:51.066 A:middle
Core ML.

00:29:51.866 --> 00:29:52.956 A:middle
If your custom layer that does

00:29:52.956 --> 00:29:54.356 A:middle
not have any parameters, then

00:29:54.356 --> 00:29:56.626 A:middle
you load, then you do not need

00:29:56.626 --> 00:29:57.616 A:middle
to do, do this.

00:29:58.356 --> 00:29:59.516 A:middle
If your layer has lots of

00:29:59.516 --> 00:30:01.116 A:middle
parameters, they can all go


00:29:59.516 --> 00:30:01.116 A:middle
parameters, they can all go

00:30:01.116 --> 00:30:02.466 A:middle
here, and they will all be

00:30:02.466 --> 00:30:04.736 A:middle
encapsulated inside the Core ML

00:30:04.736 --> 00:30:05.066 A:middle
model.

00:30:05.996 --> 00:30:07.486 A:middle
So as you might have noticed

00:30:07.486 --> 00:30:09.096 A:middle
that all I did here was very

00:30:09.096 --> 00:30:10.576 A:middle
similar to how you would define

00:30:10.576 --> 00:30:11.466 A:middle
a class, right?

00:30:11.786 --> 00:30:12.656 A:middle
You give a class name.

00:30:12.656 --> 00:30:14.106 A:middle
Maybe a description, maybe some

00:30:14.106 --> 00:30:14.746 A:middle
parameters.

00:30:15.546 --> 00:30:16.646 A:middle
So now let me execute this.

00:30:20.066 --> 00:30:20.916 A:middle
And now we see that the

00:30:20.916 --> 00:30:23.606 A:middle
converter went, conversion went

00:30:25.196 --> 00:30:25.336 A:middle
fine.

00:30:25.596 --> 00:30:30.216 A:middle
So let me this is behaving very

00:30:30.216 --> 00:30:31.636 A:middle
weirdly for some reason.

00:30:35.296 --> 00:30:36.836 A:middle
If you don't mind, I'm just

00:30:36.886 --> 00:30:38.656 A:middle
going to delete this all.

00:30:40.556 --> 00:30:42.396 A:middle
So let me visualize this model,

00:30:42.676 --> 00:30:44.186 A:middle
and you can do that very simply

00:30:44.356 --> 00:30:47.136 A:middle
using function in Core ML Tools.

00:30:47.636 --> 00:30:48.596 A:middle
That's called visualize spec.

00:30:50.766 --> 00:30:53.256 A:middle
And here you can see a

00:30:53.256 --> 00:30:54.256 A:middle
visualization of the model.

00:30:54.586 --> 00:30:56.166 A:middle
So as we can see, we have the

00:30:56.216 --> 00:30:56.996 A:middle
[inaudible] and some layers

00:30:56.996 --> 00:30:57.416 A:middle
there.

00:30:57.876 --> 00:31:00.336 A:middle
And this is our custom layer.


00:30:57.876 --> 00:31:00.336 A:middle
And this is our custom layer.

00:31:00.336 --> 00:31:01.936 A:middle
And if I click on this, I see

00:31:01.936 --> 00:31:03.176 A:middle
the parameters that it has.

00:31:03.176 --> 00:31:04.806 A:middle
So this is the name of the class

00:31:05.086 --> 00:31:05.706 A:middle
that I gave.

00:31:06.256 --> 00:31:07.816 A:middle
And this, and these are the

00:31:07.816 --> 00:31:08.746 A:middle
parameters that I set.

00:31:10.026 --> 00:31:10.886 A:middle
It's always a good idea to

00:31:10.886 --> 00:31:13.146 A:middle
visualize your Core ML model

00:31:13.146 --> 00:31:14.176 A:middle
before you drag and drop just to

00:31:14.176 --> 00:31:15.906 A:middle
see if everything looks fine.

00:31:17.346 --> 00:31:20.546 A:middle
Okay. This is the wrong

00:31:20.546 --> 00:31:20.946 A:middle
notebook.

00:31:21.256 --> 00:31:23.036 A:middle
Okay. And now I'll save out this

00:31:23.036 --> 00:31:23.406 A:middle
model.

00:31:30.256 --> 00:31:32.106 A:middle
And now let's take a look at

00:31:32.106 --> 00:31:32.606 A:middle
this model.

00:31:33.846 --> 00:31:35.816 A:middle
So let me close this.

00:31:37.756 --> 00:31:37.926 A:middle
Okay.

00:31:42.086 --> 00:31:44.696 A:middle
Let me actually let me navigate

00:31:44.816 --> 00:31:50.476 A:middle
to the directory that I have.

00:31:51.976 --> 00:31:52.736 A:middle
And here's my model.

00:31:52.736 --> 00:31:53.946 A:middle
So if I click on it and see it

00:31:53.946 --> 00:31:55.336 A:middle
in Xcode just to see how it

00:31:55.336 --> 00:31:55.666 A:middle
looks.

00:31:55.666 --> 00:31:58.756 A:middle
We can see that it has the

00:31:58.756 --> 00:32:00.266 A:middle
custom description here.


00:31:58.756 --> 00:32:00.266 A:middle
custom description here.

00:32:01.546 --> 00:32:03.186 A:middle
Okay. Let me go back to slides.

00:32:04.516 --> 00:32:09.896 A:middle
[ Applause ]

00:32:10.396 --> 00:32:12.466 A:middle
So what we just saw was with a

00:32:12.466 --> 00:32:15.126 A:middle
few simple lines, we could

00:32:15.156 --> 00:32:16.386 A:middle
exhibit a convert a function to

00:32:16.546 --> 00:32:17.246 A:middle
Core ML.

00:32:17.246 --> 00:32:19.496 A:middle
And the process is pretty much

00:32:19.526 --> 00:32:21.266 A:middle
the same if you are using the

00:32:21.266 --> 00:32:23.566 A:middle
TensorFlow converter or the ONNX

00:32:23.566 --> 00:32:23.956 A:middle
converter.

00:32:25.266 --> 00:32:26.596 A:middle
So we have our model here on the

00:32:26.596 --> 00:32:27.176 A:middle
left-hand side.

00:32:27.616 --> 00:32:29.286 A:middle
The custom layer model with the

00:32:29.286 --> 00:32:29.856 A:middle
parameters.

00:32:30.496 --> 00:32:31.646 A:middle
Now when you drag and drop this

00:32:31.646 --> 00:32:33.966 A:middle
model into Xcode, you will need

00:32:33.966 --> 00:32:35.336 A:middle
to provide the implementation of

00:32:35.426 --> 00:32:36.456 A:middle
the class.

00:32:36.456 --> 00:32:38.186 A:middle
In a file say, for example,

00:32:38.186 --> 00:32:38.476 A:middle
[inaudible].

00:32:38.476 --> 00:32:39.536 A:middle
And this is how it would look

00:32:39.536 --> 00:32:39.726 A:middle
like.

00:32:40.206 --> 00:32:41.646 A:middle
So you have your class, so

00:32:42.006 --> 00:32:44.986 A:middle
you'll have the initializer

00:32:45.246 --> 00:32:45.706 A:middle
function.

00:32:45.996 --> 00:32:47.456 A:middle
So this would be just

00:32:47.456 --> 00:32:49.106 A:middle
initializing any parameters that

00:32:49.106 --> 00:32:50.116 A:middle
we had in the model.

00:32:50.486 --> 00:32:52.636 A:middle
And then the main function in

00:32:52.636 --> 00:32:55.906 A:middle
this class would be evaluate.

00:32:56.846 --> 00:32:58.996 A:middle
This is where the actual

00:32:59.266 --> 00:33:00.706 A:middle
implementation of whatever


00:32:59.266 --> 00:33:00.706 A:middle
implementation of whatever

00:33:00.706 --> 00:33:01.876 A:middle
mathematical function the layer

00:33:01.876 --> 00:33:03.396 A:middle
is supposed to perform will go

00:33:03.596 --> 00:33:04.376 A:middle
here, in here.

00:33:04.806 --> 00:33:05.976 A:middle
And then there's one more

00:33:05.976 --> 00:33:07.266 A:middle
function called output shape or

00:33:07.266 --> 00:33:07.946 A:middle
input shapes.

00:33:08.136 --> 00:33:10.036 A:middle
This just specifies the size of

00:33:10.036 --> 00:33:11.496 A:middle
the output area that the layer

00:33:11.496 --> 00:33:12.106 A:middle
produces.

00:33:12.556 --> 00:33:14.906 A:middle
This helps Core ML in allocating

00:33:14.906 --> 00:33:17.116 A:middle
the buffer size at load time so

00:33:17.116 --> 00:33:18.726 A:middle
that your app is more efficient

00:33:19.076 --> 00:33:19.646 A:middle
at runtime.

00:33:21.966 --> 00:33:24.256 A:middle
So we just saw how you would

00:33:24.326 --> 00:33:26.136 A:middle
tackle a new layer in a neural

00:33:26.136 --> 00:33:26.516 A:middle
network.

00:33:26.826 --> 00:33:29.446 A:middle
There's a very similar concept

00:33:29.506 --> 00:33:30.556 A:middle
to a custom layer, and it's

00:33:30.556 --> 00:33:32.336 A:middle
called custom model.

00:33:32.336 --> 00:33:35.426 A:middle
It has the same idea, but it's

00:33:35.426 --> 00:33:36.766 A:middle
sort of more generic.

00:33:37.226 --> 00:33:39.966 A:middle
So with a custom model, you can

00:33:39.966 --> 00:33:41.546 A:middle
deal with any sort of network.

00:33:41.806 --> 00:33:43.696 A:middle
It need not be a neural -- it

00:33:43.726 --> 00:33:44.856 A:middle
need not be a neural network.

00:33:45.306 --> 00:33:46.876 A:middle
And basically gives you just

00:33:46.876 --> 00:33:48.646 A:middle
more flexibility overall.

00:33:50.156 --> 00:33:51.796 A:middle
So let me summarize the session.

00:33:52.356 --> 00:33:57.026 A:middle
We saw how much more rich is

00:33:57.076 --> 00:33:58.656 A:middle
this ecosystem around Core ML

00:33:58.656 --> 00:34:00.116 A:middle
Tools and that's great for you


00:33:58.656 --> 00:34:00.116 A:middle
Tools and that's great for you

00:34:00.116 --> 00:34:00.396 A:middle
guys.

00:34:00.396 --> 00:34:01.346 A:middle
Because now you have lot of

00:34:01.436 --> 00:34:03.416 A:middle
choice to get Core ML models

00:34:03.416 --> 00:34:03.726 A:middle
from.

00:34:04.476 --> 00:34:06.526 A:middle
We saw how easy it was to

00:34:06.846 --> 00:34:09.666 A:middle
quantize this, quantize Core ML

00:34:09.666 --> 00:34:10.235 A:middle
model.

00:34:10.716 --> 00:34:12.485 A:middle
And we saw that with a few lines

00:34:13.156 --> 00:34:16.565 A:middle
of code, we could easily

00:34:16.565 --> 00:34:18.295 A:middle
integrate a new custom layer in

00:34:19.356 --> 00:34:20.456 A:middle
the model.

00:34:20.806 --> 00:34:22.136 A:middle
You can find more information at

00:34:22.136 --> 00:34:23.206 A:middle
our documentation page.

00:34:23.596 --> 00:34:25.476 A:middle
And come to the labs and talk to

00:34:25.476 --> 00:34:25.626 A:middle
us.

00:34:26.176 --> 00:34:26.966 A:middle
Okay, thank you.

00:34:27.507 --> 00:34:29.507 A:middle
[ Applause ]
