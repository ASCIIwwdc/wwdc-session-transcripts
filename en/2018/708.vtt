WEBVTT

00:00:07.016 --> 00:00:15.500 A:middle
[ Music ]

00:00:20.516 --> 00:00:26.786 A:middle
[ Applause ]

00:00:27.286 --> 00:00:27.936 A:middle
&gt;&gt; Good morning.

00:00:28.826 --> 00:00:31.276 A:middle
Welcome. I'm Michael, and this

00:00:31.276 --> 00:00:33.316 A:middle
session is about what's new in

00:00:33.316 --> 00:00:33.816 A:middle
Core ML.

00:00:35.626 --> 00:00:37.786 A:middle
So introduced a year ago, Core

00:00:37.846 --> 00:00:39.286 A:middle
ML is all about making it

00:00:39.286 --> 00:00:41.346 A:middle
unbelievably simple for you to

00:00:41.346 --> 00:00:42.476 A:middle
integrate machine learning

00:00:42.476 --> 00:00:43.846 A:middle
models into your app.

00:00:45.436 --> 00:00:46.696 A:middle
It's been wonderful to see the

00:00:46.696 --> 00:00:48.286 A:middle
adoption over the past year.

00:00:49.946 --> 00:00:51.276 A:middle
We hope it has all of you

00:00:51.276 --> 00:00:53.316 A:middle
thinking about what great new

00:00:53.316 --> 00:00:55.776 A:middle
experiences you can enable if

00:00:55.776 --> 00:00:57.496 A:middle
your app had the ability to do

00:00:57.496 --> 00:00:59.756 A:middle
things like understand the

00:00:59.756 --> 00:01:03.826 A:middle
content of images, or perhaps,

00:00:59.756 --> 00:01:03.826 A:middle
content of images, or perhaps,

00:01:04.726 --> 00:01:05.876 A:middle
analyze some text.

00:01:08.696 --> 00:01:11.266 A:middle
What could you do if your app

00:01:11.266 --> 00:01:13.026 A:middle
could reason about audio or

00:01:13.026 --> 00:01:16.536 A:middle
music, or interpret your users'

00:01:16.536 --> 00:01:17.956 A:middle
actions based on their motion

00:01:17.956 --> 00:01:21.506 A:middle
activity, or even transform or

00:01:21.506 --> 00:01:22.976 A:middle
generate new content for them?

00:01:24.416 --> 00:01:26.106 A:middle
All of this, and much, much

00:01:26.106 --> 00:01:28.336 A:middle
more, is easily within reach.

00:01:29.046 --> 00:01:30.356 A:middle
And that's because this type of

00:01:30.356 --> 00:01:32.496 A:middle
functionality can be encoded in

00:01:32.496 --> 00:01:33.356 A:middle
a Core ML model.

00:01:35.146 --> 00:01:36.786 A:middle
Now if we take a peek inside one

00:01:36.786 --> 00:01:39.126 A:middle
of these, we may find a neural

00:01:39.126 --> 00:01:41.396 A:middle
network, tree ensemble, or some

00:01:41.396 --> 00:01:42.426 A:middle
other model architecture.

00:01:43.686 --> 00:01:44.876 A:middle
They may have millions of

00:01:44.926 --> 00:01:46.876 A:middle
parameters, the values of which

00:01:46.876 --> 00:01:47.976 A:middle
have been learned from large

00:01:47.976 --> 00:01:48.666 A:middle
amounts of data.

00:01:50.596 --> 00:01:52.686 A:middle
But for you, you could focus on

00:01:52.686 --> 00:01:53.446 A:middle
a single file.

00:01:54.166 --> 00:01:55.716 A:middle
You can focus on the

00:01:55.716 --> 00:01:57.566 A:middle
functionality it provides and

00:01:57.566 --> 00:01:59.816 A:middle
the experience it enables rather

00:01:59.816 --> 00:02:00.816 A:middle
than those implementation

00:01:59.816 --> 00:02:00.816 A:middle
than those implementation

00:02:00.816 --> 00:02:01.216 A:middle
details.

00:02:04.806 --> 00:02:06.556 A:middle
Adding a Core ML model to your

00:02:06.926 --> 00:02:08.106 A:middle
app is simple as adding that

00:02:08.196 --> 00:02:09.666 A:middle
file to your Xcode project.

00:02:11.226 --> 00:02:12.456 A:middle
Xcode will give you a simple

00:02:12.456 --> 00:02:15.206 A:middle
view, describe what it does in

00:02:15.236 --> 00:02:17.106 A:middle
terms of the inputs it requires

00:02:17.416 --> 00:02:18.616 A:middle
and the outputs it provides.

00:02:20.136 --> 00:02:21.476 A:middle
Xcode will take this one step

00:02:21.476 --> 00:02:22.576 A:middle
further and generate an

00:02:22.576 --> 00:02:25.186 A:middle
interface for you, so that

00:02:25.186 --> 00:02:26.336 A:middle
interacting with this model is

00:02:26.336 --> 00:02:29.206 A:middle
just a few lines of code, one to

00:02:29.206 --> 00:02:31.916 A:middle
load the model, one to make a

00:02:31.916 --> 00:02:34.526 A:middle
prediction, and sometimes one to

00:02:34.526 --> 00:02:35.876 A:middle
pull out the specific output you

00:02:35.876 --> 00:02:36.516 A:middle
are interested in.

00:02:38.146 --> 00:02:39.206 A:middle
Note that in some cases you

00:02:39.206 --> 00:02:40.116 A:middle
don't even have to write back

00:02:40.146 --> 00:02:41.806 A:middle
code because Core ML integrates

00:02:41.806 --> 00:02:43.296 A:middle
with some of our higher-level

00:02:43.296 --> 00:02:44.966 A:middle
APIs and allows you to customize

00:02:44.966 --> 00:02:46.416 A:middle
their behavior if you give them

00:02:46.416 --> 00:02:47.286 A:middle
a Core ML model.

00:02:47.736 --> 00:02:49.576 A:middle
So with Vision, this is done

00:02:49.576 --> 00:02:51.106 A:middle
through the VNCoreML Request

00:02:51.106 --> 00:02:51.496 A:middle
object.

00:02:51.946 --> 00:02:53.106 A:middle
And in the new Natural Language

00:02:53.106 --> 00:02:54.586 A:middle
framework, you can instantiate

00:02:54.686 --> 00:02:56.536 A:middle
an MLModel from a CoreML model.

00:02:57.246 --> 00:03:00.786 A:middle
So that's Core ML in a nutshell.

00:02:57.246 --> 00:03:00.786 A:middle
So that's Core ML in a nutshell.

00:03:01.606 --> 00:03:02.576 A:middle
But we are here to talk about

00:03:02.576 --> 00:03:03.096 A:middle
what's new.

00:03:04.326 --> 00:03:05.826 A:middle
We took all the great feedback

00:03:06.026 --> 00:03:06.886 A:middle
we received from you over the

00:03:06.886 --> 00:03:08.796 A:middle
past year and focused on some

00:03:09.116 --> 00:03:11.086 A:middle
key enhancements to CoreML 2.

00:03:12.586 --> 00:03:13.666 A:middle
And we are going to talk about

00:03:13.666 --> 00:03:14.736 A:middle
these in two sessions.

00:03:15.656 --> 00:03:16.836 A:middle
In the first session, the one

00:03:16.836 --> 00:03:17.626 A:middle
you are all sitting in right

00:03:17.626 --> 00:03:18.786 A:middle
now, we are going to talk about

00:03:18.786 --> 00:03:20.176 A:middle
what's new from the perspective

00:03:20.176 --> 00:03:21.646 A:middle
of your app.

00:03:21.846 --> 00:03:23.146 A:middle
In the second session, which

00:03:23.146 --> 00:03:24.716 A:middle
starts immediately after this at

00:03:24.716 --> 00:03:26.736 A:middle
10 a.m. after a short break, we

00:03:26.736 --> 00:03:29.046 A:middle
are going to talk about tools

00:03:29.366 --> 00:03:30.786 A:middle
and how you can update and

00:03:30.786 --> 00:03:32.186 A:middle
convert models to take advantage

00:03:32.186 --> 00:03:33.456 A:middle
of the new features in Core ML

00:03:33.496 --> 00:03:33.676 A:middle
2.

00:03:36.936 --> 00:03:37.836 A:middle
When it comes to your app, we

00:03:37.836 --> 00:03:39.426 A:middle
are going to focus on three key

00:03:39.426 --> 00:03:39.896 A:middle
areas.

00:03:40.736 --> 00:03:42.746 A:middle
The first is how you can reduce

00:03:42.796 --> 00:03:44.656 A:middle
the size and number of models

00:03:45.076 --> 00:03:46.246 A:middle
using your app while still

00:03:46.246 --> 00:03:47.616 A:middle
getting the same functionality.

00:03:48.886 --> 00:03:50.206 A:middle
Then we'll look about how you

00:03:50.206 --> 00:03:51.846 A:middle
can get more performance out of

00:03:51.846 --> 00:03:52.496 A:middle
a single model.

00:03:52.996 --> 00:03:55.286 A:middle
And then we'll conclude about

00:03:55.336 --> 00:03:57.086 A:middle
how using Core ML will allow you

00:03:57.086 --> 00:03:58.196 A:middle
to keep pace with the

00:03:58.196 --> 00:03:59.686 A:middle
state-of-the-art and rapidly

00:03:59.686 --> 00:04:00.946 A:middle
moving field of machine

00:03:59.686 --> 00:04:00.946 A:middle
moving field of machine

00:04:00.946 --> 00:04:01.236 A:middle
learning.

00:04:02.286 --> 00:04:03.466 A:middle
So to kick it off, let's talk

00:04:03.466 --> 00:04:04.346 A:middle
about model size.

00:04:04.486 --> 00:04:05.366 A:middle
I'm going to hand it off to

00:04:05.366 --> 00:04:05.966 A:middle
Francesco.

00:04:06.516 --> 00:04:09.996 A:middle
[ Applause ]

00:04:10.496 --> 00:04:11.166 A:middle
&gt;&gt; Thank you Michael.

00:04:12.876 --> 00:04:15.936 A:middle
Hello. Every ways to reduce the

00:04:15.936 --> 00:04:17.476 A:middle
size of your Core ML app is very

00:04:17.476 --> 00:04:18.005 A:middle
important.

00:04:18.636 --> 00:04:19.935 A:middle
My name is Francesco, and I am

00:04:19.935 --> 00:04:21.286 A:middle
going to introduce quantization

00:04:21.375 --> 00:04:23.266 A:middle
and flexible shapes, two new

00:04:23.266 --> 00:04:25.086 A:middle
features in Core ML 2 that can

00:04:25.086 --> 00:04:28.306 A:middle
help reduce your app size.

00:04:28.516 --> 00:04:30.326 A:middle
So Core ML, [inaudible] why

00:04:30.326 --> 00:04:32.506 A:middle
should you learn in models and

00:04:33.316 --> 00:04:33.486 A:middle
device.

00:04:33.636 --> 00:04:34.786 A:middle
This gives your app four key

00:04:34.786 --> 00:04:36.316 A:middle
advantages compared to running

00:04:36.316 --> 00:04:36.966 A:middle
them in the cloud.

00:04:37.766 --> 00:04:39.746 A:middle
First of all, user privacy is

00:04:39.746 --> 00:04:40.476 A:middle
fully respected.

00:04:40.476 --> 00:04:42.066 A:middle
There are many machine-learning

00:04:42.066 --> 00:04:42.976 A:middle
models on device.

00:04:43.876 --> 00:04:45.406 A:middle
We guarantee that the data never

00:04:45.406 --> 00:04:46.836 A:middle
leaves the device of the user.

00:04:48.016 --> 00:04:49.636 A:middle
Second, it can help you achieve

00:04:49.636 --> 00:04:50.626 A:middle
real-time performance.

00:04:52.046 --> 00:04:52.956 A:middle
And for silicon [phonetic] and

00:04:52.956 --> 00:04:54.956 A:middle
devices are super- efficient for

00:04:54.956 --> 00:04:56.136 A:middle
machine-learning workloads.

00:04:57.106 --> 00:04:58.226 A:middle
Furthermore, you don't have to

00:04:58.226 --> 00:04:59.796 A:middle
maintain and pay for Internet

00:04:59.796 --> 00:05:00.256 A:middle
servers.

00:04:59.796 --> 00:05:00.256 A:middle
servers.

00:05:01.346 --> 00:05:02.486 A:middle
And Core ML inference is

00:05:02.486 --> 00:05:04.976 A:middle
available anywhere at any time

00:05:05.546 --> 00:05:06.806 A:middle
despite natural connectivity

00:05:06.806 --> 00:05:07.226 A:middle
issues.

00:05:07.916 --> 00:05:09.256 A:middle
All these great benefits come

00:05:09.256 --> 00:05:10.706 A:middle
with the fact that you now need

00:05:10.706 --> 00:05:12.106 A:middle
to store your machine-learning

00:05:12.106 --> 00:05:13.076 A:middle
models on device.

00:05:14.046 --> 00:05:15.246 A:middle
And if the machine-learning

00:05:15.246 --> 00:05:17.326 A:middle
models are big, then you might

00:05:17.326 --> 00:05:18.506 A:middle
be concerned about the size of

00:05:18.506 --> 00:05:18.866 A:middle
your app.

00:05:19.796 --> 00:05:21.416 A:middle
For example, you are -- you have

00:05:21.416 --> 00:05:22.846 A:middle
your [inaudible] map, and it's

00:05:22.846 --> 00:05:23.856 A:middle
full of cool features.

00:05:24.446 --> 00:05:25.846 A:middle
And your users are very happy

00:05:25.846 --> 00:05:26.246 A:middle
about it.

00:05:26.836 --> 00:05:28.156 A:middle
And now you want to take

00:05:28.156 --> 00:05:28.996 A:middle
advantage of the new

00:05:28.996 --> 00:05:31.096 A:middle
opportunities offered by machine

00:05:31.096 --> 00:05:32.526 A:middle
learning on device, and you want

00:05:32.526 --> 00:05:33.936 A:middle
to add new amazing capabilities

00:05:33.936 --> 00:05:34.946 A:middle
to your app.

00:05:34.946 --> 00:05:36.166 A:middle
So what you do, you train some

00:05:36.166 --> 00:05:38.126 A:middle
Core ML models and you add them

00:05:38.126 --> 00:05:39.446 A:middle
to your app.

00:05:39.806 --> 00:05:41.046 A:middle
What this means is that your app

00:05:41.046 --> 00:05:42.716 A:middle
has become more awesome and your

00:05:42.716 --> 00:05:43.916 A:middle
users are even happier.

00:05:45.176 --> 00:05:46.546 A:middle
But some of them might notice

00:05:47.006 --> 00:05:48.456 A:middle
that your app has increased in

00:05:48.456 --> 00:05:49.276 A:middle
size a little bit.

00:05:49.966 --> 00:05:51.376 A:middle
It's not uncommon to see that

00:05:51.376 --> 00:05:53.066 A:middle
apps grow up, either to tens or

00:05:53.106 --> 00:05:54.366 A:middle
hundreds of megabytes after

00:05:54.366 --> 00:05:55.216 A:middle
adding machine-learning

00:05:55.216 --> 00:05:56.106 A:middle
capabilities to them.

00:05:57.546 --> 00:05:58.926 A:middle
And as you keep adding more and

00:05:58.926 --> 00:06:01.216 A:middle
more features to your app, your

00:05:58.926 --> 00:06:01.216 A:middle
more features to your app, your

00:06:01.216 --> 00:06:02.806 A:middle
app size might simply become out

00:06:02.806 --> 00:06:03.286 A:middle
of control.

00:06:04.696 --> 00:06:05.756 A:middle
So that's the first thing that

00:06:05.756 --> 00:06:06.666 A:middle
you can do about it.

00:06:06.976 --> 00:06:08.736 A:middle
And if these machine-learning

00:06:08.736 --> 00:06:10.076 A:middle
models are supporting other

00:06:10.116 --> 00:06:11.946 A:middle
features to your app, you can

00:06:11.946 --> 00:06:13.506 A:middle
keep them outside your initial

00:06:13.506 --> 00:06:13.956 A:middle
bundle.

00:06:13.956 --> 00:06:17.196 A:middle
And then as the user uses the

00:06:17.196 --> 00:06:18.666 A:middle
other features, you can download

00:06:18.666 --> 00:06:20.546 A:middle
them on demand, compile them on

00:06:20.546 --> 00:06:21.036 A:middle
device.

00:06:21.796 --> 00:06:23.036 A:middle
So this -- in this case, this

00:06:23.036 --> 00:06:24.356 A:middle
user is happy in the beginning

00:06:24.356 --> 00:06:26.166 A:middle
because the installation size is

00:06:26.166 --> 00:06:26.746 A:middle
unchanged.

00:06:27.246 --> 00:06:28.956 A:middle
But since the user downloads and

00:06:28.956 --> 00:06:30.106 A:middle
uses all the current Core ML

00:06:30.106 --> 00:06:32.156 A:middle
functionality in your app, at

00:06:32.156 --> 00:06:33.406 A:middle
the end of the day the size of

00:06:33.406 --> 00:06:34.786 A:middle
the -- of your app is still

00:06:34.786 --> 00:06:35.046 A:middle
large.

00:06:36.216 --> 00:06:38.096 A:middle
So wouldn't it be better if,

00:06:38.096 --> 00:06:40.766 A:middle
instead, we could tackle this

00:06:40.806 --> 00:06:43.646 A:middle
problem by reducing the size of

00:06:43.646 --> 00:06:44.616 A:middle
the models itself?

00:06:46.016 --> 00:06:47.496 A:middle
This would give us a smaller

00:06:47.496 --> 00:06:48.886 A:middle
bundle in case we ship the

00:06:48.886 --> 00:06:51.716 A:middle
models inside the app, faster

00:06:51.716 --> 00:06:54.026 A:middle
and smaller downloads if instead

00:06:54.026 --> 00:06:55.346 A:middle
of shipping the models in the

00:06:55.346 --> 00:06:56.206 A:middle
app we download them.

00:06:56.206 --> 00:06:58.936 A:middle
And in any case, your app will

00:06:58.936 --> 00:07:00.406 A:middle
enjoy a lower memory footprint.

00:06:58.936 --> 00:07:00.406 A:middle
enjoy a lower memory footprint.

00:07:00.906 --> 00:07:02.456 A:middle
Using less memory is better for

00:07:02.456 --> 00:07:04.176 A:middle
the performance of your app and

00:07:04.176 --> 00:07:05.446 A:middle
great for the system in general.

00:07:06.286 --> 00:07:08.046 A:middle
So let's see how we can

00:07:08.046 --> 00:07:09.616 A:middle
decompose the size of a Core ML

00:07:09.616 --> 00:07:11.026 A:middle
app into factors to better

00:07:11.026 --> 00:07:11.886 A:middle
tackle this problem.

00:07:13.386 --> 00:07:14.276 A:middle
First, there is the number of

00:07:14.276 --> 00:07:14.666 A:middle
models.

00:07:14.906 --> 00:07:16.046 A:middle
This depends on how many

00:07:16.716 --> 00:07:18.006 A:middle
machine-learning functionalities

00:07:18.006 --> 00:07:18.536 A:middle
your app has.

00:07:19.286 --> 00:07:20.186 A:middle
Then there is the number of

00:07:20.266 --> 00:07:20.576 A:middle
weights.

00:07:21.466 --> 00:07:23.156 A:middle
The number of weights depends on

00:07:23.156 --> 00:07:24.166 A:middle
the architecture that you have

00:07:24.166 --> 00:07:24.926 A:middle
chosen to solve your

00:07:24.926 --> 00:07:25.816 A:middle
machine-learning problem.

00:07:26.446 --> 00:07:29.026 A:middle
As Michael was mentioning, the

00:07:29.026 --> 00:07:30.216 A:middle
number of weight -- the weights

00:07:30.416 --> 00:07:31.336 A:middle
are the place in which the

00:07:31.336 --> 00:07:33.236 A:middle
machine-learning model stores

00:07:33.236 --> 00:07:34.796 A:middle
the information that it has been

00:07:34.796 --> 00:07:35.666 A:middle
learning during training.

00:07:36.336 --> 00:07:38.036 A:middle
So it is -- if it has been

00:07:38.036 --> 00:07:39.826 A:middle
trained to do a complex task,

00:07:39.966 --> 00:07:41.556 A:middle
it's not uncommon to see a model

00:07:41.556 --> 00:07:43.366 A:middle
requiring tens of millions of

00:07:43.416 --> 00:07:43.696 A:middle
weights.

00:07:45.216 --> 00:07:46.736 A:middle
Finally, there is the size of

00:07:46.776 --> 00:07:47.196 A:middle
the weight.

00:07:47.386 --> 00:07:48.456 A:middle
How are we storing these

00:07:48.886 --> 00:07:50.086 A:middle
parameters that we are learning

00:07:50.086 --> 00:07:50.666 A:middle
during training?

00:07:51.796 --> 00:07:52.846 A:middle
Let's focus on this factor

00:07:52.846 --> 00:07:53.166 A:middle
first.

00:07:54.466 --> 00:07:55.776 A:middle
For neural networks, we have

00:07:55.776 --> 00:07:57.346 A:middle
several options to represent and

00:07:57.346 --> 00:07:57.976 A:middle
store the weights.

00:07:59.476 --> 00:08:00.966 A:middle
And the first, really, is of

00:07:59.476 --> 00:08:00.966 A:middle
And the first, really, is of

00:08:00.966 --> 00:08:02.606 A:middle
Core ML in iOS 11.

00:08:03.486 --> 00:08:04.816 A:middle
Neural networks were stored

00:08:04.816 --> 00:08:07.296 A:middle
using floating-point 32-bit

00:08:07.506 --> 00:08:07.856 A:middle
weights.

00:08:08.496 --> 00:08:12.466 A:middle
In iOS 11.2, we heard your

00:08:12.466 --> 00:08:13.856 A:middle
feedback and we introduced half

00:08:13.856 --> 00:08:16.006 A:middle
precision floating-point 16

00:08:16.006 --> 00:08:16.246 A:middle
weight.

00:08:16.676 --> 00:08:18.986 A:middle
This gives your app half the

00:08:18.986 --> 00:08:21.126 A:middle
storage required for the same

00:08:21.126 --> 00:08:21.646 A:middle
accuracy.

00:08:22.236 --> 00:08:23.756 A:middle
But this year we wanted to take

00:08:23.756 --> 00:08:25.616 A:middle
several steps further, and we

00:08:25.616 --> 00:08:27.116 A:middle
are introducing quantized

00:08:27.116 --> 00:08:27.336 A:middle
weights.

00:08:28.576 --> 00:08:29.836 A:middle
With quantized weights we are no

00:08:29.836 --> 00:08:31.526 A:middle
longer restricted to use either

00:08:31.526 --> 00:08:33.756 A:middle
Float 32 or Float 16 values.

00:08:34.155 --> 00:08:35.236 A:middle
But neural networks can be

00:08:35.236 --> 00:08:37.956 A:middle
encoded using 8 bits, 4 bits,

00:08:38.476 --> 00:08:40.515 A:middle
any bits all the way down to 1

00:08:41.876 --> 00:08:41.976 A:middle
bit.

00:08:42.186 --> 00:08:43.876 A:middle
So let's now see what

00:08:43.876 --> 00:08:45.166 A:middle
quantization here is.

00:08:46.336 --> 00:08:47.746 A:middle
Here we are representing a

00:08:47.746 --> 00:08:49.176 A:middle
subset of the weights of our

00:08:49.176 --> 00:08:49.986 A:middle
neural networks.

00:08:50.546 --> 00:08:52.186 A:middle
As we can see, these weights can

00:08:52.186 --> 00:08:54.166 A:middle
take any value in a continuous

00:08:54.166 --> 00:08:54.486 A:middle
range.

00:08:55.536 --> 00:08:57.426 A:middle
This means that in theory, a

00:08:57.426 --> 00:08:58.726 A:middle
single weight can take an

00:08:58.726 --> 00:09:00.086 A:middle
infinite number of possible

00:08:58.726 --> 00:09:00.086 A:middle
infinite number of possible

00:09:00.086 --> 00:09:00.576 A:middle
values.

00:09:01.096 --> 00:09:02.506 A:middle
So in practice, in neural

00:09:02.506 --> 00:09:04.476 A:middle
networks we store weights using

00:09:04.476 --> 00:09:07.186 A:middle
floater 32 point -- Float 32

00:09:07.186 --> 00:09:07.586 A:middle
numbers.

00:09:08.096 --> 00:09:09.256 A:middle
This means that this weight can

00:09:09.256 --> 00:09:10.946 A:middle
take billions of values to

00:09:10.946 --> 00:09:13.206 A:middle
better represent the -- their

00:09:13.206 --> 00:09:14.036 A:middle
continuous nature.

00:09:14.576 --> 00:09:16.146 A:middle
But it turns out the neural

00:09:16.146 --> 00:09:18.006 A:middle
networks also work with lower

00:09:18.006 --> 00:09:18.716 A:middle
precision weights.

00:09:19.946 --> 00:09:21.776 A:middle
Quantization is the process it

00:09:21.826 --> 00:09:23.536 A:middle
takes to discontinue strings of

00:09:23.536 --> 00:09:25.566 A:middle
values and constrains them to

00:09:25.566 --> 00:09:27.926 A:middle
take a very small and discrete

00:09:28.016 --> 00:09:30.606 A:middle
subset of possible values.

00:09:31.176 --> 00:09:32.956 A:middle
For example, here quantization

00:09:32.956 --> 00:09:34.156 A:middle
has turned this continuous

00:09:34.156 --> 00:09:37.156 A:middle
spectrum of weights into only

00:09:37.156 --> 00:09:39.196 A:middle
256 possible values.

00:09:39.536 --> 00:09:41.156 A:middle
So before quantization, the

00:09:41.156 --> 00:09:42.556 A:middle
weights would take any possible

00:09:42.556 --> 00:09:42.986 A:middle
values.

00:09:43.316 --> 00:09:44.906 A:middle
After quantization, they only

00:09:44.906 --> 00:09:46.926 A:middle
have 256 options.

00:09:47.976 --> 00:09:50.326 A:middle
Now since its weight can be

00:09:50.366 --> 00:09:52.796 A:middle
taken from this small set, Core

00:09:52.796 --> 00:09:54.366 A:middle
ML now needs only 8 bits of

00:09:54.366 --> 00:09:55.606 A:middle
stored information of a weight.

00:09:56.876 --> 00:09:58.856 A:middle
But nothing can stop us here.

00:09:59.296 --> 00:09:59.976 A:middle
We can go further.

00:10:00.066 --> 00:10:01.966 A:middle
And for example, we can

00:10:02.436 --> 00:10:04.276 A:middle
constrain the network to take,

00:10:04.276 --> 00:10:06.156 A:middle
instead of one of 56 different

00:10:06.156 --> 00:10:08.056 A:middle
values, for example, just 8.

00:10:08.536 --> 00:10:11.246 A:middle
And since now we now have only 8

00:10:11.246 --> 00:10:13.926 A:middle
options, Core ML will need 3-bit

00:10:13.966 --> 00:10:15.966 A:middle
values per weight to store your

00:10:15.966 --> 00:10:16.306 A:middle
model.

00:10:17.556 --> 00:10:19.726 A:middle
There are now some details about

00:10:19.726 --> 00:10:20.806 A:middle
how we are going to choose these

00:10:20.806 --> 00:10:22.316 A:middle
values to represent the weights.

00:10:22.786 --> 00:10:24.486 A:middle
They can be uniformly

00:10:24.516 --> 00:10:26.256 A:middle
distributed in this range, and

00:10:26.256 --> 00:10:27.576 A:middle
in this case we have linear

00:10:27.576 --> 00:10:30.276 A:middle
quantization instead in lookup

00:10:30.356 --> 00:10:33.856 A:middle
table quantization, we can have

00:10:33.906 --> 00:10:35.356 A:middle
these values scattered in this

00:10:35.356 --> 00:10:37.146 A:middle
range in an arbitrary manner.

00:10:37.766 --> 00:10:39.246 A:middle
So let's see practically how

00:10:39.246 --> 00:10:40.906 A:middle
quantization can help us reduce

00:10:41.066 --> 00:10:41.826 A:middle
the size of our model.

00:10:41.826 --> 00:10:43.256 A:middle
In this example, you are

00:10:43.256 --> 00:10:45.506 A:middle
focusing on Resnet50, which is a

00:10:45.506 --> 00:10:47.186 A:middle
common architecture used by many

00:10:47.186 --> 00:10:48.526 A:middle
applications for many different

00:10:48.526 --> 00:10:48.846 A:middle
tasks.

00:10:50.056 --> 00:10:52.106 A:middle
It includes 25 million trained

00:10:52.106 --> 00:10:54.426 A:middle
parameters and this means that

00:10:54.426 --> 00:10:56.186 A:middle
you have to use 32-bit floats to

00:10:56.186 --> 00:10:56.916 A:middle
represent it.

00:10:57.946 --> 00:10:59.386 A:middle
Then the total model size is

00:10:59.386 --> 00:11:00.616 A:middle
more than 100 megabytes.

00:10:59.386 --> 00:11:00.616 A:middle
more than 100 megabytes.

00:11:01.196 --> 00:11:03.856 A:middle
If we quantize it to 8-bits,

00:11:04.396 --> 00:11:05.616 A:middle
then the architecture hasn't

00:11:05.616 --> 00:11:07.476 A:middle
changed; we still have 25

00:11:07.476 --> 00:11:08.886 A:middle
million parameters.

00:11:09.616 --> 00:11:11.606 A:middle
But we are now using only 1 byte

00:11:12.686 --> 00:11:14.236 A:middle
to store a single weight, and

00:11:14.236 --> 00:11:15.396 A:middle
this means that the model size

00:11:15.396 --> 00:11:16.856 A:middle
is reduced by a factor of 4x.

00:11:17.326 --> 00:11:18.686 A:middle
It's only -- it now only takes

00:11:18.686 --> 00:11:20.066 A:middle
26 megabytes to store this

00:11:20.066 --> 00:11:20.366 A:middle
model.

00:11:20.906 --> 00:11:22.046 A:middle
And we can go further.

00:11:22.426 --> 00:11:23.386 A:middle
We can use that quantized

00:11:23.386 --> 00:11:24.926 A:middle
representation that only uses 4

00:11:24.926 --> 00:11:26.716 A:middle
bits per weight in this model

00:11:27.286 --> 00:11:28.586 A:middle
and end up with a model that is

00:11:28.586 --> 00:11:28.976 A:middle
even smaller.

00:11:29.516 --> 00:11:36.016 A:middle
[ Applause ]

00:11:36.516 --> 00:11:38.986 A:middle
And again, Core ML supports all

00:11:38.986 --> 00:11:40.626 A:middle
the quantization modes all the

00:11:40.626 --> 00:11:43.876 A:middle
way down to 8 bits.

00:11:44.296 --> 00:11:47.096 A:middle
Now quantization is a powerful

00:11:47.096 --> 00:11:48.886 A:middle
technique to take an existing

00:11:48.886 --> 00:11:50.286 A:middle
architecture and of a smaller

00:11:50.286 --> 00:11:50.896 A:middle
version of it.

00:11:51.396 --> 00:11:52.616 A:middle
But how can you obtain quantized

00:11:52.616 --> 00:11:52.896 A:middle
model?

00:11:54.516 --> 00:11:56.486 A:middle
If you have any neural

00:11:56.486 --> 00:11:58.306 A:middle
networking in Core ML format,

00:11:58.306 --> 00:11:59.626 A:middle
you can use Core ML Tools to

00:11:59.626 --> 00:12:00.586 A:middle
obtain a quantized

00:11:59.626 --> 00:12:00.586 A:middle
obtain a quantized

00:12:00.586 --> 00:12:01.476 A:middle
representation of it.

00:12:01.716 --> 00:12:03.496 A:middle
So Core ML 2 should quantize for

00:12:03.496 --> 00:12:04.216 A:middle
you automatically.

00:12:05.516 --> 00:12:07.386 A:middle
Or you can train quantized

00:12:07.386 --> 00:12:07.756 A:middle
models.

00:12:09.316 --> 00:12:10.596 A:middle
You can either train quantized

00:12:10.596 --> 00:12:11.416 A:middle
-- with a quantization

00:12:11.416 --> 00:12:12.866 A:middle
constraint from scratch, or

00:12:12.866 --> 00:12:14.256 A:middle
retrain existing models with

00:12:14.256 --> 00:12:15.406 A:middle
quantization constraints.

00:12:16.376 --> 00:12:17.426 A:middle
After you have obtained your

00:12:17.426 --> 00:12:18.336 A:middle
quantized model with your

00:12:18.336 --> 00:12:19.556 A:middle
training tools, you can then

00:12:19.556 --> 00:12:21.616 A:middle
convert it to Core ML as usual.

00:12:22.156 --> 00:12:23.466 A:middle
And nothing will change in the

00:12:23.466 --> 00:12:24.596 A:middle
app in the way you use the

00:12:24.596 --> 00:12:24.976 A:middle
model.

00:12:25.626 --> 00:12:27.466 A:middle
Inside the model, the numbers

00:12:27.466 --> 00:12:28.716 A:middle
are going to be stored in

00:12:28.716 --> 00:12:29.826 A:middle
different precision, but the

00:12:29.826 --> 00:12:31.816 A:middle
interface for using the model

00:12:32.176 --> 00:12:32.976 A:middle
will not change at all.

00:12:35.216 --> 00:12:36.856 A:middle
However, we always have to

00:12:36.856 --> 00:12:38.426 A:middle
consider that quantized models

00:12:38.986 --> 00:12:40.016 A:middle
have lower-precision

00:12:40.016 --> 00:12:41.586 A:middle
approximations of the original

00:12:41.586 --> 00:12:43.446 A:middle
reference floating-point models.

00:12:44.256 --> 00:12:45.686 A:middle
And this means that quantized

00:12:45.686 --> 00:12:47.166 A:middle
models come with an accuracy

00:12:47.166 --> 00:12:48.316 A:middle
versus size-of-the-model

00:12:48.316 --> 00:12:48.836 A:middle
tradeoff.

00:12:49.736 --> 00:12:51.436 A:middle
This tradeoff is model dependent

00:12:51.546 --> 00:12:53.096 A:middle
and use case dependent.

00:12:53.096 --> 00:12:55.166 A:middle
And it's also a very active area

00:12:55.166 --> 00:12:55.706 A:middle
of research.

00:12:56.456 --> 00:12:57.846 A:middle
So it's always recommended to

00:12:57.936 --> 00:12:58.846 A:middle
check the accuracy of the

00:12:58.846 --> 00:13:00.786 A:middle
quantized model and compare it

00:12:58.846 --> 00:13:00.786 A:middle
quantized model and compare it

00:13:00.786 --> 00:13:02.396 A:middle
with the referenced

00:13:02.396 --> 00:13:04.186 A:middle
floating-point version for

00:13:04.186 --> 00:13:05.586 A:middle
relevant this data and form

00:13:05.586 --> 00:13:07.076 A:middle
metrics that are valid for your

00:13:07.076 --> 00:13:07.876 A:middle
app and use case.

00:13:08.966 --> 00:13:10.786 A:middle
Now let's see a demo of how we

00:13:10.786 --> 00:13:13.586 A:middle
can use -- adopt quantized

00:13:13.586 --> 00:13:14.796 A:middle
models to reduce the size of an

00:13:14.796 --> 00:13:14.936 A:middle
app.

00:13:15.516 --> 00:13:18.500 A:middle
[ Applause ]

00:13:25.236 --> 00:13:26.556 A:middle
I would like to show you a style

00:13:26.556 --> 00:13:27.566 A:middle
transfer app.

00:13:28.116 --> 00:13:29.416 A:middle
In style transfer, a neural

00:13:29.416 --> 00:13:31.356 A:middle
network has been trained to

00:13:31.356 --> 00:13:33.456 A:middle
render user images using styles

00:13:33.456 --> 00:13:34.326 A:middle
that have been learned by

00:13:34.326 --> 00:13:35.646 A:middle
watching paintings or other

00:13:35.646 --> 00:13:36.086 A:middle
images.

00:13:36.696 --> 00:13:37.616 A:middle
So let me load my app.

00:13:37.616 --> 00:13:41.216 A:middle
As we can see, I am shipping

00:13:41.216 --> 00:13:43.226 A:middle
this app with four styles; City,

00:13:43.226 --> 00:13:45.066 A:middle
Glass, Oils and Waves.

00:13:45.656 --> 00:13:47.606 A:middle
And then I can pick images from

00:13:47.606 --> 00:13:49.256 A:middle
the photo library of the users

00:13:49.256 --> 00:13:51.096 A:middle
and then process them blending

00:13:51.096 --> 00:13:52.736 A:middle
them in different styles right

00:13:52.736 --> 00:13:53.226 A:middle
on device.

00:13:53.976 --> 00:13:55.406 A:middle
So this is the original image,

00:13:56.136 --> 00:13:57.076 A:middle
and I am going to render the

00:13:57.076 --> 00:13:57.716 A:middle
City style,

00:13:59.636 --> 00:13:59.976 A:middle
Glass,

00:14:02.336 --> 00:14:02.706 A:middle
Oils,

00:14:04.506 --> 00:14:04.956 A:middle
and Waves.

00:14:07.076 --> 00:14:08.746 A:middle
Let's see how this app has been

00:14:08.746 --> 00:14:09.516 A:middle
built in Xcode.

00:14:10.966 --> 00:14:13.296 A:middle
This app uses Core ML and Vision

00:14:13.396 --> 00:14:15.406 A:middle
API to perform this stylization.

00:14:16.226 --> 00:14:17.786 A:middle
And as we can see, we have four

00:14:17.786 --> 00:14:19.586 A:middle
Core ML models here bundled in

00:14:19.586 --> 00:14:21.436 A:middle
Xcode; City, Glass, Oils, and

00:14:21.436 --> 00:14:22.706 A:middle
Waves, the same ones we are

00:14:22.706 --> 00:14:23.406 A:middle
seeing in the app.

00:14:23.406 --> 00:14:25.796 A:middle
And we can see -- we can inspect

00:14:25.796 --> 00:14:26.336 A:middle
this model.

00:14:26.766 --> 00:14:27.976 A:middle
These are seen as quantized

00:14:27.976 --> 00:14:29.166 A:middle
model, so each one of these

00:14:29.166 --> 00:14:31.246 A:middle
models is 6.7 megabytes of this

00:14:31.246 --> 00:14:31.876 A:middle
space on disk.

00:14:33.096 --> 00:14:34.266 A:middle
We see that the models take an

00:14:34.266 --> 00:14:35.226 A:middle
input image of a certain

00:14:35.226 --> 00:14:37.706 A:middle
resolution and produce an image

00:14:37.706 --> 00:14:39.176 A:middle
called Stylized of the same

00:14:39.176 --> 00:14:39.716 A:middle
resolution.

00:14:41.016 --> 00:14:43.106 A:middle
Now we want to investigate how

00:14:43.106 --> 00:14:44.736 A:middle
much storage space and memory

00:14:44.736 --> 00:14:46.366 A:middle
space we can use by -- we can

00:14:46.366 --> 00:14:47.796 A:middle
save by switching to quantized,

00:14:47.796 --> 00:14:48.096 A:middle
models.

00:14:48.096 --> 00:14:49.716 A:middle
So I have been playing with Core

00:14:49.716 --> 00:14:52.656 A:middle
ML Tools and obtained quantizer

00:14:52.656 --> 00:14:55.236 A:middle
presentation for these models.

00:14:56.216 --> 00:14:57.486 A:middle
And for a tutorial about how to

00:14:57.486 --> 00:14:59.446 A:middle
obtain these models, stay for

00:14:59.446 --> 00:15:01.056 A:middle
Part 2 that is going to cover

00:14:59.446 --> 00:15:01.056 A:middle
Part 2 that is going to cover

00:15:01.056 --> 00:15:02.766 A:middle
quantization with Core ML Tools

00:15:02.766 --> 00:15:02.976 A:middle
in detail.

00:15:04.126 --> 00:15:05.396 A:middle
So I want to focus first on the

00:15:05.396 --> 00:15:07.856 A:middle
Glass style and see how the

00:15:07.856 --> 00:15:09.416 A:middle
different quantization versions

00:15:09.696 --> 00:15:10.746 A:middle
work for these styles.

00:15:11.836 --> 00:15:13.646 A:middle
So all I have to do is drag

00:15:13.646 --> 00:15:15.076 A:middle
these new models inside the

00:15:15.076 --> 00:15:17.756 A:middle
Xcode project, and rerun the

00:15:17.876 --> 00:15:17.943 A:middle
app.

00:15:18.046 --> 00:15:19.976 A:middle
And then we are going to see how

00:15:19.976 --> 00:15:20.806 A:middle
these models behave.

00:15:21.296 --> 00:15:23.986 A:middle
First we can see that the size

00:15:23.986 --> 00:15:25.856 A:middle
has been greatly reduced.

00:15:25.856 --> 00:15:27.906 A:middle
For example, the 8-bit version

00:15:27.906 --> 00:15:29.906 A:middle
already from 6 or 7 megabytes

00:15:29.906 --> 00:15:30.976 A:middle
went down to just 1.7.

00:15:31.516 --> 00:15:36.576 A:middle
[ Applause ]

00:15:37.076 --> 00:15:39.226 A:middle
In 4-bit, we can save even more,

00:15:39.226 --> 00:15:40.876 A:middle
and now the model is less than 1

00:15:40.876 --> 00:15:41.336 A:middle
megabyte.

00:15:41.936 --> 00:15:43.286 A:middle
In 3-bit, it is -- that's even

00:15:43.286 --> 00:15:44.956 A:middle
smaller, at 49 kilobytes.

00:15:44.956 --> 00:15:45.946 A:middle
And so on.

00:15:47.106 --> 00:15:49.326 A:middle
Now let's go back to the app.

00:15:50.826 --> 00:15:52.616 A:middle
Let's make this same image for

00:15:52.616 --> 00:15:54.486 A:middle
reference and apply the Glass

00:15:54.536 --> 00:15:56.276 A:middle
style in the original version.

00:15:57.136 --> 00:15:58.266 A:middle
Still looks as before.

00:15:59.016 --> 00:16:00.676 A:middle
Now we can compare it with the

00:15:59.016 --> 00:16:00.676 A:middle
Now we can compare it with the

00:16:00.676 --> 00:16:01.716 A:middle
8-bit version.

00:16:02.036 --> 00:16:05.796 A:middle
And you can see nothing has

00:16:05.866 --> 00:16:06.186 A:middle
changed.

00:16:06.796 --> 00:16:07.806 A:middle
This is because 8-bit

00:16:07.806 --> 00:16:09.156 A:middle
quantization methods are very

00:16:09.156 --> 00:16:09.536 A:middle
solid.

00:16:10.806 --> 00:16:13.176 A:middle
We can also venture further and

00:16:13.176 --> 00:16:14.956 A:middle
try the 4-bit version of this

00:16:14.956 --> 00:16:15.316 A:middle
model.

00:16:15.866 --> 00:16:18.256 A:middle
Wow. The results are still

00:16:18.256 --> 00:16:18.626 A:middle
great.

00:16:19.896 --> 00:16:21.366 A:middle
And now let's try the 3-bit

00:16:21.366 --> 00:16:21.836 A:middle
version.

00:16:24.556 --> 00:16:26.386 A:middle
We see that there are -- we see

00:16:26.386 --> 00:16:27.506 A:middle
the first color shift.

00:16:27.506 --> 00:16:29.116 A:middle
So it probably is good if we go

00:16:29.116 --> 00:16:30.426 A:middle
and check with the designers if

00:16:30.426 --> 00:16:32.446 A:middle
this effect is still acceptable.

00:16:33.196 --> 00:16:34.596 A:middle
And now, as we see the 2-bit

00:16:34.596 --> 00:16:37.256 A:middle
version, this is not really what

00:16:37.256 --> 00:16:37.926 A:middle
we were looking for.

00:16:37.926 --> 00:16:39.056 A:middle
Maybe we will save it for a

00:16:39.056 --> 00:16:40.376 A:middle
horror app, but I am not going

00:16:40.376 --> 00:16:40.976 A:middle
to show this to the designer.

00:16:41.516 --> 00:16:46.026 A:middle
[ Applause ]

00:16:46.526 --> 00:16:47.526 A:middle
Let's go back to the 4-bit

00:16:47.566 --> 00:16:48.626 A:middle
version and hide this one.

00:16:49.196 --> 00:16:51.056 A:middle
This was just a reminder that

00:16:51.056 --> 00:16:52.686 A:middle
quantized models are

00:16:52.686 --> 00:16:54.156 A:middle
approximation of the original

00:16:54.156 --> 00:16:54.356 A:middle
models.

00:16:55.216 --> 00:16:56.366 A:middle
So it's always recommended to

00:16:56.416 --> 00:16:57.866 A:middle
check them and compare with the

00:16:57.866 --> 00:16:58.696 A:middle
original versions.

00:16:59.106 --> 00:17:00.716 A:middle
Now for every model and

00:16:59.106 --> 00:17:00.716 A:middle
Now for every model and

00:17:00.716 --> 00:17:02.156 A:middle
quantization technique, there is

00:17:02.156 --> 00:17:03.356 A:middle
always a point in which things

00:17:03.356 --> 00:17:06.526 A:middle
start to mismatch.

00:17:06.526 --> 00:17:08.376 A:middle
Now we -- after some discussion

00:17:08.376 --> 00:17:09.526 A:middle
with the designer, extensive

00:17:09.526 --> 00:17:10.846 A:middle
evaluation of many images, we

00:17:10.846 --> 00:17:12.156 A:middle
decided to ship the 4-bit

00:17:12.156 --> 00:17:13.415 A:middle
version of this model, which is

00:17:13.715 --> 00:17:15.086 A:middle
the smallest size for the best

00:17:15.086 --> 00:17:15.506 A:middle
quality.

00:17:16.955 --> 00:17:17.886 A:middle
So let's remove all the

00:17:17.886 --> 00:17:19.566 A:middle
floating-point version of the

00:17:19.566 --> 00:17:21.445 A:middle
models that were taking a lot of

00:17:21.445 --> 00:17:23.996 A:middle
space in our app and replace

00:17:23.996 --> 00:17:25.976 A:middle
them with the 4-bit version.

00:17:30.736 --> 00:17:32.516 A:middle
And now let's run the app one

00:17:32.516 --> 00:17:32.956 A:middle
last time.

00:17:40.226 --> 00:17:41.846 A:middle
OK. Let's pick the same image

00:17:41.846 --> 00:17:48.886 A:middle
again and show all the styles.

00:17:48.886 --> 00:17:54.186 A:middle
This was the City, Glass, Oils,

00:17:54.926 --> 00:17:56.996 A:middle
and big Wave.

00:17:58.306 --> 00:18:02.076 A:middle
So in this demo we saw how we

00:17:58.306 --> 00:18:02.076 A:middle
So in this demo we saw how we

00:18:02.076 --> 00:18:04.336 A:middle
started with four models and

00:18:04.336 --> 00:18:06.606 A:middle
they were huge, in 32-bit -- or

00:18:06.606 --> 00:18:08.716 A:middle
total app size was 27 megabytes.

00:18:09.426 --> 00:18:10.966 A:middle
Then we evaluated the quality

00:18:10.966 --> 00:18:12.466 A:middle
and switched to 4-bit models,

00:18:13.026 --> 00:18:14.406 A:middle
and the total size of our app

00:18:14.406 --> 00:18:16.526 A:middle
went down to just 3.4 megabytes.

00:18:16.806 --> 00:18:16.873 A:middle
Now --

00:18:17.516 --> 00:18:22.646 A:middle
[ Applause ]

00:18:23.146 --> 00:18:24.646 A:middle
This doesn't cost us anything in

00:18:24.646 --> 00:18:26.756 A:middle
terms of quality because all

00:18:26.756 --> 00:18:27.646 A:middle
these versions -- these

00:18:27.646 --> 00:18:29.456 A:middle
quantized versions look the

00:18:29.456 --> 00:18:31.626 A:middle
same, and the quality is still

00:18:31.626 --> 00:18:32.076 A:middle
amazing.

00:18:32.616 --> 00:18:36.006 A:middle
We showed how quantization can

00:18:36.006 --> 00:18:37.446 A:middle
help us reduce the size of an

00:18:37.446 --> 00:18:39.166 A:middle
app by reducing the size of the

00:18:39.166 --> 00:18:40.796 A:middle
weight at the very microscopic

00:18:40.796 --> 00:18:41.046 A:middle
level.

00:18:41.046 --> 00:18:44.356 A:middle
Now let's see how we can reduce

00:18:44.356 --> 00:18:45.546 A:middle
the number of models that your

00:18:45.546 --> 00:18:45.976 A:middle
app needs.

00:18:46.666 --> 00:18:48.866 A:middle
In the most straightforward

00:18:48.866 --> 00:18:51.446 A:middle
case, if your app has three

00:18:51.446 --> 00:18:52.936 A:middle
machine-learning functionalities

00:18:53.306 --> 00:18:54.496 A:middle
then you need three different

00:18:54.496 --> 00:18:55.536 A:middle
machine-learning models.

00:18:56.156 --> 00:18:58.206 A:middle
But in some cases, it is

00:18:58.206 --> 00:19:01.206 A:middle
possible to have the same model

00:18:58.206 --> 00:19:01.206 A:middle
possible to have the same model

00:19:01.426 --> 00:19:03.116 A:middle
to support two different

00:19:03.116 --> 00:19:03.676 A:middle
functions.

00:19:04.406 --> 00:19:06.006 A:middle
For example, you can train a

00:19:06.006 --> 00:19:06.906 A:middle
multi-task model.

00:19:06.906 --> 00:19:09.316 A:middle
And multi-task models has been

00:19:09.316 --> 00:19:10.936 A:middle
trained to perform multiple

00:19:10.936 --> 00:19:11.626 A:middle
things at once.

00:19:12.446 --> 00:19:13.656 A:middle
There is an example about style

00:19:13.656 --> 00:19:14.776 A:middle
transferring, the Turi Create

00:19:14.836 --> 00:19:16.276 A:middle
session about multi-task models.

00:19:16.936 --> 00:19:19.366 A:middle
Or in some cases, you can use a

00:19:19.366 --> 00:19:20.856 A:middle
yet new feature in Core ML

00:19:21.146 --> 00:19:22.596 A:middle
called Flexible Shapes and

00:19:22.596 --> 00:19:23.076 A:middle
Sizes.

00:19:24.386 --> 00:19:27.006 A:middle
Let's go back to our Style

00:19:27.006 --> 00:19:27.726 A:middle
Transfer demo.

00:19:28.166 --> 00:19:30.516 A:middle
In Xcode we saw that the size of

00:19:30.516 --> 00:19:31.916 A:middle
the input image and the output

00:19:31.916 --> 00:19:34.826 A:middle
image was encoded in part of the

00:19:34.826 --> 00:19:35.966 A:middle
definition of the model.

00:19:36.816 --> 00:19:38.066 A:middle
But what if we want to run the

00:19:38.066 --> 00:19:39.666 A:middle
same style on different image

00:19:39.666 --> 00:19:40.346 A:middle
resolution?

00:19:41.056 --> 00:19:42.576 A:middle
What if we want to run the same

00:19:42.576 --> 00:19:44.746 A:middle
network on different image

00:19:44.746 --> 00:19:45.226 A:middle
sizes?

00:19:46.796 --> 00:19:49.156 A:middle
For example, the user might want

00:19:49.156 --> 00:19:50.726 A:middle
to see a high-definition style

00:19:50.726 --> 00:19:51.186 A:middle
transfer.

00:19:51.826 --> 00:19:53.906 A:middle
So they use -- they give us a

00:19:53.906 --> 00:19:54.906 A:middle
high-definition image.

00:19:55.646 --> 00:19:57.316 A:middle
Now if I were Core ML model all

00:19:57.316 --> 00:19:58.546 A:middle
it takes is a lower resolution

00:19:58.546 --> 00:20:00.766 A:middle
as an input, all we can do as

00:19:58.546 --> 00:20:00.766 A:middle
as an input, all we can do as

00:20:00.766 --> 00:20:03.436 A:middle
developers is size -- or resize

00:20:03.436 --> 00:20:06.236 A:middle
the image down, process it, and

00:20:06.236 --> 00:20:06.976 A:middle
then scale it back up.

00:20:07.786 --> 00:20:09.346 A:middle
This is not really going to

00:20:09.346 --> 00:20:10.206 A:middle
amaze the user.

00:20:10.206 --> 00:20:14.336 A:middle
Even in the past, we could

00:20:14.336 --> 00:20:16.146 A:middle
reship this model with Corel ML

00:20:16.146 --> 00:20:18.206 A:middle
Tools and make it accept any

00:20:18.206 --> 00:20:20.396 A:middle
resolution, in particular, a

00:20:20.396 --> 00:20:21.826 A:middle
higher-resolution image.

00:20:23.246 --> 00:20:24.756 A:middle
So even in the past we could do

00:20:24.756 --> 00:20:27.096 A:middle
this feature and feed directly

00:20:27.096 --> 00:20:28.716 A:middle
the high-resolution image to a

00:20:28.716 --> 00:20:30.446 A:middle
Corel ML model, producing a

00:20:30.586 --> 00:20:31.576 A:middle
high-definition result.

00:20:31.576 --> 00:20:34.366 A:middle
This is because we wanted to

00:20:34.366 --> 00:20:36.436 A:middle
introduce a finer detail in the

00:20:36.436 --> 00:20:38.706 A:middle
stylization and the way finer

00:20:38.706 --> 00:20:40.966 A:middle
strokes that are amazing when

00:20:40.966 --> 00:20:42.406 A:middle
you zoom in, because they have

00:20:42.706 --> 00:20:43.826 A:middle
-- they add a lot of work into

00:20:43.826 --> 00:20:44.516 A:middle
the final image.

00:20:44.516 --> 00:20:47.646 A:middle
So in the past we could do it,

00:20:47.646 --> 00:20:49.656 A:middle
but we could do it by

00:20:49.656 --> 00:20:51.116 A:middle
duplicating the model and

00:20:51.116 --> 00:20:52.606 A:middle
creating two different versions:

00:20:53.096 --> 00:20:54.576 A:middle
one for the standard definition

00:20:54.576 --> 00:20:55.376 A:middle
and one for the high

00:20:55.376 --> 00:20:56.046 A:middle
definitions.

00:20:56.516 --> 00:20:57.796 A:middle
And this, of course, means that

00:20:57.796 --> 00:20:59.726 A:middle
our app is twice as much the

00:20:59.726 --> 00:21:01.356 A:middle
size -- besides the fact that

00:20:59.726 --> 00:21:01.356 A:middle
size -- besides the fact that

00:21:01.356 --> 00:21:02.516 A:middle
the network has been trained to

00:21:02.516 --> 00:21:03.736 A:middle
support any resolution.

00:21:04.526 --> 00:21:05.186 A:middle
Not anymore.

00:21:05.586 --> 00:21:06.866 A:middle
We are introducing flexible

00:21:06.866 --> 00:21:07.246 A:middle
shapes.

00:21:07.816 --> 00:21:09.036 A:middle
And with flexible shapes, you

00:21:09.036 --> 00:21:10.526 A:middle
have -- if you have -- you can

00:21:10.526 --> 00:21:12.286 A:middle
have the single model to process

00:21:12.286 --> 00:21:13.776 A:middle
more resolutions and many more

00:21:13.776 --> 00:21:14.426 A:middle
resolutions.

00:21:14.806 --> 00:21:15.926 A:middle
So now in Xcode --

00:21:16.516 --> 00:21:20.886 A:middle
[ Applause ]

00:21:21.386 --> 00:21:23.416 A:middle
-- in Xcode you are going to see

00:21:24.366 --> 00:21:26.016 A:middle
that this -- the input is still

00:21:26.016 --> 00:21:27.986 A:middle
an image, but the size of the

00:21:27.986 --> 00:21:29.746 A:middle
full resolution, the model also

00:21:29.746 --> 00:21:31.896 A:middle
accepts flexible resolutions.

00:21:32.066 --> 00:21:33.806 A:middle
In this simple example, SD and

00:21:34.106 --> 00:21:34.173 A:middle
HD.

00:21:34.173 --> 00:21:37.566 A:middle
This means that now you have to

00:21:37.566 --> 00:21:38.786 A:middle
ship a single model.

00:21:38.786 --> 00:21:41.426 A:middle
You don't have to have any

00:21:41.426 --> 00:21:42.266 A:middle
redundant code.

00:21:43.316 --> 00:21:44.316 A:middle
And if you need to switch

00:21:44.316 --> 00:21:45.726 A:middle
between standard definition and

00:21:45.726 --> 00:21:46.906 A:middle
high definition, you can do it

00:21:46.906 --> 00:21:48.386 A:middle
much faster because we don't

00:21:48.386 --> 00:21:49.456 A:middle
need to reload the model from

00:21:49.456 --> 00:21:50.876 A:middle
scratch; we just need to resize

00:21:51.496 --> 00:21:51.566 A:middle
it.

00:21:52.236 --> 00:21:54.186 A:middle
You have two options to specify

00:21:54.186 --> 00:21:55.716 A:middle
the flexibility of the model.

00:21:57.026 --> 00:21:58.556 A:middle
You can define a range for its

00:21:58.556 --> 00:22:00.416 A:middle
dimension, so you can define a

00:21:58.556 --> 00:22:00.416 A:middle
dimension, so you can define a

00:22:00.416 --> 00:22:02.036 A:middle
minimal width and height and the

00:22:02.036 --> 00:22:03.096 A:middle
maximum width and height.

00:22:03.646 --> 00:22:05.356 A:middle
And then at inference pick any

00:22:05.356 --> 00:22:06.136 A:middle
value in between.

00:22:06.766 --> 00:22:08.276 A:middle
But there is also another way.

00:22:08.576 --> 00:22:10.486 A:middle
You can enumerate all the shapes

00:22:10.486 --> 00:22:11.326 A:middle
that you are going to use.

00:22:11.736 --> 00:22:12.936 A:middle
For example, all different

00:22:12.936 --> 00:22:14.606 A:middle
aspect ratios, all different

00:22:14.606 --> 00:22:16.536 A:middle
resolutions, and this is better

00:22:16.536 --> 00:22:17.306 A:middle
for performance.

00:22:17.586 --> 00:22:19.086 A:middle
Core ML knows more about your

00:22:19.086 --> 00:22:21.006 A:middle
use case earlier, so it can --

00:22:21.006 --> 00:22:22.236 A:middle
it has the opportunities of

00:22:22.236 --> 00:22:24.106 A:middle
performing more optimizations.

00:22:24.926 --> 00:22:26.496 A:middle
And it also gives your app a

00:22:26.496 --> 00:22:27.776 A:middle
smaller tested surface.

00:22:28.286 --> 00:22:30.956 A:middle
Now which models are flexible?

00:22:30.956 --> 00:22:32.996 A:middle
Which models can be trained to

00:22:32.996 --> 00:22:34.526 A:middle
support multiple resolutions?

00:22:35.206 --> 00:22:37.426 A:middle
Fully convolutional neural

00:22:37.426 --> 00:22:39.746 A:middle
networks, commonly used for MS

00:22:39.746 --> 00:22:41.636 A:middle
processing tasks such as style

00:22:41.636 --> 00:22:43.746 A:middle
transfer, image enhancement,

00:22:43.916 --> 00:22:45.786 A:middle
super resolution, and so on --

00:22:46.016 --> 00:22:47.716 A:middle
and some of the architecture.

00:22:48.446 --> 00:22:50.956 A:middle
Core ML Tools can check if a

00:22:50.956 --> 00:22:52.636 A:middle
model has this capability for

00:22:52.636 --> 00:22:52.786 A:middle
you.

00:22:54.236 --> 00:22:55.516 A:middle
So we still have the number of

00:22:55.516 --> 00:22:57.126 A:middle
models Core ML uses in flexible

00:22:57.126 --> 00:22:58.606 A:middle
sizes, and the size of the

00:22:58.606 --> 00:22:59.516 A:middle
weights can be reduced by

00:22:59.516 --> 00:23:00.246 A:middle
quantization.

00:22:59.516 --> 00:23:00.246 A:middle
quantization.

00:23:00.646 --> 00:23:01.606 A:middle
But what about the number of

00:23:01.686 --> 00:23:01.956 A:middle
weights?

00:23:03.176 --> 00:23:05.426 A:middle
Core ML, given the fact that it

00:23:05.426 --> 00:23:07.006 A:middle
supports many, many different

00:23:07.006 --> 00:23:08.356 A:middle
architecture at any framework,

00:23:08.956 --> 00:23:10.886 A:middle
has always helped you choose the

00:23:10.886 --> 00:23:12.556 A:middle
right -- the model of the right

00:23:12.606 --> 00:23:13.706 A:middle
size for your machine-learning

00:23:13.706 --> 00:23:14.086 A:middle
problem.

00:23:14.636 --> 00:23:17.206 A:middle
So Core ML can help you tackle

00:23:17.206 --> 00:23:19.376 A:middle
the size of your app using this

00:23:20.036 --> 00:23:21.106 A:middle
-- all these three factors.

00:23:21.476 --> 00:23:23.046 A:middle
In any case, the inference is

00:23:23.046 --> 00:23:24.246 A:middle
going to be super performant.

00:23:24.246 --> 00:23:26.706 A:middle
And to introduce new features in

00:23:26.706 --> 00:23:28.066 A:middle
performance and customization,

00:23:28.126 --> 00:23:29.196 A:middle
let's welcome Bill March.

00:23:29.746 --> 00:23:29.976 A:middle
Thank you.

00:23:30.516 --> 00:23:36.776 A:middle
[ Applause ]

00:23:37.276 --> 00:23:38.346 A:middle
&gt;&gt; Thank you.

00:23:39.826 --> 00:23:41.056 A:middle
One of the fundamental design

00:23:41.056 --> 00:23:42.416 A:middle
principles of Core ML from the

00:23:42.416 --> 00:23:43.886 A:middle
very beginning has been that it

00:23:43.886 --> 00:23:45.166 A:middle
should give your app the best

00:23:45.166 --> 00:23:46.206 A:middle
possible performance.

00:23:46.806 --> 00:23:48.106 A:middle
And in keeping with that goal,

00:23:48.176 --> 00:23:49.256 A:middle
I'd like to highlight a new

00:23:49.256 --> 00:23:50.526 A:middle
feature of Core ML to help

00:23:50.526 --> 00:23:52.176 A:middle
ensure that your app will shine

00:23:52.176 --> 00:23:53.376 A:middle
on any Apple device.

00:23:54.286 --> 00:23:55.986 A:middle
Let's take a look at the style

00:23:55.986 --> 00:23:57.426 A:middle
transfer example that Francesco

00:23:57.426 --> 00:23:57.966 A:middle
showed us.

00:23:58.356 --> 00:23:59.316 A:middle
From the perspective of your

00:23:59.316 --> 00:24:01.186 A:middle
app, it takes an image of an

00:23:59.316 --> 00:24:01.186 A:middle
app, it takes an image of an

00:24:01.186 --> 00:24:02.776 A:middle
input and simply returns the

00:24:02.776 --> 00:24:03.806 A:middle
stylized image.

00:24:04.346 --> 00:24:05.736 A:middle
And there are two key components

00:24:05.736 --> 00:24:06.976 A:middle
that go into making this happen:

00:24:07.416 --> 00:24:09.896 A:middle
first, the MLModel file, which

00:24:09.896 --> 00:24:11.366 A:middle
stores the particular parameters

00:24:11.366 --> 00:24:13.686 A:middle
needed to apply this style; and

00:24:13.686 --> 00:24:15.626 A:middle
second, the inference engine,

00:24:15.766 --> 00:24:17.546 A:middle
which takes in the MLModel and

00:24:17.546 --> 00:24:18.746 A:middle
the image and performs the

00:24:18.746 --> 00:24:20.096 A:middle
calculations necessary to

00:24:20.166 --> 00:24:20.886 A:middle
produce the result.

00:24:21.886 --> 00:24:23.006 A:middle
So let's peek under the hood of

00:24:23.006 --> 00:24:24.286 A:middle
this inference engine and see

00:24:24.286 --> 00:24:25.416 A:middle
how we leverage Apple's

00:24:25.416 --> 00:24:27.146 A:middle
technology to perform this style

00:24:27.146 --> 00:24:28.416 A:middle
transfer efficiently.

00:24:30.156 --> 00:24:31.606 A:middle
This model is an example of a

00:24:31.606 --> 00:24:33.126 A:middle
neural network, which consists

00:24:33.126 --> 00:24:34.466 A:middle
of a series of mathematical

00:24:34.466 --> 00:24:35.866 A:middle
operations called layers.

00:24:36.406 --> 00:24:37.566 A:middle
Each layer applies some

00:24:37.566 --> 00:24:39.026 A:middle
transformation to the image,

00:24:39.026 --> 00:24:40.376 A:middle
finally resulting in the

00:24:40.376 --> 00:24:41.566 A:middle
stylized output.

00:24:41.996 --> 00:24:43.996 A:middle
The model stores weights for

00:24:43.996 --> 00:24:45.506 A:middle
each layer which determine the

00:24:45.556 --> 00:24:47.086 A:middle
particular transformation and

00:24:47.086 --> 00:24:48.106 A:middle
the style that we are going to

00:24:48.106 --> 00:24:48.496 A:middle
apply.

00:24:49.576 --> 00:24:50.696 A:middle
The Core ML neural network

00:24:50.696 --> 00:24:52.366 A:middle
inference engine has highly

00:24:52.366 --> 00:24:53.866 A:middle
optimized implementations for

00:24:53.866 --> 00:24:54.776 A:middle
each of these layers.

00:24:55.176 --> 00:24:57.236 A:middle
On the GPU, we use MTL shaders.

00:24:57.466 --> 00:24:58.686 A:middle
On the CPU we can use

00:24:58.686 --> 00:24:59.946 A:middle
Accelerate, the proficient

00:24:59.946 --> 00:25:00.676 A:middle
calculation.

00:24:59.946 --> 00:25:00.676 A:middle
calculation.

00:25:01.296 --> 00:25:02.626 A:middle
And we can dispatch different

00:25:02.626 --> 00:25:03.836 A:middle
parts of the computation to

00:25:03.836 --> 00:25:04.976 A:middle
different pieces of hardware

00:25:04.976 --> 00:25:06.526 A:middle
dynamically depending on the

00:25:06.526 --> 00:25:08.256 A:middle
model, the device state, and

00:25:08.256 --> 00:25:08.986 A:middle
other factors.

00:25:10.156 --> 00:25:12.736 A:middle
We can also find opportunities

00:25:12.736 --> 00:25:14.406 A:middle
to fuse layers in the network,

00:25:14.406 --> 00:25:15.836 A:middle
resulting in fewer overall

00:25:15.836 --> 00:25:17.156 A:middle
computations being needed.

00:25:18.206 --> 00:25:20.336 A:middle
We are able to optimize here

00:25:20.336 --> 00:25:21.916 A:middle
because we know what's going on.

00:25:22.206 --> 00:25:23.196 A:middle
We know the details of the

00:25:23.196 --> 00:25:24.736 A:middle
model; they are contained in the

00:25:24.736 --> 00:25:26.216 A:middle
MLModel file that you provided

00:25:26.216 --> 00:25:26.576 A:middle
to us.

00:25:27.016 --> 00:25:28.126 A:middle
And we know the details of the

00:25:28.126 --> 00:25:30.036 A:middle
inference engine and the device

00:25:30.176 --> 00:25:32.316 A:middle
because we designed them.

00:25:32.316 --> 00:25:33.716 A:middle
We can take care of all of these

00:25:33.716 --> 00:25:35.856 A:middle
optimizations for you, and you

00:25:35.856 --> 00:25:37.376 A:middle
can focus on delivering the best

00:25:37.376 --> 00:25:38.656 A:middle
user experience in your app.

00:25:39.086 --> 00:25:41.586 A:middle
But what about your workload?

00:25:42.436 --> 00:25:44.136 A:middle
What about, in particular, if

00:25:44.136 --> 00:25:45.286 A:middle
you need to make multiple

00:25:45.286 --> 00:25:45.986 A:middle
predictions?

00:25:47.196 --> 00:25:48.406 A:middle
If Core ML doesn't know about

00:25:48.406 --> 00:25:50.186 A:middle
it, then Core ML can't optimize

00:25:50.186 --> 00:25:50.486 A:middle
for it.

00:25:51.486 --> 00:25:53.096 A:middle
So in the past, if you had a

00:25:53.096 --> 00:25:55.886 A:middle
workload like this, you needed

00:25:55.886 --> 00:25:57.356 A:middle
to do something like this: a

00:25:57.356 --> 00:25:58.926 A:middle
simple for loop wrapped around a

00:25:58.926 --> 00:26:00.436 A:middle
call to the existing Core ML

00:25:58.926 --> 00:26:00.436 A:middle
call to the existing Core ML

00:26:00.436 --> 00:26:01.246 A:middle
prediction API.

00:26:01.246 --> 00:26:03.416 A:middle
So you'd loop over some array of

00:26:03.416 --> 00:26:04.766 A:middle
inputs and produce an array of

00:26:04.766 --> 00:26:05.336 A:middle
outputs.

00:26:05.716 --> 00:26:08.166 A:middle
Let's take a closer look at what

00:26:08.166 --> 00:26:10.216 A:middle
happens under the hood when this

00:26:10.216 --> 00:26:11.206 A:middle
-- when we are doing this.

00:26:12.146 --> 00:26:13.636 A:middle
For each image, we will need to

00:26:13.636 --> 00:26:15.096 A:middle
do some kind of preprocessing

00:26:15.096 --> 00:26:15.366 A:middle
work.

00:26:15.686 --> 00:26:16.876 A:middle
If nothing else, we need to send

00:26:16.876 --> 00:26:18.216 A:middle
the data down to the GPU.

00:26:19.066 --> 00:26:20.316 A:middle
Once we have done that, we can

00:26:20.316 --> 00:26:21.976 A:middle
do the calculation and produce

00:26:21.976 --> 00:26:22.816 A:middle
the output image.

00:26:23.046 --> 00:26:23.526 A:middle
But then there is a

00:26:23.526 --> 00:26:25.546 A:middle
postprocessing step in which we

00:26:25.546 --> 00:26:26.736 A:middle
need to retrieve the data from

00:26:26.736 --> 00:26:28.046 A:middle
the GPU and return it to your

00:26:28.046 --> 00:26:28.296 A:middle
app.

00:26:29.816 --> 00:26:31.176 A:middle
The key to improving this

00:26:31.176 --> 00:26:32.716 A:middle
picture is to eliminate the

00:26:32.716 --> 00:26:34.526 A:middle
bubbles in the GPU pipeline.

00:26:35.896 --> 00:26:36.876 A:middle
This results in greater

00:26:36.876 --> 00:26:38.136 A:middle
performance for two major

00:26:38.136 --> 00:26:38.726 A:middle
reasons.

00:26:38.926 --> 00:26:40.696 A:middle
First, since there is no time

00:26:40.696 --> 00:26:42.306 A:middle
when the GPU is idle the overall

00:26:42.306 --> 00:26:43.516 A:middle
compute time is reduced.

00:26:44.066 --> 00:26:45.996 A:middle
And second, because the GPU is

00:26:45.996 --> 00:26:47.856 A:middle
kept working continuously, it's

00:26:47.856 --> 00:26:49.006 A:middle
able to operate in a higher

00:26:49.006 --> 00:26:51.036 A:middle
performance state and reduce the

00:26:51.036 --> 00:26:52.886 A:middle
time necessary to compute each

00:26:52.886 --> 00:26:54.146 A:middle
particular output.

00:26:55.616 --> 00:26:56.596 A:middle
But so much of the appeal in

00:26:56.596 --> 00:26:57.806 A:middle
Core ML is that you don't have

00:26:57.806 --> 00:26:59.236 A:middle
to worry about any details like

00:26:59.236 --> 00:26:59.706 A:middle
this at all.

00:27:00.106 --> 00:27:01.706 A:middle
In fact, for your app all you

00:27:01.706 --> 00:27:02.666 A:middle
are really concerned with for

00:27:02.666 --> 00:27:05.336 A:middle
your users is going from a long

00:27:05.336 --> 00:27:06.636 A:middle
time to get results to a short

00:27:06.676 --> 00:27:06.886 A:middle
time.

00:27:07.846 --> 00:27:10.096 A:middle
So this year we are introducing

00:27:10.096 --> 00:27:11.796 A:middle
a new batch API that will allow

00:27:11.796 --> 00:27:13.026 A:middle
you to do exactly this.

00:27:14.096 --> 00:27:15.336 A:middle
Where before you needed to loop

00:27:15.336 --> 00:27:16.386 A:middle
over your inputs and call

00:27:16.386 --> 00:27:18.056 A:middle
separate predictions, the new

00:27:18.056 --> 00:27:19.156 A:middle
API is very simple.

00:27:20.466 --> 00:27:22.516 A:middle
One-line predictions, it

00:27:22.516 --> 00:27:24.366 A:middle
consumes an input -- an array of

00:27:24.366 --> 00:27:25.946 A:middle
inputs and produces an array of

00:27:25.946 --> 00:27:26.566 A:middle
outputs.

00:27:26.936 --> 00:27:27.826 A:middle
Core ML will take care of the

00:27:27.826 --> 00:27:27.976 A:middle
rest.

00:27:28.516 --> 00:27:34.336 A:middle
[ Applause ]

00:27:34.836 --> 00:27:35.716 A:middle
So let's see it in action.

00:27:36.446 --> 00:27:38.696 A:middle
So in keeping with our style

00:27:38.696 --> 00:27:40.386 A:middle
transfer example, let's look at

00:27:40.386 --> 00:27:41.376 A:middle
the case where we wanted to

00:27:41.376 --> 00:27:42.776 A:middle
apply a style to our entire

00:27:42.776 --> 00:27:43.586 A:middle
photo library.

00:27:43.956 --> 00:27:45.046 A:middle
So here I have a simple app

00:27:45.116 --> 00:27:46.066 A:middle
that's going to do just that.

00:27:46.066 --> 00:27:47.926 A:middle
I am going to apply a style to

00:27:47.926 --> 00:27:48.946 A:middle
200 images.

00:27:49.206 --> 00:27:51.396 A:middle
On the left, as in your left,

00:27:51.976 --> 00:27:54.026 A:middle
there is an implementation using

00:27:54.026 --> 00:27:55.526 A:middle
last year's API in a for loop.

00:27:55.936 --> 00:27:57.636 A:middle
And on the right we have the new

00:27:57.636 --> 00:27:58.276 A:middle
batch API.

00:27:58.626 --> 00:27:59.346 A:middle
So let's get started.

00:28:00.466 --> 00:28:01.156 A:middle
We are off.

00:28:03.596 --> 00:28:04.606 A:middle
And we can see the new is

00:28:04.606 --> 00:28:05.176 A:middle
already done.

00:28:05.176 --> 00:28:06.756 A:middle
We'll wait a moment for last

00:28:06.756 --> 00:28:08.916 A:middle
year's technology, and there we

00:28:08.916 --> 00:28:08.983 A:middle
go.

00:28:09.516 --> 00:28:14.796 A:middle
[ Applause ]

00:28:15.296 --> 00:28:16.556 A:middle
In this example we see a

00:28:16.556 --> 00:28:18.006 A:middle
noticeable improvement with the

00:28:18.006 --> 00:28:18.756 A:middle
new batch API.

00:28:19.136 --> 00:28:20.836 A:middle
And in general, the improvement

00:28:20.836 --> 00:28:22.096 A:middle
you'll see in your app depends

00:28:22.096 --> 00:28:23.906 A:middle
on the model and the device and

00:28:23.906 --> 00:28:24.546 A:middle
the workload.

00:28:25.016 --> 00:28:26.186 A:middle
But if you have a large number

00:28:26.186 --> 00:28:27.746 A:middle
of predictions to call, use the

00:28:27.746 --> 00:28:29.606 A:middle
new API and give Core ML every

00:28:29.606 --> 00:28:30.606 A:middle
opportunity to accelerate your

00:28:30.606 --> 00:28:30.976 A:middle
computation.

00:28:35.666 --> 00:28:36.796 A:middle
Of course, the most

00:28:36.796 --> 00:28:37.806 A:middle
high-performance app in the

00:28:37.806 --> 00:28:39.746 A:middle
world isn't terribly exciting if

00:28:39.746 --> 00:28:40.596 A:middle
it's not delivering an

00:28:40.596 --> 00:28:41.846 A:middle
experience that you want for

00:28:41.846 --> 00:28:42.526 A:middle
your users.

00:28:43.666 --> 00:28:44.966 A:middle
We want to ensure that no matter

00:28:44.966 --> 00:28:46.846 A:middle
what that experience is, or what

00:28:46.846 --> 00:28:48.546 A:middle
it could be in the future, Core

00:28:48.546 --> 00:28:50.066 A:middle
ML will be just as performant

00:28:50.066 --> 00:28:51.336 A:middle
and simple to use as ever.

00:28:52.486 --> 00:28:53.286 A:middle
But the field of machine

00:28:53.286 --> 00:28:54.656 A:middle
learning is growing rapidly.

00:28:55.186 --> 00:28:56.086 A:middle
How will we keep up?

00:28:56.786 --> 00:28:57.926 A:middle
And just how rapidly?

00:28:57.976 --> 00:28:59.406 A:middle
Well let me tell you a little

00:28:59.406 --> 00:29:00.636 A:middle
bit of a personal story about

00:28:59.406 --> 00:29:00.636 A:middle
bit of a personal story about

00:29:00.666 --> 00:29:00.876 A:middle
that.

00:29:02.246 --> 00:29:03.496 A:middle
Let's take a look at a

00:29:03.496 --> 00:29:05.196 A:middle
deceptively simple question that

00:29:05.196 --> 00:29:06.346 A:middle
we can answer with machine

00:29:06.346 --> 00:29:06.626 A:middle
learning.

00:29:07.686 --> 00:29:09.256 A:middle
Given an image, what I want to

00:29:09.256 --> 00:29:11.536 A:middle
know: Are there any horses in

00:29:11.536 --> 00:29:11.603 A:middle
it?

00:29:12.576 --> 00:29:14.346 A:middle
So I think I heard a chuckle or

00:29:14.346 --> 00:29:14.626 A:middle
two.

00:29:14.626 --> 00:29:15.666 A:middle
Maybe this seems like kind of a

00:29:15.666 --> 00:29:16.836 A:middle
silly challenge problem.

00:29:16.836 --> 00:29:18.626 A:middle
Small children love this, by the

00:29:18.626 --> 00:29:18.856 A:middle
way.

00:29:19.086 --> 00:29:22.056 A:middle
But -- so way, way back in the

00:29:22.106 --> 00:29:23.356 A:middle
past, when I was first starting

00:29:23.356 --> 00:29:24.446 A:middle
graduate school and I was

00:29:24.446 --> 00:29:25.746 A:middle
thinking about this problem and

00:29:25.746 --> 00:29:26.736 A:middle
first learning about machine

00:29:26.736 --> 00:29:28.476 A:middle
learning, my insights on the top

00:29:28.476 --> 00:29:29.426 A:middle
came down to something like

00:29:29.426 --> 00:29:32.306 A:middle
this: I don't know -- seems

00:29:32.306 --> 00:29:32.546 A:middle
hard.

00:29:33.226 --> 00:29:34.276 A:middle
I don't really have any good

00:29:34.276 --> 00:29:34.776 A:middle
idea for you.

00:29:35.896 --> 00:29:38.106 A:middle
So a few years pass.

00:29:38.266 --> 00:29:39.716 A:middle
I get older, hopefully a little

00:29:39.716 --> 00:29:40.316 A:middle
bit wiser.

00:29:40.316 --> 00:29:41.466 A:middle
But certainly the field is

00:29:41.466 --> 00:29:42.676 A:middle
moving very, very quickly,

00:29:42.676 --> 00:29:43.836 A:middle
because there started to be a

00:29:43.836 --> 00:29:45.176 A:middle
lot of exciting new results

00:29:45.176 --> 00:29:46.686 A:middle
using deep neural networks.

00:29:47.736 --> 00:29:48.926 A:middle
And so then my view on this

00:29:48.926 --> 00:29:49.676 A:middle
problem changed.

00:29:49.676 --> 00:29:50.716 A:middle
And suddenly, wow, this

00:29:50.716 --> 00:29:52.036 A:middle
cutting-edge research can really

00:29:52.036 --> 00:29:53.366 A:middle
answer these kind of questions,

00:29:53.366 --> 00:29:54.756 A:middle
and computers can catch up with

00:29:54.756 --> 00:29:55.556 A:middle
small children and horse

00:29:55.556 --> 00:29:56.196 A:middle
recognition technology.

00:29:56.196 --> 00:29:56.976 A:middle
What an exciting development.

00:29:57.516 --> 00:29:59.866 A:middle
[ Laughter ]

00:30:00.366 --> 00:30:02.366 A:middle
So a few more years pass.

00:30:02.486 --> 00:30:03.856 A:middle
Now I work at Apple, and my

00:30:03.856 --> 00:30:05.226 A:middle
perspective on this problem has

00:30:05.226 --> 00:30:06.476 A:middle
changed again.

00:30:06.476 --> 00:30:09.256 A:middle
Now, just grab Create ML.

00:30:09.436 --> 00:30:10.656 A:middle
The UI is lovely.

00:30:10.656 --> 00:30:11.696 A:middle
You'll have a horse classifier

00:30:11.696 --> 00:30:12.526 A:middle
in just a few minutes.

00:30:13.366 --> 00:30:15.046 A:middle
So, you know, if you are a

00:30:15.046 --> 00:30:16.226 A:middle
machine learning expert, maybe

00:30:16.226 --> 00:30:17.136 A:middle
you are looking at this and you

00:30:17.136 --> 00:30:18.016 A:middle
are thinking, "Oh, this guy

00:30:18.016 --> 00:30:18.926 A:middle
doesn't know what he is talking

00:30:18.926 --> 00:30:19.196 A:middle
about.

00:30:19.196 --> 00:30:20.506 A:middle
You know, in 2007 I knew how to

00:30:20.506 --> 00:30:21.376 A:middle
solve that problem.

00:30:21.376 --> 00:30:23.336 A:middle
In 2012 I'd solved it a hundred

00:30:23.336 --> 00:30:23.646 A:middle
times."

00:30:24.506 --> 00:30:25.076 A:middle
Not my point.

00:30:25.756 --> 00:30:27.146 A:middle
If you are someone who cares

00:30:27.146 --> 00:30:29.446 A:middle
about long-lasting, high-quality

00:30:29.446 --> 00:30:30.976 A:middle
software, this should make you

00:30:30.976 --> 00:30:32.796 A:middle
nervous, because in 11 years we

00:30:32.796 --> 00:30:34.136 A:middle
have seen the entire picture of

00:30:34.136 --> 00:30:35.126 A:middle
this problem turn over.

00:30:36.246 --> 00:30:37.386 A:middle
So let's take a look at a few

00:30:37.386 --> 00:30:38.676 A:middle
more features in Core ML that

00:30:38.676 --> 00:30:40.146 A:middle
can help set your mind at ease.

00:30:41.606 --> 00:30:43.346 A:middle
To do that, let's open the hood

00:30:43.346 --> 00:30:44.586 A:middle
once again and peek at one of

00:30:44.586 --> 00:30:46.176 A:middle
these new horse finder models,

00:30:46.656 --> 00:30:48.406 A:middle
which is, once again, a neural

00:30:48.406 --> 00:30:48.766 A:middle
network.

00:30:49.526 --> 00:30:51.106 A:middle
As we have illustrated before,

00:30:51.306 --> 00:30:52.696 A:middle
the neural network consists of a

00:30:52.696 --> 00:30:54.266 A:middle
series of highly optimized

00:30:54.266 --> 00:30:54.696 A:middle
layers.

00:30:54.746 --> 00:30:56.266 A:middle
It is a series of layers, and we

00:30:56.266 --> 00:30:57.176 A:middle
have highly optimized

00:30:57.176 --> 00:30:58.586 A:middle
implementations for each of them

00:30:58.626 --> 00:30:59.916 A:middle
in our inference engine.

00:31:00.656 --> 00:31:02.396 A:middle
Our list of supported operations

00:31:02.396 --> 00:31:03.806 A:middle
is large and always growing,

00:31:04.306 --> 00:31:05.416 A:middle
trying to keep up with new

00:31:05.416 --> 00:31:06.506 A:middle
developments in the field.

00:31:07.436 --> 00:31:08.746 A:middle
But what if there is a layer

00:31:08.746 --> 00:31:10.116 A:middle
that just isn't supported in

00:31:10.116 --> 00:31:10.646 A:middle
Core ML?

00:31:11.256 --> 00:31:14.476 A:middle
In the past, you either needed

00:31:14.476 --> 00:31:15.926 A:middle
to wait or you needed a

00:31:15.926 --> 00:31:16.516 A:middle
different model.

00:31:16.516 --> 00:31:18.866 A:middle
But what if this layer is the

00:31:19.126 --> 00:31:20.596 A:middle
key horse-finding layer?

00:31:21.076 --> 00:31:22.166 A:middle
This is the breakthrough that

00:31:22.166 --> 00:31:23.626 A:middle
your horse app was waiting for.

00:31:23.966 --> 00:31:24.836 A:middle
Can you afford to wait?

00:31:26.546 --> 00:31:27.816 A:middle
Given the speed of machine

00:31:27.816 --> 00:31:28.566 A:middle
learning, this could be a

00:31:28.566 --> 00:31:29.426 A:middle
serious obstacle.

00:31:31.216 --> 00:31:33.376 A:middle
So we introduced custom layers

00:31:33.376 --> 00:31:34.406 A:middle
for neural network models.

00:31:35.076 --> 00:31:37.176 A:middle
Now if a neural network layer is

00:31:37.176 --> 00:31:38.866 A:middle
missing, you can provide an

00:31:38.866 --> 00:31:40.926 A:middle
implementation with -- will mesh

00:31:41.116 --> 00:31:42.326 A:middle
seamlessly with the rest of the

00:31:42.326 --> 00:31:43.186 A:middle
Core ML model.

00:31:44.026 --> 00:31:45.606 A:middle
Inside the model, the custom

00:31:45.606 --> 00:31:46.876 A:middle
layer stores the name of an

00:31:46.876 --> 00:31:48.166 A:middle
implementing class -- the

00:31:48.166 --> 00:31:49.726 A:middle
AAPLCustomHorseLayer in this

00:31:49.726 --> 00:31:50.126 A:middle
case.

00:31:50.906 --> 00:31:52.626 A:middle
The implementation class fills

00:31:52.626 --> 00:31:53.656 A:middle
the role of the missing

00:31:53.656 --> 00:31:54.976 A:middle
implementation in the inference

00:31:54.976 --> 00:31:55.286 A:middle
engine.

00:31:55.906 --> 00:31:56.936 A:middle
Just like the layer is built

00:31:56.936 --> 00:31:59.406 A:middle
into Core ML, the implementation

00:31:59.406 --> 00:32:00.856 A:middle
provided here should be general

00:31:59.406 --> 00:32:00.856 A:middle
provided here should be general

00:32:00.856 --> 00:32:02.336 A:middle
and applicable to any instance

00:32:02.336 --> 00:32:02.916 A:middle
of the new layer.

00:32:04.736 --> 00:32:05.986 A:middle
It simply needs to be included

00:32:05.986 --> 00:32:07.206 A:middle
in your app at runtime.

00:32:07.706 --> 00:32:08.996 A:middle
Then the parameters for this

00:32:08.996 --> 00:32:10.196 A:middle
particular layer are

00:32:10.196 --> 00:32:11.946 A:middle
encapsulated in the ML model

00:32:12.106 --> 00:32:13.466 A:middle
with the rest of the information

00:32:13.466 --> 00:32:14.136 A:middle
about the model.

00:32:15.996 --> 00:32:17.196 A:middle
Implementing a custom layer is

00:32:17.196 --> 00:32:17.476 A:middle
simple.

00:32:18.136 --> 00:32:19.576 A:middle
We expose an MLCustomLayer

00:32:19.576 --> 00:32:20.196 A:middle
protocol.

00:32:20.556 --> 00:32:21.816 A:middle
You simply provide methods to

00:32:21.816 --> 00:32:23.356 A:middle
initialize the layer based on

00:32:23.356 --> 00:32:24.846 A:middle
the data stored in the ML model.

00:32:26.096 --> 00:32:27.176 A:middle
You'll need to provide a method

00:32:27.176 --> 00:32:28.516 A:middle
that tells us how much space to

00:32:28.516 --> 00:32:29.826 A:middle
allocate for the outputs of the

00:32:29.826 --> 00:32:31.846 A:middle
layer, and then a method that

00:32:31.846 --> 00:32:32.846 A:middle
does the computation.

00:32:34.676 --> 00:32:36.596 A:middle
Plus, you can add this

00:32:36.596 --> 00:32:38.406 A:middle
flexibility without sacrificing

00:32:38.406 --> 00:32:39.626 A:middle
the performance of your model as

00:32:39.626 --> 00:32:39.946 A:middle
a whole.

00:32:40.976 --> 00:32:41.996 A:middle
The protocol includes an

00:32:41.996 --> 00:32:43.336 A:middle
optional method, which allows

00:32:43.336 --> 00:32:44.506 A:middle
you to provide us with a MTL

00:32:44.506 --> 00:32:45.946 A:middle
shader implementation of your

00:32:45.946 --> 00:32:47.436 A:middle
model -- of the layer, excuse

00:32:47.436 --> 00:32:47.556 A:middle
me.

00:32:47.936 --> 00:32:50.056 A:middle
If you give us this, then it can

00:32:50.056 --> 00:32:51.556 A:middle
be encoded in the same command

00:32:51.556 --> 00:32:52.796 A:middle
buffer as the rest of the Core

00:32:52.796 --> 00:32:53.866 A:middle
ML computation.

00:32:53.866 --> 00:32:55.026 A:middle
So there is no extra overhead

00:32:55.026 --> 00:32:56.196 A:middle
from additional encodings or

00:32:56.196 --> 00:32:57.346 A:middle
multiple trips to and from the

00:32:57.346 --> 00:32:57.786 A:middle
GPU.

00:32:58.536 --> 00:32:59.826 A:middle
If you don't provide this, then

00:32:59.826 --> 00:33:01.026 A:middle
we'll simply evaluate the layer

00:32:59.826 --> 00:33:01.026 A:middle
we'll simply evaluate the layer

00:33:01.026 --> 00:33:02.546 A:middle
on the CPU with no other work on

00:33:02.546 --> 00:33:03.506 A:middle
your part.

00:33:04.256 --> 00:33:06.016 A:middle
So no matter how quickly

00:33:06.016 --> 00:33:07.186 A:middle
advancements in neural network

00:33:07.186 --> 00:33:08.466 A:middle
models may happen, you have a

00:33:08.466 --> 00:33:09.716 A:middle
way to keep up with Core ML.

00:33:10.556 --> 00:33:11.936 A:middle
But there are limitations.

00:33:12.836 --> 00:33:14.196 A:middle
Custom layers only work for

00:33:14.196 --> 00:33:15.476 A:middle
neural network models, and they

00:33:15.476 --> 00:33:16.816 A:middle
only take inputs and outputs

00:33:16.816 --> 00:33:18.066 A:middle
which are ML MultiArrays.

00:33:18.066 --> 00:33:19.986 A:middle
This is a natural way to

00:33:19.986 --> 00:33:21.326 A:middle
interact with neural networks.

00:33:21.686 --> 00:33:22.706 A:middle
But the machine learning field

00:33:22.706 --> 00:33:24.406 A:middle
is hardly restricted to only

00:33:24.406 --> 00:33:25.556 A:middle
advancing in this area.

00:33:26.656 --> 00:33:27.666 A:middle
In fact, when I was first

00:33:27.666 --> 00:33:28.406 A:middle
learning about image

00:33:28.406 --> 00:33:29.996 A:middle
recognition, almost no one was

00:33:29.996 --> 00:33:31.416 A:middle
talking about neural networks as

00:33:31.416 --> 00:33:32.626 A:middle
a solution to that problem.

00:33:32.986 --> 00:33:34.006 A:middle
And you can see today it's the

00:33:34.006 --> 00:33:37.186 A:middle
absolute state of the art.

00:33:37.186 --> 00:33:38.686 A:middle
And it's not hard to imagine

00:33:38.686 --> 00:33:39.896 A:middle
machine-learning-enabled app

00:33:39.896 --> 00:33:41.426 A:middle
experiences where custom layers

00:33:41.426 --> 00:33:42.426 A:middle
simply wouldn't fit.

00:33:42.906 --> 00:33:44.666 A:middle
For instance, a machine-learning

00:33:44.666 --> 00:33:46.246 A:middle
app might use a neural network

00:33:46.246 --> 00:33:47.446 A:middle
to embed an image in some

00:33:47.446 --> 00:33:49.356 A:middle
similarity space, then look up

00:33:49.356 --> 00:33:50.576 A:middle
similar images using a

00:33:50.576 --> 00:33:51.776 A:middle
nearest-neighbor method or

00:33:51.776 --> 00:33:53.456 A:middle
locality-sensitive hashing -- or

00:33:53.456 --> 00:33:54.626 A:middle
even some other approach.

00:33:56.896 --> 00:33:58.306 A:middle
A model might combine audio and

00:33:58.306 --> 00:33:59.736 A:middle
motion data to provide a bit of

00:33:59.736 --> 00:34:01.096 A:middle
needed encouragement to someone

00:33:59.736 --> 00:34:01.096 A:middle
needed encouragement to someone

00:34:01.096 --> 00:34:02.126 A:middle
who doesn't always close his

00:34:02.126 --> 00:34:02.506 A:middle
rings.

00:34:04.826 --> 00:34:06.336 A:middle
Or even a completely new model

00:34:06.336 --> 00:34:07.776 A:middle
type we haven't even imagined

00:34:07.776 --> 00:34:08.886 A:middle
yet that enables novel

00:34:08.886 --> 00:34:10.286 A:middle
experiences for your users.

00:34:11.076 --> 00:34:12.266 A:middle
In all these cases, it would be

00:34:12.266 --> 00:34:13.116 A:middle
great if we could have the

00:34:13.116 --> 00:34:14.545 A:middle
simplicity and portability of

00:34:14.545 --> 00:34:16.496 A:middle
Core ML without having to

00:34:16.496 --> 00:34:18.106 A:middle
sacrifice the flexibility to

00:34:18.106 --> 00:34:19.096 A:middle
keep up with the field.

00:34:19.815 --> 00:34:22.005 A:middle
So we are introducing custom

00:34:22.005 --> 00:34:22.505 A:middle
models.

00:34:23.496 --> 00:34:25.056 A:middle
A Core ML custom model allows

00:34:25.056 --> 00:34:26.045 A:middle
you to encapsulate the

00:34:26.045 --> 00:34:27.315 A:middle
implementation of a part of a

00:34:27.315 --> 00:34:28.576 A:middle
computation that's missing

00:34:28.735 --> 00:34:29.686 A:middle
inside Core ML.

00:34:30.436 --> 00:34:32.045 A:middle
Just like for custom layers, the

00:34:32.045 --> 00:34:33.286 A:middle
model stores the name of an

00:34:33.286 --> 00:34:34.456 A:middle
implementation class.

00:34:34.956 --> 00:34:36.216 A:middle
The class fills the role of the

00:34:36.216 --> 00:34:37.606 A:middle
general inference engine for

00:34:37.606 --> 00:34:38.656 A:middle
this type of model.

00:34:39.116 --> 00:34:40.906 A:middle
Then the parameters are stored

00:34:40.906 --> 00:34:42.235 A:middle
in the ML Model just like

00:34:42.266 --> 00:34:42.626 A:middle
before.

00:34:43.386 --> 00:34:44.826 A:middle
This allows the model to be

00:34:44.826 --> 00:34:46.366 A:middle
updated as an asset in your app

00:34:46.366 --> 00:34:48.286 A:middle
without having to touch code.

00:34:50.005 --> 00:34:51.806 A:middle
And implementing a custom model

00:34:51.806 --> 00:34:52.556 A:middle
is simple as well.

00:34:52.896 --> 00:34:54.016 A:middle
We expose a protocol,

00:34:54.065 --> 00:34:55.216 A:middle
MLCustomModel.

00:34:55.466 --> 00:34:56.485 A:middle
You provide methods to

00:34:56.485 --> 00:34:57.876 A:middle
initialize based on the data

00:34:57.876 --> 00:34:59.126 A:middle
stored in the ML Model.

00:34:59.126 --> 00:35:01.076 A:middle
And you provide a method to

00:34:59.126 --> 00:35:01.076 A:middle
And you provide a method to

00:35:01.076 --> 00:35:02.206 A:middle
compute the prediction on an

00:35:02.206 --> 00:35:02.656 A:middle
input.

00:35:02.976 --> 00:35:04.626 A:middle
There is an optional method to

00:35:04.626 --> 00:35:06.146 A:middle
provide a batch implementation

00:35:06.146 --> 00:35:07.276 A:middle
if there are opportunities in

00:35:07.276 --> 00:35:08.776 A:middle
this particular model type to

00:35:08.776 --> 00:35:10.006 A:middle
have optimizations there.

00:35:10.286 --> 00:35:11.276 A:middle
And if not, we'll call the

00:35:11.276 --> 00:35:13.126 A:middle
single prediction in a for loop.

00:35:14.026 --> 00:35:16.046 A:middle
And using a customized model in

00:35:16.046 --> 00:35:17.316 A:middle
your app is largely the same

00:35:17.316 --> 00:35:18.696 A:middle
workflow as any other Core ML

00:35:18.696 --> 00:35:19.066 A:middle
model.

00:35:19.646 --> 00:35:21.156 A:middle
In Xcode, a model with

00:35:21.156 --> 00:35:22.936 A:middle
customized components will have

00:35:22.936 --> 00:35:24.986 A:middle
a dependency section listing the

00:35:24.986 --> 00:35:26.286 A:middle
names of the implementations

00:35:26.286 --> 00:35:27.366 A:middle
needed along with a short

00:35:27.366 --> 00:35:27.956 A:middle
description.

00:35:28.576 --> 00:35:29.816 A:middle
Just include these in your app,

00:35:29.936 --> 00:35:30.836 A:middle
and you are ready to go.

00:35:31.696 --> 00:35:33.616 A:middle
The prediction API is unchanged,

00:35:33.616 --> 00:35:34.946 A:middle
whether for single predictions

00:35:34.996 --> 00:35:35.646 A:middle
or batch.

00:35:37.016 --> 00:35:39.476 A:middle
So custom layers and custom

00:35:39.476 --> 00:35:41.086 A:middle
models allow you to use the

00:35:41.086 --> 00:35:42.806 A:middle
power and simplicity of Core ML

00:35:42.806 --> 00:35:43.896 A:middle
without sacrificing the

00:35:43.896 --> 00:35:45.676 A:middle
flexibility needed to keep up

00:35:45.676 --> 00:35:46.886 A:middle
with the fast-paced area of

00:35:46.886 --> 00:35:47.496 A:middle
machine learning.

00:35:48.566 --> 00:35:49.896 A:middle
For new neural network layers,

00:35:50.096 --> 00:35:52.016 A:middle
custom layers allow you to make

00:35:52.066 --> 00:35:53.566 A:middle
use of the many optimizations

00:35:53.566 --> 00:35:54.716 A:middle
already present in the neural

00:35:54.716 --> 00:35:56.176 A:middle
network inference engine in Core

00:35:56.176 --> 00:35:56.446 A:middle
ML.

00:35:57.096 --> 00:35:58.956 A:middle
Custom models are more flexible

00:35:59.816 --> 00:36:01.686 A:middle
for types and functionality, but

00:35:59.816 --> 00:36:01.686 A:middle
for types and functionality, but

00:36:01.686 --> 00:36:02.656 A:middle
they do require more

00:36:02.656 --> 00:36:04.006 A:middle
implementation work on your

00:36:05.296 --> 00:36:05.436 A:middle
part.

00:36:05.806 --> 00:36:07.316 A:middle
Both forms of customization

00:36:07.316 --> 00:36:08.916 A:middle
allow you to encapsulate model

00:36:08.916 --> 00:36:10.346 A:middle
parameters in an ML model,

00:36:10.826 --> 00:36:12.176 A:middle
making the model portable and

00:36:12.176 --> 00:36:13.156 A:middle
your code simpler.

00:36:14.516 --> 00:36:16.816 A:middle
And we've only been able to

00:36:16.816 --> 00:36:18.146 A:middle
touch on a few of the great new

00:36:18.146 --> 00:36:19.506 A:middle
features in Core ML 2.

00:36:20.146 --> 00:36:22.336 A:middle
Please download the beta, try

00:36:22.336 --> 00:36:22.976 A:middle
them out for yourself.

00:36:27.066 --> 00:36:28.556 A:middle
Core ML has many great new

00:36:28.556 --> 00:36:29.926 A:middle
features to reduce your app

00:36:29.926 --> 00:36:31.916 A:middle
size, improve performance, and

00:36:31.916 --> 00:36:33.136 A:middle
ensure flexibility and

00:36:33.136 --> 00:36:34.476 A:middle
compatibility with the latest

00:36:34.476 --> 00:36:35.406 A:middle
developments in machine

00:36:35.406 --> 00:36:35.686 A:middle
learning.

00:36:36.496 --> 00:36:37.886 A:middle
We showed you how quantization

00:36:37.886 --> 00:36:39.676 A:middle
can reduce model size, how the

00:36:39.676 --> 00:36:41.336 A:middle
new batch API can enable more

00:36:41.336 --> 00:36:43.156 A:middle
efficient processing, and how

00:36:43.156 --> 00:36:44.936 A:middle
custom layers and custom models

00:36:44.936 --> 00:36:46.226 A:middle
can help you bring cutting-edge

00:36:46.226 --> 00:36:47.446 A:middle
machine learning to your app.

00:36:48.416 --> 00:36:49.756 A:middle
Combined with our great new tool

00:36:49.756 --> 00:36:51.236 A:middle
for training models in Create

00:36:51.236 --> 00:36:52.746 A:middle
ML, there are more ways than

00:36:52.746 --> 00:36:54.596 A:middle
ever to add ML-enabled features

00:36:54.596 --> 00:36:56.176 A:middle
to your app and support great

00:36:56.176 --> 00:36:57.736 A:middle
new experiences for your users.

00:36:59.576 --> 00:37:01.376 A:middle
After just a short break, we'll

00:36:59.576 --> 00:37:01.376 A:middle
After just a short break, we'll

00:37:01.376 --> 00:37:02.986 A:middle
be back right here to take a

00:37:02.986 --> 00:37:04.066 A:middle
deeper look at some of these

00:37:04.066 --> 00:37:04.626 A:middle
features.

00:37:04.896 --> 00:37:06.286 A:middle
In particular, we'll show you

00:37:06.286 --> 00:37:07.686 A:middle
how to use our Core ML Tools

00:37:07.686 --> 00:37:09.576 A:middle
software to start reducing model

00:37:09.576 --> 00:37:11.596 A:middle
sizes and customizing your user

00:37:11.596 --> 00:37:13.096 A:middle
experiences with Core ML today.

00:37:13.096 --> 00:37:13.976 A:middle
Thank you.

00:37:14.516 --> 00:37:20.500 A:middle
[ Applause ]
