WEBVTT

00:00:07.516 --> 00:00:16.500 A:middle
[ Music ]

00:00:19.516 --> 00:00:25.500 A:middle
[ Applause ]

00:00:26.456 --> 00:00:27.606 A:middle
&gt;&gt; Hello, everybody.

00:00:27.986 --> 00:00:29.526 A:middle
I'm very excited to be here

00:00:29.526 --> 00:00:31.446 A:middle
today to talk about

00:00:31.446 --> 00:00:34.326 A:middle
understanding ARKit Tracking and

00:00:34.326 --> 00:00:36.496 A:middle
Detection to empower you to

00:00:36.496 --> 00:00:38.796 A:middle
create great augmented reality

00:00:38.796 --> 00:00:39.576 A:middle
experiences.

00:00:40.596 --> 00:00:42.316 A:middle
My name is Marion and I'm from

00:00:42.346 --> 00:00:43.266 A:middle
the ARKit Team.

00:00:43.266 --> 00:00:44.756 A:middle
And what about you?

00:00:46.126 --> 00:00:47.786 A:middle
Are you an experienced ARKit

00:00:47.786 --> 00:00:49.206 A:middle
developer, already, but you are

00:00:49.206 --> 00:00:50.736 A:middle
interested in what's going on

00:00:50.736 --> 00:00:51.276 A:middle
under the hood?

00:00:51.896 --> 00:00:53.316 A:middle
Then, this talk is for you.

00:00:53.316 --> 00:00:56.596 A:middle
Or you may be new to ARKit.

00:00:57.546 --> 00:00:59.606 A:middle
Then, you'll learn different

00:00:59.606 --> 00:01:01.046 A:middle
kind of tracking technologies,

00:00:59.606 --> 00:01:01.046 A:middle
kind of tracking technologies,

00:01:01.206 --> 00:01:02.876 A:middle
as well as some basics and

00:01:02.966 --> 00:01:04.676 A:middle
terminology used in augmented

00:01:04.676 --> 00:01:06.756 A:middle
reality, which will then help

00:01:06.756 --> 00:01:08.176 A:middle
you to create your very own

00:01:08.176 --> 00:01:09.416 A:middle
first augmented reality

00:01:09.416 --> 00:01:10.016 A:middle
experience.

00:01:10.726 --> 00:01:14.586 A:middle
So, let's get started.

00:01:15.106 --> 00:01:17.926 A:middle
What's tracking?

00:01:18.066 --> 00:01:19.536 A:middle
Tracking provides your camera

00:01:19.836 --> 00:01:22.906 A:middle
viewing position and orientation

00:01:23.156 --> 00:01:24.956 A:middle
into your physical environment,

00:01:25.376 --> 00:01:26.806 A:middle
which will then allow you to

00:01:26.806 --> 00:01:29.036 A:middle
augment virtual content into

00:01:29.036 --> 00:01:30.096 A:middle
your camera's view.

00:01:31.146 --> 00:01:33.656 A:middle
In this video, for example, the

00:01:33.696 --> 00:01:35.836 A:middle
front table and the chairs is

00:01:35.836 --> 00:01:39.206 A:middle
virtual content augmented on top

00:01:39.926 --> 00:01:41.416 A:middle
of the real physical terrace.

00:01:42.756 --> 00:01:44.006 A:middle
This, by the way, is Ikea.

00:01:45.296 --> 00:01:47.186 A:middle
And the virtual content will

00:01:47.186 --> 00:01:48.926 A:middle
appear always virtually correct.

00:01:49.746 --> 00:01:52.086 A:middle
Correct placement, correct size,

00:01:52.686 --> 00:01:53.816 A:middle
and correct perspective

00:01:53.816 --> 00:01:54.516 A:middle
appearance.

00:01:55.066 --> 00:01:56.836 A:middle
So, different tracking

00:01:56.836 --> 00:01:58.906 A:middle
technologies are just providing

00:01:59.076 --> 00:02:00.606 A:middle
a difference reference system

00:01:59.076 --> 00:02:00.606 A:middle
a difference reference system

00:02:00.906 --> 00:02:01.586 A:middle
for the camera.

00:02:01.816 --> 00:02:03.556 A:middle
Meaning the camera with respect

00:02:03.556 --> 00:02:05.246 A:middle
to your world, the camera with

00:02:05.246 --> 00:02:07.106 A:middle
respect to an image, or maybe, a

00:02:07.106 --> 00:02:07.916 A:middle
3D object.

00:02:09.086 --> 00:02:11.186 A:middle
And we'll talk about those

00:02:11.296 --> 00:02:12.396 A:middle
different kind of tracking

00:02:12.396 --> 00:02:14.116 A:middle
technologies in the next hour,

00:02:14.936 --> 00:02:16.476 A:middle
such that you'll be able to make

00:02:16.476 --> 00:02:17.626 A:middle
the right choice for your

00:02:17.626 --> 00:02:18.796 A:middle
specific use case.

00:02:19.216 --> 00:02:22.296 A:middle
We'll talk about the already

00:02:22.296 --> 00:02:24.196 A:middle
existing AR technologies'

00:02:24.486 --> 00:02:26.726 A:middle
Orientation Tracking, World

00:02:26.866 --> 00:02:28.876 A:middle
Tracking, and Plane Detection.

00:02:29.616 --> 00:02:31.376 A:middle
Before we then have a close look

00:02:31.496 --> 00:02:33.466 A:middle
at our new tracking and

00:02:33.466 --> 00:02:36.536 A:middle
detection technologies which

00:02:36.536 --> 00:02:38.076 A:middle
came out now with ARKit 2.

00:02:38.696 --> 00:02:40.316 A:middle
Which are saving and loading

00:02:40.316 --> 00:02:43.126 A:middle
maps, image tracking, and object

00:02:43.186 --> 00:02:43.776 A:middle
detection.

00:02:45.266 --> 00:02:46.926 A:middle
But before diving deep into

00:02:46.926 --> 00:02:48.886 A:middle
those technologies, let's start

00:02:48.946 --> 00:02:51.416 A:middle
with a very short recap of ARKit

00:02:51.416 --> 00:02:52.646 A:middle
like on a high level.

00:02:53.176 --> 00:02:54.166 A:middle
This is, specifically,

00:02:54.166 --> 00:02:55.936 A:middle
interesting if you are new to

00:02:55.936 --> 00:02:56.276 A:middle
ARKit.

00:02:56.786 --> 00:03:00.556 A:middle
So, the first thing you'll do is

00:02:56.786 --> 00:03:00.556 A:middle
So, the first thing you'll do is

00:03:00.556 --> 00:03:01.826 A:middle
create an ARSession.

00:03:02.496 --> 00:03:04.576 A:middle
An ARSession is the object that

00:03:04.576 --> 00:03:06.946 A:middle
handles everything from

00:03:06.946 --> 00:03:09.686 A:middle
configuring to running the AR

00:03:09.686 --> 00:03:11.226 A:middle
technologies.

00:03:11.476 --> 00:03:14.226 A:middle
And also, returning the results

00:03:14.226 --> 00:03:15.696 A:middle
of the AR technologies.

00:03:16.196 --> 00:03:19.406 A:middle
You then, have to describe what

00:03:19.496 --> 00:03:20.796 A:middle
kind of technologies you

00:03:20.856 --> 00:03:21.866 A:middle
actually want to run.

00:03:22.266 --> 00:03:23.256 A:middle
Like, what kind of tracking

00:03:23.256 --> 00:03:25.136 A:middle
technologies and what kind of

00:03:25.136 --> 00:03:26.376 A:middle
features should be enabled, like

00:03:26.436 --> 00:03:28.096 A:middle
Plane Detection, for example.

00:03:28.966 --> 00:03:32.506 A:middle
You'll then, take this specific

00:03:32.506 --> 00:03:35.406 A:middle
ARConfiguration and call run

00:03:36.256 --> 00:03:39.476 A:middle
method on your instance of the

00:03:39.476 --> 00:03:40.116 A:middle
ARSession.

00:03:41.576 --> 00:03:43.206 A:middle
Then, the ARSession, internally,

00:03:44.306 --> 00:03:47.336 A:middle
will start configuring an

00:03:47.336 --> 00:03:49.596 A:middle
AVCaptureSession to start

00:03:50.096 --> 00:03:52.716 A:middle
receiving the images, as well as

00:03:52.716 --> 00:03:55.096 A:middle
a Core Motion  manager to begin

00:03:55.096 --> 00:03:56.816 A:middle
receiving the motion sensor, so,

00:03:57.326 --> 00:03:57.616 A:middle
data.

00:03:57.616 --> 00:03:58.626 A:middle
So, this is, basically, the

00:03:58.626 --> 00:04:01.496 A:middle
built-in input system from your

00:03:58.626 --> 00:04:01.496 A:middle
built-in input system from your

00:04:01.496 --> 00:04:02.666 A:middle
device for ARKit.

00:04:04.066 --> 00:04:07.066 A:middle
Now, after processing the

00:04:07.066 --> 00:04:09.306 A:middle
results are returned in ARFrames

00:04:09.306 --> 00:04:10.926 A:middle
at 60 frames per second.

00:04:12.126 --> 00:04:14.226 A:middle
An ARFrame is a snapshot in time

00:04:14.226 --> 00:04:15.396 A:middle
which gives you everything you

00:04:15.396 --> 00:04:17.055 A:middle
need to render your augmented

00:04:17.055 --> 00:04:17.875 A:middle
reality scene.

00:04:18.366 --> 00:04:20.776 A:middle
Like, the captured camera image,

00:04:20.776 --> 00:04:22.326 A:middle
which would then be, which will

00:04:22.326 --> 00:04:23.576 A:middle
be rendered in the background of

00:04:23.576 --> 00:04:25.176 A:middle
your augmented reality scenario.

00:04:25.826 --> 00:04:27.136 A:middle
As well as a track camera

00:04:27.136 --> 00:04:29.456 A:middle
motion, which will then be

00:04:29.456 --> 00:04:31.256 A:middle
applied to your virtual camera

00:04:31.706 --> 00:04:34.406 A:middle
to render the virtual content

00:04:34.446 --> 00:04:36.746 A:middle
from the same perspective as the

00:04:36.746 --> 00:04:37.586 A:middle
physical camera.

00:04:38.766 --> 00:04:40.426 A:middle
It also contains information

00:04:40.876 --> 00:04:42.086 A:middle
about the environment.

00:04:42.086 --> 00:04:43.146 A:middle
Like, for example, detected

00:04:43.146 --> 00:04:43.536 A:middle
planes.

00:04:43.536 --> 00:04:46.806 A:middle
So, let's now start with our

00:04:46.806 --> 00:04:48.436 A:middle
first tracking technology and

00:04:48.436 --> 00:04:49.216 A:middle
build up from there.

00:04:51.586 --> 00:04:53.176 A:middle
Orientation Tracking.

00:04:54.386 --> 00:04:56.396 A:middle
Orientation Tracking tracks,

00:04:56.536 --> 00:04:56.976 A:middle
guess what?

00:04:57.176 --> 00:04:58.156 A:middle
Orientation.

00:04:58.676 --> 00:05:00.746 A:middle
Meaning it tracks the rotation,

00:04:58.676 --> 00:05:00.746 A:middle
Meaning it tracks the rotation,

00:05:00.746 --> 00:05:01.226 A:middle
only.

00:05:02.136 --> 00:05:03.206 A:middle
You can think about it as you

00:05:03.206 --> 00:05:05.456 A:middle
can only use your hat to view

00:05:05.456 --> 00:05:06.916 A:middle
virtual content, which also,

00:05:07.156 --> 00:05:09.046 A:middle
only allows rotation.

00:05:09.966 --> 00:05:11.716 A:middle
Meaning you can experience the

00:05:11.866 --> 00:05:13.406 A:middle
virtual content from the same

00:05:13.406 --> 00:05:15.656 A:middle
positional point of view, but no

00:05:15.656 --> 00:05:17.276 A:middle
change in the position is going

00:05:17.276 --> 00:05:17.856 A:middle
to be tracked.

00:05:19.596 --> 00:05:21.546 A:middle
The rotation data is tracked

00:05:21.546 --> 00:05:22.836 A:middle
around three axles.

00:05:22.946 --> 00:05:24.276 A:middle
That's why it's also, sometimes,

00:05:24.316 --> 00:05:25.676 A:middle
called the three degrees of

00:05:25.676 --> 00:05:26.416 A:middle
freedom tracking.

00:05:26.416 --> 00:05:28.926 A:middle
You can use it, for example, in

00:05:28.926 --> 00:05:30.846 A:middle
a spherical virtual environment.

00:05:30.846 --> 00:05:32.256 A:middle
Like, for example, experience a

00:05:32.256 --> 00:05:35.066 A:middle
360-degree video, in which the

00:05:35.066 --> 00:05:36.496 A:middle
virtual content can be viewed

00:05:36.496 --> 00:05:38.116 A:middle
from the same positional point.

00:05:39.346 --> 00:05:41.226 A:middle
You can also, use it to augment

00:05:41.226 --> 00:05:43.196 A:middle
objects that are very far away.

00:05:44.336 --> 00:05:46.296 A:middle
Orientation Tracking is not

00:05:46.396 --> 00:05:48.236 A:middle
suited for physical world

00:05:48.236 --> 00:05:49.746 A:middle
augmentation, in which you want

00:05:49.746 --> 00:05:50.886 A:middle
to view the content from

00:05:50.886 --> 00:05:52.666 A:middle
different points of views.

00:05:54.226 --> 00:05:56.416 A:middle
So, let's now have a look at

00:05:56.556 --> 00:05:58.016 A:middle
what happens under the hood when

00:05:58.016 --> 00:05:59.486 A:middle
Orientation Tracking is running.

00:06:00.036 --> 00:06:02.676 A:middle
It is, actually, quite simple.

00:06:03.106 --> 00:06:05.016 A:middle
It only uses the rotation data

00:06:05.016 --> 00:06:07.166 A:middle
from core motion, which applies

00:06:07.326 --> 00:06:09.146 A:middle
sensor fusion to the motion

00:06:09.146 --> 00:06:09.986 A:middle
sensors data.

00:06:11.426 --> 00:06:13.626 A:middle
As motion data is provided at a

00:06:13.756 --> 00:06:15.696 A:middle
higher frequency than the camera

00:06:15.696 --> 00:06:17.936 A:middle
image, Orientation Tracking

00:06:18.206 --> 00:06:20.736 A:middle
takes the latest motion data

00:06:20.736 --> 00:06:22.616 A:middle
from Core Motion, once the camera

00:06:22.616 --> 00:06:23.636 A:middle
image is available.

00:06:23.776 --> 00:06:25.766 A:middle
And then, returns both results

00:06:26.086 --> 00:06:27.206 A:middle
in an ARFrame.

00:06:27.526 --> 00:06:28.066 A:middle
So, that's it.

00:06:28.126 --> 00:06:28.956 A:middle
Very simple.

00:06:29.686 --> 00:06:31.726 A:middle
So, please note that the camera

00:06:31.726 --> 00:06:33.266 A:middle
feed is not processed in

00:06:33.266 --> 00:06:34.526 A:middle
Orientation Tracking.

00:06:34.856 --> 00:06:35.956 A:middle
Meaning there's no computer

00:06:35.956 --> 00:06:37.036 A:middle
version under the hood here.

00:06:38.286 --> 00:06:40.376 A:middle
Now, to run Orientation Tracking

00:06:40.986 --> 00:06:43.096 A:middle
you only need to configure your

00:06:43.286 --> 00:06:45.876 A:middle
ARSession with an AROrientation

00:06:45.876 --> 00:06:47.256 A:middle
TrackingConfiguration.

00:06:48.266 --> 00:06:49.686 A:middle
The results will then be

00:06:49.686 --> 00:06:52.216 A:middle
returned in an ARCamera object

00:06:52.996 --> 00:06:55.116 A:middle
provided by the ARFrames.

00:06:55.116 --> 00:06:57.926 A:middle
Now, an ARCamera object always

00:06:57.926 --> 00:06:59.826 A:middle
contains the transform, which in

00:06:59.826 --> 00:07:01.106 A:middle
this case of Orientation

00:06:59.826 --> 00:07:01.106 A:middle
this case of Orientation

00:07:01.106 --> 00:07:02.936 A:middle
Tracking, only contains the

00:07:02.936 --> 00:07:05.006 A:middle
rotation data of your tracked

00:07:05.116 --> 00:07:06.016 A:middle
physical camera.

00:07:07.096 --> 00:07:09.616 A:middle
Alternatively, the rotation is

00:07:09.666 --> 00:07:11.596 A:middle
also represented in eulerAngles.

00:07:12.276 --> 00:07:13.956 A:middle
You can use whatever fits best

00:07:14.066 --> 00:07:14.306 A:middle
to you.

00:07:16.866 --> 00:07:18.686 A:middle
Let's now move over to more

00:07:18.686 --> 00:07:20.326 A:middle
advanced tracking technologies.

00:07:21.156 --> 00:07:22.546 A:middle
We'll start with World Tracking.

00:07:23.076 --> 00:07:25.406 A:middle
World Tracking tracks your

00:07:25.676 --> 00:07:28.186 A:middle
camera viewing orientation, and

00:07:28.186 --> 00:07:30.096 A:middle
also, the change in position

00:07:30.316 --> 00:07:32.056 A:middle
into your physical environment

00:07:32.326 --> 00:07:34.196 A:middle
without any prior information

00:07:34.196 --> 00:07:35.226 A:middle
about your environment.

00:07:36.266 --> 00:07:37.656 A:middle
Here, you can see on the left

00:07:37.656 --> 00:07:41.316 A:middle
side the real camera's view into

00:07:41.316 --> 00:07:42.766 A:middle
the environment, while on the

00:07:42.766 --> 00:07:45.416 A:middle
right side you see the tracked

00:07:45.546 --> 00:07:47.466 A:middle
camera motion while exploring

00:07:47.466 --> 00:07:50.506 A:middle
the world represented in the

00:07:50.506 --> 00:07:51.756 A:middle
coordinate system.

00:07:52.986 --> 00:07:54.406 A:middle
Let's now explain better, what

00:07:54.506 --> 00:07:55.466 A:middle
happens here, when World

00:07:55.536 --> 00:07:56.376 A:middle
Tracking is running.

00:07:56.896 --> 00:08:00.406 A:middle
World Tracking uses a motion

00:07:56.896 --> 00:08:00.406 A:middle
World Tracking uses a motion

00:08:00.406 --> 00:08:03.236 A:middle
sensor, the motion data of your

00:08:03.236 --> 00:08:05.436 A:middle
device's accelerometer and

00:08:05.436 --> 00:08:08.376 A:middle
gyroscope to compute its change

00:08:08.376 --> 00:08:10.796 A:middle
in orientation and translation

00:08:11.116 --> 00:08:12.436 A:middle
on a high frequency.

00:08:14.706 --> 00:08:16.926 A:middle
It also provides its information

00:08:17.116 --> 00:08:19.276 A:middle
in correct scale in meters.

00:08:20.656 --> 00:08:23.056 A:middle
In literature, just this part of

00:08:23.106 --> 00:08:24.306 A:middle
the tracking system is also

00:08:24.306 --> 00:08:25.976 A:middle
called Inertial Odometry.

00:08:27.076 --> 00:08:29.046 A:middle
While this motion data provides

00:08:29.106 --> 00:08:31.026 A:middle
good motion information for

00:08:31.026 --> 00:08:32.826 A:middle
movement across small time

00:08:32.826 --> 00:08:34.226 A:middle
intervals and whenever there's

00:08:34.275 --> 00:08:36.566 A:middle
like, sudden movement, it does

00:08:36.645 --> 00:08:38.635 A:middle
drift over larger time intervals

00:08:39.015 --> 00:08:40.456 A:middle
as the data is not ideally

00:08:40.616 --> 00:08:42.126 A:middle
precise and subject to

00:08:42.126 --> 00:08:43.296 A:middle
cumulative errors.

00:08:44.496 --> 00:08:45.616 A:middle
That's why it cannot be used

00:08:45.906 --> 00:08:47.446 A:middle
just by its own for tracking.

00:08:48.036 --> 00:08:51.456 A:middle
Now, to compensate this drift,

00:08:51.696 --> 00:08:53.416 A:middle
World Tracking, additionally,

00:08:53.516 --> 00:08:55.506 A:middle
applies a computer vision

00:08:55.506 --> 00:08:58.656 A:middle
process in which it uses the

00:08:58.656 --> 00:08:59.606 A:middle
camera frames.

00:09:00.706 --> 00:09:02.426 A:middle
This technology provides a

00:09:02.506 --> 00:09:05.276 A:middle
higher accuracy, but at the cost

00:09:05.276 --> 00:09:06.546 A:middle
of computation time.

00:09:08.056 --> 00:09:10.176 A:middle
Also, this technology is

00:09:10.226 --> 00:09:12.596 A:middle
sensitive to fast camera motions

00:09:12.666 --> 00:09:14.396 A:middle
and this results in motion blur

00:09:14.396 --> 00:09:15.296 A:middle
in the camera frames.

00:09:16.516 --> 00:09:18.626 A:middle
Now, this vision only part of

00:09:18.626 --> 00:09:20.926 A:middle
the system is also called Visual

00:09:20.926 --> 00:09:21.666 A:middle
Odometry.

00:09:22.016 --> 00:09:24.276 A:middle
Now, by fusing those both

00:09:24.276 --> 00:09:26.576 A:middle
systems, computer vision and

00:09:26.576 --> 00:09:28.916 A:middle
motion, ARKit takes the best of

00:09:28.986 --> 00:09:30.016 A:middle
those both systems.

00:09:30.646 --> 00:09:32.086 A:middle
From computer vision, it takes

00:09:32.086 --> 00:09:34.116 A:middle
a high accuracy over the larger

00:09:34.116 --> 00:09:34.946 A:middle
time intervals.

00:09:35.566 --> 00:09:37.576 A:middle
And from the motion data it

00:09:37.576 --> 00:09:39.586 A:middle
takes the high update rates and

00:09:39.586 --> 00:09:41.356 A:middle
good precision for the smaller

00:09:41.356 --> 00:09:43.446 A:middle
time intervals, as well as the

00:09:43.446 --> 00:09:44.276 A:middle
metric scale.

00:09:44.856 --> 00:09:47.456 A:middle
Now, by combining those both

00:09:47.456 --> 00:09:49.446 A:middle
systems World Tracking can skip

00:09:49.586 --> 00:09:51.036 A:middle
the computer vision processing

00:09:51.246 --> 00:09:52.886 A:middle
for some of those frames, while

00:09:52.886 --> 00:09:54.736 A:middle
still keeping an efficient and

00:09:54.776 --> 00:09:55.966 A:middle
responsive tracking.

00:09:56.956 --> 00:09:58.856 A:middle
This frees CPU resources, which

00:09:58.856 --> 00:10:00.356 A:middle
you can then, additionally, use

00:09:58.856 --> 00:10:00.356 A:middle
you can then, additionally, use

00:10:00.356 --> 00:10:00.956 A:middle
for your apps.

00:10:02.876 --> 00:10:04.376 A:middle
In Literature, this combined

00:10:04.376 --> 00:10:06.456 A:middle
technology is also called Visual

00:10:06.506 --> 00:10:07.696 A:middle
Inertial Odometry.

00:10:08.916 --> 00:10:11.456 A:middle
Let's have a closer look at the

00:10:11.456 --> 00:10:13.306 A:middle
visual part of it.

00:10:14.116 --> 00:10:15.926 A:middle
So, within the computer version

00:10:15.926 --> 00:10:19.286 A:middle
process interesting regions in

00:10:19.286 --> 00:10:21.096 A:middle
the camera images are extracted,

00:10:21.356 --> 00:10:22.696 A:middle
like here, the blue and the

00:10:22.696 --> 00:10:23.526 A:middle
orange dot.

00:10:24.406 --> 00:10:26.006 A:middle
And they are extracted such that

00:10:26.006 --> 00:10:27.496 A:middle
they can robustly all to be

00:10:27.946 --> 00:10:30.246 A:middle
extracted and other images of

00:10:30.286 --> 00:10:31.516 A:middle
the same environment.

00:10:33.016 --> 00:10:34.096 A:middle
Those interesting regions are

00:10:34.096 --> 00:10:35.166 A:middle
also called features.

00:10:36.496 --> 00:10:37.616 A:middle
Now, those features are then

00:10:37.616 --> 00:10:39.996 A:middle
matched between multiple images

00:10:40.196 --> 00:10:42.326 A:middle
over the camera stream based on

00:10:42.326 --> 00:10:43.686 A:middle
their similarity and their

00:10:43.686 --> 00:10:44.326 A:middle
appearance.

00:10:45.176 --> 00:10:46.716 A:middle
And what then happens is pretty

00:10:46.716 --> 00:10:48.566 A:middle
much how you are able to see 3D

00:10:48.566 --> 00:10:49.286 A:middle
with your eyes.

00:10:50.176 --> 00:10:51.576 A:middle
You have two of them and they

00:10:51.576 --> 00:10:53.516 A:middle
are within the sidewise small

00:10:53.516 --> 00:10:54.226 A:middle
distance.

00:10:55.056 --> 00:10:56.976 A:middle
And this parallax between the

00:10:56.976 --> 00:10:58.796 A:middle
eyes is important as this

00:10:58.796 --> 00:11:00.636 A:middle
results in slightly different

00:10:58.796 --> 00:11:00.636 A:middle
results in slightly different

00:11:00.636 --> 00:11:02.236 A:middle
views into the environment,

00:11:02.606 --> 00:11:04.706 A:middle
which allows you to see stereo

00:11:04.816 --> 00:11:06.056 A:middle
and perceive the depth.

00:11:07.106 --> 00:11:08.406 A:middle
And this is what ARKit now,

00:11:08.406 --> 00:11:10.176 A:middle
also, does with the different

00:11:10.176 --> 00:11:12.076 A:middle
views of the same camera stream

00:11:12.076 --> 00:11:12.956 A:middle
during the process of

00:11:13.026 --> 00:11:14.156 A:middle
triangulation.

00:11:14.736 --> 00:11:16.206 A:middle
And it does it once there's

00:11:16.256 --> 00:11:18.046 A:middle
enough parallax present.

00:11:18.896 --> 00:11:20.786 A:middle
It computes the missing depth

00:11:20.786 --> 00:11:22.736 A:middle
information for those matched

00:11:22.816 --> 00:11:23.306 A:middle
features.

00:11:23.706 --> 00:11:26.826 A:middle
Meaning those 2D features from

00:11:26.826 --> 00:11:28.556 A:middle
the image are now reconstructed

00:11:28.556 --> 00:11:29.316 A:middle
in 3D.

00:11:30.806 --> 00:11:32.066 A:middle
Please, note that this

00:11:32.066 --> 00:11:34.476 A:middle
reconstruction to be successful,

00:11:35.706 --> 00:11:37.536 A:middle
the camera position must have

00:11:37.676 --> 00:11:39.776 A:middle
changed by a translation to

00:11:39.776 --> 00:11:41.316 A:middle
provide enough parallax.

00:11:42.356 --> 00:11:44.086 A:middle
For example, with the sidewise

00:11:44.086 --> 00:11:44.626 A:middle
movement.

00:11:44.966 --> 00:11:47.746 A:middle
The pure rotation does not give

00:11:47.746 --> 00:11:48.856 A:middle
enough information here.

00:11:50.536 --> 00:11:52.606 A:middle
So, this is your first small map

00:11:52.606 --> 00:11:53.646 A:middle
of your environment.

00:11:53.646 --> 00:11:55.826 A:middle
In ARKit we call this a World

00:11:55.906 --> 00:11:56.136 A:middle
map.

00:11:57.396 --> 00:11:59.826 A:middle
In this same moment, also, the

00:11:59.826 --> 00:12:01.626 A:middle
camera's positions and

00:11:59.826 --> 00:12:01.626 A:middle
camera's positions and

00:12:01.626 --> 00:12:04.226 A:middle
orientations of your sequences

00:12:04.226 --> 00:12:06.546 A:middle
are computed, denoted with a C

00:12:06.546 --> 00:12:06.766 A:middle
here.

00:12:07.476 --> 00:12:08.546 A:middle
Meaning, your World Tracking

00:12:08.546 --> 00:12:09.396 A:middle
just initialized.

00:12:09.396 --> 00:12:10.496 A:middle
This is the moment of

00:12:10.526 --> 00:12:12.496 A:middle
initialization of the tracking

00:12:12.496 --> 00:12:12.746 A:middle
system.

00:12:12.746 --> 00:12:15.886 A:middle
Please note that also in this

00:12:15.886 --> 00:12:17.326 A:middle
moment of this initial

00:12:17.326 --> 00:12:19.216 A:middle
reconstruction of the World map,

00:12:19.406 --> 00:12:21.246 A:middle
the world origin was defined.

00:12:21.986 --> 00:12:23.676 A:middle
And it is set to the first

00:12:23.886 --> 00:12:25.846 A:middle
camera's origin of the

00:12:25.846 --> 00:12:27.326 A:middle
triangulated frames.

00:12:28.046 --> 00:12:29.896 A:middle
And it is also set to be gravity

00:12:29.896 --> 00:12:30.296 A:middle
aligned.

00:12:31.056 --> 00:12:33.446 A:middle
It's denoted with a W in the

00:12:34.096 --> 00:12:34.286 A:middle
slides.

00:12:34.906 --> 00:12:35.786 A:middle
So, you now have a small

00:12:35.786 --> 00:12:37.246 A:middle
representation of your real

00:12:37.246 --> 00:12:39.366 A:middle
environment reconstructed as a

00:12:39.366 --> 00:12:40.896 A:middle
World map in its own world

00:12:40.896 --> 00:12:42.046 A:middle
coordinates system.

00:12:42.836 --> 00:12:44.446 A:middle
And you have your current camera

00:12:44.596 --> 00:12:46.646 A:middle
tracked with respect to the same

00:12:46.646 --> 00:12:48.646 A:middle
world coordinate system.

00:12:50.896 --> 00:12:53.186 A:middle
You can now start adding virtual

00:12:53.186 --> 00:12:56.436 A:middle
content to augment them into the

00:12:56.436 --> 00:12:57.056 A:middle
camera's view.

00:12:58.656 --> 00:13:01.016 A:middle
Now, to place virtual content

00:12:58.656 --> 00:13:01.016 A:middle
Now, to place virtual content

00:13:01.066 --> 00:13:03.416 A:middle
correctly to an ARSession, you

00:13:03.416 --> 00:13:06.256 A:middle
should use ARAnchors from ARKit,

00:13:06.636 --> 00:13:07.826 A:middle
which are denoted with an A

00:13:07.826 --> 00:13:08.096 A:middle
here.

00:13:09.536 --> 00:13:12.326 A:middle
ARAnchors are reference points

00:13:12.536 --> 00:13:14.076 A:middle
within this World map, within

00:13:14.076 --> 00:13:15.816 A:middle
this world coordinates system.

00:13:16.486 --> 00:13:18.386 A:middle
And you should use them because

00:13:18.386 --> 00:13:20.686 A:middle
the World Tracking might update

00:13:20.686 --> 00:13:22.206 A:middle
them during the tracking.

00:13:22.206 --> 00:13:23.636 A:middle
Meaning that, also, all the

00:13:23.636 --> 00:13:25.336 A:middle
virtual content that is assigned

00:13:25.336 --> 00:13:27.776 A:middle
to it will be updated and

00:13:27.886 --> 00:13:31.626 A:middle
correctly augmented into the

00:13:32.456 --> 00:13:32.746 A:middle
camera's view.

00:13:32.746 --> 00:13:34.446 A:middle
So, now that you've used the

00:13:34.446 --> 00:13:36.436 A:middle
ARAnchors you can add virtual

00:13:36.466 --> 00:13:38.356 A:middle
content to the anchor, which

00:13:38.386 --> 00:13:40.496 A:middle
will them be augmented correctly

00:13:40.886 --> 00:13:44.116 A:middle
into the current camera's view.

00:13:45.576 --> 00:13:48.676 A:middle
From now on, this created 3D

00:13:48.676 --> 00:13:51.026 A:middle
World map of your environment is

00:13:51.026 --> 00:13:52.526 A:middle
your reference system for the

00:13:52.526 --> 00:13:53.236 A:middle
World Tracking.

00:13:54.046 --> 00:13:55.506 A:middle
It is used to reference new

00:13:55.506 --> 00:13:56.456 A:middle
images against.

00:13:57.076 --> 00:13:58.796 A:middle
And features are matched from

00:13:58.796 --> 00:14:01.806 A:middle
image to image and triangulated.

00:13:58.796 --> 00:14:01.806 A:middle
image to image and triangulated.

00:14:02.666 --> 00:14:04.216 A:middle
And at the same time, also, new

00:14:04.216 --> 00:14:05.776 A:middle
robust features are extracted,

00:14:06.106 --> 00:14:08.246 A:middle
matched, and triangulated, which

00:14:08.246 --> 00:14:10.496 A:middle
are then extending your World

00:14:10.556 --> 00:14:10.786 A:middle
map.

00:14:11.286 --> 00:14:13.346 A:middle
Meaning ARKit is learning your

00:14:13.346 --> 00:14:14.036 A:middle
environment.

00:14:15.926 --> 00:14:17.006 A:middle
This then allows, again, the

00:14:17.006 --> 00:14:18.686 A:middle
computation of tracking updates

00:14:18.946 --> 00:14:20.936 A:middle
of the current camera's position

00:14:20.936 --> 00:14:21.956 A:middle
and orientation.

00:14:23.086 --> 00:14:24.986 A:middle
And finally, the correct

00:14:24.986 --> 00:14:26.796 A:middle
augmentation into the current

00:14:26.796 --> 00:14:27.426 A:middle
camera's view.

00:14:27.946 --> 00:14:31.966 A:middle
While you continue to explore

00:14:31.966 --> 00:14:33.786 A:middle
the world, World Tracking will

00:14:33.786 --> 00:14:35.706 A:middle
continue to track your physical

00:14:35.706 --> 00:14:37.826 A:middle
camera and continue to learn

00:14:38.076 --> 00:14:39.426 A:middle
your physical environment.

00:14:40.506 --> 00:14:42.866 A:middle
But over time, the augmentation

00:14:42.866 --> 00:14:45.506 A:middle
might drift slightly, which can

00:14:45.506 --> 00:14:47.366 A:middle
be noticed like you can see in

00:14:47.366 --> 00:14:49.096 A:middle
the left image, in a small

00:14:49.096 --> 00:14:51.086 A:middle
offset of the augmentation.

00:14:52.366 --> 00:14:54.736 A:middle
This is because even small

00:14:55.216 --> 00:14:58.016 A:middle
offsets, even small errors will

00:14:58.016 --> 00:14:59.786 A:middle
become noticeable when

00:14:59.786 --> 00:15:01.496 A:middle
accumulated over time.

00:14:59.786 --> 00:15:01.496 A:middle
accumulated over time.

00:15:03.486 --> 00:15:05.286 A:middle
Now, when the device comes back

00:15:05.426 --> 00:15:07.216 A:middle
to a similar view, which was

00:15:07.216 --> 00:15:09.036 A:middle
already explored before, like

00:15:09.086 --> 00:15:10.956 A:middle
for example, the starting point

00:15:10.956 --> 00:15:11.756 A:middle
where we started the

00:15:11.756 --> 00:15:14.116 A:middle
exploration, ARKit can perform

00:15:14.116 --> 00:15:15.846 A:middle
another optimization step.

00:15:16.546 --> 00:15:18.086 A:middle
And this addition makes, a

00:15:18.086 --> 00:15:19.496 A:middle
Visual Intertial Odometry

00:15:19.496 --> 00:15:21.986 A:middle
system, makes the system that

00:15:21.986 --> 00:15:24.176 A:middle
ARKit supplies to a Visual

00:15:24.286 --> 00:15:26.206 A:middle
Inertial SLAM System.

00:15:27.376 --> 00:15:28.826 A:middle
So, let's bring back this first

00:15:28.916 --> 00:15:30.726 A:middle
image where the World Tracking

00:15:30.726 --> 00:15:32.476 A:middle
started the exploration.

00:15:33.956 --> 00:15:35.136 A:middle
So, what happens now is that

00:15:35.136 --> 00:15:37.166 A:middle
World Tracking will check how

00:15:37.166 --> 00:15:39.246 A:middle
well the tracking information

00:15:39.446 --> 00:15:41.076 A:middle
and the World map of the current

00:15:41.076 --> 00:15:43.666 A:middle
view aligns with the past views,

00:15:43.926 --> 00:15:45.276 A:middle
like the one from the beginning.

00:15:45.276 --> 00:15:48.586 A:middle
And will then perform the

00:15:48.586 --> 00:15:51.886 A:middle
optimization step and align the

00:15:52.066 --> 00:15:54.316 A:middle
current information and the

00:15:54.316 --> 00:15:56.306 A:middle
current World map with your real

00:15:56.306 --> 00:15:57.666 A:middle
physical environment.

00:15:58.816 --> 00:16:00.256 A:middle
Have you noticed that during

00:15:58.816 --> 00:16:00.256 A:middle
Have you noticed that during

00:16:00.256 --> 00:16:01.946 A:middle
this step, also the ARAnchor was

00:16:01.946 --> 00:16:02.536 A:middle
updated?

00:16:03.006 --> 00:16:04.476 A:middle
And that is the reason why you

00:16:04.476 --> 00:16:07.036 A:middle
should use ARAnchors when adding

00:16:07.036 --> 00:16:08.426 A:middle
virtual content to your

00:16:08.826 --> 00:16:09.436 A:middle
scenario.

00:16:09.956 --> 00:16:14.396 A:middle
In this video, you can see the

00:16:14.396 --> 00:16:16.956 A:middle
same step again with a real

00:16:16.956 --> 00:16:17.716 A:middle
camera feed.

00:16:18.116 --> 00:16:19.396 A:middle
On the left side you see the

00:16:19.396 --> 00:16:20.736 A:middle
camera's view into the

00:16:20.736 --> 00:16:22.596 A:middle
environment, and also, features

00:16:22.596 --> 00:16:25.026 A:middle
which are tracked in the images.

00:16:25.406 --> 00:16:26.986 A:middle
And on the right side, you see a

00:16:26.986 --> 00:16:28.356 A:middle
bird eye's view onto the

00:16:28.356 --> 00:16:30.596 A:middle
scenario, showing what ARKit

00:16:30.596 --> 00:16:33.166 A:middle
knows about it and showing the

00:16:34.016 --> 00:16:35.726 A:middle
3D reconstruction of the

00:16:35.726 --> 00:16:36.386 A:middle
environment.

00:16:36.996 --> 00:16:39.506 A:middle
The colors of the points are

00:16:39.506 --> 00:16:41.346 A:middle
just encoding the height of the

00:16:41.346 --> 00:16:43.306 A:middle
reconstructed points with blue

00:16:43.306 --> 00:16:45.036 A:middle
being the ground floor and red

00:16:45.096 --> 00:16:46.746 A:middle
being the table and the chairs.

00:16:47.376 --> 00:16:51.216 A:middle
Once the camera returns back to

00:16:51.216 --> 00:16:52.576 A:middle
a similar view it has seen

00:16:52.576 --> 00:16:54.546 A:middle
before, like here the starting

00:16:54.546 --> 00:16:56.636 A:middle
point, ARKit will now apply this

00:16:56.636 --> 00:16:57.896 A:middle
optimization step.

00:16:57.996 --> 00:16:59.436 A:middle
So, just pay attention to the

00:16:59.436 --> 00:17:00.716 A:middle
point cloud and the camera

00:16:59.436 --> 00:17:00.716 A:middle
point cloud and the camera

00:17:00.796 --> 00:17:01.416 A:middle
trajectory.

00:17:02.826 --> 00:17:04.106 A:middle
Have you noticed the update?

00:17:04.556 --> 00:17:05.506 A:middle
Let me show you, once more.

00:17:05.996 --> 00:17:10.866 A:middle
This update aligns the ARKit

00:17:10.866 --> 00:17:12.175 A:middle
knowledge with your real

00:17:12.286 --> 00:17:15.016 A:middle
physical world, and also, the

00:17:15.016 --> 00:17:17.536 A:middle
camera movement and results in

00:17:17.536 --> 00:17:19.425 A:middle
the better augmentation for the

00:17:19.425 --> 00:17:20.626 A:middle
coming camera frames.

00:17:21.955 --> 00:17:23.306 A:middle
By the way, all those

00:17:23.306 --> 00:17:24.935 A:middle
computations of World Tracking,

00:17:25.435 --> 00:17:27.425 A:middle
and also, all this information

00:17:27.425 --> 00:17:28.886 A:middle
about your learned environment,

00:17:29.346 --> 00:17:31.076 A:middle
everything is done on your

00:17:31.076 --> 00:17:31.996 A:middle
device only.

00:17:32.136 --> 00:17:33.496 A:middle
And all this information, also,

00:17:33.496 --> 00:17:35.096 A:middle
stays on your device only.

00:17:35.096 --> 00:17:37.836 A:middle
So, how can you use this complex

00:17:37.836 --> 00:17:40.566 A:middle
technology, now, in your app?

00:17:41.926 --> 00:17:45.606 A:middle
It is actually quite simple.

00:17:45.716 --> 00:17:47.486 A:middle
To run World Tracking you just

00:17:47.526 --> 00:17:49.766 A:middle
configure your ARSession with an

00:17:49.816 --> 00:17:51.896 A:middle
ARWorldTrackingConfiguration.

00:17:52.976 --> 00:17:55.376 A:middle
Again, its results are returned

00:17:55.376 --> 00:17:57.336 A:middle
in an ARCamera object of the

00:17:57.336 --> 00:17:57.956 A:middle
ARFrame.

00:18:00.216 --> 00:18:03.636 A:middle
An ARCamera object, again,

00:18:03.636 --> 00:18:05.626 A:middle
contains the transform, which in

00:18:05.626 --> 00:18:06.946 A:middle
this case of World Tracking

00:18:07.496 --> 00:18:08.806 A:middle
contains, additionally, to the

00:18:08.806 --> 00:18:11.016 A:middle
rotation, also, the translation

00:18:11.016 --> 00:18:12.116 A:middle
of the track camera.

00:18:13.286 --> 00:18:15.106 A:middle
Additionally, the ARCamera also

00:18:15.106 --> 00:18:16.886 A:middle
contains information about the

00:18:16.886 --> 00:18:18.556 A:middle
tracking state and

00:18:18.556 --> 00:18:19.746 A:middle
trackingStateReason.

00:18:20.166 --> 00:18:22.366 A:middle
This will provide some

00:18:22.366 --> 00:18:24.206 A:middle
information about the current

00:18:24.306 --> 00:18:25.216 A:middle
tracking quality.

00:18:26.736 --> 00:18:27.976 A:middle
So, tracking quality.

00:18:28.516 --> 00:18:29.906 A:middle
Have you ever experienced

00:18:30.016 --> 00:18:32.376 A:middle
opening an AR app and the

00:18:32.376 --> 00:18:34.476 A:middle
tracking worked very poorly or

00:18:34.476 --> 00:18:35.646 A:middle
maybe it didn't work at all?

00:18:36.276 --> 00:18:37.346 A:middle
How did that feel?

00:18:38.446 --> 00:18:39.846 A:middle
Maybe frustrating?

00:18:39.846 --> 00:18:40.796 A:middle
You might not open the app,

00:18:40.796 --> 00:18:41.166 A:middle
again.

00:18:42.026 --> 00:18:43.566 A:middle
So, how can you get a higher

00:18:43.566 --> 00:18:45.326 A:middle
tracking quality for your app?

00:18:46.726 --> 00:18:48.626 A:middle
For this, we need to understand

00:18:48.626 --> 00:18:49.846 A:middle
the main factors that are

00:18:49.846 --> 00:18:51.016 A:middle
influencing the tracking

00:18:51.016 --> 00:18:51.646 A:middle
quality.

00:18:52.156 --> 00:18:53.436 A:middle
And I want to highlight three of

00:18:53.476 --> 00:18:53.856 A:middle
them here.

00:18:55.056 --> 00:18:56.756 A:middle
First of all, World Tracking

00:18:56.756 --> 00:18:58.996 A:middle
relies on a constant stream of

00:18:59.056 --> 00:19:01.136 A:middle
camera images and sensor data.

00:18:59.056 --> 00:19:01.136 A:middle
camera images and sensor data.

00:19:01.556 --> 00:19:02.876 A:middle
If this is interrupted for too

00:19:02.876 --> 00:19:04.996 A:middle
long, tracking will become

00:19:04.996 --> 00:19:05.456 A:middle
limited.

00:19:06.926 --> 00:19:08.676 A:middle
Second, World Tracking also

00:19:08.676 --> 00:19:10.326 A:middle
works best in textured and

00:19:10.326 --> 00:19:12.306 A:middle
well-lit environments because

00:19:12.646 --> 00:19:14.656 A:middle
World Tracking uses those

00:19:14.656 --> 00:19:16.726 A:middle
visually robust points to map

00:19:16.726 --> 00:19:18.256 A:middle
and finally triangulate its

00:19:18.316 --> 00:19:18.916 A:middle
location.

00:19:18.916 --> 00:19:20.976 A:middle
It is important that there is

00:19:20.976 --> 00:19:23.136 A:middle
enough visual complexity in the

00:19:23.136 --> 00:19:23.686 A:middle
environment.

00:19:24.706 --> 00:19:26.216 A:middle
If this is not the case because

00:19:26.466 --> 00:19:28.046 A:middle
it's, for example, too dark or

00:19:28.046 --> 00:19:29.666 A:middle
you're looking against a white

00:19:29.666 --> 00:19:31.606 A:middle
wall, then also, the tracking

00:19:31.606 --> 00:19:32.656 A:middle
will perform poorly.

00:19:33.166 --> 00:19:36.936 A:middle
And third, also, World Tracking

00:19:36.936 --> 00:19:38.646 A:middle
works best in static

00:19:38.646 --> 00:19:39.366 A:middle
environments.

00:19:40.326 --> 00:19:42.026 A:middle
If too much of what your camera

00:19:42.106 --> 00:19:45.166 A:middle
sees is moving, then the visual

00:19:45.166 --> 00:19:47.036 A:middle
data won't correspond with the

00:19:47.036 --> 00:19:50.526 A:middle
motion data, which might result

00:19:50.526 --> 00:19:51.776 A:middle
in the potential drift.

00:19:52.846 --> 00:19:54.706 A:middle
Also, device itself should not

00:19:54.706 --> 00:19:55.936 A:middle
be on a moving platform like a

00:19:55.936 --> 00:19:57.456 A:middle
bus or an elevator.

00:19:58.326 --> 00:19:59.726 A:middle
Because in those moments the

00:19:59.726 --> 00:20:01.176 A:middle
motion sensor would actually

00:19:59.726 --> 00:20:01.176 A:middle
motion sensor would actually

00:20:01.176 --> 00:20:02.706 A:middle
sense a motion like going up or

00:20:02.706 --> 00:20:04.476 A:middle
down in the elevator while,

00:20:04.476 --> 00:20:06.766 A:middle
visually, your environment had

00:20:06.816 --> 00:20:07.566 A:middle
not changed.

00:20:08.066 --> 00:20:11.846 A:middle
So, how can you get notified

00:20:11.846 --> 00:20:13.966 A:middle
about the tracking quality that

00:20:14.036 --> 00:20:15.036 A:middle
the user is currently

00:20:15.036 --> 00:20:16.926 A:middle
experiencing with your app?

00:20:18.236 --> 00:20:19.916 A:middle
ARKit monitors its tracking

00:20:19.916 --> 00:20:20.586 A:middle
performance.

00:20:21.186 --> 00:20:22.776 A:middle
We applied machine learning,

00:20:23.076 --> 00:20:24.536 A:middle
which was trained on thousands

00:20:24.536 --> 00:20:26.546 A:middle
of data sets to which we had the

00:20:26.546 --> 00:20:28.706 A:middle
information how well tracking

00:20:28.706 --> 00:20:30.266 A:middle
performed in those situations.

00:20:31.776 --> 00:20:33.276 A:middle
To train a classifier, which

00:20:33.276 --> 00:20:35.036 A:middle
tells you how tracking performs,

00:20:35.036 --> 00:20:36.846 A:middle
we used annotations like the

00:20:36.846 --> 00:20:39.486 A:middle
number of visual-- visible

00:20:39.566 --> 00:20:41.136 A:middle
features tracked in the image

00:20:41.936 --> 00:20:43.576 A:middle
and also, the current velocity

00:20:43.576 --> 00:20:44.346 A:middle
of the device.

00:20:45.496 --> 00:20:47.866 A:middle
Now, during runtime, the health

00:20:47.866 --> 00:20:50.636 A:middle
of tracking is determined based

00:20:50.636 --> 00:20:51.746 A:middle
on those parameters.

00:20:52.616 --> 00:20:55.116 A:middle
In this video, we can see how

00:20:55.116 --> 00:20:57.026 A:middle
the health estimate, which can

00:20:57.026 --> 00:20:58.406 A:middle
be seen-- which, is reported in

00:20:58.406 --> 00:21:00.846 A:middle
the lower left, gets worse when

00:20:58.406 --> 00:21:00.846 A:middle
the lower left, gets worse when

00:21:00.846 --> 00:21:03.926 A:middle
the camera is covered while we

00:21:03.926 --> 00:21:05.576 A:middle
are still moving and exploring

00:21:05.576 --> 00:21:06.296 A:middle
the environment.

00:21:07.796 --> 00:21:09.596 A:middle
It also shows how it returns

00:21:09.596 --> 00:21:11.336 A:middle
back to normal after the camera

00:21:11.336 --> 00:21:12.496 A:middle
view is uncovered.

00:21:14.036 --> 00:21:16.066 A:middle
Now, ARKit simplifies its

00:21:16.066 --> 00:21:18.586 A:middle
information for you by providing

00:21:18.586 --> 00:21:19.596 A:middle
a tracking state.

00:21:20.466 --> 00:21:22.186 A:middle
And the tracking state can have

00:21:22.316 --> 00:21:23.426 A:middle
three different values.

00:21:23.426 --> 00:21:26.776 A:middle
It can be normal, which is the

00:21:26.856 --> 00:21:29.676 A:middle
healthy state and is the case in

00:21:29.676 --> 00:21:30.546 A:middle
most of the time.

00:21:30.546 --> 00:21:31.566 A:middle
It's the case in most of the

00:21:31.566 --> 00:21:31.956 A:middle
times.

00:21:32.506 --> 00:21:34.096 A:middle
And it can also be limited,

00:21:34.236 --> 00:21:35.396 A:middle
which is whenever tracking

00:21:35.396 --> 00:21:36.426 A:middle
performs poorly.

00:21:37.486 --> 00:21:40.116 A:middle
If that's the case, then the

00:21:40.116 --> 00:21:42.056 A:middle
limited state will also come

00:21:42.056 --> 00:21:43.786 A:middle
along with the reason, like

00:21:43.946 --> 00:21:45.406 A:middle
insufficient features or

00:21:45.406 --> 00:21:47.766 A:middle
excessive motion or being

00:21:47.806 --> 00:21:49.606 A:middle
currently in the initialization

00:21:49.606 --> 00:21:49.966 A:middle
phase.

00:21:50.716 --> 00:21:53.496 A:middle
It can also be not available,

00:21:53.496 --> 00:21:55.256 A:middle
which means that tracking did

00:21:55.256 --> 00:21:56.086 A:middle
not start yet.

00:21:57.116 --> 00:21:58.826 A:middle
Now, whenever the tracking state

00:21:58.916 --> 00:22:01.146 A:middle
changes, a delegate is called.

00:21:58.916 --> 00:22:01.146 A:middle
changes, a delegate is called.

00:22:01.416 --> 00:22:03.636 A:middle
The camera did change tracking

00:22:03.636 --> 00:22:04.076 A:middle
state.

00:22:05.216 --> 00:22:06.116 A:middle
And this gives you the

00:22:06.116 --> 00:22:08.666 A:middle
opportunity to notify the user

00:22:08.896 --> 00:22:10.346 A:middle
when a limited state has been

00:22:10.346 --> 00:22:11.096 A:middle
encountered.

00:22:12.116 --> 00:22:13.226 A:middle
You should, then, give

00:22:13.426 --> 00:22:15.426 A:middle
informative and actionable

00:22:15.776 --> 00:22:17.936 A:middle
feedback what the user can do to

00:22:17.936 --> 00:22:19.626 A:middle
improve his tracking situation,

00:22:20.136 --> 00:22:22.566 A:middle
as most of it is actually in the

00:22:22.566 --> 00:22:23.396 A:middle
user's hand.

00:22:23.776 --> 00:22:25.706 A:middle
Like for example, as we learned

00:22:25.706 --> 00:22:27.996 A:middle
before, like a sidewise movement

00:22:28.096 --> 00:22:30.656 A:middle
to allow initialization or

00:22:31.206 --> 00:22:32.236 A:middle
making sure there's enough

00:22:32.236 --> 00:22:34.316 A:middle
adequate lighting for enough

00:22:34.416 --> 00:22:35.646 A:middle
visual complexity.

00:22:36.236 --> 00:22:39.646 A:middle
So, let me wrap up the World

00:22:39.646 --> 00:22:40.446 A:middle
Tracking for you.

00:22:42.136 --> 00:22:46.156 A:middle
World Tracking tracks your

00:22:46.156 --> 00:22:48.046 A:middle
camera 6 degree of freedom

00:22:48.276 --> 00:22:51.556 A:middle
orientation and position with

00:22:51.556 --> 00:22:53.356 A:middle
respect to your surrounding

00:22:53.356 --> 00:22:55.286 A:middle
environment and without any

00:22:55.286 --> 00:22:56.916 A:middle
prior information about your

00:22:56.916 --> 00:22:59.066 A:middle
environment, which then allows

00:22:59.606 --> 00:23:01.866 A:middle
the physical world augmentation

00:22:59.606 --> 00:23:01.866 A:middle
the physical world augmentation

00:23:01.866 --> 00:23:02.836 A:middle
in which the content can

00:23:02.836 --> 00:23:04.776 A:middle
actually be viewed from any kind

00:23:04.776 --> 00:23:05.166 A:middle
of view.

00:23:06.636 --> 00:23:08.676 A:middle
Also, World Tracking creates a

00:23:08.676 --> 00:23:11.196 A:middle
World map, which becomes the

00:23:11.196 --> 00:23:13.206 A:middle
tracking's reference system to

00:23:13.206 --> 00:23:14.876 A:middle
localize new camera images

00:23:14.916 --> 00:23:15.416 A:middle
against.

00:23:17.266 --> 00:23:18.386 A:middle
To create a great user

00:23:18.386 --> 00:23:20.466 A:middle
experience, the tracking quality

00:23:20.466 --> 00:23:22.696 A:middle
should be monitored and feedback

00:23:22.786 --> 00:23:24.476 A:middle
and guidance should be provided

00:23:24.476 --> 00:23:25.196 A:middle
to your user.

00:23:25.736 --> 00:23:28.676 A:middle
And World Tracking runs on your

00:23:28.676 --> 00:23:29.606 A:middle
device only.

00:23:30.056 --> 00:23:31.516 A:middle
And all results stay on your

00:23:31.516 --> 00:23:31.926 A:middle
device.

00:23:33.446 --> 00:23:34.716 A:middle
If you have not done it already,

00:23:35.576 --> 00:23:37.206 A:middle
try out one of our developer

00:23:37.206 --> 00:23:37.836 A:middle
examples.

00:23:37.936 --> 00:23:39.116 A:middle
For example, the Build Your

00:23:39.116 --> 00:23:41.546 A:middle
First AR Experience, and play a

00:23:41.546 --> 00:23:43.726 A:middle
bit around, just 15 minutes with

00:23:43.726 --> 00:23:44.866 A:middle
the tracking quality in

00:23:44.866 --> 00:23:46.536 A:middle
different situations; light

00:23:46.616 --> 00:23:48.036 A:middle
situations or movements.

00:23:48.536 --> 00:23:50.476 A:middle
And always remember to guide the

00:23:50.476 --> 00:23:53.596 A:middle
user whenever he encounters a

00:23:53.826 --> 00:23:56.696 A:middle
limited tracking situation to

00:23:56.696 --> 00:23:58.646 A:middle
guarantee that he has a great

00:23:58.886 --> 00:24:00.036 A:middle
tracking experience.

00:23:58.886 --> 00:24:00.036 A:middle
tracking experience.

00:24:01.456 --> 00:24:04.586 A:middle
So, World Tracking is about the

00:24:04.586 --> 00:24:06.566 A:middle
camera-- where your camera is

00:24:06.566 --> 00:24:08.526 A:middle
with respect to your physical

00:24:08.526 --> 00:24:09.186 A:middle
environment.

00:24:10.156 --> 00:24:13.026 A:middle
Let's now talk about how the

00:24:13.026 --> 00:24:14.536 A:middle
virtual content can interact

00:24:14.836 --> 00:24:16.776 A:middle
with the physical environment.

00:24:17.136 --> 00:24:19.156 A:middle
And this is possible with Plane

00:24:19.156 --> 00:24:19.726 A:middle
Detection.

00:24:22.916 --> 00:24:24.876 A:middle
The following video, again, from

00:24:24.876 --> 00:24:26.576 A:middle
the Ikea app, shows a great use

00:24:26.576 --> 00:24:28.146 A:middle
case for the Plane Detection,

00:24:28.466 --> 00:24:30.626 A:middle
placing virtual objects into

00:24:30.626 --> 00:24:32.566 A:middle
your physical environment, and

00:24:32.566 --> 00:24:34.426 A:middle
then interacting with it.

00:24:35.296 --> 00:24:37.786 A:middle
So first, please note how, also,

00:24:37.786 --> 00:24:39.256 A:middle
in the Ikea app the user is

00:24:39.256 --> 00:24:41.156 A:middle
guided to make some movement.

00:24:42.296 --> 00:24:44.326 A:middle
Then, once a horizontal plane is

00:24:44.376 --> 00:24:46.796 A:middle
detected, the virtual table set

00:24:46.796 --> 00:24:48.836 A:middle
is displayed and is waiting to

00:24:48.836 --> 00:24:49.916 A:middle
be placed by you.

00:24:51.256 --> 00:24:52.856 A:middle
Once you position it, rotate it

00:24:52.856 --> 00:24:54.646 A:middle
as you want it, you can lock the

00:24:54.646 --> 00:24:55.886 A:middle
object in its environment.

00:24:56.126 --> 00:24:56.976 A:middle
And did you notice the

00:24:56.976 --> 00:24:59.206 A:middle
interaction between the detected

00:24:59.256 --> 00:25:01.156 A:middle
ground plane and the table set

00:24:59.256 --> 00:25:01.156 A:middle
ground plane and the table set

00:25:01.376 --> 00:25:02.196 A:middle
in the moment of locking?

00:25:02.196 --> 00:25:04.166 A:middle
It kind of bounces shortly on

00:25:04.166 --> 00:25:04.916 A:middle
the ground plane.

00:25:05.276 --> 00:25:06.986 A:middle
And this is possible because we

00:25:06.986 --> 00:25:08.416 A:middle
know where the ground plane is.

00:25:09.556 --> 00:25:10.996 A:middle
So, let's have a look at what

00:25:10.996 --> 00:25:12.696 A:middle
happened under the hood here.

00:25:14.016 --> 00:25:16.086 A:middle
Plane Detection uses the World

00:25:16.166 --> 00:25:18.726 A:middle
map provided by the world I just

00:25:18.726 --> 00:25:20.656 A:middle
talked about, just talked about

00:25:20.656 --> 00:25:22.726 A:middle
a moment ago, which is

00:25:22.726 --> 00:25:24.246 A:middle
represented here in those yellow

00:25:24.246 --> 00:25:24.716 A:middle
points.

00:25:25.176 --> 00:25:28.446 A:middle
And then, it uses them to detect

00:25:28.446 --> 00:25:30.966 A:middle
surfaces that are horizontal or

00:25:30.966 --> 00:25:32.936 A:middle
vertical, like the ground, the

00:25:32.936 --> 00:25:34.586 A:middle
bench, and the small wall.

00:25:35.386 --> 00:25:36.946 A:middle
It does this by accumulating

00:25:36.946 --> 00:25:38.746 A:middle
information over multiple

00:25:38.746 --> 00:25:39.266 A:middle
ARFrames.

00:25:40.096 --> 00:25:42.496 A:middle
So, as the user moves around the

00:25:42.596 --> 00:25:44.316 A:middle
scene, more and more information

00:25:44.316 --> 00:25:45.416 A:middle
about the real surface is

00:25:45.416 --> 00:25:46.026 A:middle
acquired.

00:25:46.896 --> 00:25:48.086 A:middle
It also allows the Plane

00:25:48.086 --> 00:25:49.966 A:middle
Detection to provide and like

00:25:50.296 --> 00:25:52.056 A:middle
extent the surface, like a

00:25:52.056 --> 00:25:52.916 A:middle
convex hull.

00:25:53.386 --> 00:25:58.006 A:middle
If multiple planes belonging to

00:25:58.006 --> 00:26:00.676 A:middle
the same physical surface are

00:25:58.006 --> 00:26:00.676 A:middle
the same physical surface are

00:26:00.676 --> 00:26:02.876 A:middle
detected, like in this part now,

00:26:02.876 --> 00:26:04.636 A:middle
the green and the purple one,

00:26:05.226 --> 00:26:06.726 A:middle
then they will be merged once

00:26:06.726 --> 00:26:07.926 A:middle
they start overlapping.

00:26:08.596 --> 00:26:11.356 A:middle
If horizontal and vertical

00:26:11.356 --> 00:26:13.026 A:middle
planes intersect they are

00:26:13.026 --> 00:26:14.366 A:middle
clipped at the line of

00:26:14.366 --> 00:26:15.666 A:middle
intersection, which is actually

00:26:15.666 --> 00:26:18.156 A:middle
a new feature in ARKit 2.

00:26:19.876 --> 00:26:21.776 A:middle
Plane Detection is designed to

00:26:21.776 --> 00:26:24.686 A:middle
have very little overhead as it

00:26:24.766 --> 00:26:27.166 A:middle
repurposes the mapped 3D points

00:26:27.166 --> 00:26:28.296 A:middle
from the World Tracking.

00:26:29.086 --> 00:26:31.166 A:middle
And then it fits planes into

00:26:31.166 --> 00:26:33.386 A:middle
those point clouds and over time

00:26:33.696 --> 00:26:36.346 A:middle
continuously aggregates more and

00:26:36.346 --> 00:26:38.326 A:middle
more points and merge the planes

00:26:38.606 --> 00:26:40.286 A:middle
that start to overlap.

00:26:40.936 --> 00:26:42.516 A:middle
Therefore, it takes some time

00:26:42.516 --> 00:26:44.156 A:middle
until the first planes are

00:26:44.156 --> 00:26:44.806 A:middle
detected.

00:26:46.096 --> 00:26:47.276 A:middle
What does that mean for you?

00:26:48.856 --> 00:26:50.726 A:middle
If your app is started, there

00:26:50.726 --> 00:26:53.036 A:middle
might not directly be planes to

00:26:53.036 --> 00:26:55.516 A:middle
place objects on or to interact

00:26:55.626 --> 00:26:55.846 A:middle
with.

00:26:57.266 --> 00:26:58.786 A:middle
If the detection of a plane is

00:26:58.876 --> 00:27:01.496 A:middle
mandatory for your experience,

00:26:58.876 --> 00:27:01.496 A:middle
mandatory for your experience,

00:27:01.786 --> 00:27:04.006 A:middle
you should again guide the user

00:27:04.426 --> 00:27:06.006 A:middle
to move the camera with enough

00:27:06.106 --> 00:27:08.956 A:middle
translation to ensure a dense

00:27:08.956 --> 00:27:11.616 A:middle
reconstruction based on the

00:27:11.616 --> 00:27:13.396 A:middle
parallax, and also, enough

00:27:13.396 --> 00:27:15.086 A:middle
visual complexity in the scene.

00:27:15.976 --> 00:27:18.096 A:middle
Again, for the reconstruction, a

00:27:18.096 --> 00:27:20.416 A:middle
rotation only is not enough.

00:27:20.946 --> 00:27:23.856 A:middle
Now, how can you enable the

00:27:23.856 --> 00:27:24.796 A:middle
Plane Detection?

00:27:25.786 --> 00:27:26.966 A:middle
It's, again, very simple.

00:27:27.116 --> 00:27:28.676 A:middle
As the Plane Detection reuses

00:27:28.676 --> 00:27:30.286 A:middle
the 3D map from the World

00:27:30.286 --> 00:27:32.486 A:middle
Tracking, it can be configured

00:27:32.646 --> 00:27:33.316 A:middle
by using the

00:27:33.316 --> 00:27:35.126 A:middle
ARWorldTrackingConfiguration.

00:27:35.646 --> 00:27:38.346 A:middle
Then, the property

00:27:38.346 --> 00:27:40.706 A:middle
planeDetection just needs to be

00:27:40.706 --> 00:27:41.986 A:middle
set to either horizontal,

00:27:41.986 --> 00:27:43.626 A:middle
vertical, or like in this case,

00:27:43.706 --> 00:27:43.926 A:middle
both.

00:27:45.116 --> 00:27:47.446 A:middle
And then, just call your

00:27:47.446 --> 00:27:49.046 A:middle
ARSession with this

00:27:49.106 --> 00:27:50.226 A:middle
configuration.

00:27:50.476 --> 00:27:52.146 A:middle
And the detection of the planes

00:27:52.146 --> 00:27:52.856 A:middle
will be started.

00:27:53.526 --> 00:27:56.086 A:middle
Now, how are those, the results

00:27:56.086 --> 00:27:57.686 A:middle
of the detected planes returned

00:27:57.686 --> 00:27:58.576 A:middle
to you?

00:28:01.496 --> 00:28:03.216 A:middle
The detected planes are returned

00:28:03.216 --> 00:28:05.006 A:middle
as an ARPlaneAnchor.

00:28:05.936 --> 00:28:07.996 A:middle
An ARPlaneAnchor is a subclass

00:28:07.996 --> 00:28:08.946 A:middle
of an ARAnchor.

00:28:10.106 --> 00:28:11.696 A:middle
Each ARAnchor provides a

00:28:11.876 --> 00:28:14.476 A:middle
transform containing the

00:28:14.476 --> 00:28:16.326 A:middle
information where the anchor is

00:28:16.606 --> 00:28:17.566 A:middle
in your World map.

00:28:18.126 --> 00:28:20.216 A:middle
Now, a plane anchor,

00:28:20.216 --> 00:28:21.436 A:middle
specifically, also has

00:28:21.436 --> 00:28:24.026 A:middle
information about the geometry

00:28:24.026 --> 00:28:25.886 A:middle
of the surface of the plane,

00:28:26.816 --> 00:28:27.856 A:middle
which is represented in two

00:28:27.856 --> 00:28:28.976 A:middle
alternative ways.

00:28:29.316 --> 00:28:31.256 A:middle
Either like a bounding box with

00:28:31.256 --> 00:28:35.096 A:middle
a center and an extent, or as a

00:28:35.096 --> 00:28:37.306 A:middle
3D mesh describing the shape of

00:28:37.356 --> 00:28:39.336 A:middle
the convex hull of the detected

00:28:39.386 --> 00:28:41.306 A:middle
plane and its geometry property.

00:28:42.636 --> 00:28:44.646 A:middle
To get notified about detected

00:28:44.736 --> 00:28:48.266 A:middle
planes, delegates are going to

00:28:48.266 --> 00:28:51.266 A:middle
be called whenever planes are

00:28:51.266 --> 00:28:53.676 A:middle
added, updated, or removed.

00:28:54.656 --> 00:28:57.086 A:middle
This will then allow you to use

00:28:57.086 --> 00:28:59.566 A:middle
those planes, as well as react

00:28:59.686 --> 00:29:01.486 A:middle
to any updates.

00:28:59.686 --> 00:29:01.486 A:middle
to any updates.

00:29:01.666 --> 00:29:02.926 A:middle
Now, what can you do with

00:29:03.006 --> 00:29:03.366 A:middle
planes?

00:29:04.866 --> 00:29:05.956 A:middle
Like what we've seen before on

00:29:05.956 --> 00:29:07.296 A:middle
the Ikea app, these are great

00:29:07.296 --> 00:29:07.796 A:middle
examples.

00:29:08.026 --> 00:29:09.496 A:middle
Place virtual objects, for

00:29:09.496 --> 00:29:10.916 A:middle
example, with hit testing.

00:29:12.046 --> 00:29:13.606 A:middle
Or you can interact with some,

00:29:13.606 --> 00:29:15.186 A:middle
for example, physically.

00:29:15.286 --> 00:29:17.846 A:middle
Like we've seen bouncing is a

00:29:17.846 --> 00:29:18.706 A:middle
possibility.

00:29:19.816 --> 00:29:21.966 A:middle
Or you can also use it by adding

00:29:21.966 --> 00:29:23.226 A:middle
an occlusion plane into the

00:29:23.226 --> 00:29:25.346 A:middle
detected plane, which will then

00:29:25.496 --> 00:29:27.286 A:middle
hide all the virtual content

00:29:27.336 --> 00:29:30.036 A:middle
below or behind the added

00:29:30.036 --> 00:29:30.916 A:middle
occlusion plane.

00:29:32.616 --> 00:29:34.436 A:middle
So, let me summarize what we've

00:29:34.436 --> 00:29:35.386 A:middle
already gone through.

00:29:36.536 --> 00:29:37.756 A:middle
We've had a look at the

00:29:37.816 --> 00:29:41.176 A:middle
Orientation Tracking, the World

00:29:41.286 --> 00:29:43.816 A:middle
Tracking, and the Plane

00:29:43.816 --> 00:29:44.386 A:middle
Detection.

00:29:45.306 --> 00:29:47.666 A:middle
Next, Michele will explain, in

00:29:47.666 --> 00:29:48.986 A:middle
depth, our new tracking

00:29:48.986 --> 00:29:50.316 A:middle
technologies, which were

00:29:50.386 --> 00:29:52.976 A:middle
introduced in ARKit 2.

00:29:53.046 --> 00:29:54.246 A:middle
So, welcome Michele.

00:29:55.176 --> 00:29:57.500 A:middle
[ Applause ]

00:29:58.396 --> 00:29:59.186 A:middle
&gt;&gt; Thank you, Marion.

00:30:00.506 --> 00:30:01.766 A:middle
My name is Michele, and it's a

00:30:01.766 --> 00:30:02.766 A:middle
pleasure to continue with the

00:30:02.766 --> 00:30:03.636 A:middle
remaining topics of this

00:30:03.636 --> 00:30:03.886 A:middle
session.

00:30:05.136 --> 00:30:07.886 A:middle
Next up is saving and loading

00:30:07.886 --> 00:30:08.326 A:middle
maps.

00:30:08.936 --> 00:30:10.486 A:middle
This is a feature that allows to

00:30:10.486 --> 00:30:11.816 A:middle
store all the information that

00:30:11.816 --> 00:30:12.906 A:middle
are required in a session.

00:30:13.336 --> 00:30:14.256 A:middle
So, that it can literally be

00:30:14.256 --> 00:30:16.216 A:middle
restored in another session at a

00:30:16.216 --> 00:30:18.116 A:middle
later point in time to create

00:30:18.336 --> 00:30:19.706 A:middle
augmented reality experiences

00:30:19.706 --> 00:30:21.146 A:middle
that persist to a particular

00:30:21.146 --> 00:30:21.486 A:middle
place.

00:30:22.406 --> 00:30:23.566 A:middle
Or that could, also, be stored

00:30:23.566 --> 00:30:25.416 A:middle
by another device to create

00:30:25.606 --> 00:30:27.296 A:middle
multiple user augmented reality

00:30:27.296 --> 00:30:27.906 A:middle
experiences.

00:30:28.646 --> 00:30:30.000 A:middle
Let's take a look at an example.

00:30:37.076 --> 00:30:38.876 A:middle
What you see here is a guy;

00:30:38.876 --> 00:30:40.906 A:middle
let's name him Andre, that's

00:30:40.906 --> 00:30:41.956 A:middle
walking around the table with

00:30:41.956 --> 00:30:43.506 A:middle
his device having an augmented

00:30:43.506 --> 00:30:44.416 A:middle
reality experience.

00:30:45.366 --> 00:30:47.596 A:middle
And you can see his device now

00:30:47.986 --> 00:30:48.856 A:middle
is making this seem more

00:30:48.856 --> 00:30:50.766 A:middle
interesting by adding a virtual

00:30:50.766 --> 00:30:52.506 A:middle
vase on the table.

00:30:54.556 --> 00:30:56.746 A:middle
A few minutes later his friends

00:30:56.986 --> 00:30:58.126 A:middle
arrive at the same scene.

00:30:58.366 --> 00:31:00.116 A:middle
And now, they're both looking at

00:30:58.366 --> 00:31:00.116 A:middle
And now, they're both looking at

00:31:00.116 --> 00:31:00.576 A:middle
the scene.

00:31:00.656 --> 00:31:01.926 A:middle
You're going to see Andre's

00:31:02.446 --> 00:31:04.196 A:middle
device on the left and his

00:31:04.196 --> 00:31:05.126 A:middle
friend on the right now.

00:31:06.546 --> 00:31:07.446 A:middle
So, you can see that they're

00:31:07.706 --> 00:31:08.836 A:middle
looking at the same space.

00:31:08.926 --> 00:31:09.886 A:middle
They can see each other.

00:31:10.266 --> 00:31:11.806 A:middle
But most importantly, they see

00:31:11.806 --> 00:31:13.386 A:middle
the same virtual content.

00:31:14.286 --> 00:31:15.446 A:middle
They're having a shared

00:31:15.446 --> 00:31:19.246 A:middle
augmented reality experience.

00:31:19.246 --> 00:31:21.456 A:middle
So, what we have seen in these

00:31:21.456 --> 00:31:23.126 A:middle
examples can be discovered in

00:31:23.186 --> 00:31:23.916 A:middle
three stages.

00:31:24.266 --> 00:31:25.816 A:middle
First, Andre went around the

00:31:25.816 --> 00:31:27.416 A:middle
table and acquired the World

00:31:27.416 --> 00:31:27.616 A:middle
map.

00:31:28.886 --> 00:31:30.246 A:middle
Then, the World map was shared

00:31:30.536 --> 00:31:31.506 A:middle
across devices.

00:31:32.276 --> 00:31:34.346 A:middle
And then, his friend's device

00:31:34.496 --> 00:31:36.056 A:middle
re-localized to the World map.

00:31:37.496 --> 00:31:38.986 A:middle
This means that ARKit was

00:31:38.986 --> 00:31:40.536 A:middle
able to understand in the new

00:31:40.536 --> 00:31:41.646 A:middle
device that this was the same

00:31:41.646 --> 00:31:42.936 A:middle
place as the other device,

00:31:43.586 --> 00:31:45.406 A:middle
computed the precise position of

00:31:45.406 --> 00:31:46.556 A:middle
the device with respect to the

00:31:46.556 --> 00:31:48.356 A:middle
map, and then, started tracking

00:31:48.356 --> 00:31:50.106 A:middle
from there just like the new

00:31:50.106 --> 00:31:51.346 A:middle
device acquired the World map

00:31:51.346 --> 00:31:51.746 A:middle
itself.

00:31:52.376 --> 00:31:54.956 A:middle
We're going to go into more

00:31:54.956 --> 00:31:56.196 A:middle
detail about these three phases.

00:31:56.866 --> 00:31:59.246 A:middle
But first, let's review what's

00:31:59.246 --> 00:32:00.426 A:middle
in the World map.

00:31:59.246 --> 00:32:00.426 A:middle
in the World map.

00:32:01.156 --> 00:32:02.866 A:middle
The World map includes all the

00:32:02.866 --> 00:32:04.966 A:middle
tracking data that are needed

00:32:04.966 --> 00:32:06.476 A:middle
for the system to be localized,

00:32:06.986 --> 00:32:08.546 A:middle
which includes the feature

00:32:08.546 --> 00:32:09.866 A:middle
points as Marion greatly

00:32:09.866 --> 00:32:10.506 A:middle
explained before.

00:32:10.876 --> 00:32:12.596 A:middle
As well as local appearance for

00:32:12.596 --> 00:32:13.000 A:middle
this point.

00:32:17.046 --> 00:32:18.496 A:middle
They also contain all the

00:32:18.496 --> 00:32:19.826 A:middle
anchors that were added to the

00:32:19.826 --> 00:32:21.806 A:middle
session, either by the users,

00:32:21.916 --> 00:32:23.636 A:middle
like planes, for example.

00:32:24.896 --> 00:32:26.126 A:middle
I mean by the system-- like

00:32:26.126 --> 00:32:26.456 A:middle
planes.

00:32:26.456 --> 00:32:28.126 A:middle
Or by the users, like the vase,

00:32:28.446 --> 00:32:29.646 A:middle
as we have seen in the example.

00:32:30.746 --> 00:32:33.786 A:middle
This data is serializable and

00:32:33.786 --> 00:32:35.536 A:middle
available to you so that you can

00:32:35.536 --> 00:32:37.346 A:middle
create compelling persistent or

00:32:37.826 --> 00:32:39.326 A:middle
multiple user augmented reality

00:32:39.326 --> 00:32:39.966 A:middle
experiences.

00:32:40.326 --> 00:32:41.486 A:middle
So, now let's take a look at the

00:32:41.486 --> 00:32:43.526 A:middle
first stage, which is acquiring

00:32:43.526 --> 00:32:44.006 A:middle
the World map.

00:32:44.756 --> 00:32:47.576 A:middle
We can play back the first video

00:32:48.056 --> 00:32:49.396 A:middle
where Andre went around the

00:32:49.396 --> 00:32:51.026 A:middle
table that you can see his

00:32:51.026 --> 00:32:52.596 A:middle
device on the left, here.

00:32:53.256 --> 00:32:57.216 A:middle
And on the right, you see the

00:32:57.216 --> 00:32:59.126 A:middle
World map from a top view as

00:32:59.126 --> 00:33:00.426 A:middle
acquired by the tracking system.

00:32:59.126 --> 00:33:00.426 A:middle
acquired by the tracking system.

00:33:00.776 --> 00:33:02.516 A:middle
You can recognize the circle is the table

00:33:03.096 --> 00:33:04.566 A:middle
and the chair around it.

00:33:05.136 --> 00:33:06.956 A:middle
There's a few things to pay

00:33:06.956 --> 00:33:08.106 A:middle
attention to during this

00:33:08.186 --> 00:33:09.276 A:middle
acquisition process.

00:33:10.076 --> 00:33:11.986 A:middle
First, everything that Marion

00:33:11.986 --> 00:33:13.396 A:middle
said during tracking also

00:33:13.396 --> 00:33:13.946 A:middle
applies here.

00:33:14.456 --> 00:33:15.606 A:middle
So, we want enough visual

00:33:15.606 --> 00:33:17.356 A:middle
complexity on the scene to get

00:33:17.446 --> 00:33:18.626 A:middle
dense feature points on the map.

00:33:19.546 --> 00:33:21.276 A:middle
And the scene must be static.

00:33:22.086 --> 00:33:22.856 A:middle
Of course, we can deal with

00:33:22.856 --> 00:33:24.276 A:middle
minor changes, as you have seen

00:33:24.276 --> 00:33:25.466 A:middle
the tablecloth moving by the

00:33:25.466 --> 00:33:25.716 A:middle
wind.

00:33:26.026 --> 00:33:27.216 A:middle
But the scene must be mostly

00:33:27.566 --> 00:33:27.956 A:middle
static.

00:33:29.036 --> 00:33:30.606 A:middle
In addition, when we are

00:33:30.606 --> 00:33:31.936 A:middle
specifically acquiring a World

00:33:31.936 --> 00:33:33.856 A:middle
map for sharing we want to go

00:33:33.856 --> 00:33:35.316 A:middle
around the environment from

00:33:35.316 --> 00:33:36.636 A:middle
multiple points of view.

00:33:37.516 --> 00:33:38.746 A:middle
In particular, we want to cover

00:33:38.746 --> 00:33:41.046 A:middle
all the direction from which we

00:33:41.106 --> 00:33:42.786 A:middle
want to later be localized from.

00:33:45.366 --> 00:33:46.926 A:middle
To make this easy, we also made

00:33:47.356 --> 00:33:50.296 A:middle
available a world mapping status

00:33:50.296 --> 00:33:51.736 A:middle
which gives you information

00:33:51.736 --> 00:33:52.566 A:middle
about the World map.

00:33:53.446 --> 00:33:54.816 A:middle
If you guys have been to the

00:33:54.816 --> 00:33:56.066 A:middle
What's New in ARKit talk,

00:33:56.636 --> 00:33:57.506 A:middle
Arsalan greatly expand this

00:33:57.546 --> 00:33:58.936 A:middle
to quickly recap.

00:33:59.576 --> 00:34:00.766 A:middle
When you start the session the

00:33:59.576 --> 00:34:00.766 A:middle
When you start the session the

00:34:00.766 --> 00:34:02.196 A:middle
World map status will start

00:34:02.336 --> 00:34:02.776 A:middle
limited.

00:34:02.776 --> 00:34:03.936 A:middle
And then, will switch to

00:34:04.056 --> 00:34:06.746 A:middle
extending as more of the scene is

00:34:06.746 --> 00:34:07.616 A:middle
learned by the device.

00:34:08.036 --> 00:34:09.176 A:middle
And then, finally, we go to

00:34:09.176 --> 00:34:10.856 A:middle
mapped when the system is

00:34:10.856 --> 00:34:12.156 A:middle
confident you're staying in the

00:34:12.156 --> 00:34:12.766 A:middle
same place.

00:34:13.726 --> 00:34:15.206 A:middle
And that's when you want to save

00:34:15.206 --> 00:34:16.976 A:middle
the map in the mapped state.

00:34:17.626 --> 00:34:20.315 A:middle
So, that's good information.

00:34:20.315 --> 00:34:22.056 A:middle
But this is mostly on the user

00:34:22.056 --> 00:34:24.326 A:middle
side applied to acquire the

00:34:24.326 --> 00:34:24.746 A:middle
session.

00:34:24.966 --> 00:34:26.076 A:middle
So, what does this mean to you

00:34:26.156 --> 00:34:26.926 A:middle
as a developer?

00:34:27.505 --> 00:34:29.085 A:middle
That you need to guide the user.

00:34:30.275 --> 00:34:32.255 A:middle
So, we can indicate the mapping

00:34:32.326 --> 00:34:33.985 A:middle
status and even disabling the

00:34:33.985 --> 00:34:35.835 A:middle
saving or sharing of the World

00:34:35.835 --> 00:34:37.886 A:middle
map until the mapping status

00:34:37.926 --> 00:34:39.476 A:middle
goes to the mapped state.

00:34:39.996 --> 00:34:44.255 A:middle
We can also, monitor the

00:34:44.326 --> 00:34:45.576 A:middle
tracking quality during the

00:34:45.576 --> 00:34:48.696 A:middle
acquisition session and report

00:34:48.926 --> 00:34:50.045 A:middle
to the user if the tracking

00:34:50.045 --> 00:34:51.356 A:middle
state has been limited for more

00:34:51.356 --> 00:34:52.146 A:middle
than a few seconds.

00:34:53.056 --> 00:34:54.356 A:middle
And maybe even give an option to

00:34:54.356 --> 00:34:56.036 A:middle
restart the acquisition session.

00:34:56.476 --> 00:34:58.496 A:middle
On the receiving end of the

00:34:58.496 --> 00:35:00.726 A:middle
device, we can also guide the

00:34:58.496 --> 00:35:00.726 A:middle
device, we can also guide the

00:35:00.726 --> 00:35:01.956 A:middle
user to better localization

00:35:01.956 --> 00:35:02.346 A:middle
process.

00:35:03.116 --> 00:35:04.856 A:middle
So, when we are, again, in the

00:35:04.856 --> 00:35:06.776 A:middle
acquisition device, when we are

00:35:06.776 --> 00:35:08.196 A:middle
in the map state we can take a

00:35:08.196 --> 00:35:10.296 A:middle
picture of the scene and then,

00:35:10.396 --> 00:35:11.496 A:middle
ship that together with the

00:35:11.496 --> 00:35:11.946 A:middle
World map.

00:35:11.986 --> 00:35:13.596 A:middle
And on the receiving end we can

00:35:13.596 --> 00:35:16.026 A:middle
ask the user find this view to

00:35:16.026 --> 00:35:17.766 A:middle
start your shared experience.

00:35:18.386 --> 00:35:20.946 A:middle
That was how to acquire the

00:35:20.946 --> 00:35:21.336 A:middle
World map.

00:35:21.506 --> 00:35:22.876 A:middle
Now, let's see how you can share

00:35:22.876 --> 00:35:24.666 A:middle
the World map.

00:35:24.966 --> 00:35:26.366 A:middle
First, you can get the World map

00:35:26.366 --> 00:35:27.386 A:middle
by simply calling the

00:35:27.496 --> 00:35:29.926 A:middle
getCurrentWorldMap method in the

00:35:29.926 --> 00:35:30.536 A:middle
ARSession.

00:35:30.896 --> 00:35:32.596 A:middle
And this will give you the World

00:35:33.186 --> 00:35:33.286 A:middle
map.

00:35:34.376 --> 00:35:37.156 A:middle
The World map is a serializable

00:35:37.156 --> 00:35:37.476 A:middle
class.

00:35:37.566 --> 00:35:38.856 A:middle
So, then we can simply use

00:35:38.956 --> 00:35:40.576 A:middle
NSKeyedArchiver utility to

00:35:40.576 --> 00:35:42.106 A:middle
serialize it to a binary stream

00:35:42.106 --> 00:35:44.446 A:middle
of data, which then, you can

00:35:44.446 --> 00:35:46.276 A:middle
either save to disk in case of a

00:35:46.276 --> 00:35:48.086 A:middle
single user persistent

00:35:48.166 --> 00:35:48.946 A:middle
application.

00:35:49.706 --> 00:35:52.126 A:middle
Or you can share it across

00:35:52.156 --> 00:35:52.636 A:middle
devices.

00:35:52.906 --> 00:35:54.626 A:middle
And for that, you can use the

00:35:54.626 --> 00:35:56.176 A:middle
MultiPeerConnectivity framework,

00:35:56.936 --> 00:35:58.196 A:middle
which has great feature like

00:35:58.196 --> 00:36:00.186 A:middle
automatic device, nearby device

00:35:58.196 --> 00:36:00.186 A:middle
automatic device, nearby device

00:36:00.186 --> 00:36:01.856 A:middle
discovery, and allows efficient

00:36:01.856 --> 00:36:04.356 A:middle
communication of data between

00:36:04.356 --> 00:36:04.876 A:middle
devices.

00:36:05.626 --> 00:36:06.996 A:middle
We also, have an example of how

00:36:06.996 --> 00:36:09.396 A:middle
to use that in ARKit called

00:36:09.396 --> 00:36:11.036 A:middle
Creating a Multiuser AR

00:36:11.036 --> 00:36:12.126 A:middle
Experience that you can check

00:36:12.126 --> 00:36:13.516 A:middle
out on our developer website.

00:36:14.076 --> 00:36:16.836 A:middle
On the receiving end of the

00:36:16.836 --> 00:36:18.086 A:middle
device, once you've got the

00:36:18.086 --> 00:36:20.096 A:middle
World map let's see how you can

00:36:20.096 --> 00:36:21.046 A:middle
set up the World Tracking

00:36:21.046 --> 00:36:22.126 A:middle
configuration to use it.

00:36:22.426 --> 00:36:22.986 A:middle
Very simple.

00:36:22.986 --> 00:36:24.916 A:middle
You just set the initial World

00:36:24.916 --> 00:36:27.366 A:middle
map property to that World map.

00:36:28.266 --> 00:36:29.986 A:middle
When you run the session, the

00:36:29.986 --> 00:36:31.426 A:middle
system will try to find that

00:36:31.526 --> 00:36:32.376 A:middle
previous World map.

00:36:33.636 --> 00:36:35.336 A:middle
But it may take some time, even

00:36:35.336 --> 00:36:36.366 A:middle
because the user may not be

00:36:36.366 --> 00:36:37.546 A:middle
pointing at the same scene as

00:36:37.546 --> 00:36:37.906 A:middle
before.

00:36:38.756 --> 00:36:39.656 A:middle
So, how do we know when

00:36:39.656 --> 00:36:40.606 A:middle
localization happen?

00:36:41.686 --> 00:36:43.776 A:middle
That information is available in

00:36:43.776 --> 00:36:44.666 A:middle
the tracking state.

00:36:44.666 --> 00:36:45.896 A:middle
So, as soon as you start the

00:36:45.896 --> 00:36:47.596 A:middle
session with the initial World

00:36:47.596 --> 00:36:49.256 A:middle
map, the tracking state will be

00:36:49.256 --> 00:36:50.326 A:middle
limited with reason

00:36:50.536 --> 00:36:51.366 A:middle
Relocalizing.

00:36:51.906 --> 00:36:54.226 A:middle
Note that you will still get the

00:36:54.226 --> 00:36:55.816 A:middle
tracking data available here,

00:36:56.176 --> 00:36:58.726 A:middle
but the world origin will be the

00:36:58.726 --> 00:37:00.376 A:middle
first camera, just like a new

00:36:58.726 --> 00:37:00.376 A:middle
first camera, just like a new

00:37:00.376 --> 00:37:00.826 A:middle
session.

00:37:01.336 --> 00:37:04.156 A:middle
As soon as the user points the

00:37:04.156 --> 00:37:05.606 A:middle
device to the same scene, the

00:37:05.606 --> 00:37:06.616 A:middle
system will localize.

00:37:07.076 --> 00:37:08.136 A:middle
The tracking state will go to

00:37:08.136 --> 00:37:09.866 A:middle
normal and the world origin will

00:37:09.866 --> 00:37:11.576 A:middle
be the same as the recorded

00:37:11.616 --> 00:37:12.036 A:middle
World map.

00:37:13.046 --> 00:37:15.496 A:middle
At this point, all your previous

00:37:15.496 --> 00:37:16.856 A:middle
anchors are also available in

00:37:16.856 --> 00:37:17.916 A:middle
your session, so you can put

00:37:17.976 --> 00:37:19.226 A:middle
back the virtual content.

00:37:21.816 --> 00:37:23.506 A:middle
Note here that because of what's

00:37:23.506 --> 00:37:24.986 A:middle
happening behind the hood,

00:37:25.206 --> 00:37:26.806 A:middle
behind the scenes, is that we're

00:37:26.806 --> 00:37:28.206 A:middle
matching those feature points,

00:37:28.616 --> 00:37:29.996 A:middle
there needs to be enough visual

00:37:29.996 --> 00:37:32.306 A:middle
similarity between the scenes

00:37:32.306 --> 00:37:33.326 A:middle
where you acquired the World map

00:37:33.536 --> 00:37:34.666 A:middle
and the scene where you want to

00:37:34.666 --> 00:37:35.146 A:middle
relocalize.

00:37:36.116 --> 00:37:37.406 A:middle
So, if you go back to this table

00:37:37.406 --> 00:37:38.696 A:middle
at night, chances are it's not

00:37:38.696 --> 00:37:41.946 A:middle
going to work very well.

00:37:42.206 --> 00:37:44.116 A:middle
And that was how you can create

00:37:44.506 --> 00:37:46.516 A:middle
multiple user experiences or

00:37:46.516 --> 00:37:47.686 A:middle
persistent experiences using the

00:37:47.686 --> 00:37:49.526 A:middle
saving and loading map.

00:37:50.536 --> 00:37:54.846 A:middle
Next, image tracking.

00:37:54.846 --> 00:37:56.306 A:middle
So, augmented reality is all

00:37:56.306 --> 00:37:59.186 A:middle
about adding visual content on

00:37:59.186 --> 00:38:00.306 A:middle
top of the physical world.

00:37:59.186 --> 00:38:00.306 A:middle
top of the physical world.

00:38:00.536 --> 00:38:01.686 A:middle
And on the physical world,

00:38:01.686 --> 00:38:02.866 A:middle
images are found everywhere.

00:38:02.866 --> 00:38:05.226 A:middle
Think about art pieces, pieces of art hanging on the wall the

00:38:05.226 --> 00:38:07.086 A:middle
world, magazine covers,

00:38:07.426 --> 00:38:08.336 A:middle
advertisements.

00:38:08.816 --> 00:38:10.136 A:middle
Image tracking is a tool that

00:38:10.136 --> 00:38:11.506 A:middle
allows you to recognize those

00:38:11.506 --> 00:38:13.976 A:middle
physical images and build

00:38:14.216 --> 00:38:15.366 A:middle
augmented reality experiences

00:38:15.366 --> 00:38:15.876 A:middle
around them.

00:38:17.876 --> 00:38:18.746 A:middle
Let's see an example.

00:38:20.006 --> 00:38:21.896 A:middle
You can see here; two images

00:38:21.896 --> 00:38:23.356 A:middle
being tracked simultaneously.

00:38:24.426 --> 00:38:26.766 A:middle
On the left, a beautiful

00:38:27.206 --> 00:38:29.376 A:middle
elephant is put on top of the

00:38:29.376 --> 00:38:30.716 A:middle
physical image of the elephant.

00:38:31.606 --> 00:38:32.896 A:middle
On the right, the physical image

00:38:32.896 --> 00:38:35.116 A:middle
turned into a virtual screen.

00:38:36.186 --> 00:38:37.626 A:middle
Note also, that the images can

00:38:37.626 --> 00:38:39.016 A:middle
freely move around the

00:38:39.016 --> 00:38:40.966 A:middle
environment as tracking around

00:38:40.966 --> 00:38:42.146 A:middle
at 60 frames per second.

00:38:43.306 --> 00:38:45.146 A:middle
Let's talk about looking at

00:38:45.146 --> 00:38:46.866 A:middle
what's happening behind the

00:38:47.156 --> 00:38:47.476 A:middle
scenes.

00:38:47.476 --> 00:38:48.996 A:middle
So, let's say you have an image

00:38:48.996 --> 00:38:50.526 A:middle
like this one of the elephant

00:38:50.896 --> 00:38:52.186 A:middle
and you want to find it in a

00:38:52.236 --> 00:38:52.876 A:middle
scene like this.

00:38:54.266 --> 00:38:55.556 A:middle
We're using grayscale for this.

00:38:55.556 --> 00:38:56.956 A:middle
And the first type is pretty

00:38:56.956 --> 00:38:58.426 A:middle
similar to what we do in

00:38:58.426 --> 00:38:58.836 A:middle
tracking.

00:38:58.936 --> 00:38:59.956 A:middle
So, we'll track those

00:39:00.046 --> 00:39:01.526 A:middle
interesting points from both the

00:39:01.526 --> 00:39:03.256 A:middle
reference image and the current

00:39:03.316 --> 00:39:03.566 A:middle
scene.

00:39:04.676 --> 00:39:05.996 A:middle
And then, we try to go in the

00:39:05.996 --> 00:39:07.356 A:middle
current scene and match those

00:39:07.356 --> 00:39:08.616 A:middle
features to the one on the

00:39:08.616 --> 00:39:09.346 A:middle
reference image.

00:39:10.436 --> 00:39:11.696 A:middle
By applying some projected

00:39:11.696 --> 00:39:12.866 A:middle
geometry and linear algebra,

00:39:13.186 --> 00:39:14.366 A:middle
this is enough to give an

00:39:14.456 --> 00:39:16.126 A:middle
initial estimation of the

00:39:16.126 --> 00:39:17.286 A:middle
position orientation of the

00:39:17.286 --> 00:39:18.556 A:middle
image with respect to the

00:39:18.556 --> 00:39:19.156 A:middle
current scene.

00:39:20.516 --> 00:39:21.386 A:middle
But we don't stop here.

00:39:22.566 --> 00:39:23.486 A:middle
In order to give you a really

00:39:23.486 --> 00:39:25.836 A:middle
precise pose and track at 60

00:39:25.836 --> 00:39:27.846 A:middle
frames per second, we then do a

00:39:27.846 --> 00:39:29.276 A:middle
dense tracking stage.

00:39:29.986 --> 00:39:31.316 A:middle
So, with that initial estimate

00:39:31.976 --> 00:39:33.136 A:middle
we take the pixels from the

00:39:33.136 --> 00:39:36.486 A:middle
current scene and warp them back

00:39:36.566 --> 00:39:38.266 A:middle
to a rectangular shape like you

00:39:38.266 --> 00:39:40.616 A:middle
see on the right-- top right

00:39:40.696 --> 00:39:40.916 A:middle
there.

00:39:41.306 --> 00:39:42.776 A:middle
So, that's a reconstructed image

00:39:43.536 --> 00:39:45.296 A:middle
by warping the pixels of the

00:39:45.296 --> 00:39:46.576 A:middle
current image into the

00:39:46.576 --> 00:39:46.986 A:middle
rectangle.

00:39:47.916 --> 00:39:48.766 A:middle
We can then compare the

00:39:48.766 --> 00:39:50.456 A:middle
reconstructed image with a

00:39:50.506 --> 00:39:51.726 A:middle
reference image that we have

00:39:51.726 --> 00:39:54.056 A:middle
available to create an error

00:39:54.056 --> 00:39:55.106 A:middle
image like the one you see

00:39:55.106 --> 00:39:55.436 A:middle
below.

00:39:56.636 --> 00:39:58.296 A:middle
We then optimize the position

00:39:58.296 --> 00:39:59.836 A:middle
orientation of the image, such

00:39:59.906 --> 00:40:00.976 A:middle
that that error is minimized.

00:39:59.906 --> 00:40:00.976 A:middle
that that error is minimized.

00:40:03.366 --> 00:40:04.786 A:middle
So, what this means to you that

00:40:04.786 --> 00:40:06.796 A:middle
the pose would be really

00:40:06.796 --> 00:40:07.266 A:middle
accurate.

00:40:08.146 --> 00:40:09.706 A:middle
Thank you.

00:40:10.526 --> 00:40:12.076 A:middle
And will still track at 60

00:40:12.076 --> 00:40:12.826 A:middle
frames per second.

00:40:15.296 --> 00:40:16.906 A:middle
So, let's see how we can do all

00:40:16.906 --> 00:40:18.126 A:middle
of this in ARKit.

00:40:18.916 --> 00:40:21.856 A:middle
As usual, the ARKit API is

00:40:21.856 --> 00:40:22.506 A:middle
really simple.

00:40:22.566 --> 00:40:24.566 A:middle
We have three simple steps.

00:40:24.606 --> 00:40:26.686 A:middle
First, we want to collect all

00:40:26.686 --> 00:40:27.596 A:middle
the reference images.

00:40:28.486 --> 00:40:30.616 A:middle
Then, we set up the AR Session

00:40:30.616 --> 00:40:31.346 A:middle
Configuration.

00:40:31.606 --> 00:40:32.646 A:middle
There are two options here.

00:40:33.166 --> 00:40:34.306 A:middle
One is the World Tracking

00:40:34.306 --> 00:40:35.756 A:middle
configuration that gives, also,

00:40:35.756 --> 00:40:37.426 A:middle
the device position.

00:40:37.426 --> 00:40:38.316 A:middle
And this is the one we have

00:40:38.406 --> 00:40:39.296 A:middle
talked, so far.

00:40:39.856 --> 00:40:41.506 A:middle
And in iOS12, introduced a new

00:40:41.506 --> 00:40:42.576 A:middle
configuration, which is a

00:40:42.576 --> 00:40:43.766 A:middle
standalone image tracking

00:40:43.766 --> 00:40:44.476 A:middle
configuration.

00:40:44.936 --> 00:40:47.906 A:middle
Once you start the session you

00:40:47.906 --> 00:40:49.896 A:middle
will start receiving the results

00:40:49.896 --> 00:40:52.326 A:middle
in the form of an ARImageAnchor.

00:40:53.296 --> 00:40:54.306 A:middle
We're now going into more

00:40:54.306 --> 00:40:55.626 A:middle
details of these three steps,

00:40:56.036 --> 00:40:57.456 A:middle
starting from the reference

00:40:57.456 --> 00:40:57.886 A:middle
images.

00:40:58.426 --> 00:41:01.286 A:middle
The easiest way to add reference

00:40:58.426 --> 00:41:01.286 A:middle
The easiest way to add reference

00:41:01.286 --> 00:41:02.836 A:middle
images to your application is

00:41:02.896 --> 00:41:04.746 A:middle
through the, Xcode asset

00:41:04.746 --> 00:41:05.106 A:middle
catalog.

00:41:06.146 --> 00:41:08.066 A:middle
You simply create an AR Resource

00:41:08.066 --> 00:41:09.646 A:middle
Groups and drag and drop your

00:41:09.646 --> 00:41:10.386 A:middle
images in there.

00:41:11.566 --> 00:41:13.026 A:middle
Next, you have to set the

00:41:13.026 --> 00:41:14.446 A:middle
physical dimension of the image,

00:41:14.556 --> 00:41:16.196 A:middle
which you can do on the property

00:41:16.196 --> 00:41:17.436 A:middle
window on the top right.

00:41:17.946 --> 00:41:20.656 A:middle
Setting the physical dimension

00:41:21.096 --> 00:41:22.336 A:middle
is a requirement and there's a

00:41:22.336 --> 00:41:23.126 A:middle
few reason for that.

00:41:24.466 --> 00:41:26.126 A:middle
First, it allows the pose of the

00:41:26.126 --> 00:41:27.816 A:middle
image to be in physical scale.

00:41:28.386 --> 00:41:29.776 A:middle
Which means, also, your content

00:41:29.816 --> 00:41:31.076 A:middle
will be in physical scale.

00:41:31.186 --> 00:41:32.666 A:middle
In ARKit, everything is in

00:41:32.666 --> 00:41:33.986 A:middle
meters, so also, your visual

00:41:33.986 --> 00:41:36.226 A:middle
content will be in meters.

00:41:37.056 --> 00:41:38.626 A:middle
In addition, it's especially

00:41:38.626 --> 00:41:40.266 A:middle
important to set the correct

00:41:40.266 --> 00:41:41.586 A:middle
physical dimension of the image

00:41:41.946 --> 00:41:43.166 A:middle
in case we combine the image

00:41:43.216 --> 00:41:44.046 A:middle
tracking with the World

00:41:44.046 --> 00:41:44.476 A:middle
Tracking.

00:41:44.936 --> 00:41:46.206 A:middle
As this will give immediately

00:41:46.526 --> 00:41:48.616 A:middle
consistent pose between the

00:41:48.616 --> 00:41:51.196 A:middle
image and the world.

00:41:51.196 --> 00:41:52.826 A:middle
Let's see some example of this

00:41:53.256 --> 00:41:54.166 A:middle
reference images.

00:41:54.816 --> 00:41:57.366 A:middle
You can see here, two beautiful

00:41:57.366 --> 00:41:57.806 A:middle
images.

00:41:58.476 --> 00:41:59.776 A:middle
These images will work really

00:41:59.776 --> 00:42:01.066 A:middle
great with image tracking.

00:41:59.776 --> 00:42:01.066 A:middle
great with image tracking.

00:42:01.156 --> 00:42:03.426 A:middle
They have high texture, high

00:42:03.426 --> 00:42:04.956 A:middle
level of contrast, well

00:42:04.956 --> 00:42:06.376 A:middle
distributed histograms, as well

00:42:06.376 --> 00:42:07.356 A:middle
as they do not contain

00:42:07.356 --> 00:42:08.416 A:middle
repetitive structures.

00:42:08.536 --> 00:42:10.486 A:middle
There are, also, other kinds of

00:42:10.486 --> 00:42:12.316 A:middle
images that will work less good

00:42:12.316 --> 00:42:12.936 A:middle
with the system.

00:42:13.436 --> 00:42:14.726 A:middle
You can see an example of this

00:42:15.456 --> 00:42:16.176 A:middle
on the right.

00:42:17.116 --> 00:42:19.606 A:middle
And if we take a look at these

00:42:19.696 --> 00:42:21.596 A:middle
top two examples, you can see

00:42:21.596 --> 00:42:23.466 A:middle
that the good image we have a

00:42:23.466 --> 00:42:25.036 A:middle
lot of those interesting points.

00:42:25.536 --> 00:42:26.306 A:middle
And you can see that the

00:42:26.306 --> 00:42:27.716 A:middle
histogram is well distributed

00:42:27.716 --> 00:42:28.686 A:middle
across the whole range.

00:42:29.316 --> 00:42:30.126 A:middle
While on the Snow image,

00:42:30.126 --> 00:42:33.096 A:middle
there's only a few of those

00:42:33.096 --> 00:42:34.286 A:middle
interesting points and the

00:42:34.286 --> 00:42:36.116 A:middle
histogram is all skewed toward

00:42:36.116 --> 00:42:36.536 A:middle
the whites.

00:42:37.236 --> 00:42:40.536 A:middle
You can get an estimation of how

00:42:40.536 --> 00:42:42.066 A:middle
good an image will be directly

00:42:42.066 --> 00:42:42.916 A:middle
in Xcode.

00:42:44.076 --> 00:42:46.066 A:middle
As soon as you drag an image in

00:42:46.066 --> 00:42:48.396 A:middle
there, the image is analyzed and

00:42:48.576 --> 00:42:50.316 A:middle
problems are reported to you in the form

00:42:50.316 --> 00:42:52.206 A:middle
of warnings to give you early

00:42:52.276 --> 00:42:53.826 A:middle
feedback, even before you run

00:42:53.826 --> 00:42:54.516 A:middle
your application.

00:42:55.646 --> 00:42:56.666 A:middle
For example, if you click on

00:42:56.666 --> 00:42:59.086 A:middle
this bottom image that could be

00:42:59.086 --> 00:43:01.836 A:middle
a magazine page, for example, we

00:42:59.086 --> 00:43:01.836 A:middle
a magazine page, for example, we

00:43:01.836 --> 00:43:04.396 A:middle
can see that the Xcode says that

00:43:04.556 --> 00:43:05.776 A:middle
the histogram is not well

00:43:05.776 --> 00:43:06.436 A:middle
distributed.

00:43:06.676 --> 00:43:07.626 A:middle
In fact, you can see there's a

00:43:07.626 --> 00:43:09.326 A:middle
lot of whites in the image.

00:43:09.476 --> 00:43:10.626 A:middle
And it would also say that this

00:43:10.626 --> 00:43:11.716 A:middle
image contains repetitive

00:43:11.716 --> 00:43:14.486 A:middle
structures, mainly caused by the

00:43:15.476 --> 00:43:15.636 A:middle
text.

00:43:15.776 --> 00:43:18.076 A:middle
Another example, if you have two

00:43:18.076 --> 00:43:20.126 A:middle
images which are too similar and

00:43:20.126 --> 00:43:22.176 A:middle
are at risk of being confused at

00:43:22.506 --> 00:43:24.366 A:middle
detection time, also, Xcode

00:43:24.366 --> 00:43:25.166 A:middle
warns you about that.

00:43:25.906 --> 00:43:26.986 A:middle
You can see an example of these

00:43:26.986 --> 00:43:28.906 A:middle
two images of the same mountain

00:43:28.906 --> 00:43:29.756 A:middle
range, the Sierra.

00:43:30.156 --> 00:43:32.446 A:middle
There's a few things that we can

00:43:32.446 --> 00:43:33.506 A:middle
do to deal with this warning.

00:43:33.936 --> 00:43:34.956 A:middle
For example, let's go back to

00:43:34.956 --> 00:43:38.236 A:middle
this image that had repetitive

00:43:38.236 --> 00:43:40.746 A:middle
structures and not well

00:43:40.746 --> 00:43:41.746 A:middle
distributed histograms.

00:43:42.516 --> 00:43:44.186 A:middle
You can try to identify a region

00:43:44.186 --> 00:43:45.126 A:middle
of this image which is

00:43:45.126 --> 00:43:46.596 A:middle
distinctive enough, like in this

00:43:46.596 --> 00:43:48.736 A:middle
case, for example, the actual

00:43:48.736 --> 00:43:49.536 A:middle
image of the page.

00:43:50.106 --> 00:43:51.276 A:middle
And then, you can crop that out

00:43:51.276 --> 00:43:52.756 A:middle
and use this as the reference

00:43:52.756 --> 00:43:53.556 A:middle
image, instead.

00:43:53.876 --> 00:43:55.126 A:middle
Which will give you, of course,

00:43:55.476 --> 00:43:56.756 A:middle
all the warnings are going to be

00:43:56.756 --> 00:43:58.296 A:middle
removed and will give you better

00:43:58.946 --> 00:43:59.716 A:middle
tracking quality.

00:44:00.026 --> 00:44:03.106 A:middle
Another thing that we can do is

00:44:03.436 --> 00:44:06.376 A:middle
use multiple AR Resource Groups.

00:44:07.496 --> 00:44:09.176 A:middle
This allow many more images to

00:44:09.396 --> 00:44:10.146 A:middle
be detected.

00:44:10.416 --> 00:44:11.726 A:middle
As we recommend to have a

00:44:11.726 --> 00:44:13.816 A:middle
maximum of 25 images per group

00:44:14.236 --> 00:44:15.296 A:middle
to keep your experience

00:44:15.296 --> 00:44:16.736 A:middle
efficient and responsive.

00:44:17.936 --> 00:44:19.016 A:middle
But you can have as many groups

00:44:19.246 --> 00:44:19.826 A:middle
as you want.

00:44:20.036 --> 00:44:21.896 A:middle
And then, you can switch between

00:44:21.896 --> 00:44:23.156 A:middle
groups programmatically.

00:44:23.376 --> 00:44:26.076 A:middle
For example, if you are want to

00:44:26.076 --> 00:44:27.076 A:middle
create an augmented reality

00:44:27.076 --> 00:44:28.846 A:middle
experience in a museum that may

00:44:28.846 --> 00:44:30.646 A:middle
have hundreds of images.

00:44:31.816 --> 00:44:33.396 A:middle
Usually though, those images are

00:44:33.396 --> 00:44:34.586 A:middle
actually physically located in

00:44:34.586 --> 00:44:35.336 A:middle
different rooms.

00:44:35.646 --> 00:44:37.646 A:middle
So, what you can do is put the

00:44:38.246 --> 00:44:39.756 A:middle
images that physically will be

00:44:39.756 --> 00:44:41.146 A:middle
present in the room into a

00:44:41.146 --> 00:44:41.476 A:middle
group.

00:44:41.686 --> 00:44:43.036 A:middle
And images of another room into

00:44:43.036 --> 00:44:43.596 A:middle
another group.

00:44:44.246 --> 00:44:45.806 A:middle
And then use, for example, core

00:44:45.806 --> 00:44:47.586 A:middle
location to switch between

00:44:48.306 --> 00:44:48.686 A:middle
rooms.

00:44:49.186 --> 00:44:52.116 A:middle
Note also, that you can have

00:44:52.446 --> 00:44:53.906 A:middle
similar images, now, as long as

00:44:53.946 --> 00:44:55.126 A:middle
they are in different groups.

00:44:55.896 --> 00:44:57.716 A:middle
So, that was all about reference

00:44:57.746 --> 00:44:58.266 A:middle
images.

00:44:58.266 --> 00:45:00.606 A:middle
Let's now, see our two

00:44:58.266 --> 00:45:00.606 A:middle
Let's now, see our two

00:45:01.166 --> 00:45:01.916 A:middle
configurations.

00:45:03.006 --> 00:45:05.286 A:middle
The ARImageTrackingConfiguration

00:45:05.416 --> 00:45:06.876 A:middle
is a new standalone image

00:45:06.876 --> 00:45:07.836 A:middle
tracking configuration, which

00:45:07.866 --> 00:45:09.256 A:middle
means it doesn't run the World

00:45:09.256 --> 00:45:09.646 A:middle
Tracking.

00:45:10.636 --> 00:45:11.846 A:middle
Which also, means there is no

00:45:12.136 --> 00:45:12.916 A:middle
world origin.

00:45:13.216 --> 00:45:14.676 A:middle
So, every image will be given to

00:45:14.676 --> 00:45:15.976 A:middle
you with respect to the current

00:45:15.976 --> 00:45:16.526 A:middle
camera view.

00:45:18.246 --> 00:45:19.346 A:middle
You can also combine image

00:45:19.406 --> 00:45:20.906 A:middle
tracking with a World Tracking

00:45:20.906 --> 00:45:21.646 A:middle
configuration.

00:45:22.466 --> 00:45:24.536 A:middle
And in this case, you will have

00:45:24.536 --> 00:45:25.636 A:middle
all the scene understanding

00:45:25.636 --> 00:45:27.506 A:middle
capability available like Plane

00:45:27.506 --> 00:45:29.356 A:middle
Detection, light estimation,

00:45:29.356 --> 00:45:30.026 A:middle
everything else.

00:45:31.066 --> 00:45:32.796 A:middle
So, what is more appropriate to

00:45:32.796 --> 00:45:34.376 A:middle
use which configurations?

00:45:35.066 --> 00:45:35.536 A:middle
Let's see.

00:45:35.626 --> 00:45:37.106 A:middle
So, in the

00:45:37.516 --> 00:45:39.916 A:middle
ARImageTrackingConfigurations is

00:45:39.916 --> 00:45:41.226 A:middle
really tailored for use cases

00:45:41.226 --> 00:45:42.906 A:middle
which are built around images.

00:45:43.196 --> 00:45:44.286 A:middle
We can see an example on the

00:45:44.286 --> 00:45:44.826 A:middle
left here.

00:45:46.606 --> 00:45:48.516 A:middle
We can have an image that could

00:45:48.516 --> 00:45:49.666 A:middle
be a page of a textbook.

00:45:50.506 --> 00:45:51.576 A:middle
And to make the experience more

00:45:51.576 --> 00:45:53.976 A:middle
engaging, we are overlaying

00:45:54.286 --> 00:45:54.746 A:middle
dynamic graph.

00:45:54.746 --> 00:45:55.846 A:middle
In this case, on how to build an

00:45:55.846 --> 00:45:56.796 A:middle
equilateral triangle.

00:45:57.866 --> 00:45:58.746 A:middle
So, you can see that this

00:45:58.746 --> 00:46:00.256 A:middle
experience is really tailored

00:45:58.746 --> 00:46:00.256 A:middle
experience is really tailored

00:46:00.256 --> 00:46:00.966 A:middle
around an image.

00:46:01.116 --> 00:46:03.046 A:middle
If you have, let's see this

00:46:03.046 --> 00:46:03.826 A:middle
other example.

00:46:04.546 --> 00:46:05.516 A:middle
Image tracking is used to

00:46:05.606 --> 00:46:07.266 A:middle
trigger some content that then

00:46:07.266 --> 00:46:09.326 A:middle
goes beyond the extent of the

00:46:09.326 --> 00:46:09.496 A:middle
image.

00:46:09.496 --> 00:46:12.086 A:middle
In this case, you want to use

00:46:12.086 --> 00:46:13.726 A:middle
the ARWorldTrackingConfiguration

00:46:13.806 --> 00:46:14.906 A:middle
as you will need the device

00:46:14.906 --> 00:46:16.506 A:middle
position to keep track of that

00:46:16.506 --> 00:46:19.856 A:middle
content outside the image.

00:46:20.076 --> 00:46:21.146 A:middle
Also, note that the image

00:46:21.246 --> 00:46:23.576 A:middle
tracking doesn't use the motion

00:46:23.576 --> 00:46:24.676 A:middle
data, which means it can also be

00:46:24.676 --> 00:46:26.506 A:middle
used on a bus or an elevator,

00:46:27.076 --> 00:46:28.146 A:middle
where the motion data don't

00:46:28.146 --> 00:46:29.896 A:middle
agree with the visual data.

00:46:30.916 --> 00:46:33.036 A:middle
So, let's see now, how we can do

00:46:33.036 --> 00:46:33.756 A:middle
this in code.

00:46:35.236 --> 00:46:36.916 A:middle
You can easily recognize those

00:46:36.946 --> 00:46:37.716 A:middle
three steps here.

00:46:37.936 --> 00:46:39.836 A:middle
The first one is to gather all

00:46:39.836 --> 00:46:40.476 A:middle
the images.

00:46:40.876 --> 00:46:41.796 A:middle
And there's a convenience

00:46:41.796 --> 00:46:43.396 A:middle
function for that in the

00:46:43.566 --> 00:46:45.436 A:middle
ARReferenceImage class that

00:46:45.436 --> 00:46:47.436 A:middle
gathers all the images that are

00:46:47.436 --> 00:46:48.366 A:middle
in a particular group.

00:46:48.556 --> 00:46:50.226 A:middle
In this case, it's named Room1.

00:46:51.896 --> 00:46:53.256 A:middle
We can then simply set the

00:46:53.256 --> 00:46:55.626 A:middle
trackingImages property to those

00:46:55.626 --> 00:46:56.306 A:middle
images in the

00:46:56.306 --> 00:46:58.156 A:middle
ARImageTrackingConfigurations.

00:46:58.586 --> 00:46:59.516 A:middle
And run the session.

00:47:00.866 --> 00:47:02.766 A:middle
You will then start receiving

00:47:02.766 --> 00:47:04.306 A:middle
the results, for example, to the

00:47:04.306 --> 00:47:06.346 A:middle
session:didUpdate anchors

00:47:06.466 --> 00:47:08.266 A:middle
delegate method, where you can

00:47:08.266 --> 00:47:10.096 A:middle
check if the anchors is of type

00:47:10.146 --> 00:47:11.156 A:middle
ARImageAnchor.

00:47:12.606 --> 00:47:14.186 A:middle
In the anchor, you will find, of

00:47:14.246 --> 00:47:15.386 A:middle
course, the position and

00:47:15.386 --> 00:47:17.336 A:middle
orientation of the image, as

00:47:17.336 --> 00:47:18.216 A:middle
well as the reference image

00:47:18.216 --> 00:47:18.656 A:middle
itself.

00:47:18.716 --> 00:47:19.856 A:middle
Where you can find, for example,

00:47:19.856 --> 00:47:21.836 A:middle
the name of the image as you

00:47:21.836 --> 00:47:23.156 A:middle
named it in the asset catalog so

00:47:23.156 --> 00:47:24.126 A:middle
that you know which image has

00:47:24.126 --> 00:47:24.746 A:middle
been detected.

00:47:25.986 --> 00:47:26.966 A:middle
There's also another Boolean

00:47:26.966 --> 00:47:28.936 A:middle
property, which tells you if

00:47:28.936 --> 00:47:30.086 A:middle
this image is currently being

00:47:30.086 --> 00:47:33.666 A:middle
tracked in the frame.

00:47:33.866 --> 00:47:35.306 A:middle
Note here that other than these

00:47:35.306 --> 00:47:36.496 A:middle
use cases that we have seen so

00:47:36.496 --> 00:47:38.576 A:middle
far when you build experiences

00:47:38.576 --> 00:47:40.546 A:middle
around images, image detection

00:47:40.546 --> 00:47:43.196 A:middle
and tracking allows a few more

00:47:43.976 --> 00:47:44.126 A:middle
things.

00:47:45.226 --> 00:47:47.496 A:middle
For example, if two devices are

00:47:47.496 --> 00:47:48.866 A:middle
looking at the same physical

00:47:48.866 --> 00:47:51.446 A:middle
image, you can detect this image

00:47:51.446 --> 00:47:52.416 A:middle
from both devices.

00:47:52.846 --> 00:47:54.586 A:middle
And this will give you a shared

00:47:54.586 --> 00:47:56.066 A:middle
coordinate system that you can

00:47:56.066 --> 00:47:57.626 A:middle
then use as an alternative way

00:47:58.106 --> 00:47:59.386 A:middle
to have a shared experience.

00:48:01.476 --> 00:48:03.846 A:middle
Another example, if you happen

00:48:03.846 --> 00:48:05.226 A:middle
to know where an image is

00:48:05.346 --> 00:48:06.916 A:middle
physically located in the world,

00:48:08.296 --> 00:48:09.826 A:middle
like for example, you know that

00:48:09.826 --> 00:48:11.446 A:middle
the map of this park is in the

00:48:11.446 --> 00:48:12.076 A:middle
physical world.

00:48:12.456 --> 00:48:14.446 A:middle
You can use image tracking to

00:48:14.446 --> 00:48:15.956 A:middle
get the position of the device

00:48:15.956 --> 00:48:17.576 A:middle
with respect to the image and,

00:48:17.576 --> 00:48:19.636 A:middle
therefore, also the position of

00:48:19.636 --> 00:48:20.656 A:middle
the device with respect to the

00:48:20.656 --> 00:48:21.946 A:middle
world, which, you can then use,

00:48:21.946 --> 00:48:23.956 A:middle
for example, to overlay

00:48:24.196 --> 00:48:26.396 A:middle
directions really attached to

00:48:26.396 --> 00:48:27.086 A:middle
the physical world.

00:48:27.646 --> 00:48:31.376 A:middle
So, that concludes the image

00:48:31.436 --> 00:48:31.836 A:middle
tracking.

00:48:31.836 --> 00:48:34.006 A:middle
Let's now go and look at the

00:48:34.066 --> 00:48:35.266 A:middle
Object Detection.

00:48:38.016 --> 00:48:39.546 A:middle
So, with image tracking we have

00:48:39.546 --> 00:48:41.716 A:middle
seen how we can detect images,

00:48:41.716 --> 00:48:43.676 A:middle
which are planar objects in the

00:48:43.676 --> 00:48:44.406 A:middle
physical world.

00:48:45.376 --> 00:48:46.786 A:middle
Object detection extends this

00:48:46.786 --> 00:48:48.256 A:middle
concept to the third dimension

00:48:48.346 --> 00:48:49.806 A:middle
allowing the detection of more

00:48:49.806 --> 00:48:50.586 A:middle
generic objects.

00:48:51.046 --> 00:48:53.626 A:middle
Note, though, that this object

00:48:53.816 --> 00:48:55.826 A:middle
will be assumed to be static in

00:48:55.826 --> 00:48:57.086 A:middle
the scene, unlike images that

00:48:57.086 --> 00:48:57.676 A:middle
can move around.

00:48:58.866 --> 00:49:00.046 A:middle
We can see an example here.

00:48:58.866 --> 00:49:00.046 A:middle
We can see an example here.

00:49:00.046 --> 00:49:02.566 A:middle
That's the Nefertiti bust.

00:49:02.716 --> 00:49:04.266 A:middle
It's a statue that could be

00:49:04.266 --> 00:49:05.326 A:middle
present in a museum.

00:49:05.636 --> 00:49:07.166 A:middle
And now, you can detect it with

00:49:07.166 --> 00:49:07.576 A:middle
ARKit.

00:49:08.216 --> 00:49:10.376 A:middle
And then, for example, display

00:49:10.376 --> 00:49:12.296 A:middle
some information on top of the

00:49:12.296 --> 00:49:14.476 A:middle
physical object.

00:49:15.466 --> 00:49:17.536 A:middle
Note also that in the object

00:49:17.736 --> 00:49:18.936 A:middle
detection in ARKit, we are

00:49:18.936 --> 00:49:20.516 A:middle
talking about specific instances

00:49:20.516 --> 00:49:21.116 A:middle
of an object.

00:49:21.646 --> 00:49:22.506 A:middle
So, we're not talking about

00:49:22.506 --> 00:49:23.986 A:middle
detecting statues in general,

00:49:24.346 --> 00:49:26.406 A:middle
but this particular instance of

00:49:26.406 --> 00:49:27.456 A:middle
the Nefertiti statue.

00:49:28.836 --> 00:49:29.986 A:middle
So, how do we represent these

00:49:29.986 --> 00:49:31.006 A:middle
objects in ARKit?

00:49:31.626 --> 00:49:33.366 A:middle
You first need to scan the

00:49:33.366 --> 00:49:33.736 A:middle
object.

00:49:33.806 --> 00:49:35.086 A:middle
So, really, there's two steps to

00:49:35.086 --> 00:49:35.196 A:middle
it.

00:49:35.316 --> 00:49:36.936 A:middle
First, you scan the object and

00:49:36.936 --> 00:49:38.336 A:middle
then you can detect it.

00:49:39.096 --> 00:49:40.546 A:middle
Let's talk about the scanning

00:49:40.546 --> 00:49:42.446 A:middle
part, which mostly is going to

00:49:42.446 --> 00:49:44.166 A:middle
be on your side as a developer,

00:49:44.886 --> 00:49:46.016 A:middle
to basically, create that

00:49:46.016 --> 00:49:47.296 A:middle
representation of the object

00:49:47.406 --> 00:49:49.176 A:middle
that can be used for detection.

00:49:51.276 --> 00:49:53.716 A:middle
Internally, an object is

00:49:53.716 --> 00:49:55.616 A:middle
represented in a similar way as

00:49:55.616 --> 00:49:56.196 A:middle
the world map.

00:49:56.776 --> 00:49:58.576 A:middle
You can see an example of the 3D

00:49:58.916 --> 00:50:00.456 A:middle
feature points of the Nefertiti

00:49:58.916 --> 00:50:00.456 A:middle
feature points of the Nefertiti

00:50:00.456 --> 00:50:01.886 A:middle
statue there on the left.

00:50:03.056 --> 00:50:04.876 A:middle
And to scan the object, you can

00:50:05.006 --> 00:50:06.516 A:middle
use the Scanning and Detecting

00:50:06.516 --> 00:50:08.496 A:middle
3D Objects developer sample

00:50:08.496 --> 00:50:10.416 A:middle
that's available on the website.

00:50:11.706 --> 00:50:13.136 A:middle
And note here, that the

00:50:13.136 --> 00:50:14.766 A:middle
detection quality that you will

00:50:14.766 --> 00:50:17.306 A:middle
get at runtime, later, is highly

00:50:17.306 --> 00:50:18.616 A:middle
affected by the quality of the

00:50:18.616 --> 00:50:18.986 A:middle
scan.

00:50:19.776 --> 00:50:21.826 A:middle
So, let's spend a few moments to

00:50:21.826 --> 00:50:23.266 A:middle
see how we can get the best

00:50:23.336 --> 00:50:27.386 A:middle
quality during the scanning.

00:50:27.546 --> 00:50:29.066 A:middle
Once you build and run this

00:50:29.066 --> 00:50:30.666 A:middle
developer sample you will see

00:50:30.786 --> 00:50:31.916 A:middle
something like this on your

00:50:31.916 --> 00:50:32.406 A:middle
device.

00:50:33.286 --> 00:50:35.976 A:middle
The first step is to find the

00:50:35.976 --> 00:50:37.566 A:middle
region of space around your

00:50:37.566 --> 00:50:37.926 A:middle
object.

00:50:39.036 --> 00:50:40.066 A:middle
The application will try to

00:50:40.066 --> 00:50:41.556 A:middle
automatically estimate this

00:50:41.586 --> 00:50:42.946 A:middle
bounding box, exploiting

00:50:42.946 --> 00:50:43.946 A:middle
different feature points.

00:50:44.896 --> 00:50:46.206 A:middle
But you can always adjust this

00:50:46.256 --> 00:50:48.856 A:middle
box by dragging on a side to

00:50:49.066 --> 00:50:50.726 A:middle
shrink it or make it larger.

00:50:52.876 --> 00:50:54.506 A:middle
Note here, that what is really

00:50:54.506 --> 00:50:55.766 A:middle
important that when you go

00:50:55.766 --> 00:50:57.266 A:middle
around the object you make sure

00:50:57.266 --> 00:50:58.796 A:middle
that you don't cut any of the

00:50:58.796 --> 00:51:00.806 A:middle
interesting points of the

00:50:58.796 --> 00:51:00.806 A:middle
interesting points of the

00:51:00.806 --> 00:51:01.166 A:middle
object.

00:51:01.856 --> 00:51:03.606 A:middle
You can also, rotate the box

00:51:03.606 --> 00:51:04.956 A:middle
with a two-finger gesture from

00:51:04.996 --> 00:51:05.146 A:middle
top.

00:51:05.856 --> 00:51:08.406 A:middle
So, make sure that this box is

00:51:08.606 --> 00:51:09.666 A:middle
around the object and not

00:51:09.696 --> 00:51:11.286 A:middle
cutting any interesting part of

00:51:11.916 --> 00:51:11.986 A:middle
it.

00:51:13.016 --> 00:51:14.746 A:middle
The next part is the actual

00:51:14.746 --> 00:51:15.246 A:middle
scanning.

00:51:16.376 --> 00:51:19.356 A:middle
In this phase what we want to do

00:51:19.356 --> 00:51:21.426 A:middle
is really go around the objects

00:51:21.736 --> 00:51:23.486 A:middle
from all the points of view that

00:51:23.486 --> 00:51:24.846 A:middle
you think your users will want

00:51:24.846 --> 00:51:25.806 A:middle
to detect it later.

00:51:27.056 --> 00:51:28.386 A:middle
In order to make it easy for you

00:51:28.386 --> 00:51:30.096 A:middle
to understand which part of the

00:51:30.096 --> 00:51:31.656 A:middle
objects have been, already,

00:51:31.656 --> 00:51:33.376 A:middle
acquired like this beautiful

00:51:33.376 --> 00:51:34.356 A:middle
tile representation.

00:51:34.726 --> 00:51:36.046 A:middle
And you also can see a

00:51:36.456 --> 00:51:37.676 A:middle
percentage on top which tells

00:51:37.676 --> 00:51:38.786 A:middle
you how many tiles have already

00:51:38.786 --> 00:51:39.366 A:middle
been acquired.

00:51:40.406 --> 00:51:41.786 A:middle
And it's really important in

00:51:41.786 --> 00:51:43.716 A:middle
this phase that you spend time

00:51:43.716 --> 00:51:45.126 A:middle
on the regions of the object

00:51:45.126 --> 00:51:46.546 A:middle
which have a lot of features

00:51:46.726 --> 00:51:47.726 A:middle
that are distinctive enough.

00:51:47.726 --> 00:51:49.446 A:middle
And you go close enough to

00:51:49.446 --> 00:51:50.346 A:middle
capture all the details.

00:51:50.686 --> 00:51:52.046 A:middle
And again, that you really go

00:51:52.046 --> 00:51:56.246 A:middle
around from all the sides.

00:51:56.436 --> 00:51:59.456 A:middle
Like you see here.

00:51:59.686 --> 00:52:01.186 A:middle
Once you're happy with the

00:51:59.686 --> 00:52:01.186 A:middle
Once you're happy with the

00:52:01.186 --> 00:52:02.996 A:middle
coverage of your objects, you

00:52:02.996 --> 00:52:04.476 A:middle
can go to the next step, which

00:52:04.476 --> 00:52:06.656 A:middle
is allows you to adjust the

00:52:06.656 --> 00:52:09.166 A:middle
origin by simply dragging on the

00:52:09.166 --> 00:52:10.026 A:middle
coordinate system.

00:52:10.536 --> 00:52:12.496 A:middle
And this will be the coordinate

00:52:12.496 --> 00:52:13.886 A:middle
system that will be later given

00:52:13.886 --> 00:52:16.206 A:middle
to you at detection time in the

00:52:16.206 --> 00:52:16.506 A:middle
anchor.

00:52:16.506 --> 00:52:17.766 A:middle
So, make sure that you put it in

00:52:17.766 --> 00:52:19.316 A:middle
a place which makes sense for

00:52:19.316 --> 00:52:20.606 A:middle
your virtual content.

00:52:20.676 --> 00:52:25.526 A:middle
So, at this point, you have a

00:52:25.676 --> 00:52:27.286 A:middle
full representation of your

00:52:27.286 --> 00:52:30.176 A:middle
object, which you can use for

00:52:30.176 --> 00:52:30.686 A:middle
detection.

00:52:30.686 --> 00:52:33.106 A:middle
And the application will now

00:52:33.106 --> 00:52:34.976 A:middle
switch to a detection mode.

00:52:36.106 --> 00:52:37.386 A:middle
We encourage you to use this

00:52:37.386 --> 00:52:39.966 A:middle
mode to get early feedback about

00:52:39.966 --> 00:52:41.046 A:middle
the detection quality.

00:52:41.866 --> 00:52:44.776 A:middle
So, you may want to go around

00:52:44.776 --> 00:52:46.126 A:middle
the object from different points

00:52:46.126 --> 00:52:47.796 A:middle
of view and verify that the

00:52:47.796 --> 00:52:49.776 A:middle
object is detected from all

00:52:50.336 --> 00:52:51.696 A:middle
these different point of view.

00:52:51.696 --> 00:52:53.816 A:middle
You can point your device away,

00:52:53.816 --> 00:52:55.086 A:middle
come back from another angle,

00:52:55.836 --> 00:52:58.386 A:middle
and make sure that the scan was

00:52:58.386 --> 00:53:00.006 A:middle
good to detect the object.

00:52:58.386 --> 00:53:00.006 A:middle
good to detect the object.

00:53:00.526 --> 00:53:03.106 A:middle
You can also, move these objects

00:53:03.276 --> 00:53:04.506 A:middle
around so that the light

00:53:04.506 --> 00:53:05.866 A:middle
condition will be different.

00:53:06.836 --> 00:53:08.216 A:middle
And you want to make sure that

00:53:08.216 --> 00:53:09.196 A:middle
those are still detected.

00:53:09.196 --> 00:53:10.316 A:middle
This is particularly important

00:53:10.366 --> 00:53:12.536 A:middle
for objects like toys that you

00:53:12.536 --> 00:53:13.646 A:middle
don't know where they're

00:53:13.646 --> 00:53:14.986 A:middle
actually going to be physically

00:53:15.106 --> 00:53:15.546 A:middle
located.

00:53:17.096 --> 00:53:18.776 A:middle
We, also, suggest that you take

00:53:18.776 --> 00:53:20.176 A:middle
the object and put it in a

00:53:20.176 --> 00:53:21.556 A:middle
completely different environment

00:53:22.016 --> 00:53:24.036 A:middle
and still make sure that it is

00:53:24.076 --> 00:53:24.576 A:middle
detected.

00:53:25.666 --> 00:53:27.756 A:middle
In case this is not detected you

00:53:27.756 --> 00:53:28.806 A:middle
may want to go back to the

00:53:28.806 --> 00:53:31.406 A:middle
scanning and make sure that your

00:53:31.406 --> 00:53:32.506 A:middle
environment is well lit.

00:53:33.786 --> 00:53:35.956 A:middle
We really like, well lit

00:53:35.956 --> 00:53:37.156 A:middle
environment during the scanning

00:53:37.266 --> 00:53:37.916 A:middle
is very important.

00:53:38.506 --> 00:53:39.776 A:middle
If if you have a lux meter, it will

00:53:39.776 --> 00:53:41.916 A:middle
be about 500 lux will be best.

00:53:43.046 --> 00:53:45.006 A:middle
And if that is still not enough,

00:53:45.166 --> 00:53:46.556 A:middle
you may want to keep different

00:53:46.556 --> 00:53:50.446 A:middle
versions of the scans.

00:53:50.576 --> 00:53:51.826 A:middle
So, at this point, once you're

00:53:51.826 --> 00:53:53.266 A:middle
happy with the detection quality

00:53:53.266 --> 00:53:55.136 A:middle
you can simply drop the model to

00:53:55.136 --> 00:53:57.846 A:middle
your Mac and add it to the AR

00:53:57.846 --> 00:53:59.656 A:middle
Resource Groups, just like you

00:53:59.746 --> 00:54:03.016 A:middle
did for the images.

00:53:59.746 --> 00:54:03.016 A:middle
did for the images.

00:54:03.016 --> 00:54:04.526 A:middle
Also note that there are some

00:54:04.526 --> 00:54:05.886 A:middle
objects that will work really

00:54:05.886 --> 00:54:07.366 A:middle
great with this system.

00:54:07.586 --> 00:54:08.706 A:middle
Object like you can see on the

00:54:08.706 --> 00:54:08.986 A:middle
left.

00:54:09.586 --> 00:54:10.956 A:middle
First of all, they are rigid

00:54:10.956 --> 00:54:13.206 A:middle
objects and they are, also, rich

00:54:13.206 --> 00:54:14.696 A:middle
of texture, distinctive enough.

00:54:15.436 --> 00:54:16.416 A:middle
But there are also certain kinds

00:54:16.416 --> 00:54:17.526 A:middle
of object that will not work

00:54:17.686 --> 00:54:18.506 A:middle
well with the system.

00:54:19.066 --> 00:54:21.256 A:middle
You can see an example of this

00:54:21.256 --> 00:54:22.136 A:middle
on the right.

00:54:22.686 --> 00:54:24.756 A:middle
And for example, metallic,

00:54:24.826 --> 00:54:26.606 A:middle
transparent, or metallic or

00:54:26.606 --> 00:54:27.796 A:middle
reflective objects will not

00:54:27.856 --> 00:54:28.096 A:middle
work.

00:54:29.206 --> 00:54:31.496 A:middle
Or transparent objects like

00:54:31.576 --> 00:54:32.796 A:middle
glass material object will also

00:54:32.796 --> 00:54:34.376 A:middle
not work because the appearance

00:54:34.376 --> 00:54:35.236 A:middle
of these objects will really

00:54:35.236 --> 00:54:37.766 A:middle
depend on where they are in the

00:54:38.816 --> 00:54:38.936 A:middle
scene.

00:54:39.756 --> 00:54:41.076 A:middle
So, that was how to scan the

00:54:41.076 --> 00:54:41.506 A:middle
objects.

00:54:41.706 --> 00:54:43.126 A:middle
Again, make sure that you have

00:54:43.126 --> 00:54:44.026 A:middle
well-lit environment.

00:54:44.996 --> 00:54:46.596 A:middle
Let's now see how we can detect

00:54:46.646 --> 00:54:48.056 A:middle
this in ARKit.

00:54:50.046 --> 00:54:51.956 A:middle
If this looks familiar to you,

00:54:51.956 --> 00:54:53.606 A:middle
it's because the API is pretty

00:54:53.606 --> 00:54:54.536 A:middle
similar to the one of the

00:54:54.536 --> 00:54:55.006 A:middle
images.

00:54:55.586 --> 00:54:56.716 A:middle
We have a convenience method

00:54:56.816 --> 00:54:58.586 A:middle
to gather all the objects in a

00:54:58.586 --> 00:54:58.896 A:middle
group.

00:54:59.506 --> 00:55:00.656 A:middle
This time is in the

00:54:59.506 --> 00:55:00.656 A:middle
This time is in the

00:55:00.656 --> 00:55:01.916 A:middle
ARReferenceObjects class.

00:55:02.806 --> 00:55:05.136 A:middle
And to configure your

00:55:05.136 --> 00:55:07.056 A:middle
ARWorldTracking configuration,

00:55:07.056 --> 00:55:08.516 A:middle
you simply pass this object to

00:55:08.516 --> 00:55:10.876 A:middle
the detectionObjects property.

00:55:13.206 --> 00:55:15.566 A:middle
Once you run the session, again,

00:55:15.566 --> 00:55:17.236 A:middle
you will find your results.

00:55:18.306 --> 00:55:19.356 A:middle
And in this case, you want to

00:55:19.356 --> 00:55:21.656 A:middle
check for the ARObjectAnchor,

00:55:22.386 --> 00:55:23.656 A:middle
which will give you the position

00:55:23.786 --> 00:55:25.436 A:middle
and orientation of the object

00:55:25.436 --> 00:55:27.866 A:middle
with respect to the world.

00:55:28.716 --> 00:55:30.346 A:middle
And also, the name of the object

00:55:30.346 --> 00:55:33.086 A:middle
as was given in the asset

00:55:35.016 --> 00:55:35.246 A:middle
catalog.

00:55:35.246 --> 00:55:36.516 A:middle
So, you guys may have noticed

00:55:36.706 --> 00:55:38.316 A:middle
some similarities between the

00:55:38.316 --> 00:55:40.506 A:middle
object detection and the world

00:55:40.506 --> 00:55:42.726 A:middle
mapping relocalization.

00:55:43.426 --> 00:55:44.206 A:middle
But there's also few

00:55:44.206 --> 00:55:44.806 A:middle
differences.

00:55:44.856 --> 00:55:46.316 A:middle
So, in the case of the object

00:55:46.316 --> 00:55:48.406 A:middle
detection we are always giving

00:55:48.746 --> 00:55:50.446 A:middle
the object position with respect

00:55:50.446 --> 00:55:50.946 A:middle
to the world.

00:55:51.506 --> 00:55:52.426 A:middle
While in the world map

00:55:52.426 --> 00:55:53.966 A:middle
relocalization is the camera

00:55:53.966 --> 00:55:56.066 A:middle
itself that adjusts to the

00:55:56.066 --> 00:55:56.816 A:middle
previous world map.

00:55:58.286 --> 00:56:00.306 A:middle
In addition, you can detect

00:55:58.286 --> 00:56:00.306 A:middle
In addition, you can detect

00:56:00.536 --> 00:56:01.336 A:middle
multiple objects.

00:56:01.966 --> 00:56:03.626 A:middle
And object detection works best

00:56:03.736 --> 00:56:05.306 A:middle
for objects which are tabletop,

00:56:05.496 --> 00:56:06.376 A:middle
furniture sized.

00:56:07.146 --> 00:56:08.446 A:middle
While, the world map is really

00:56:08.446 --> 00:56:09.536 A:middle
the whole scene that's been

00:56:09.776 --> 00:56:10.266 A:middle
acquired.

00:56:10.776 --> 00:56:13.756 A:middle
With this insight, we conclude the

00:56:13.756 --> 00:56:14.576 A:middle
object detection.

00:56:14.686 --> 00:56:16.266 A:middle
Let's summarize what you have

00:56:17.796 --> 00:56:19.336 A:middle
seen, today.

00:56:19.516 --> 00:56:21.876 A:middle
Orientation tracking tracks only

00:56:21.876 --> 00:56:23.696 A:middle
the rotation of the device and

00:56:23.696 --> 00:56:25.756 A:middle
can be used to explore statical

00:56:25.756 --> 00:56:26.416 A:middle
environments.

00:56:27.946 --> 00:56:29.266 A:middle
World Tracking is the fully

00:56:29.266 --> 00:56:30.776 A:middle
featured position and

00:56:30.776 --> 00:56:32.246 A:middle
orientation tracking, which will

00:56:32.246 --> 00:56:33.926 A:middle
give you the device position

00:56:33.926 --> 00:56:36.286 A:middle
with respect to a world origin.

00:56:37.016 --> 00:56:38.776 A:middle
And enables all the scene

00:56:38.776 --> 00:56:40.486 A:middle
understanding capabilities like

00:56:41.166 --> 00:56:43.366 A:middle
the Plane Detection, which will

00:56:44.476 --> 00:56:46.256 A:middle
make you able to interact with

00:56:46.256 --> 00:56:48.546 A:middle
the physical, horizontal, and

00:56:48.546 --> 00:56:50.226 A:middle
vertical planes where you can

00:56:50.226 --> 00:56:51.636 A:middle
then put virtual objects.

00:56:51.996 --> 00:56:55.306 A:middle
We have seen how you can create

00:56:55.656 --> 00:56:57.036 A:middle
persistent or multiuser

00:56:57.036 --> 00:56:58.926 A:middle
experiences with the saving and

00:56:58.926 --> 00:57:00.116 A:middle
loading map features in the

00:56:58.926 --> 00:57:00.116 A:middle
loading map features in the

00:57:00.116 --> 00:57:00.566 A:middle
ARKit2.

00:57:01.516 --> 00:57:03.036 A:middle
And how you can detect physical

00:57:03.036 --> 00:57:04.816 A:middle
images and track them at 60

00:57:04.816 --> 00:57:06.096 A:middle
frames per second with the image

00:57:06.096 --> 00:57:08.596 A:middle
tracking and how you can detect

00:57:08.596 --> 00:57:09.916 A:middle
more generic objects with the

00:57:09.916 --> 00:57:11.176 A:middle
object detections.

00:57:12.576 --> 00:57:15.216 A:middle
And with this, I really hope you

00:57:15.216 --> 00:57:15.946 A:middle
guys have a better

00:57:15.946 --> 00:57:17.496 A:middle
understanding, now, of all the

00:57:17.496 --> 00:57:19.126 A:middle
different tracking technology

00:57:19.586 --> 00:57:20.816 A:middle
that are present in ARKit and

00:57:20.816 --> 00:57:22.016 A:middle
how they work behind the scenes.

00:57:23.006 --> 00:57:24.666 A:middle
And how you can get the best

00:57:24.806 --> 00:57:25.976 A:middle
quality out of it.

00:57:26.436 --> 00:57:28.106 A:middle
And we're really looking forward

00:57:28.106 --> 00:57:29.226 A:middle
to see what you guys are going

00:57:29.226 --> 00:57:29.656 A:middle
to do with that.

00:57:30.976 --> 00:57:32.416 A:middle
More information can be found at

00:57:32.486 --> 00:57:33.636 A:middle
the session link in the

00:57:33.636 --> 00:57:34.496 A:middle
developer website.

00:57:34.496 --> 00:57:36.226 A:middle
And we have an ARKit Lab

00:57:36.536 --> 00:57:37.586 A:middle
tomorrow, 9 a.m.

00:57:38.356 --> 00:57:39.806 A:middle
We will both, me and Marion will

00:57:39.806 --> 00:57:41.786 A:middle
be there answering any question

00:57:41.786 --> 00:57:42.666 A:middle
on ARKit you may have.

00:57:43.756 --> 00:57:44.976 A:middle
And with that, thank you, very

00:57:44.976 --> 00:57:46.976 A:middle
much and enjoy the bash.

00:57:47.516 --> 00:57:53.506 A:middle
[ Applause ]
