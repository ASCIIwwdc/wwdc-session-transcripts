WEBVTT

00:00:06.300 --> 00:00:08.130
>> Good morning everyone and welcome.

00:00:08.130 --> 00:00:09.280
My name is Brad Moore.

00:00:09.280 --> 00:00:14.730
I work on keyboards, text editing and of course this
and today Josh Shaffer and I are going to walk you

00:00:14.730 --> 00:00:20.440
through Gesture Recognition, which is a new way
of dealing with Multi-Touch input on iPhone OS.

00:00:20.440 --> 00:00:22.300
Now before we get into the nuts and bolts

00:00:22.300 --> 00:00:26.480
of Gesture Recognition I think it's
worth asking why does this matter to us?

00:00:26.480 --> 00:00:29.230
Why do we care about Multi-Touch processing?

00:00:29.230 --> 00:00:32.770
And at one level, simply there's no alternative.

00:00:32.770 --> 00:00:37.010
IOS, iPad, iPhone, they are all
built around touch-screen interfaces.

00:00:37.010 --> 00:00:41.550
So if you want to respond to user events
they are going to be Touch events.

00:00:41.550 --> 00:00:47.630
But more importantly touch-screen interfaces
make for a really incredible user experience.

00:00:47.630 --> 00:00:52.450
And you as app developers want to tap
into that magical interactive quality.

00:00:52.450 --> 00:00:54.090
So how do you get there?

00:00:54.090 --> 00:00:57.320
Well if you want to live up to the
potential one thing you want to make note

00:00:57.320 --> 00:01:00.790
of is why Touch interfaces are easy to use.

00:01:00.790 --> 00:01:03.550
On our system, it's direct manipulation.

00:01:03.550 --> 00:01:07.240
You put the fingers on the screen and
you physically manipulate content.

00:01:07.240 --> 00:01:10.260
It feels great and nothing could be simpler.

00:01:10.260 --> 00:01:17.240
When the iPad first launched you saw videos up on
YouTube of first-time computer users including toddlers,

00:01:17.240 --> 00:01:23.250
a 99-year-old woman, even a cat and they spend one
minute with the iPad and they know how to use it.

00:01:23.250 --> 00:01:25.550
And it's because of this direct manipulation.

00:01:25.550 --> 00:01:29.390
It's easy and it feels great so
you want to be shooting for that.

00:01:29.390 --> 00:01:33.530
Another reason it's easy to use is that we
have a common set of gestures and behaviors.

00:01:33.530 --> 00:01:35.920
It's a vocabulary that works system wide.

00:01:35.920 --> 00:01:41.370
So, for instance, when you encounter new content
one of the first things you do is you tap on it.

00:01:41.370 --> 00:01:44.310
You tap on an icon in the Home screen an app launches.

00:01:44.310 --> 00:01:48.000
You tap on a text field, a keyboard launches, comes up.

00:01:48.000 --> 00:01:50.060
You tap on a button and it responds.

00:01:50.060 --> 00:01:53.720
Tap is so easy, so familiar, and if you come from the mouse

00:01:53.720 --> 00:01:57.970
and pointer world we don't begrudge
that, it's similar to a click.

00:01:57.970 --> 00:02:01.590
Something that has no counterpart in
the mouse and pointer world is pinch.

00:02:01.590 --> 00:02:08.640
You put two fingers down on the screen move them together
or apart and the content just magically tracks and scales.

00:02:08.640 --> 00:02:10.340
It feels great.

00:02:10.340 --> 00:02:15.550
There's no counterpart in the mouse and pointer world
because it's a really difficult behavior to implement.

00:02:15.550 --> 00:02:21.410
You either have a fiddly slider or a
text box you enter a percentage into.

00:02:21.410 --> 00:02:26.070
On iPhone we have pervasive scalable
content because pinch is so natural

00:02:26.070 --> 00:02:28.950
and it works all over the place and users expect it.

00:02:28.950 --> 00:02:31.060
Other gestures we have include swipe.

00:02:31.060 --> 00:02:34.410
You swipe between photos, items in a set.

00:02:34.410 --> 00:02:39.650
If you swipe an item in a list it will let you delete that.

00:02:39.650 --> 00:02:41.440
We have a pan gesture.

00:02:41.440 --> 00:02:46.000
When you encounter scalable content you just put
your finger down on the screen and move it along

00:02:46.000 --> 00:02:51.180
and the content glides beautifully with your
finger and it has a little inertial scroll.

00:02:51.180 --> 00:02:56.070
It works sometimes free form, sometimes
it's locked to a particular direction,

00:02:56.070 --> 00:02:58.410
but it works everywhere and it works great.

00:02:58.410 --> 00:03:03.150
One other gesture we support is a bit more
advanced, it's the press-and-hold gesture.

00:03:03.150 --> 00:03:06.280
And it's kind of hard to discover
because you put you finger there

00:03:06.280 --> 00:03:09.130
and don't move it for a while and then something happens.

00:03:09.130 --> 00:03:11.730
But it gives you advanced things on the Home screen.

00:03:11.730 --> 00:03:13.490
An application icon starts jiggling.

00:03:13.490 --> 00:03:15.490
You can move it around.

00:03:15.490 --> 00:03:20.160
In a text field you can move the
cursor, and bring up a magnifier.

00:03:20.160 --> 00:03:24.380
And in all sorts of places it brings up the copy/paste UI.

00:03:24.380 --> 00:03:29.900
Now I said it's a little less discoverable but because it's
consistent across the system, once you've encountered it

00:03:29.900 --> 00:03:34.300
in just one place you know to try it
everywhere and that goes for all these gestures.

00:03:34.300 --> 00:03:39.870
It's a toolbox that users carry around with
them and expect them to work in your apps.

00:03:39.870 --> 00:03:41.910
So it's easy for users.

00:03:41.910 --> 00:03:44.030
I think you know where this is going.

00:03:44.030 --> 00:03:45.180
It's hard to write.

00:03:45.180 --> 00:03:49.240
There are unique challenges to touch-screen
interfaces that use developer space.

00:03:49.240 --> 00:03:52.550
One of those is the limited precision
inherent in touch input.

00:03:52.550 --> 00:03:59.160
We give you a coordinate that at first blush looks kind
of like a mouse coordinate but it's nothing of the sort

00:03:59.160 --> 00:04:04.430
because it's backed by a finger, which
has all the precision of a wobbly sausage.

00:04:04.430 --> 00:04:07.690
You try to put it where you want and it doesn't go there.

00:04:07.690 --> 00:04:12.860
You try to move it by a certain amount and it
doesn't move in the direction or the amount you want.

00:04:12.860 --> 00:04:16.610
You can't even hold it steady, which is the
easiest thing in the world with a mouse.

00:04:16.610 --> 00:04:23.260
So you can't really think of that coordinate as a
precise coordinate so much as a disc of possibilities.

00:04:23.260 --> 00:04:27.120
Another problem you face is the multi in Multi-Touch.

00:04:27.120 --> 00:04:31.680
Again there is an analogy when you put your
finger down you move it and you lift it.

00:04:31.680 --> 00:04:35.990
It's almost like mouse down, mouse move, mouse up right?

00:04:35.990 --> 00:04:40.690
Well sort of, but only if your computer
has 20 mice attached to it and each one

00:04:40.690 --> 00:04:43.940
of them is controlling an independent pointer.

00:04:43.940 --> 00:04:49.600
So the added overhead of those different
points of onscreen that you have to track,

00:04:49.600 --> 00:04:53.380
let alone trying to make sense of
them, makes your lives a lot harder.

00:04:53.380 --> 00:04:58.320
But I should mention that these two problems I've
listed here you can solve systematically if you ensure

00:04:58.320 --> 00:05:02.720
that your User Interface elements are
of a certain size and if you make sure

00:05:02.720 --> 00:05:09.890
that your User Interface elements respond
independently to Touch events you can get really far.

00:05:09.890 --> 00:05:13.200
There's another problem though that's
really hard to solve systematically

00:05:13.200 --> 00:05:17.910
and that's the inherent ambiguity of Multi-Touch interfaces.

00:05:17.910 --> 00:05:18.830
Now what do I mean by that?

00:05:18.830 --> 00:05:26.140
Well I'll take Safari as an example but you come to
Safari, you come to a page and you put your finger

00:05:26.140 --> 00:05:28.590
on the screen and you don't know what to do.

00:05:28.590 --> 00:05:30.990
The finger is over a link, maybe you follow a link.

00:05:30.990 --> 00:05:33.750
It could be a tap.

00:05:33.750 --> 00:05:39.130
But it could very well also be a double-tap so when
the finger lifts you don't want to immediately respond

00:05:39.130 --> 00:05:43.460
to that tap but maybe it's not
just one finger that's coming down.

00:05:43.460 --> 00:05:49.930
If you wait a bit another finger is going to come
down and it's a pinch and then again maybe one

00:05:49.930 --> 00:05:54.330
or more fingers are going to come down and move a
bit in the same direction, in which case it's a pan.

00:05:54.330 --> 00:06:01.260
And maybe it's going to wait there for a while and
not move dramatically in which case they want to bring

00:06:01.260 --> 00:06:07.570
up an action sheet and as if those weren't enough gesture
support, you also have the possibility that they're going

00:06:07.570 --> 00:06:12.710
to tap and then hold, in which case we
bring up an accelerated selection you want.

00:06:12.710 --> 00:06:16.130
So this a bewildering array of possibilities.

00:06:16.130 --> 00:06:19.770
And you see this sort of thing not
just in Safari but all sorts of apps

00:06:19.770 --> 00:06:23.990
and different context and it leads to a paralysis.

00:06:23.990 --> 00:06:27.890
When a finger comes in contact with the
screen, I know I want to do something

00:06:27.890 --> 00:06:30.840
but I don't know what gesture it is so I have to wait.

00:06:30.840 --> 00:06:32.300
I have to guess.

00:06:32.300 --> 00:06:36.210
So there are these suboptimal solutions.

00:06:36.210 --> 00:06:38.520
And you can understand the first, waiting.

00:06:38.520 --> 00:06:45.460
It comes out of a very good intention of
responding to the full set of gestures, right.

00:06:45.460 --> 00:06:50.350
I wait until the finger is off the screen or I
wait until a certain amount of time has gone by

00:06:50.350 --> 00:06:55.740
and then I can make my determination very accurately.

00:06:55.740 --> 00:07:00.560
But what it leads to in practice is I move
my fingers and then the content follows

00:07:00.560 --> 00:07:05.600
and that just kills the direct experience,
the direct manipulation experience.

00:07:05.600 --> 00:07:10.620
So as a user and a framework developer
nothing makes me sadder or more frustrated.

00:07:10.620 --> 00:07:13.370
So we can't do that, it would make me sad.

00:07:13.370 --> 00:07:18.230
If you recognize the problem of latency,
well maybe we'll say I'll take my best guess.

00:07:18.230 --> 00:07:19.720
I'll take my shot at it.

00:07:19.720 --> 00:07:25.800
And when a finger comes down well if it's in an
area of the screen that isn't otherwise interesting,

00:07:25.800 --> 00:07:28.200
I'll wait for another finger to
come down, see if it's a pinch.

00:07:28.200 --> 00:07:35.340
But if it comes down on a link I can reasonably infer
that I should follow the link and it does minimize latency

00:07:35.340 --> 00:07:40.330
but it's pretty frustrating cause you swipe over
a link to move the page and you follow a link.

00:07:40.330 --> 00:07:43.590
So that's not a solution either.

00:07:43.590 --> 00:07:50.220
What usually ends up happening at this point is you
say, "Boy, I've got this wonderful Multi-Touch interface

00:07:50.220 --> 00:07:53.420
but I'm going to have a mono-gesture interface.

00:07:53.420 --> 00:07:55.170
I'm going to tap everywhere.

00:07:55.170 --> 00:07:56.990
I tap to zoom.

00:07:56.990 --> 00:08:01.840
I tap to follow a link and you
litter your application with buttons.

00:08:01.840 --> 00:08:08.030
And, yeah, tap is direct manipulation
but you can do so much better.

00:08:08.030 --> 00:08:09.830
Now I should say there's a fourth solution.

00:08:09.830 --> 00:08:12.620
Some of you out there recognize all of these problems

00:08:12.620 --> 00:08:19.090
and you write a very tight state machine
so there's a minimal amount of latency.

00:08:19.090 --> 00:08:23.790
You correctly infer user intent and things just work great,

00:08:23.790 --> 00:08:29.830
except they don't because you haven't used our own
definition, the system-wide definition of pinch,

00:08:29.830 --> 00:08:32.400
swipe and scroll, you've got bizzaro swipe.

00:08:32.400 --> 00:08:37.420
So the user comes into your app and it's self-consistent
but it's not consistent with any other app on the system.

00:08:37.420 --> 00:08:41.240
So even when you do things perfectly you fail.

00:08:41.240 --> 00:08:45.520
So touch-screen interfaces are
easy to use but hard to write.

00:08:45.520 --> 00:08:52.900
Obviously our goal today and with UIGestureRecognizer
is to make it very easy for you guys to use.

00:08:52.900 --> 00:09:00.610
So the topics we're going to cover include a brief recap
of Touch handling, we'll contrast that to gesture handling

00:09:00.610 --> 00:09:02.950
and we're going to step in the mechanics of gesture handling

00:09:02.950 --> 00:09:08.350
so you know what UIGestureRecognizer is doing
on your behalf when you're employing it.

00:09:08.350 --> 00:09:11.840
Then we're going to go into the
nitty gritty details of the API.

00:09:11.840 --> 00:09:17.230
Finally we'll talk about how gesture interactors
and gesture recognizers interact with each other

00:09:17.230 --> 00:09:20.480
and how they interact with normal Touch event delivery.

00:09:20.480 --> 00:09:25.890
So first off, Touch handling, this should all be old hat.

00:09:25.890 --> 00:09:30.070
A finger comes down on the screen
and it's bound to a UITouch right.

00:09:30.070 --> 00:09:35.910
For the lifetime of that finger's contact with
the screen the same UITouch is associated with it.

00:09:35.910 --> 00:09:41.830
Also when a finger comes down on the screen its hit
test to a view and that UITouch is bound to that view

00:09:41.830 --> 00:09:44.830
for the duration of the finger's contact with the screen.

00:09:44.830 --> 00:09:51.390
And finally changes to that touch are delivered
to the UIView via a set of methods on UIResponder.

00:09:51.390 --> 00:09:54.650
Touch is began, touch is moved,
touch is ended and touch is canceled.

00:09:54.650 --> 00:09:56.390
All this sounds familiar right?

00:09:56.390 --> 00:10:00.930
So how would you go about detecting a
gesture using this raw event handling?

00:10:00.930 --> 00:10:08.220
Well first of all you need to subclass UIView so you
can store state and respond to these responder methods.

00:10:08.220 --> 00:10:13.590
Here I'm looking for a swipe so
I store away the first touch.

00:10:13.590 --> 00:10:16.720
And I store away its initial location.

00:10:16.720 --> 00:10:22.850
So when touch is began I see if it's the first
touch and if so I pull it out, get it's location.

00:10:22.850 --> 00:10:28.680
When touch is moved I look at the current location and
then I say, "Well if it's beyond some horizontal threshold,

00:10:28.680 --> 00:10:34.790
and beneath some vertical threshold, looks like a
swipe" and then touch is ended, I just clear the state.

00:10:34.790 --> 00:10:36.880
Not so bad, right?

00:10:36.880 --> 00:10:38.650
Well there are a lot of problems with this.

00:10:38.650 --> 00:10:43.390
One I've had to subclass to store
state, two it's not reusable.

00:10:43.390 --> 00:10:49.420
If I go to any other view on the system
I have to copy/paste this code at best

00:10:49.420 --> 00:10:52.510
and three, this is a horrible definition of swipe.

00:10:52.510 --> 00:10:55.290
Please recognize this don't paste this code into your app.

00:10:55.290 --> 00:11:04.220
There is a point here and point here, and if my finger moves
between them along any path it's going to count as a swipe

00:11:04.220 --> 00:11:06.170
and it doesn't pay attention to philosophy.

00:11:06.170 --> 00:11:08.640
There's so many nuances it doesn't get right.

00:11:08.640 --> 00:11:13.050
So you might go in and try to tweak it and
you'll have a more restrictive definition.

00:11:13.050 --> 00:11:15.150
But it then it's not going to be
the system definition at all.

00:11:15.150 --> 00:11:19.590
So this is a mess and it's a state
of the art until UIGestureRecognizer.

00:11:19.590 --> 00:11:23.280
So let's see what gesture handling is.

00:11:24.290 --> 00:11:32.320
Well to use gesture recognizers you instantiate a
pre-defined UIGestureRecognizer, optionally configure it.

00:11:32.320 --> 00:11:34.370
You set up one or more handlers.

00:11:34.370 --> 00:11:40.350
These are your target action pairs and then
you just add it to the view and you're done.

00:11:40.350 --> 00:11:44.860
So let's look at this example of
swiping with UIGestureRecognizer.

00:11:44.860 --> 00:11:47.600
Here I've subclassed UIView, but I don't really need to

00:11:47.600 --> 00:11:51.910
but in its initWithFrame method I'm
allocating the SwipeGestureRecognizer.

00:11:51.910 --> 00:11:57.540
I'm setting its target to self and its
action to the swipeRecognized method.

00:11:57.540 --> 00:12:02.840
So when the gesture is recognized
swipeRecognized will be called on me.

00:12:02.840 --> 00:12:09.180
Then I add it to the view and because the view retains
the GestureRecognizer I just release it, and that's it.

00:12:09.180 --> 00:12:10.590
This is reuseable.

00:12:10.590 --> 00:12:15.740
I don't have to do any of the work of detecting
the gesture, that's left to the GestureRecognizer

00:12:15.740 --> 00:12:20.680
and best of all I get the system definition of swipe.

00:12:20.680 --> 00:12:26.900
So when a user comes to my app it feels
just like every other app on the system.

00:12:26.900 --> 00:12:28.770
Okay so hopefully that whets your appetite.

00:12:28.770 --> 00:12:31.220
We'll get into more details later.

00:12:31.220 --> 00:12:37.120
But now the mechanics, how does Gesture Recognition work?

00:12:37.120 --> 00:12:42.520
Well we're familiar when a finger makes contact
with the screen that it's hit test to a view.

00:12:42.520 --> 00:12:46.280
Then a sequence of touches are
delivered to that view over time.

00:12:46.280 --> 00:12:52.420
But when you add a Gesture Recognizer to
the mix the gesture receives the touches

00:12:52.420 --> 00:12:54.620
in addition to the view receiving the touches.

00:12:54.620 --> 00:12:59.200
So it looks kind of like this when the
swipe comes through and it compares it

00:12:59.200 --> 00:13:02.390
to its own internal definition of the gesture.

00:13:02.390 --> 00:13:04.770
So let's look at that again more slowly.

00:13:04.770 --> 00:13:13.200
A swipe comes down and as it finds something that
it deems to be a swipe it notices that it's a match

00:13:13.200 --> 00:13:18.640
and it messages your handlers, your
target-action pairs and things proceed.

00:13:18.640 --> 00:13:21.750
It's actually very uncommon for a
gesture to be recognized though.

00:13:21.750 --> 00:13:29.050
Most of the time Gesture Recognizers are not matching,
so what happens there is something comes down,

00:13:29.050 --> 00:13:35.380
something happens to make it reject the
definition and then no messages are sent

00:13:35.380 --> 00:13:38.470
to your target-action pairs and things proceed.

00:13:39.540 --> 00:13:42.300
So if we add another Gesture Recognizer to the mix.

00:13:42.300 --> 00:13:47.320
We're still getting the multicast touch
delivery but something I want to point out is

00:13:47.320 --> 00:13:50.180
that Gesture Recognizers don't know about each other.

00:13:50.180 --> 00:13:54.180
They independently process touch-info sequences.

00:13:54.180 --> 00:13:58.120
For example, I have a Tap Recognizer
and a Swipe Recognizer here.

00:13:58.120 --> 00:14:02.460
They're going to be looking at sequences of
touch input and not know a thing about the other

00:14:02.460 --> 00:14:05.110
and they'll independently transition to a recognized

00:14:05.110 --> 00:14:11.750
or failed state messaging target-action
pairs along the way as needed.

00:14:11.750 --> 00:14:16.870
So let's look at a more complicated example
where there are multiple views to consider,

00:14:16.870 --> 00:14:19.350
we have a container view and two subviews.

00:14:19.350 --> 00:14:24.250
Well let's add Gesture Recognizers to
the first subview and the second subview.

00:14:24.250 --> 00:14:31.000
An important property of Gesture Recognition
is that if a touch isn't delivered to a view,

00:14:31.000 --> 00:14:38.020
then that Gesture Recognizer doesn't do any processing
and similarly a touch is delivered to a view.

00:14:38.020 --> 00:14:41.710
But when a touch is delivered to a view
its Gesture Recognizers receive it.

00:14:41.710 --> 00:14:47.140
So if subview one receives the touch
the Tap Gesture Recognizer sees it.

00:14:47.140 --> 00:14:51.660
If subview two receives the touch the
Swipe Gesture Recognizer receives it

00:14:51.660 --> 00:14:57.360
and the Tap Gesture Recognizer does not
receive any touches delivered to subview 2.

00:14:57.360 --> 00:15:01.990
And just to drive home the example, if a touch is delivered

00:15:01.990 --> 00:15:06.180
to the container view absolutely no Gesture
Recognizer sees that input sequence.

00:15:06.180 --> 00:15:12.660
And this is really important because this contextual
analysis means we're not considering all possible gestures

00:15:12.660 --> 00:15:14.130
at all possible times.

00:15:14.130 --> 00:15:17.710
We're only doing this as needed and that's
really important on an embedded device

00:15:17.710 --> 00:15:21.430
where you don't have infinite processing resources.

00:15:21.430 --> 00:15:25.990
So let's extend this example and add a
Gesture Recognizer to the container view.

00:15:25.990 --> 00:15:28.760
We've added a Pinch Gesture Recognizer here.

00:15:28.760 --> 00:15:34.100
And normally views only receive
touches that are hit test to that view

00:15:34.100 --> 00:15:37.400
and you might expect the same thing
from Gesture Recognizers.

00:15:37.400 --> 00:15:41.100
But what actually happens is the Gesture
Recognizer receives touches delivered

00:15:41.100 --> 00:15:44.230
to that view and to descendents of that view.

00:15:44.230 --> 00:15:49.100
So here we're going to have a finger come down in the
first subview, a finger come down in the second subview

00:15:49.100 --> 00:15:57.220
and only the Pinch Gesture Recognizer
is going to know about both of them.

00:15:57.220 --> 00:15:59.300
Okay and it matches.

00:15:59.300 --> 00:16:01.690
So really that's how it works in a nutshell.

00:16:01.690 --> 00:16:08.060
We send Touch events to more than one thing,
the Gesture Recognizer in addition to the view

00:16:08.060 --> 00:16:13.600
and Gesture Recognizers independently compare that
input sequence to their own internal definition.

00:16:13.600 --> 00:16:17.180
They only do this analysis if they've
been attached to a view.

00:16:17.180 --> 00:16:22.510
If no Gesture Recognizer is attached no processing
is going on and it works across multiple views.

00:16:22.510 --> 00:16:28.970
Specifically a Gesture Recognizer receives touches both
delivered both to its view and descendents of that view.

00:16:28.970 --> 00:16:33.830
So finally into the nuts and bolts of the API.

00:16:33.830 --> 00:16:42.050
Well first thing worth messaging is that UIGestureRecognizer
is available both on IOS 3.2 and IOS 4.0.

00:16:42.050 --> 00:16:48.900
So you have it available to you on iPad and iPhone
and UIGestureRecognizer is an abstract base class.

00:16:48.900 --> 00:16:55.820
So it has a lot of useful properties and methods
but you don't want to instantiate it directly.

00:16:55.820 --> 00:17:02.190
That said we provide of course, a huge number of
built-in subclasses that you do want to instantiate.

00:17:02.190 --> 00:17:10.060
Among these we have TapGestureRecognizer and of course
that looks for taps but also double-taps and 2-finger taps.

00:17:10.060 --> 00:17:12.290
We have PinchGestureRecognizer.

00:17:12.290 --> 00:17:14.620
We have SwipeGestureRecognizer.

00:17:14.620 --> 00:17:23.040
We have PanGestureRecognizer and that's
a bit more forgiving than its definition.

00:17:23.040 --> 00:17:28.090
Notice the path that took was slow and all over the screen.

00:17:28.090 --> 00:17:35.260
LongPressGestureRecognizer both recognizes long presses
and long presses that have some number of taps before them

00:17:35.260 --> 00:17:39.100
and continue to move after they've been recognized.

00:17:39.100 --> 00:17:43.390
And one other one that I didn't mention
earlier is the Rotation Gesture.

00:17:43.390 --> 00:17:50.090
It's available on iPad and some Contacts like Photos and
you get it as one of the built-in Gesture Recognizers.

00:17:50.090 --> 00:17:52.300
Now this set is pretty comprehensive.

00:17:52.300 --> 00:17:55.150
I don't think you'll often have to go outside of it

00:17:55.150 --> 00:17:59.610
but it's worth noting the UIGestureRecognizer
is designed from the get-go to be subclassed.

00:17:59.610 --> 00:18:06.650
So if you have some reason to define a custom gesture
or otherwise encapsulate your touch handling code,

00:18:06.650 --> 00:18:09.050
feel free to subclass UIGestureRecognizer.

00:18:09.050 --> 00:18:12.230
We're not going to talk about that now
and you can come to the following session

00:18:12.230 --> 00:18:15.040
if you're curious but it's worth pointing that out.

00:18:15.040 --> 00:18:17.760
Okay, So you have UIGestureRecognizer.

00:18:17.760 --> 00:18:23.580
The most important thing you can do with it is
establish a handler and that's reflected in the fact

00:18:23.580 --> 00:18:27.040
that its initializer takes a target-action pair.

00:18:27.040 --> 00:18:33.400
And if you want to manipulate the target-action pairs
after you've created it you can add and remove targets.

00:18:33.400 --> 00:18:38.580
And the action method takes an optional
UIGestureRecognizer as an argument.

00:18:38.580 --> 00:18:43.930
You almost always want to take the GestureRecognizer
as an argument and then within the handler,

00:18:43.930 --> 00:18:46.980
within the action method you want to do something.

00:18:46.980 --> 00:18:48.520
Well what do you do?

00:18:48.520 --> 00:18:55.050
In certain cases if you have a really tiny view and it has
a Tap Recognizer on it, the fact that the gesture occurred

00:18:55.050 --> 00:18:58.560
at all is sufficient information to process it.

00:18:58.560 --> 00:19:04.650
But in a much larger number of cases you need more
information and Gesture Recognizers have properties

00:19:04.650 --> 00:19:08.160
and methods associated with them that
make it really easy to handle them.

00:19:08.160 --> 00:19:14.260
So the first of these that seems interesting
is location, the location on screen.

00:19:14.260 --> 00:19:18.380
We have a location and view method on UIGestureRecognizer

00:19:18.380 --> 00:19:21.320
and it means different things for
different concrete subclasses.

00:19:21.320 --> 00:19:26.620
For instance, in a Swipe Gesture it's going
to be the point at which the swipe started.

00:19:26.620 --> 00:19:32.250
And in a Pinch Gesture it's going to be
the center of the pinch, natural right.

00:19:32.250 --> 00:19:35.880
Occasionally you want a little more
information so you might expect us

00:19:35.880 --> 00:19:40.680
to just expose an NS array of touches
associated with the gesture.

00:19:40.680 --> 00:19:48.180
But what we actually do is similar to that but we
implement location of touchInView on UIGestureRecognizer.

00:19:48.180 --> 00:19:50.160
Why don't we give the array of touches?

00:19:50.160 --> 00:19:53.800
Well take for example a DoubleTapRecognizer.

00:19:53.800 --> 00:19:58.640
By the time the gesture is Recognizer the finger
and the touch associated with it are long gone.

00:19:58.640 --> 00:20:04.730
So we store away that information and location of
touchInView and make it available to you at a later time.

00:20:04.730 --> 00:20:08.970
Another property that you might want to
pull out of GestureRecognizer is the state.

00:20:08.970 --> 00:20:14.400
There is a state property on UIGestureRecognizer
of type UIGestureRecognizerState.

00:20:14.400 --> 00:20:17.340
So what values does the UIGestureRecognizerState take on?

00:20:17.340 --> 00:20:20.400
Well I want to get out of the way that there are a couple

00:20:20.400 --> 00:20:24.650
that for your purposes are uninteresting,
they're just for bookkeeping.

00:20:24.650 --> 00:20:31.490
The possible state and the failed state although they are
very important in the implementation of Gesture Recognition,

00:20:31.490 --> 00:20:36.010
you'll never receive your action methods while
things are in this state, so disregard them.

00:20:36.010 --> 00:20:38.770
Then there's Discrete Recognizers.

00:20:38.770 --> 00:20:43.440
A Discrete Gesture Recognizer is
something that recognizes a gesture

00:20:43.440 --> 00:20:45.820
at a particulate moment in time and then it's done with it.

00:20:45.820 --> 00:20:47.650
There are no further updates.

00:20:47.650 --> 00:20:53.440
So you're action method will fire just once and when
that happens Discrete Gesture Recognizers are always

00:20:53.440 --> 00:20:56.640
in the UIGestureRecognizerStateRecognized state.

00:20:56.640 --> 00:20:58.310
That was a mouthful.

00:20:58.310 --> 00:21:04.710
So since it's always in that state maybe it's not very
interesting to you and you don't even need to check it.

00:21:04.710 --> 00:21:12.410
But one case you care about immensely is Continuous
Recognizers, recognizers like Pinch and Rotate that happen

00:21:12.410 --> 00:21:17.390
over time and will cause your action
method to be fired multiple times.

00:21:17.390 --> 00:21:21.290
Here there is the Began, Changed, Ended and Canceled state.

00:21:21.290 --> 00:21:23.410
And let's look at that visually.

00:21:23.410 --> 00:21:25.230
We have the Began state.

00:21:25.230 --> 00:21:30.190
Every Gesture Recognizer that's continuous starts
out in the Began state when you receive it.

00:21:30.190 --> 00:21:32.130
It might move to the Changed state.

00:21:32.130 --> 00:21:38.420
It might stay in the Changed state for a while and it will
definitely eventually move to a terminal state, like Ended.

00:21:38.420 --> 00:21:45.180
Now one thing I want to point out there is an
important difference between Gesture Recognizer states

00:21:45.180 --> 00:21:47.990
and touch phases, which are similar right?

00:21:47.990 --> 00:21:56.580
You have began, changed, ended, similar to began,
moved, ended but the problem with touch phases is

00:21:56.580 --> 00:22:00.390
that in the began phase you don't really
know that you can perform any action.

00:22:00.390 --> 00:22:02.400
In Gesture Recognizers you know for sure

00:22:02.400 --> 00:22:08.240
because we've already waited however much time
is necessary to correctly identify the gesture.

00:22:08.240 --> 00:22:14.440
So in your Began state, you can go and start
updating the UI and save any initial state.

00:22:14.440 --> 00:22:17.220
In the Changed state you can continue updating the UI

00:22:17.220 --> 00:22:22.140
and in the Ended state you can tear
down any transit UI and confirm changes.

00:22:22.140 --> 00:22:23.920
Now I mentioned there's a Canceled state as well.

00:22:23.920 --> 00:22:26.980
So the state diagram actually looks
like this from either the Began

00:22:26.980 --> 00:22:29.440
or Changed states it can move straight to the Canceled.

00:22:29.440 --> 00:22:36.740
That will happen say when a phone call comes in or we notice
that the phone is up to your face with the proximity sensor.

00:22:36.740 --> 00:22:42.720
You want to handle this similarly to the Ended
state but ideally you won't confirm any user action

00:22:42.720 --> 00:22:45.980
because the user didn't actually finish that gesture.

00:22:45.980 --> 00:22:50.800
So let's look at an example of dealing
with a ContinuousStateGestureRecognizer.

00:22:50.800 --> 00:22:56.140
Here we're going to handle a LongPressGestureRecognizer
and we're going

00:22:56.140 --> 00:22:59.250
to try do something similar to
what happens on the Home screen.

00:22:59.250 --> 00:23:05.440
Your finger is going press something, you want it to start
jiggling and you move it around as your finger changes.

00:23:05.440 --> 00:23:10.760
So the first thing we'll do is pull
off the view and the currentLocation.

00:23:10.760 --> 00:23:17.880
We'll use these in every example and note that
UIGestureRecognizer has a handy backpointer to its UIView.

00:23:17.880 --> 00:23:21.310
It's been added to a view and you
can access it from the handler.

00:23:21.310 --> 00:23:23.480
You want to do that all the time.

00:23:23.480 --> 00:23:28.400
And the location we're getting in the view is
super view so we have a steady reference frame.

00:23:28.400 --> 00:23:30.200
And then we're going to switch on State.

00:23:30.200 --> 00:23:31.680
It will go to through four states.

00:23:31.680 --> 00:23:39.060
In the first we'll store away the startLocation
and the centerOffset so we can make geometry deltas

00:23:39.060 --> 00:23:44.010
in the further states and we'll start
the animation, we'll beginJiggling.

00:23:44.010 --> 00:23:48.050
In StateChanged we're just going
to update the center of the icon.

00:23:48.050 --> 00:23:55.600
In StateEnded we're going to update the center and
stop the animations because it's been confirmed

00:23:55.600 --> 00:24:01.260
and in StateCanceled we'll do something similar but this
time we're going to restore the center to the startLocation

00:24:01.260 --> 00:24:04.040
because something interrupted the gesture.

00:24:04.040 --> 00:24:08.320
So all those states were common
to every UIGestureRecognizer.

00:24:08.320 --> 00:24:11.830
There's a lot of specialized state on
the concrete subclasses we provide you

00:24:11.830 --> 00:24:16.020
that is useful for those particular subclasses.

00:24:16.020 --> 00:24:20.230
For instance UIPinchGestureRecognizer has a scale with it.

00:24:20.230 --> 00:24:25.030
And UIPanGestureRecognizer has an associated translation.

00:24:25.030 --> 00:24:28.950
These method names, these property
names should be a very big clue

00:24:28.950 --> 00:24:34.070
that these Concrete Gesture Recognizers are
intended to be used in a very particular way.

00:24:34.070 --> 00:24:35.760
I can't emphasize this enough.

00:24:35.760 --> 00:24:41.340
We've done all the trouble of decoupling
Gesture Recognition from Gesture Handling.

00:24:41.340 --> 00:24:47.480
But that doesn't mean that you can mix and match
in any old fashion and have a good user experience.

00:24:47.480 --> 00:24:53.290
It's entirely possibly to have your PinchGesture start
rotating the view, but it's not going to make any sense.

00:24:53.290 --> 00:24:57.150
So please try to use the properties appropriately.

00:24:57.150 --> 00:25:01.190
OK So that's how we handle gestures.

00:25:01.190 --> 00:25:04.260
Sometimes you want to customize a
Gesture Recognition just slightly.

00:25:04.260 --> 00:25:10.280
There's a common way to override behavior without
subclassing in Cocoa Touch and that's delegation.

00:25:10.280 --> 00:25:13.370
Well UIGestureRecognizer does indeed have a delegate.

00:25:13.370 --> 00:25:19.760
It conforms to the UIGestureRecognizerDelegate property,
protocol and it has a number of optional methods.

00:25:19.760 --> 00:25:25.790
The first of these GestureRecognizerShouldBegin
allows you to decide at the last moment

00:25:25.790 --> 00:25:29.280
that a Gesture Recognizer is not going to be recognized.

00:25:29.280 --> 00:25:33.710
It is a way of restricting the definition
of a gesture without subclassing

00:25:33.710 --> 00:25:37.280
and without reconfiguring it in any other way.

00:25:37.280 --> 00:25:40.920
Another delegate method the
GestureRecognizerShouldReceiveTouch allows you

00:25:40.920 --> 00:25:44.170
to prevent a gesture from even
seeing a touch in the first place.

00:25:44.170 --> 00:25:48.980
This is useful perhaps if you have a Gesture
Recognizer on your outer container view

00:25:48.980 --> 00:25:53.650
and there's one tiny descendent view that
you don't want to interact with gestures.

00:25:53.650 --> 00:25:55.770
So we're looking at this visually.

00:25:55.770 --> 00:26:01.190
What happens when a swipe comes in and the Gesture
Recognizer says, "Yes it matches my definition?"

00:26:01.190 --> 00:26:05.640
Well it first sends a message to its
delegate asking if it should begin.

00:26:05.640 --> 00:26:09.380
And so delegate says, "No" the
Gesture Recognizer never transitions

00:26:09.380 --> 00:26:14.130
to the RecognizedState and no action methods are sent.

00:26:14.130 --> 00:26:19.740
In the other delegate method when a finger comes in
contact with the view and its hit test to the view,

00:26:19.740 --> 00:26:23.730
we at that point send a message to the delegate
asking if it should even receive the touch.

00:26:23.730 --> 00:26:30.200
And if you say no to that, the SwipeRecognizer
never even receives the touch sequence.

00:26:30.200 --> 00:26:37.810
So you can use the delegate to override every definition
of UIGestureRecognizer or any UIGestureRecognizer

00:26:37.810 --> 00:26:43.580
but the subclasses offer many more ways to
configure those particular Gesture Recognizers.

00:26:43.580 --> 00:26:49.930
So to give an example TapGestureRecognizer
allows you to override both the number of taps,

00:26:49.930 --> 00:26:53.990
say single versus double and the number of touches required.

00:26:53.990 --> 00:27:01.030
Say you want to implement a 2-finger tap like you
use in Maps to scroll out, or zoom out rather.

00:27:01.030 --> 00:27:02.510
There is PanGestureRecognizer.

00:27:02.510 --> 00:27:07.370
You can specify both a minimum number of
touches and a maximum number of touches.

00:27:07.370 --> 00:27:12.310
And UILongPressGestureRecognizer, which has
a huge number of configurable properties.

00:27:12.310 --> 00:27:17.540
You can specify a number of taps
required beforehand and it defaults to 0

00:27:17.540 --> 00:27:22.140
but if you set it 1 say you'll
get a tap and a half behavior.

00:27:22.140 --> 00:27:27.200
If you change the number of touches required
you can get a 2-finger Long Press Gesture.

00:27:27.200 --> 00:27:32.730
You can also tweak the minimum press
duration and even the allowable movement.

00:27:32.730 --> 00:27:39.710
So the built-in subclasses are highly, highly configurable
and the jury is out on whether that's a good thing or not;

00:27:39.710 --> 00:27:47.750
so I want to take this opportunity to caution you all
against overly configuring these in very exotic ways.

00:27:47.750 --> 00:27:56.520
If you create a 4-finger Triple-Tap Gesture it's entirely
possible, we've given you the rope to hang yourself by

00:27:56.520 --> 00:28:00.050
but it's going to make for a very
poor user experience for two reasons.

00:28:00.050 --> 00:28:03.200
It's going to be inconsistent with the rest of the system.

00:28:03.200 --> 00:28:09.790
So if for some reason users come to love that gesture
in your app it's not going to work anywhere else.

00:28:09.790 --> 00:28:15.480
And, as sort of the flipside to this is it's going
to be very hard to discover it because it's not going

00:28:15.480 --> 00:28:20.810
to exist anywhere else, so users will come to
your app and not know to try that awesome gesture.

00:28:20.810 --> 00:28:27.500
So you don't want to hide complex or interesting
behaviors behind undiscoverable gestures.

00:28:27.500 --> 00:28:32.070
That said, if you're determined to shoot
yourself in the foot, let us do it for you

00:28:32.070 --> 00:28:37.420
and use the UIGestureRecognizer
and configure it however you want.

00:28:37.420 --> 00:28:40.720
Okay so that's how to use the API.

00:28:40.720 --> 00:28:54.900
And now Josh Schaffer is going to come on stage
and actually show you in a demo how it works.

00:28:54.900 --> 00:28:55.080
[Applause]

00:28:55.080 --> 00:28:57.000
>> Alright thanks Brad.

00:28:57.000 --> 00:29:01.480
So I'm really excited to share this with you guys
because we've been doing some touch handling things

00:29:01.480 --> 00:29:05.390
over the last couple of years and it just got a lot easier.

00:29:05.390 --> 00:29:08.170
So we're going to do two demos today actually.

00:29:08.170 --> 00:29:13.790
And I've gone ahead and downloaded a couple of the
samples off developer.apple.com and we'll take a look

00:29:13.790 --> 00:29:19.480
at how we did them before and see how we can
modify them to make them significantly easier now.

00:29:19.480 --> 00:29:20.630
So you may have seen this before.

00:29:20.630 --> 00:29:23.860
It's part of the Scroll View Suite
set of sample applications that went

00:29:23.860 --> 00:29:26.620
out with last year's Mastering iPhone Scroll View session.

00:29:26.620 --> 00:29:32.090
And you can pinch in using a UIScrollView and
scroll around and that's all part of UIScrollView.

00:29:32.090 --> 00:29:37.500
But additionally we have the ability to
double-tap to zoom in and 2-finger tap to zoom out.

00:29:37.500 --> 00:29:41.550
Now the way that was implemented last year and
in this sample code that you can download was

00:29:41.550 --> 00:29:46.220
through a custom subclass of UIImageView
called a TapDetectingImageView.

00:29:46.220 --> 00:29:52.720
And this TapDetectingImageView has a whole bunch of I bars
to attract state and defines its own delegate protocol

00:29:52.720 --> 00:29:57.080
to inform the class that allocated it that there were taps.

00:29:57.080 --> 00:30:00.460
Double-taps and 2-finger taps so that's all down here

00:30:00.460 --> 00:30:05.160
and then within the implementation file there's all
this code just to figure out whether we're tapping,

00:30:05.160 --> 00:30:09.610
double-tapping, 2-finger tapping and then
in the end we have to notify our delegate

00:30:09.610 --> 00:30:11.600
that one of these things has actually happened.

00:30:11.600 --> 00:30:16.960
So the first thing we're going to do this year is
select those two files for the TapDetectingImageView

00:30:16.960 --> 00:30:24.040
and just delete them [Applause] and now we're done.

00:30:24.040 --> 00:30:30.370
No, So we had first allocated our
TapDetectingImageView here in our root view controller.

00:30:30.370 --> 00:30:31.390
So we now can just go back.

00:30:31.390 --> 00:30:36.930
We don't need a subclass at all anymore and
we're just going to create a UIImageView instead.

00:30:36.930 --> 00:30:41.690
And additionally now we'll delete this setDelegate because
UIImageView doesn't actually have a delegate protocol

00:30:41.690 --> 00:30:44.270
so that would not have actually compiled.

00:30:44.270 --> 00:30:48.560
And now we can just allocate a couple of
UITapGestureRecognizers, actually three because we want

00:30:48.560 --> 00:30:51.100
to configure tap, double-tap, and 2-finger tap.

00:30:51.100 --> 00:30:54.790
So we'll go ahead and create three of them.

00:30:54.790 --> 00:30:59.480
We've got three direct instantiations
of UITapGestureRecognizer initing

00:30:59.480 --> 00:31:01.980
with target self and three different selectors.

00:31:01.980 --> 00:31:06.680
We've got handleSingleTap, handleDoubleTap and handleTwoFingerTap.

00:31:06.680 --> 00:31:09.590
Now all of these by default are
configured to be single-finger,

00:31:09.590 --> 00:31:14.200
single-tap because that's the default allocation
you get when you allocate a TapGestureRecognizer.

00:31:14.200 --> 00:31:19.350
So for our double-tap we'll have to set the number of
taps required to 2 and for our 2-finger tap the number

00:31:19.350 --> 00:31:23.410
of touches required to 2, but that's all the
configuration that we have to do in order

00:31:23.410 --> 00:31:25.640
to get all three of these different behaviors.

00:31:25.640 --> 00:31:29.810
So with that done we can just attach them to
that UIImageView so we'll call imageView

00:31:29.810 --> 00:31:32.710
addGestureRecognizer: Single-tap,
Double-tap and 2-finger tap.

00:31:32.710 --> 00:31:36.500
And now that they're all retained by
the ImageView we can just release them.

00:31:36.500 --> 00:31:39.570
We don't have to even hang on to
them in high bar or anything.

00:31:39.570 --> 00:31:42.500
So a single-tap release, double-tap
and 2-finger tap all released.

00:31:42.500 --> 00:31:46.380
So the last thing that's left to
do before this actually works is

00:31:46.380 --> 00:31:49.570
to replace those delegate methods
that we had implemented before.

00:31:49.570 --> 00:31:53.750
Now one thing to take note of in these
delegate methods is they actually provided us

00:31:53.750 --> 00:31:55.880
with a point where the taps had occurred.

00:31:55.880 --> 00:31:57.600
So we'll need to solve that as well.

00:31:57.600 --> 00:32:03.310
But first off we'll just select this and replace
our TapDetectingImageView got SingleTapAtPoint

00:32:03.310 --> 00:32:07.320
with our new action method we defined Handle
single-tap and the same for the double-tap,

00:32:07.320 --> 00:32:12.560
we'll replace that with Handle double-tap and for the
2-finger tap we'll replace that with Handle2-fingerTap.

00:32:12.560 --> 00:32:20.150
Now that last part is we no longer have this tap point to
find because that was passed to us before by our subclass.

00:32:20.150 --> 00:32:26.760
But as Brad showed us UIGestureRecognizer knows
where the gesture occurred and for any number

00:32:26.760 --> 00:32:30.140
of taps it's always the centroid of
however many fingers were involved.

00:32:30.140 --> 00:32:33.370
So if it's a 2-finger tap it's the
point that was exactly between them.

00:32:33.370 --> 00:32:39.380
So we can actually just replace the thing that was
passed in with the GestureRecognizer locationInView

00:32:39.380 --> 00:32:43.790
and since GestureRecognizer knows what view it's
attached to and that's the view we want it in,

00:32:43.790 --> 00:32:49.140
we can just get that view back from the Gesture Recognizer
and assign that to our local variable now at tap point.

00:32:49.140 --> 00:32:52.010
So we just replace exactly what
we had been calculating before

00:32:52.010 --> 00:32:58.270
with something the Gesture Recognizer is already calculating
for us, and the same thing down here in Handle 2-finger tap.

00:32:58.270 --> 00:33:00.920
So with that done there is not really anything left to do.

00:33:00.920 --> 00:33:02.150
We can build and run.

00:33:02.150 --> 00:33:04.380
We still have the same ScrollView functionality we had.

00:33:04.380 --> 00:33:05.250
We can still pinch.

00:33:05.250 --> 00:33:10.680
We can scroll around and we can still double-tap to
zoom in and 2-finger tap to zoom out with way less code

00:33:10.680 --> 00:33:15.370
than we had before and with a definition of tap
that's the same as everywhere else on the system.

00:33:15.370 --> 00:33:23.960
So, that's-- [Applause]

00:33:23.960 --> 00:33:25.010
>> Thank you Josh.

00:33:25.010 --> 00:33:31.020
As someone who's had to write and maintain
code like that before this warms my heart too.

00:33:31.020 --> 00:33:38.480
And hopefully you're looking at this and salivating and
saying, "Boy I can replace a lot of hairy code I've read."

00:33:38.480 --> 00:33:43.400
So let's now go into what happens when
there are multiple Gesture Recognizers

00:33:43.400 --> 00:33:46.440
and they're interacting with each other in some way.

00:33:46.440 --> 00:33:54.460
Well you want to think of the UIGestureRecognizer
state machine as a superposition of possibilities.

00:33:54.460 --> 00:33:58.710
It's like earlier a finger comes down on the
screen but you don't know what it is yet.

00:33:58.710 --> 00:34:05.460
But of course, we only support at most one gesture from a
single sequence of touch input and we achieve this by saying

00:34:05.460 --> 00:34:08.720
that the first Gesture Recognizer
to match is the one that wins.

00:34:08.720 --> 00:34:12.040
It's the one that gets the target-action
paired messages sent.

00:34:12.040 --> 00:34:14.920
And this works really well in a lot of cases.

00:34:14.920 --> 00:34:16.030
It reduces latency.

00:34:16.030 --> 00:34:19.800
I mean as soon as something recognizes, an action happens.

00:34:19.800 --> 00:34:22.630
And let's look at it visually.

00:34:22.630 --> 00:34:29.780
Here the swipe has been recognized and there hasn't been
anything in the touch input sequence up to this point

00:34:29.780 --> 00:34:33.230
that disqualifies the touch input sequence from being a pan.

00:34:33.230 --> 00:34:36.340
The pan is a very promiscuous Gesture Recognizer.

00:34:36.340 --> 00:34:41.710
But by virtue of the fact that swipe has been
recognized the pan will actually be excluded

00:34:41.710 --> 00:34:47.960
and only the Swipe Recognizer will
have its handlers notified.

00:34:47.960 --> 00:34:51.490
So that's how we deal with conflicts in general.

00:34:51.490 --> 00:34:59.450
But if two Gesture Recognizers recognize at exactly the
same moment in time in response to precisely the same step

00:34:59.450 --> 00:35:05.080
in the touch input sequence, well we have to
break ties and we do this by favoring views

00:35:05.080 --> 00:35:09.570
that are closest to leaf nodes in the view hierarchy.

00:35:09.570 --> 00:35:16.990
For instance if you have an outer view and an inner view
that both have TapGestureRecognizers installed and you tap

00:35:16.990 --> 00:35:22.840
on the inner view we're going to deliver the
target-action pair for the inner view not the outer view.

00:35:22.840 --> 00:35:28.790
And if two Gesture Recognizers on exactly the
same view recognize at the same moment in time,

00:35:28.790 --> 00:35:32.670
we're going to prefer the Gesture
Recognizer that was most recently added.

00:35:32.670 --> 00:35:36.130
So visually this looks like the following.

00:35:36.130 --> 00:35:41.960
We add a SwipeGestureRecognizer to both the outer
and the inner view, the container and the subview

00:35:41.960 --> 00:35:45.890
and then when a touch input sequence
come in that matches the swipe we have

00:35:45.890 --> 00:35:51.720
to choose among the different possibilities and here
we choose the more deeply nested view with subview.

00:35:51.720 --> 00:35:57.080
And that in turn excludes the outer view Swipe Recognizer.

00:35:57.080 --> 00:36:05.020
So let's go ahead and add another Swipe Recognizer to
the subview and run the same touch input sequence again

00:36:05.020 --> 00:36:10.780
and here we have to choose between Gesture Recognizers
attached to different views so we choose the subview again,

00:36:10.780 --> 00:36:17.840
but this time we go the more recently added Swipe
Recognizer so it's recognized, it's target-action pair fires

00:36:17.840 --> 00:36:20.700
and it excludes the other SwipeGestureRecognizer.

00:36:20.700 --> 00:36:24.750
So this is great.

00:36:24.750 --> 00:36:26.380
It minimizes latency.

00:36:26.380 --> 00:36:32.890
It works in a huge number of cases and hopefully
makes everyone happy but there are a few cases,

00:36:32.890 --> 00:36:35.080
well a couple of cases where it falls short.

00:36:35.080 --> 00:36:37.170
One of these is with Dependent Gestures.

00:36:37.170 --> 00:36:39.160
And what do I mean by Dependent Gestures?

00:36:39.160 --> 00:36:44.090
Well where one gesture always has to
occur before a second gesture occurs.

00:36:44.090 --> 00:36:50.610
An obvious example is tap and double-tap and
tap always happens before double-tap right?

00:36:50.610 --> 00:36:56.770
And you might think from what I've just said that if
you attach both a Single-tap and a Double-tapRecognizer

00:36:56.770 --> 00:37:01.020
to a view than the Double-tapRecognizer
is never going to fire.

00:37:01.020 --> 00:37:04.570
Well if you were paying attention to the demo
you know that's not in fact what happens.

00:37:04.570 --> 00:37:08.210
As a matter of fact, we have very
reasonable default behaviors in the UIKit.

00:37:08.210 --> 00:37:14.180
When a single-tap comes in the single-tap
recognizes, it messages its target-action pair,

00:37:14.180 --> 00:37:17.050
but it does not exclude a Double-tapGestureRecognizer.

00:37:17.050 --> 00:37:24.450
When a second tap comes in the Double-tap recognizes it as a
double-tap but it does exclude the single-tap in this case.

00:37:24.450 --> 00:37:30.240
So in a double-tap sequence we get our single-tap
firing and then we get a double-tap firing.

00:37:30.240 --> 00:37:38.780
So two action methods and it works pretty well and it works
really well if you have stackable actions on your gestures.

00:37:38.780 --> 00:37:47.100
For instance, take Notes: if you want to move the cursor,
you tap; if you want to select a word, you double-tap.

00:37:47.100 --> 00:37:52.070
Well there's no reason you can't move the
cursor immediately before selecting the word

00:37:52.070 --> 00:37:57.390
and so you can just stack these actions and
it works beautifully and there's no latency.

00:37:57.390 --> 00:38:03.200
An example where it doesn't work so well
when you have nonstackable actions is Safari.

00:38:03.200 --> 00:38:05.690
On tap we follow a link.

00:38:05.690 --> 00:38:08.240
On double-tap we zoom into the content.

00:38:08.240 --> 00:38:12.800
Well you don't want to zoom into the
content when you've just followed a link.

00:38:12.800 --> 00:38:13.880
That doesn't work at all.

00:38:13.880 --> 00:38:17.830
So there needs to be some other way to
deal with that and of course, there is.

00:38:17.830 --> 00:38:23.940
On UIGestureRecognizer we have another
method requireGestureRecognizerToFail.

00:38:23.940 --> 00:38:30.070
And requireGestureRecognizerToFail is
called by the dependee on the dependent.

00:38:30.070 --> 00:38:36.050
That is first Gesture Recognizer to
recognize calls the method against the--

00:38:36.050 --> 00:38:39.900
well you call this against the first
Gesture Recognizer to recognize

00:38:39.900 --> 00:38:42.980
against the second Gesture Recognizer to recognize.

00:38:42.980 --> 00:38:46.930
An example is a single-tap requires a double-tap to fail.

00:38:46.930 --> 00:38:50.750
So what does that look visually?

00:38:50.750 --> 00:38:56.220
Well our first tap comes in and the
Single-Tap Recognizer in some senses recognizes

00:38:56.220 --> 00:39:01.930
because it knows it's matching its internal
definition but we've imposed this failure requirement

00:39:01.930 --> 00:39:06.970
so it's not quite finished and it's
waiting for the double-tap to fail.

00:39:06.970 --> 00:39:11.870
Then when another tap comes in the
Double-Tap Gesture Recognizer says, "Ah ha this is a match."

00:39:11.870 --> 00:39:17.460
It messages its handlers and then by virtue of
being recognized it excludes the single-tap.

00:39:17.460 --> 00:39:23.450
So when a double-tap comes in we immediately get our
double-tap action method and there's no action sent

00:39:23.450 --> 00:39:30.370
for the single-tap, exactly what we want,
except consider just the single-tap case.

00:39:30.370 --> 00:39:38.630
A finger comes in and after some duration a double-tap
fails and only then does a single-tap transition fully

00:39:38.630 --> 00:39:41.280
to a recognized state and message the handlers.

00:39:41.280 --> 00:39:47.460
And notice there that this introduces latency,
which I said earlier is really, really bad.

00:39:47.460 --> 00:39:49.500
It makes me sad and you don't want to do this.

00:39:49.500 --> 00:39:55.460
What you want to do is design your User Interface
so that you can work with stackable actions.

00:39:55.460 --> 00:40:02.860
But if you absolutely cannot have stackable actions
for Dependent Gestures you can use this method.

00:40:02.860 --> 00:40:09.130
So Dependent Gestures are one example where mutually
exclusive Gesture Recognition isn't perfect.

00:40:09.130 --> 00:40:12.200
Another example is Compatible Gestures.

00:40:12.200 --> 00:40:16.800
Gestures like pinch and rotate, why
not have them happen at the same time.

00:40:16.800 --> 00:40:21.610
Well by default we're using the same
model we used for all gesture recognizers

00:40:21.610 --> 00:40:24.710
and that's no Gesture Recognizer is compatible with another.

00:40:24.710 --> 00:40:31.440
So if I put two fingers down on the screen at some point I'm
going to decide either it's a pinch or rotate but not both

00:40:31.440 --> 00:40:34.150
and my action methods won't fire for both.

00:40:34.150 --> 00:40:36.930
But this is something you can override.

00:40:36.930 --> 00:40:41.620
Using a method on the Delegate
property what I didn't mention earlier,

00:40:41.620 --> 00:40:45.950
gestureRecognizer:shouldRecognizeSimultaneouslyWithGestureRecognizer:

00:40:45.950 --> 00:40:50.770
And if you return yes to this two Gesture
Recognizers can be recognized at the same time

00:40:50.770 --> 00:40:53.330
and the target-action pairs will fire for both.

00:40:53.330 --> 00:40:58.670
So visually if I put two fingers down
and start panning and pinching together,

00:40:58.670 --> 00:41:01.760
it's entirely possible to get action methods for both.

00:41:01.760 --> 00:41:04.990
So that's how Gesture Recognizers interact with each other.

00:41:04.990 --> 00:41:10.750
I want to emphasize that the default behavior is
usually what you want but you can tweak it a little.

00:41:10.750 --> 00:41:13.440
So let's move on to Hybrid event handling.

00:41:13.440 --> 00:41:18.610
What happens when you have Gesture Recognizers
interacting with raw Touch event delivery?

00:41:18.610 --> 00:41:22.670
You have an app say that's been written
before the advent of Gesture Recognizers.

00:41:22.670 --> 00:41:23.620
Good for you.

00:41:23.620 --> 00:41:29.920
And are you worrying perhaps you need to rewrite
everything from the ground up to use this better tool?

00:41:29.920 --> 00:41:37.220
No absolutely not, the API was designed to mix
and match with responder delivery of Touch events.

00:41:37.220 --> 00:41:41.280
And as a matter of fact, you can easily add
Gesture Recognizers to your existing app.

00:41:41.280 --> 00:41:47.080
Take a particular view add a Tap Recognizer,
add a Swipe Recognizer it works great.

00:41:47.080 --> 00:41:52.460
There is one thing I should caution you about
though, if you haven't systematically been dealing

00:41:52.460 --> 00:41:59.150
with canceled touches you're going to start having a
lot of problems because when a Gesture Recognizer moves

00:41:59.150 --> 00:42:07.830
to the recognized state and its target-action pairs
are messaged, then it actually stops delivering events

00:42:07.830 --> 00:42:14.190
to the UIView when we do this, because we don't want two
distinct things handling the same input sequence that's

00:42:14.190 --> 00:42:15.910
generally not a good behavior.

00:42:15.910 --> 00:42:22.080
And so in order to stop delivery we
have to complete the state machine

00:42:22.080 --> 00:42:25.740
so we cancel those touches, so be ready for that.

00:42:25.740 --> 00:42:32.410
I also want to emphasize that Gesture Recognition is not
a replacement for raw event delivery, raw event handling.

00:42:32.410 --> 00:42:39.130
For one it builds upon and exposes the
UIResponder touch delivery methods in UITouch.

00:42:39.130 --> 00:42:44.110
But for another reason not everything fits into a gesture.

00:42:44.110 --> 00:42:46.050
Not everything is a gesture.

00:42:46.050 --> 00:42:48.150
You have free-form touch input in many cases.

00:42:48.150 --> 00:42:56.440
If I'm multi-tapping into my keyboard, if I'm taking a
piano and typing into it I want free-form touch input.

00:42:56.440 --> 00:43:02.140
So don't try to fit things that aren't
gestures into UIGestureRecognizers.

00:43:02.140 --> 00:43:08.310
So if you're curious about this, if you're curious about
how Gesture Recognizers interact with raw event delivery,

00:43:08.310 --> 00:43:13.100
if you're curious about how to subclass
Gesture Recognizers stay for the next session

00:43:13.100 --> 00:43:16.970
because Josh is going to talk all about that.

00:43:16.970 --> 00:43:31.030
Alright that's really it for the topics and we have one
last demo and Josh is going to come up here and offer it.

00:43:31.030 --> 00:43:32.020
[Applause]

00:43:32.020 --> 00:43:33.500
>> Alright thanks again Brad.

00:43:33.500 --> 00:43:41.280
So one more time we've got another app that I've
downloaded off of developer.apple.com sample code.

00:43:41.280 --> 00:43:42.430
This one you may have also seen.

00:43:42.430 --> 00:43:45.610
It's a bit older but it's the Touches sample app.

00:43:45.610 --> 00:43:53.500
It basically shows you how to track multiple touches in a
single view but manipulating multiple subviews while doing

00:43:53.500 --> 00:43:56.780
that tracking so we can grab one and drag it around.

00:43:56.780 --> 00:44:01.690
Or if we put two fingers down we can
actually grab two and drag them both around.

00:44:01.690 --> 00:44:08.170
But this was of course written using just raw Touch
events because that's all we had at that time.

00:44:08.170 --> 00:44:13.680
So let's, Oops, expand this out here
and take a look at what we were doing.

00:44:13.680 --> 00:44:19.770
So we had-- Here we are-- a view subclass
view subclass, MyView and in that view

00:44:19.770 --> 00:44:25.200
of course we implemented the touchesbegan, moved and
ended and canceled methods and we had a whole lot of code

00:44:25.200 --> 00:44:30.810
that was stashing off different touch locations
and origins and all this different stuff.

00:44:30.810 --> 00:44:36.010
We had helper methods to dispatch these Touch events, the
different subview in case we wanted to track different ones

00:44:36.010 --> 00:44:40.150
and we also had to disambiguate which views
we were trying to touch all on our own.

00:44:40.150 --> 00:44:42.810
So there's a lot of code in there
to handle that kind of stuff.

00:44:42.810 --> 00:44:47.310
And, Oops, let's add line numbers
back in here and our text editing.

00:44:47.310 --> 00:44:49.320
So we can see, Actually it's pretty small.

00:44:49.320 --> 00:44:54.370
You probably can't see there's actually 213 lines of
code in this file just to implement what we just looked

00:44:54.370 --> 00:44:57.200
at of dragging those single views around.

00:44:57.200 --> 00:45:00.520
So the first thing we'll do is of
course grab all that and delete it all.

00:45:00.520 --> 00:45:08.630
[Laughter] And we can even get rid of all these
defines we had up here and our helper methods

00:45:08.630 --> 00:45:10.600
because we don't need any of that anymore.

00:45:10.600 --> 00:45:16.620
Now we're starting out with 38 lines of code so we're kind
of back to almost an empty file and just a bit of stuff

00:45:16.620 --> 00:45:21.370
for cleaning up our memory in dealloc
We begin by adding a PanGestureRecognizer.

00:45:21.370 --> 00:45:27.030
But we actually want to be able to manipulate
all three these views and we want to be able

00:45:27.030 --> 00:45:29.390
to manipulate all three at the same time.

00:45:29.390 --> 00:45:35.510
So we'll use three different UIPanGestureRecognizers
and attach one to each of those subviews.

00:45:35.510 --> 00:45:38.830
But in order to do that I'm going to add
this one helper method so I don't have

00:45:38.830 --> 00:45:40.520
to duplicate the code all over the place.

00:45:40.520 --> 00:45:45.150
So I'm just going to addGestureRecognizerstoView
method and in that we're going to take

00:45:45.150 --> 00:45:48.570
in whatever view we want to attach the gestures to.

00:45:48.570 --> 00:45:53.860
So we'll allocate UIPanGestureRecognizer the
same way we did with the tap end with target Self

00:45:53.860 --> 00:45:58.420
and action will be PanPiece a new
selector we'll define in a minute.

00:45:58.420 --> 00:46:05.120
We're going to attach it to that view that we just passed
in so ViewAddGestureRecognizer PanGestureRecognizer

00:46:05.120 --> 00:46:08.490
and we don't have to hang on to it
again because the View is retaining it.

00:46:08.490 --> 00:46:11.530
So PanGestureRelease is all we have to there.

00:46:11.530 --> 00:46:15.520
Now because this view I know is being loaded
from a Nib I'm going to do the configuration

00:46:15.520 --> 00:46:18.850
of the Gesture Recognizers in the awakeFromNib method.

00:46:18.850 --> 00:46:24.630
So in awakeFromNib I will call AddGestureRecognizerstoView
for all three of those piece views.

00:46:24.630 --> 00:46:26.080
These were connections that were set up in IB.

00:46:26.080 --> 00:46:29.200
We didn't just do it now but it's
the same as the sample code.

00:46:29.200 --> 00:46:31.820
I wanted to change it as little as possible.

00:46:31.820 --> 00:46:34.620
So you can check it out for yourself.

00:46:34.620 --> 00:46:43.710
So now we've attached all these Gesture Recognizers so
next we actually want to implement that PanPiece method.

00:46:43.710 --> 00:46:48.500
So the strategy we're going to use
for panning the piece is very similar

00:46:48.500 --> 00:46:52.050
to what was being done in the original Touch handling code.

00:46:52.050 --> 00:46:56.530
Every view has a center point which is defined
in the coordinate space of that view's superview

00:46:56.530 --> 00:46:59.780
and that determines where in that view that subview appears.

00:46:59.780 --> 00:47:04.870
So we're just going to adjust the center point by
the amount that the user has panned their finger.

00:47:04.870 --> 00:47:07.770
So to start out with we'll figure
out which view we're trying to pan

00:47:07.770 --> 00:47:11.890
and because we've attached the GestureRecognizer
to each of the individual subviews we can just ask

00:47:11.890 --> 00:47:16.700
for the GestureRecognizer's view and
that's the thing we're trying to pan.

00:47:16.700 --> 00:47:20.990
Then we want to figure out when we want
to adjust the location and Brad talked

00:47:20.990 --> 00:47:24.600
about the different states that
a Gesture Recognizer may be in.

00:47:24.600 --> 00:47:29.230
So since we're a PanGestureRecognizer which
is continuous and we report changes over time,

00:47:29.230 --> 00:47:33.630
we want to adjust our location any time
we're in stateBegan or stateChanged.

00:47:33.630 --> 00:47:38.750
By the time we get to stateBegan, as Brad said, we
already know that we've ruled out any other possibilities.

00:47:38.750 --> 00:47:40.760
We're really trying to pan a piece.

00:47:40.760 --> 00:47:44.290
So in those two states we know we can update our location.

00:47:44.290 --> 00:47:49.060
In order to figure out what to update we want to
start out by knowing where we are now and we can get

00:47:49.060 --> 00:47:51.720
that by just pulling the center off the piece.

00:47:51.720 --> 00:47:55.940
So we'll get the piece's center and stash that in a
local CG point, which we're going to adjust in a second

00:47:55.940 --> 00:47:59.460
and in order-- and we want to know
how far we want to move it.

00:47:59.460 --> 00:48:03.880
Now normally you'd have to calculate that yourself by
figuring out the delta between the last touch location

00:48:03.880 --> 00:48:08.040
and the current touch location but
UIPanGestureRecognizer tracks it for you.

00:48:08.040 --> 00:48:13.100
So we can just ask the PanGestureRecognizer
for its translation in any particular view.

00:48:13.100 --> 00:48:18.020
And since the center is always in the coordinate space
of our View-SuperView we also want to get our translation

00:48:18.020 --> 00:48:20.390
in the coordinate space of our View-SuperView.

00:48:20.390 --> 00:48:23.650
So we'll get the GestureRecognizer
translation in View, piece in SuperView

00:48:23.650 --> 00:48:27.540
and that's the amount that we want to move our piece by.

00:48:27.540 --> 00:48:31.710
So knowing the center and how much we want to
move it we can adjust the center by that amount.

00:48:31.710 --> 00:48:37.250
So our new piece's center is CGPointMake the original
center X plus translation X and the same for the Y.

00:48:37.250 --> 00:48:41.210
Now this next part is a little bit odd.

00:48:41.210 --> 00:48:45.820
So all of the continuous Gesture Recognizers
that you'll find as part of the UIKit pan,

00:48:45.820 --> 00:48:51.260
pinch and rotate, they all report a cumulative offset.

00:48:51.260 --> 00:48:58.100
So TranslationInView normally is the total accumulated
translation for the entire duration of that gesture.

00:48:58.100 --> 00:49:01.690
So it's the translation from the original
begin point to the current location even

00:49:01.690 --> 00:49:04.790
if your action method has been fired 500 times.

00:49:04.790 --> 00:49:10.700
But as you just saw here, we're actually
adjusting our current center point by that delta.

00:49:10.700 --> 00:49:14.270
So it would actually shoot out from
under our finger if we kept doing this.

00:49:14.270 --> 00:49:18.890
But one nice thing about the property
TranslationInView and scale and rotation

00:49:18.890 --> 00:49:21.690
on the other two is that they are writable as well.

00:49:21.690 --> 00:49:27.560
So if we don't actually want a cumulative value
here, we can get deltas by just setting it back to 0

00:49:27.560 --> 00:49:30.280
after we've pulled out the current value.

00:49:30.280 --> 00:49:33.580
So in this case we'll call it
GestureRecognizerSetTranslation,

00:49:33.580 --> 00:49:37.690
CG point 0 in that same piece,
in that same SuperView rather,

00:49:37.690 --> 00:49:43.840
so that the next time our action method is called
we know it's going to be a delta from 0 instead

00:49:43.840 --> 00:49:47.500
of a total cumulative translation causing
it to shoot out from under our finger.

00:49:47.500 --> 00:49:53.650
An alternative is you could just stash away the original
point but there are cases where you may want to do that.

00:49:53.650 --> 00:49:58.920
But for purposes of our demo it means we don't
have to keep any additional state around.

00:49:58.920 --> 00:50:00.990
So that's actually all we have to do there.

00:50:00.990 --> 00:50:06.270
So if we now build and run you'll find that
we still have the exact same stuff we had.

00:50:06.270 --> 00:50:09.490
Uh oh, well we almost had it.

00:50:09.490 --> 00:50:12.140
So, this is something that's bitten people alive.

00:50:12.140 --> 00:50:16.610
So I wanted to actually make it by me so you would see it.

00:50:16.610 --> 00:50:20.270
You have probably hit this in your own code at some point.

00:50:20.270 --> 00:50:26.830
UIImageView is one of the few subclasses in
UIKit that has user action disabled by default.

00:50:26.830 --> 00:50:33.860
Now as Brad was telling us UIGestureRecognizer only
receives touches if its view is receiving touches.

00:50:33.860 --> 00:50:36.600
And since our user interaction is disabled

00:50:36.600 --> 00:50:41.250
on these ImageViews their Gesture Recognizers
aren't going to receive touches either.

00:50:41.250 --> 00:50:47.730
So, they are actually defined in our main window
Nib, just open that up and select, well maybe,

00:50:47.730 --> 00:50:53.260
this is going to be a much shorter demo
if I can't open Interface Builder,

00:50:55.010 --> 00:51:00.300
we'll open that up in there maybe this time.

00:51:00.300 --> 00:51:06.240
Oh man, what's going on?

00:51:06.240 --> 00:51:07.750
Hey there we go.

00:51:07.750 --> 00:51:09.340
Good call thanks.

00:51:09.340 --> 00:51:15.290
For some reason opening the XIB wasn't
working and opening the localization was.

00:51:15.290 --> 00:51:23.940
So anyway now we can select all three of these guys and go
over to our inspector and turn on UserAction and if we build

00:51:23.940 --> 00:51:28.050
and run now this time for sure
we can actually drag it around.

00:51:28.050 --> 00:51:34.190
Now one thing I didn't point out when I originally did this
was that when I started dragging the piece actually jumped

00:51:34.190 --> 00:51:39.380
to be underneath our cursor because we were directly
setting the center to the current touch location

00:51:39.380 --> 00:51:41.830
but that we're using deltas it's much more what we want.

00:51:41.830 --> 00:51:45.650
So actually if we're grabbing the corner it sticks
to the corner rather than jumping to the center.

00:51:45.650 --> 00:51:50.840
We can still do two at a time because Gesture
Recognizers that are not related to each other

00:51:50.840 --> 00:51:55.330
by a View hierarchy these are actually in
sibling views can recognize simultaneously.

00:51:55.330 --> 00:51:58.720
So these pans will continue to track
independent Views at the same time.

00:51:58.720 --> 00:52:07.700
And it would work for three fingers if I could do
3-fingers in the simulator but sadly no can do.

00:52:13.690 --> 00:52:15.430
[Applause] Alright so that's pretty cool right.

00:52:15.430 --> 00:52:16.880
There almost no code anymore.

00:52:16.880 --> 00:52:21.670
We've got just these 10 lines of code
that we added and deleted 213 lines.

00:52:21.670 --> 00:52:25.870
But now our design team has come to us and said, well
that's really cool I love that I can still do that

00:52:25.870 --> 00:52:28.730
and you made it better, why do I care about that.

00:52:28.730 --> 00:52:34.660
But what I really want is also to be able to pinch
in there to make them scale those views bigger.

00:52:34.660 --> 00:52:38.250
Now if you had gone and already implemented
that touch tracking before and wanted to switch

00:52:38.250 --> 00:52:43.600
to adding this new pinch ability as well, it'd probably
start to get pretty complicated and you'd have to figure

00:52:43.600 --> 00:52:49.570
out how to look at multiple touches and figure out which
views they were in and then from touches figure out how much

00:52:49.570 --> 00:52:53.510
to scale your view so you'd have to
do a bunch of math to calculate that.

00:52:53.510 --> 00:52:57.470
And it becomes pretty complex pretty quick and you end
with a big state machine that you're trying to track

00:52:57.470 --> 00:53:00.550
and a lot of additional state that you're keeping track of.

00:53:00.550 --> 00:53:03.540
But with UIGestureRecognizer it's actually quite easy.

00:53:03.540 --> 00:53:06.010
So we tell our design team, "Hey no problem.

00:53:06.010 --> 00:53:07.730
We'll get back to you in five minutes."

00:53:07.730 --> 00:53:14.800
And we allocate a UIPinchGestureRecognizer and end
at target:self selector:scalePiece add it to the View

00:53:14.800 --> 00:53:20.000
and release it, the same as everything else, but now
because we know that our design team is a little bit fickle

00:53:20.000 --> 00:53:24.370
and as soon as we implement this they're going to say I
want rotation too, we saw ahead and decide we're going

00:53:24.370 --> 00:53:30.530
to do the same thing already to save ourselves
some time so we add a UIRotationGestureRecognizer.

00:53:30.530 --> 00:53:33.990
So the action method for that will be rotatePiece.

00:53:33.990 --> 00:53:37.980
So we can come down here and add these
two new action methods below our PanPiece.

00:53:37.980 --> 00:53:42.910
So if we're going to add a ScalePiece method and this
is going to be implemented almost exactly the same way

00:53:42.910 --> 00:53:46.710
as our PanPiece method except instead of
adjusting the center point we're going

00:53:46.710 --> 00:53:49.820
to adjust the transform on the View.

00:53:49.820 --> 00:53:54.150
So we'll get out the current piece the same way
and check to see if we're in stateBegan or Changed

00:53:54.150 --> 00:53:57.670
so that we know we actually want to apply this.

00:53:57.670 --> 00:54:02.740
Now instead of getting center we're going to get the
transform off of the View, so PieceTransform and instead

00:54:02.740 --> 00:54:07.900
of getting translation from a PanGestureRecognizer
we'll get ScaleFromAPinchGestureRecognizer.

00:54:07.900 --> 00:54:13.030
And we want to scale our transform the current transform by
the scale that the GestureRecognizer has calculated for us.

00:54:13.030 --> 00:54:15.700
So we don't even have to look at the
touches and figure out how far they were

00:54:15.700 --> 00:54:17.930
or how much they moved or how to get a scale from that.

00:54:17.930 --> 00:54:19.500
It's already been done.

00:54:19.500 --> 00:54:23.700
So we'll adjust the pieces transformed
by using CGOutlineTransformScale

00:54:23.700 --> 00:54:29.360
and we'll scale the transform both horizontially and
vertically by the scale we got from the Gesture Recognizer.

00:54:29.360 --> 00:54:33.600
And as I said PanGestureRecognizer is the same as sorry,

00:54:33.600 --> 00:54:38.720
PinchGestureRecognizer is the same as
Pan in that it's a cumulative scale.

00:54:38.720 --> 00:54:46.880
So we really want deltas so we're going to go ahead and set
the scale back to 1, 1 would be the identity for a scale

00:54:46.880 --> 00:54:50.900
and we'll do the exact same thing for rotate
except instead of getting the scale out we're going

00:54:50.900 --> 00:54:55.050
to get the pieces transformed and get
the RotationGestureRecognizer as rotation

00:54:55.050 --> 00:54:59.170
and the identify value for rotation is 0.

00:54:59.170 --> 00:55:01.870
With no rotation it would be 0.

00:55:01.870 --> 00:55:05.060
And we'll have rotated with CGOutlineTransferAndRotate.

00:55:05.060 --> 00:55:10.030
So if we were to build and run now you'd find that
we actually could scale and rotate these pieces

00:55:10.030 --> 00:55:17.510
but we can't do them both at the same time because as Brad
said, they're mutually exclusive, if I could talk today,

00:55:17.510 --> 00:55:23.710
so we can do both but that's not really ideal
but there is a very quick solution to this.

00:55:23.710 --> 00:55:26.150
It's that-- long to say but quick to implement,

00:55:26.150 --> 00:55:31.960
gestureRecognizer:shouldRecognizeSimultaneouslyWithGestureRecognizer: method.

00:55:32.970 --> 00:55:34.840
[Laughter and applause] Oh well thank you.

00:55:34.840 --> 00:55:36.840
I've been practicing.

00:55:36.840 --> 00:55:41.280
So we're going to go and set the
PanGestureRecognizer's Delegate to ourself,

00:55:41.280 --> 00:55:46.310
set the PinchGestureRecognizer's Delegate to ourself and
set the RotationGestureRecognizer Delegate the same way

00:55:46.310 --> 00:55:49.430
and then we'll implement GestureRecognizer
should then recognize simultaneously

00:55:49.430 --> 00:55:54.080
with GestureRecognizer [Laughter] right
down here [Laughter] I had to say it fast

00:55:54.080 --> 00:55:57.740
because it could take the rest of the session otherwise.

00:55:57.740 --> 00:56:02.650
So what we're going to do with this one is by
default we want to keep the normal behavior.

00:56:02.650 --> 00:56:07.870
No two gestures recognized at the same time so
by default we're going to continue to return no,

00:56:07.870 --> 00:56:10.350
they should not recognize simultaneously.

00:56:10.350 --> 00:56:15.380
How you decide whether two gestures should
recognize at the same time is kind of up to you.

00:56:15.380 --> 00:56:18.770
In most cases you've probably stashed
pointers to the gesture so you can be explicit

00:56:18.770 --> 00:56:22.660
about saying these two specific
ones should recognize simultaneously

00:56:22.660 --> 00:56:28.180
but for simple implementation here we're just
going to say that any two gestures attached

00:56:28.180 --> 00:56:31.420
to the same view can recognize at the same time.

00:56:31.420 --> 00:56:37.630
So if the GestureRecognizer that we're being asked about
if its view is the same as the other GestureRecognizer

00:56:37.630 --> 00:56:40.970
that we're being asked to view, we'll
say yes they can recognize together,

00:56:40.970 --> 00:56:44.170
otherwise default behavior and nope they can't recognize.

00:56:44.170 --> 00:56:49.620
So with that just form five more lines of code,
build and run and now we can pinch, rotate,

00:56:49.620 --> 00:57:03.820
translate all at the same time and if we do in
two different views we can still move them around.

00:57:03.820 --> 00:57:08.320
[Applause] There is one last piece that I didn't go over
here and it's, I'm not going to get into too much detail

00:57:08.320 --> 00:57:14.320
about it, but we generally want to pinch
and rotate around the center of our fingers.

00:57:14.320 --> 00:57:18.520
And the rotations and scales when you apply
transference to views are applied around the center

00:57:18.520 --> 00:57:20.930
of the View not around the center of your fingers.

00:57:20.930 --> 00:57:24.910
So CALayer has a property called an anchor
point that allows you to adjust the location

00:57:24.910 --> 00:57:27.750
around which a scale and a rotation would be applied.

00:57:27.750 --> 00:57:32.480
I've written a helper method we won't look at it right now
because it's not directly related to Gesture Recognizers.

00:57:32.480 --> 00:57:37.530
But I'm going to adjust it by adjusting
anchor point to be between my fingers

00:57:37.530 --> 00:57:42.000
so that the pieces would actually remain attached
to my fingers as we were manipulating them.

00:57:42.000 --> 00:57:45.260
Oops, went a little too far in my setup things.

00:57:45.260 --> 00:57:52.970
So I'll show you that in just a second too, but now that
we've got all that working our design team has come back

00:57:52.970 --> 00:57:57.070
to us one more time and they've said, "Wow, that's
awesome you did it in five minutes, that's incredible.

00:57:57.070 --> 00:57:58.420
What can you do in ten?"

00:57:58.420 --> 00:58:04.920
[Laughter] Really we should have waited
a couple of days before we showed them.

00:58:04.920 --> 00:58:10.880
Now they want us to be able to pinch and rotate
and pan all of these views all together at one time

00:58:10.880 --> 00:58:13.390
if we're not in one view with both our fingers.

00:58:13.390 --> 00:58:18.840
So the whole collection of views all at once, so
now you wrote all this not using GestureRecognizer.

00:58:18.840 --> 00:58:21.790
You're like, "Aw geez, I have to
start over I don't know what to do."

00:58:21.790 --> 00:58:27.200
No problem with GestureRecognizer though,
we can do this with one line of code.

00:58:27.200 --> 00:58:33.640
So we're going to go back to the top and in our
awakeFormNib method, I added some warnings for you guys,

00:58:33.640 --> 00:58:38.530
I hadn't declared this as a GestureRecognizerDelegate
so don't leave those in your code when you check it in,

00:58:38.530 --> 00:58:41.300
it looks bad and it's embarrassing when you're on stage.

00:58:41.300 --> 00:58:47.070
So in our awakeFromNib method we had our
addGestureRecognizersToView helper method

00:58:47.070 --> 00:58:51.320
and we were adding them to the three pieces,
well, how about we add them to ourself as well.

00:58:51.320 --> 00:58:56.410
We are UIView subclass and these
ImageViews are subclasses of us.

00:58:56.410 --> 00:58:59.370
So add them to ourself and we're
the SuperView of these ImageViews.

00:58:59.370 --> 00:59:01.600
One line of code, let's build and run.

00:59:01.600 --> 00:59:09.670
Now we can pinch this guy, still rotate, we get a
finger outside, I can pinch and rotate all of them.

00:59:09.670 --> 00:59:20.230
[Laughter and applause] So I happened to have already
built this for an iPad, I thought I'd show you here.

00:59:20.230 --> 00:59:23.420
It's a little bit easier to see.

00:59:23.420 --> 00:59:29.220
We can now pinch this guy and we can pinch this guy and
we can pinch that guy and we can get all this fingers down

00:59:29.220 --> 00:59:36.450
and we can pinch all of them [Laughter]
and we can grab the whole set.

00:59:37.980 --> 00:59:39.730
So that's all I've got.

00:59:39.730 --> 00:59:41.090
Thanks. [Applause]

00:59:47.820 --> 00:59:48.990
>> Thanks Josh that's awesome.

00:59:48.990 --> 00:59:52.480
So just to recap Gesture Recognizers,
why you want to use them--

00:59:52.480 --> 00:59:56.640
less code to write, you can spend your
time handling gestures, not detecting them

00:59:56.640 --> 01:00:02.830
and most importantly you're going to get consistency with
the system-wide definition of gestures and how you use them.

01:00:02.830 --> 01:00:03.740
It's simple.

01:00:03.740 --> 01:00:08.850
You instantiate a Gesture Recognizer, you set your
target-action pairs, you configure it using delegate

01:00:08.850 --> 01:00:12.420
or subclass properties and you
attach it to a view and that's it.

01:00:12.420 --> 01:00:17.510
So if you want more information you can contact
our Evangelist, go online to docs or devforums.

01:00:17.510 --> 01:00:23.250
Stay for the next session if you want a subclass
UIGestureRecognizer or learn more about how

01:00:23.250 --> 01:00:27.220
to configure the Gesture Recognizers
interaction with normal touch delivery.

01:00:27.220 --> 01:00:27.750
Cool, Thank you.

