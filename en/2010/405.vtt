WEBVTT

00:00:07.330 --> 00:00:12.230
>> Kevin Calhoun: Hello, and welcome to
Session 405, "Discovering AVFoundation."

00:00:12.230 --> 00:00:18.430
Now and for the next sixty minutes you
will join me on a voyage of discovery.

00:00:18.430 --> 00:00:22.460
My name is Kevin Calhoun of Apple's
Media Systems Engineering Group,

00:00:22.460 --> 00:00:28.420
and we'll be discovering the vastly
expanded AVFoundation framework in iOS 4.

00:00:28.420 --> 00:00:33.860
We'll talk about why you would want to use this
framework and for what purpose; we'll talk in some depth

00:00:33.860 --> 00:00:40.730
about concepts underlying the use of Time Media that
inform the design of the APIs we'll be discussing

00:00:40.730 --> 00:00:46.310
in which you'll want to be familiar with to aid in your
adoption of the APIs and; of course, we'll talk specifically

00:00:46.310 --> 00:00:50.460
about tasks that you can accomplish with this API set.

00:00:50.460 --> 00:00:54.320
Now, where are we going in our
voyage of discovery this afternoon?

00:00:54.320 --> 00:00:58.320
We are going beneath the blue line.

00:00:58.320 --> 00:01:07.400
Underneath Media Player, underneath UIKit where
the pressures are and the media takes time,

00:01:07.400 --> 00:01:10.350
we are at the level of AVFoundation,
the Objective C framework

00:01:10.350 --> 00:01:14.170
which gives you a great degree of control over Time Media.

00:01:14.170 --> 00:01:20.290
We sit on top of frameworks that are familiar to you such
as Core Animation and Core Audio and also a framework

00:01:20.290 --> 00:01:23.890
which is newly public in iOS 4, Core Media.

00:01:23.890 --> 00:01:30.190
So that's where we're talking about in the
system today underneath the level of UI.

00:01:30.190 --> 00:01:36.070
Now there have been technologies on iPhone in the
past dating back to the original release and enhanced

00:01:36.070 --> 00:01:42.650
through iPhone OS 3.0 and even after that
are useful for Time Media operations.

00:01:42.650 --> 00:01:49.320
There are several very easy ways that you can use Time
Media in frameworks that sit atop the level of AVFoundation.

00:01:49.320 --> 00:01:55.720
For example, for playback the MediaPlayer
framework offers MPMoviePlayerController

00:01:55.720 --> 00:02:02.270
and MPMoviePlayerViewController well integrated
with UIKit successfully used by nearly every app

00:02:02.270 --> 00:02:05.530
on the platform that currently plays Time Media.

00:02:05.530 --> 00:02:13.240
In addition starting with iPhone OS 3.0, the browser
has offered support for the HTML5 video and audio tags.

00:02:13.240 --> 00:02:19.650
So if you have web-based content that you want to
integrate Time Media with, that's a great solution for you.

00:02:19.650 --> 00:02:24.310
Now I mentioned that AVFoundation
is vastly expanded in iOS 4.

00:02:24.310 --> 00:02:27.020
It initially was shipped with iPhone OS 3.0

00:02:27.020 --> 00:02:33.020
and in that version it offered a class called
AVAudioPlayer, which is useful for playing audio files.

00:02:33.020 --> 00:02:37.130
So, those are very easy ways to play
Time Media and may still be appropriate

00:02:37.130 --> 00:02:41.730
for your apps even now with the release of iOS 4.

00:02:41.730 --> 00:02:46.220
Similarly for Capture there are
solutions already extant on the platform.

00:02:46.220 --> 00:02:52.430
UIImagePickerController in UIKit supports
video capture as well as still image capture

00:02:52.430 --> 00:02:59.150
and AVFoundation starting iPhone S3.0 supports
AudioRecorder for recording audio files.

00:02:59.150 --> 00:03:06.640
So that's great stuff to be aware of and may be appropriate
for your app even after we survey AVFoundation together.

00:03:06.640 --> 00:03:14.200
So, the question is, why would you use AVFoundation and
iOS 4 if these other great technologies are available?

00:03:14.200 --> 00:03:19.510
The basic answer is we give you a much
larger measure of control over Time Media

00:03:19.510 --> 00:03:23.470
in AVFoundation and a lot more features as well.

00:03:23.470 --> 00:03:30.140
In particular if you need to inspect the contents of
a Time Media resource, you can get deep information.

00:03:30.140 --> 00:03:36.670
You can glean what media is available, what
metadata is present in a Time Media resource.

00:03:36.670 --> 00:03:43.460
If you need to play Time Media in ways that are more
sophisticated than you can with the other frameworks,

00:03:43.460 --> 00:03:47.260
in particular, if you want to implement
a totally custom user interface

00:03:47.260 --> 00:03:51.640
to control play back, you can do that with this API set.

00:03:51.640 --> 00:03:58.000
In addition if you wish to pull together media from
multiple sources such as iMovie does pull some video

00:03:58.000 --> 00:04:02.600
from this resource, some audio from
another resource, arrange it temporary,

00:04:02.600 --> 00:04:06.860
perform editing operations, this is the API set for you.

00:04:06.860 --> 00:04:14.390
If you want to take existing media resources either simple
ones such as simple Time Media files or complex resources

00:04:14.390 --> 00:04:18.450
such as compositions that you've edited
together and re-encode them in order

00:04:18.450 --> 00:04:22.420
to create new Time Media resources,
you can do that through this API set.

00:04:22.420 --> 00:04:28.150
And, finally, if you want full
control over the input devices

00:04:28.150 --> 00:04:32.930
that are present including the camera,
this API set has the features for you.

00:04:32.930 --> 00:04:36.850
So these are the five areas of functionality that
we're going to be talking about in this session

00:04:36.850 --> 00:04:39.890
and the two subsequent sessions so keep these five in mind.

00:04:39.890 --> 00:04:44.010
There's a bargain we're going to strike.

00:04:44.010 --> 00:04:51.820
We give you in this framework this vastly greater degree
of control over Time Media, but as you accept that power,

00:04:51.820 --> 00:04:59.090
that ability to control Time Media, you also accept
the responsibility for handling Time Media in ways

00:04:59.090 --> 00:05:01.940
that are appropriate for this type of content.

00:05:01.940 --> 00:05:09.330
So it's important to be aware of the challenges of
working with Time Media as you adopt these APIs.

00:05:09.330 --> 00:05:13.890
The most important point to make about Time
Media is an obvious one that it's intended

00:05:13.890 --> 00:05:18.190
to be processed, intended to be consumed incrementally.

00:05:18.190 --> 00:05:23.020
You see a sequence of video frames
or hear a sequence of audio samples.

00:05:23.020 --> 00:05:30.680
Time Media takes time and that point is fundamental to the
design of the APIs we're going to be talking about today.

00:05:30.680 --> 00:05:38.330
It will be necessary for you to code some forbearance
into your apps because Time Media takes time to process,

00:05:38.330 --> 00:05:45.580
not just to play but also to perform other operations
as well even operations as simple as inspection.

00:05:45.580 --> 00:05:52.580
You might not be surprised to learn that because Time Media
is intended to be represented over a period of time some

00:05:52.580 --> 00:05:59.540
of the formats in which it's delivered are not optimized to
provide summary information about the Time Media resource

00:05:59.540 --> 00:06:06.770
as a whole and as a result in order to get information about
these resources, for example, just asking a simple question

00:06:06.770 --> 00:06:11.000
like ("what's the duration of a resource?")
you may require a good deal of work

00:06:11.000 --> 00:06:14.420
to be done on your behalf to deliver the answer.

00:06:14.420 --> 00:06:22.990
So, Time Media takes time, but it doesn't monopolize
the device while it's performing an operation.

00:06:22.990 --> 00:06:28.310
You wish your apps to remain responsive to
your end user while these things are going on.

00:06:28.310 --> 00:06:35.170
So it's possible for the user to turn his or her attention
to some other task or for the device to handle an event

00:06:35.170 --> 00:06:41.950
such as an incoming phone call that changes the
circumstances of operation that you are enjoying at the time

00:06:41.950 --> 00:06:46.920
that you kicked off the operation that you're currently
performing, playback or re-encoding, for example.

00:06:46.920 --> 00:06:53.890
So it's necessary for the APIs to give you the
opportunity to respond to these changes in circumstance.

00:06:53.890 --> 00:06:59.380
Finally the last important point to make about
Time Media is that the variety of formats,

00:06:59.380 --> 00:07:07.160
the variety of delivery protocols is great and if
you wish to handle Time Media resources uniformly,

00:07:07.160 --> 00:07:12.850
it may be necessary for app to do a little bit of
additional work to take some extra steps in order

00:07:12.850 --> 00:07:17.110
to provide a uniform processing path
for the operations you have in mind.

00:07:17.110 --> 00:07:22.420
We'll get back to details about this when we
discuss playback a little bit later in the hour.

00:07:22.420 --> 00:07:24.100
Okay, but that's what to keep in mind.

00:07:24.100 --> 00:07:28.830
The five areas of functionality we offer in
AVFoundation and the challenges associated

00:07:28.830 --> 00:07:32.910
with Time Media that inform the API design.

00:07:32.910 --> 00:07:37.540
Let me give you an overview first
before we go into detail of the classes

00:07:37.540 --> 00:07:41.250
that we are making available to
you in AVFoundation in iOS 4.

00:07:41.250 --> 00:07:47.190
I mention AVFoundation first shift and
iPhone OS 3.0 and with that version

00:07:47.190 --> 00:07:53.030
of the framework there were audio-related classes
very useful, you'll still want to use these today.

00:07:53.030 --> 00:07:55.190
AVAudioSession is present.

00:07:55.190 --> 00:08:01.710
This is the class that you use in order to inform the
underlying audio system on the platform of the type

00:08:01.710 --> 00:08:04.110
of audio processing that you're performing.

00:08:04.110 --> 00:08:10.840
If you tell the audio subsystem what you're doing,
then it can arbitrate resources appropriately

00:08:10.840 --> 00:08:14.490
for what your app is doing and what
else is going on in the system.

00:08:14.490 --> 00:08:18.860
So, you're going to use this class in
connection with AVFoundation even in 4.0.

00:08:18.860 --> 00:08:27.910
Other classes in the audio category present in AVFoundation
mentioned earlier AVAudioPlayer and AVAudioRecorder,

00:08:27.910 --> 00:08:33.970
but now the expansion, the annex, the
annex, which is quite large in iOS 4

00:08:33.970 --> 00:08:38.300
in AVFoundation the five areas of
functionality we mentioned earlier.

00:08:38.300 --> 00:08:40.320
First, inspection.

00:08:40.320 --> 00:08:47.910
In order to provide uniform inspection of Time Media
resources, we offer a model object known as AVAsset,

00:08:47.910 --> 00:08:53.150
Audio Visual Asset, that allows you to get
information about the contents of a resource.

00:08:53.150 --> 00:09:00.270
Assets can contain multiple streams of media
each of which is represented by an AVAssetTrack.

00:09:00.270 --> 00:09:07.270
They also can contain collections of metadata that we
make available to you as a collection of AVMetadataItems;

00:09:07.270 --> 00:09:13.560
these are the basic inspection classes, but remember
the challenge I mentioned earlier it can take time

00:09:13.560 --> 00:09:18.840
to provide you with information about a resource
and so these classes implement a protocol known

00:09:18.840 --> 00:09:24.480
as AVAsynchronousKeyValueLoading that
extends key value coding to allow you

00:09:24.480 --> 00:09:28.960
to request the value of a property to be loaded on demand.

00:09:28.960 --> 00:09:31.930
We'll talk in great detail about how that works shortly.

00:09:31.930 --> 00:09:38.780
In addition, we allow you to represent resources as
still images, thumbnails, other types of visual previews,

00:09:38.780 --> 00:09:43.040
you would create those thumbnails and other still
image previews by means of AVAssetImageGenerator.

00:09:43.040 --> 00:09:49.190
Second area of functionality I mentioned is playback.

00:09:49.190 --> 00:09:53.410
Obviously to play Time Media, you
require a class, a controller class,

00:09:53.410 --> 00:09:57.500
that has basic play and pause types of methods.

00:09:57.500 --> 00:10:01.570
That class and this framework is
known as AVPlayer, Audiovisual Player.

00:10:01.570 --> 00:10:05.460
A strangely apt name if I do say so myself.

00:10:05.460 --> 00:10:08.880
AVPlayer plays AVPlayer items.

00:10:08.880 --> 00:10:13.780
Note that an asset is merely representation
of the contents of a Time Media resource,

00:10:13.780 --> 00:10:17.440
but does not itself carry presentation state.

00:10:17.440 --> 00:10:22.780
The presentation state for an asset is carried by
AVPlayerItem and similarly the presentation state

00:10:22.780 --> 00:10:26.200
of any asset track is carried by AVPlayerItemTrack.

00:10:26.200 --> 00:10:33.970
Now, at this level we do not have UI affordances, no views,
but it would be kind of pointless for us to allow you

00:10:33.970 --> 00:10:40.600
to play a video with no way to display it to the end
user so what we do have is a subclass of CA Layer,

00:10:40.600 --> 00:10:47.760
Core Animation Layer, known as AVPlayerLayer, which is
capable of displaying the visual output of a player.

00:10:47.760 --> 00:10:53.610
Core Animation, of course, is not only useful
for visual display, it's also useful for timing,

00:10:53.610 --> 00:11:00.600
for animations that are timed and so we also offer in our
little bag of tricks another subclass of CA Layer known

00:11:00.600 --> 00:11:07.680
as AVSynchronizedLayer, which is capable of
synchronizing a layer sub tree with a playback of an item.

00:11:07.680 --> 00:11:09.630
Very useful for synchronization.

00:11:09.630 --> 00:11:14.910
The third area of functionality, editing.

00:11:14.910 --> 00:11:20.810
I mentioned earlier that we have this
wonderful asset model that we use uniformly.

00:11:20.810 --> 00:11:26.820
It would be very nice to be able to extend that
asset model in order to describe the composition

00:11:26.820 --> 00:11:30.720
of media from multiple sources, multiple URLs.

00:11:30.720 --> 00:11:36.510
If I can do that with an asset, like any other asset
I would like that composition to have multiple tracks,

00:11:36.510 --> 00:11:42.000
perhaps multiple video tracks, multiple audio
tracks, and I would call those AVCompositionTracks

00:11:42.000 --> 00:11:48.800
and they would extend the track model by allowing me
to describe the sequence of media from multiple sources

00:11:48.800 --> 00:11:55.570
that a particular track can display, but it's not
sufficient just to be able to describe these compositions.

00:11:55.570 --> 00:11:57.720
I want to be able to create them.

00:11:57.720 --> 00:12:03.750
So, it would be nice to have mutable subclasses if
AVComposition and AVCompositionTrack that have methods

00:12:03.750 --> 00:12:06.930
for the insertion of media and other editing operations

00:12:06.930 --> 00:12:10.750
and that should give me enough to
be able to do temporal composition.

00:12:10.750 --> 00:12:15.850
It isn't quite enough to describe how
to display a temporal composition so to

00:12:15.850 --> 00:12:18.370
that we'll add a couple of additional classes.

00:12:18.370 --> 00:12:23.880
If I have multiple audio sources in my composition,
I want to describe the way they are mixed together.

00:12:23.880 --> 00:12:34.640
I might want to set their volumes relative to each
other or I might want to control the ramping of audio

00:12:34.640 --> 00:12:37.770
from one source down while another ramps up.

00:12:37.770 --> 00:12:42.450
The ability to describe that could be
available in a class known as AVAudioMix.

00:12:42.450 --> 00:12:47.480
Similarly, we can offer a means to describe
how video should be composed together.

00:12:47.480 --> 00:12:50.390
What's the front to back ordering of my video sources?

00:12:50.390 --> 00:12:52.560
What's the opacity of each of the layers?

00:12:52.560 --> 00:12:56.510
Perhaps I can ramp the opacity,
fade one down and another one up.

00:12:56.510 --> 00:12:57.990
That's AVVideoComposition.

00:12:57.990 --> 00:13:04.800
So, I have the ability to pull media
together from multiple sources and edit it.

00:13:04.800 --> 00:13:11.770
I would like to create new assets from existing ones
either complex asset like a composition or a simple one.

00:13:11.770 --> 00:13:18.790
Start with an asset, create an object known as an
AVExportSession that manages the process of export,

00:13:18.790 --> 00:13:24.750
which is going to take time so this is a controller
class that I can kick off describing the type of export

00:13:24.750 --> 00:13:30.240
that I want to perform by means of one or
another preset that's available in the framework.

00:13:30.240 --> 00:13:35.380
Once I have the export session configured,
I can run it, create the new asset,

00:13:35.380 --> 00:13:42.580
have it written out to an output URL,
and be told when the job is done.

00:13:42.580 --> 00:13:49.900
Last area of functionality in our high-level tour of
AVFoundation is pertaining to capture, input devices.

00:13:49.900 --> 00:13:55.330
I want to be able to survey the input devices
that are available in my current context.

00:13:55.330 --> 00:13:59.260
For that purpose, we can offer the AVCaptureDevice class.

00:13:59.260 --> 00:14:05.820
You can enumerate the capture devices that are available
by type, the audio devices, the cameras and so forth,

00:14:05.820 --> 00:14:08.870
and you can find out what features they offer as well

00:14:08.870 --> 00:14:13.410
to determine what features you
should make available in your app.

00:14:13.410 --> 00:14:19.450
Once you've chosen a capture device to use, you would
like to set up a capture session with the inputs

00:14:19.450 --> 00:14:24.070
to the capture sessions where is the media coming
from and to specify the outputs of the session,

00:14:24.070 --> 00:14:30.010
where is the media going, in order for you to process
media coming off of the input devices and it's helpful

00:14:30.010 --> 00:14:36.010
to have the option either to process the media in
your application, maybe you actually want to examine

00:14:36.010 --> 00:14:44.120
and process video frames you should be able to do that with
or without the option of recording that media to a file.

00:14:44.120 --> 00:14:49.280
Finally, in order to allow your end user to know
where the camera is pointing in your application,

00:14:49.280 --> 00:14:55.580
it would be helpful to have yet another subclass of CA
Layer, AVCaptureVideoPreviewLayer we might choose to call it

00:14:55.580 --> 00:15:03.130
if we're long winded, that allows you to display to your
user a preview of what the camera is currently looking at.

00:15:03.130 --> 00:15:04.410
So, there you go.

00:15:04.410 --> 00:15:06.430
We've just designed the AVFoundation Framework.

00:15:06.430 --> 00:15:08.510
Thank you for coming.

00:15:08.510 --> 00:15:15.040
[ Applause ]

00:15:15.040 --> 00:15:17.920
I will be splitting my proceeds
from this job with you all equally.

00:15:17.920 --> 00:15:19.610
Sign up in the lobby.

00:15:19.610 --> 00:15:27.380
All right so one thing I need to mention before we go into
greater detail about the use of these APIs I want to point

00:15:27.380 --> 00:15:31.980
out the fundamental framework that we
depend on in AVFoundation, Core Media.

00:15:31.980 --> 00:15:38.590
Down at this very deep level the pressures are so
great that times are represented as a rational value.

00:15:38.590 --> 00:15:40.700
That's pressure.

00:15:40.700 --> 00:15:46.010
The Core Media Framework that underlies
AVFoundation defines a number of primitives

00:15:46.010 --> 00:15:50.250
that you'll find as you survey the AVFoundation APIs.

00:15:50.250 --> 00:15:56.000
Essentially anything that starts, any type that starts
with a CM is defined in the Core Media Framework.

00:15:56.000 --> 00:15:59.460
The one that I want to point out to
you now is a representation of time,

00:15:59.460 --> 00:16:02.900
which I mentioned is a rational value known as CMTime.

00:16:02.900 --> 00:16:10.040
Have a look at the header file in CoreMediaCMTime.H
to survey the means for creating CM Times

00:16:10.040 --> 00:16:15.350
for performing arithmetic operations on
them, for comparing them and so forth.

00:16:15.350 --> 00:16:21.790
Similarly, CMTimeRange is another data structure
in Core Media that you'll want to be familiar with.

00:16:21.790 --> 00:16:26.020
The speakers who follow me this afternoon will
be highlighting more of the details of Core Media

00:16:26.020 --> 00:16:28.880
that you'll need to become familiar
with as you adopt the API set.

00:16:28.880 --> 00:16:30.650
That's a good start.

00:16:30.650 --> 00:16:36.460
So let's rise up from that very great depth so
that we can all breathe more easily, me at least,

00:16:36.460 --> 00:16:45.770
and talk in detail for the remainder of our hour
about two of the areas of functionality that we offer.

00:16:45.770 --> 00:16:51.980
We'll talk about inspection in AVFoundation,
how you find out about Time Media resources,

00:16:51.980 --> 00:16:56.710
and we'll talk about playback, how you play them and
that will carry us through to the end of the hour

00:16:56.710 --> 00:17:02.360
and the remaining three areas of functionality will
be covered over the remainder of the afternoon.

00:17:02.360 --> 00:17:04.220
So, how do you inspect?

00:17:04.220 --> 00:17:06.710
Well, obviously you want one of these AVAsset things.

00:17:06.710 --> 00:17:08.260
That's the model object.

00:17:08.260 --> 00:17:16.870
An AVAsset is the model for time-based resources that we use
uniformly that provides information about assets as a whole.

00:17:16.870 --> 00:17:19.210
What's the asset's duration?

00:17:19.210 --> 00:17:25.150
Also, you can provide presentation hints though
it doesn't carry presentation state itself,

00:17:25.150 --> 00:17:30.940
that's the job of APPlayerItem, AVAsset does
carry information about the way an asset likes

00:17:30.940 --> 00:17:34.200
to be displayed, for example, what's its natural size?

00:17:34.200 --> 00:17:44.930
Note that an asset is not constrained in any way in the
number and type of sequences of media that it can present.

00:17:44.930 --> 00:17:51.090
It can present one or more streams of audio, one
or more streams of video, and we design it this way

00:17:51.090 --> 00:17:58.360
so that we can apply this uniform model to any number, any
number of the variety of media formats that we support;

00:17:58.360 --> 00:18:02.950
the audio only ones, the video only ones and so forth.

00:18:02.950 --> 00:18:07.940
Here on the slide are some examples of formats
that we can represent by AVAsset and in addition,

00:18:07.940 --> 00:18:13.660
a couple of other objects available in the
OS that can work together with AVAsset.

00:18:13.660 --> 00:18:17.900
Objects from the MediaPlayer framework, for
example, and from the AssetsLibrary.framework.

00:18:17.900 --> 00:18:22.300
I'll give you details about that a little later.

00:18:22.300 --> 00:18:25.530
Now, if AVAsset can contain multiple sequences

00:18:25.530 --> 00:18:32.330
of media data how we represent them
each one is an instance of AVAssetTrack.

00:18:32.330 --> 00:18:35.540
Each track represents a sequence of uniform type.

00:18:35.540 --> 00:18:42.140
A track will be all video or all audio, for example,
and each track not only will have its own uniform type,

00:18:42.140 --> 00:18:46.100
it will also have its own set of format descriptions.

00:18:46.100 --> 00:18:52.800
The format descriptions tell you about the encoding of the
media; is it H264 video, for example, or something else.

00:18:52.800 --> 00:18:57.560
It's possible for there to be more than one
format description represented in a single track

00:18:57.560 --> 00:19:01.060
so the coding does not need to be
uniform across the whole thing.

00:19:01.060 --> 00:19:07.600
A track also has a timeline expressed in
terms of the timeline of its parent asset.

00:19:07.600 --> 00:19:13.160
A track doesn't need to start at time zero if its
parent asset nor does it need to play all the way

00:19:13.160 --> 00:19:16.930
through to the duration of its parent
asset, it can occupy any segment

00:19:16.930 --> 00:19:20.750
of the parent asset that's convenient for offering.

00:19:20.750 --> 00:19:24.700
Other information about tracks available via AVAssetTrack.

00:19:24.700 --> 00:19:32.210
Now a typical asset will contain a single audio track
and video track that are synchronized with each other,

00:19:32.210 --> 00:19:34.640
but as mentioned before, there's really no constraint.

00:19:34.640 --> 00:19:36.570
Any number of tracks is possible.

00:19:36.570 --> 00:19:42.420
Some of you are going to immediately rush out and author
assets with say seventeen closed-captioned tracks just

00:19:42.420 --> 00:19:47.800
to prove a point, and I say go
ahead; the model will support it.

00:19:47.800 --> 00:19:54.520
What use you might put it to other than
as a conversation piece to be discussed.

00:19:54.520 --> 00:20:01.680
Now, let's go through some specific workflow examples,
some code, some pseudocode that you might wish to write

00:20:01.680 --> 00:20:07.310
to inspect assets and here in the next several
slides I intend to give you the basic flavor

00:20:07.310 --> 00:20:09.620
of what it's like to work with this framework.

00:20:09.620 --> 00:20:14.390
Remember I mentioned that you need to
code some forbearance into your apps.

00:20:14.390 --> 00:20:17.300
Why? Because Time Media takes time.

00:20:17.300 --> 00:20:19.500
That's what I want you to take away from this session.

00:20:19.500 --> 00:20:24.390
I'll give you a very concrete example
of what that means in just a minute.

00:20:24.390 --> 00:20:34.010
To inspect an asset I will start by initializing an
AVURLAsset object, a concrete subclass of AVAsset

00:20:34.010 --> 00:20:42.770
that presents the Time Media model for any asset that can be
reference by URL, but note just because I have that instance

00:20:42.770 --> 00:20:49.020
of AVURLAsset in hand does not mean that
any work has been done on my behalf.

00:20:49.020 --> 00:20:57.930
Remember Time Media takes time, which has a corollary
applied to AVFoundation that initialization of an object

00:20:57.930 --> 00:21:05.310
in this framework does not guarantee suitability
or readiness for any particular purpose.

00:21:05.310 --> 00:21:11.560
Specifically once you have initialized
an AVURLAsset what is it ready to do?

00:21:11.560 --> 00:21:14.380
The answer is nothing yet.

00:21:14.380 --> 00:21:18.100
We have not examined the resource at all.

00:21:21.770 --> 00:21:27.140
We have not even attempted to find the host.

00:21:27.140 --> 00:21:33.240
Initialization of an AVURLAsset
from any URL will always succeed.

00:21:33.240 --> 00:21:38.820
So how do you find out what you need to know,
how you get us to do some work on your behalf?

00:21:38.820 --> 00:21:45.100
You use the AVAsynchronousKeyValueLoading protocol that
I mentioned earlier in order to tell the framework,

00:21:45.100 --> 00:21:52.420
in order to tell the asset, which values for its keys, its
properties that you wish to have loaded on your behalf.

00:21:52.420 --> 00:21:55.120
This protocol has two methods.

00:21:55.120 --> 00:22:01.240
First of all in order to find out whether any
particular value for a key such as duration

00:22:01.240 --> 00:22:07.440
or tracks is already available use
the method, statusOfValueForKeyError

00:22:07.440 --> 00:22:14.170
and this method will tell you whether the information
you seek has already been loaded on your behalf.

00:22:14.170 --> 00:22:19.420
At initialization times since we've done no
work, the status for virtually all the keys

00:22:19.420 --> 00:22:23.240
of AVAsset and AVAssetTrack will be unknown.

00:22:23.240 --> 00:22:30.060
You have to request the loading of a particular key in
order for the status to change to loading and subsequently

00:22:30.060 --> 00:22:32.750
to arrive at one of the three terminal statuses.

00:22:32.750 --> 00:22:39.100
Ideally, will arrive at the status loaded, which tells you
that, okay, now you can call the getter and get the value

00:22:39.100 --> 00:22:45.070
that you wish, the array of tracks, the
CM time for the duration, et cetera,

00:22:45.070 --> 00:22:47.780
but it's also possible for loading to fail.

00:22:47.780 --> 00:22:53.510
Remember we're doing nothing to vet the
URL when you initialize the AVURLAsset.

00:22:53.510 --> 00:22:58.690
It's only after you've requested some loading
that we may arrive at the decision that, well,

00:22:58.690 --> 00:23:04.250
the URL you reference is not a Time Media resource
at all or, oh, by the way the network's down.

00:23:04.250 --> 00:23:08.950
That failure will occur as a result of a request to load.

00:23:08.950 --> 00:23:16.890
All right so with no more suspense how do you request
the loading of a value, a key, on one of these classes?

00:23:16.890 --> 00:23:21.100
A value for a property, a declared
property in these classes.

00:23:21.100 --> 00:23:23.420
Use the other method in the protocol.

00:23:23.420 --> 00:23:29.490
Load values, plural, asynchronously, adverb,
and you have no idea how hard it was to work

00:23:29.490 --> 00:23:34.610
in an adverb into an API name, four keys.

00:23:34.610 --> 00:23:41.410
The idea here is that you decide what collection of values
you require for the operation that you're performing.

00:23:41.410 --> 00:23:46.450
Put them all together into an array, all of the
strings representing the names of the keys you want

00:23:46.450 --> 00:23:51.640
and present them all at once to the asset
via this method and then the asset will

00:23:51.640 --> 00:23:55.920
in turn do the work that's necessary
in order to do the loading.

00:23:55.920 --> 00:24:02.360
When all of the keys in the collection have
reached a terminal status, the block that you pass

00:24:02.360 --> 00:24:07.500
to load values asynchronously for keys will be
invoked and at that point you can test their status

00:24:07.500 --> 00:24:10.120
and then move on to do the appropriate thing.

00:24:10.120 --> 00:24:14.450
What does it look like in a code example?

00:24:14.450 --> 00:24:16.660
Here's how you put it all together.

00:24:16.660 --> 00:24:21.760
The first thing you do with a URL is to
initialize an instance of AVURLAsset,

00:24:21.760 --> 00:24:23.820
which at that point is not ready to tell you anything.

00:24:23.820 --> 00:24:25.510
It needs to do work.

00:24:25.510 --> 00:24:31.600
Then you would say here's the array of keys that
I require to be loaded, their values to be loaded,

00:24:31.600 --> 00:24:34.390
in order to perform the operation I'm interested in.

00:24:34.390 --> 00:24:39.830
In this particular case, I'm going to prepare an
asset for playback and what I need to load in order

00:24:39.830 --> 00:24:43.170
to play something back is its array of tracks.

00:24:43.170 --> 00:24:49.540
So, I'm going to tell the asset please load your tracks key
by invoking LoadValuesAsynchronouslyForKeys with the array

00:24:49.540 --> 00:24:55.230
that in this case contains just the one
key, and I'll supply a block that I wish

00:24:55.230 --> 00:24:57.570
to be called the net loading is complete.

00:24:57.570 --> 00:25:02.310
You can tell this is a block because
it starts with the funny hat.

00:25:02.310 --> 00:25:08.500
This particular block that you pass to this method
takes no parameters so the code that's executed

00:25:08.500 --> 00:25:11.830
when loading completes is all inside the braces there.

00:25:11.830 --> 00:25:17.140
The first thing that it does is that it checks
the status for the key I wish to have loaded;

00:25:17.140 --> 00:25:22.100
and according to the terminal state that was reached
if it's loaded, I want to update my user interface

00:25:22.100 --> 00:25:28.380
with this tracks information; if it failed to
load, I wish to report the error to the end user;

00:25:28.380 --> 00:25:34.560
or if I've canceled the loading of the values for
key zone and asset, I want to do some bookkeeping.

00:25:34.560 --> 00:25:40.940
So, that's basically what it would look like and this is
the way that you prepare assets for operations in your app,

00:25:40.940 --> 00:25:47.260
this is the means by which you code the
forbearance for the time that Time Media takes.

00:25:47.260 --> 00:25:48.540
So, let me review this really quickly.

00:25:48.540 --> 00:25:53.210
How do you inspect and load assets
in order to prepare them for use?

00:25:53.210 --> 00:25:56.820
You have information you want to find out.

00:25:56.820 --> 00:25:58.670
You know that it may take time.

00:25:58.670 --> 00:26:00.290
I'll give you a concrete example.

00:26:00.290 --> 00:26:06.730
Something as innocuous as an MP3 file can take an
enormous time in order to deliver just a very simple piece

00:26:06.730 --> 00:26:11.890
of information about the duration of
an MP3 file may require the parsing

00:26:11.890 --> 00:26:16.270
of every single audio packet in
the file in order to calculate.

00:26:16.270 --> 00:26:22.510
It's not necessarily the case, in other words, that MP3
files contain summary information about their contents.

00:26:22.510 --> 00:26:27.100
You do not want your user to have
to wait while that work goes on.

00:26:27.100 --> 00:26:35.130
You can ask for the work to be done synchronously,
but there are significant downsides to doing that.

00:26:35.130 --> 00:26:42.190
First of all, you risk having your app become unresponsive
to the user, which of course, is a total no-no.

00:26:42.190 --> 00:26:48.770
Users expect apps to respond to their control, but
there's an even worse consequence that's possible

00:26:48.770 --> 00:26:53.970
if you request these pieces of
information to be loaded synchronously.

00:26:53.970 --> 00:27:02.060
There's a watch dog on iOS 4 that watches interaction
with the Media Services available on the platform.

00:27:02.060 --> 00:27:09.060
If any one of the clients of Media Services
request an operation to be performed synchronously

00:27:09.060 --> 00:27:15.180
and that operation takes longer than a timeout value
that that watchdog manages, the watchdog will come along

00:27:15.180 --> 00:27:22.620
and kill the application that took all that time and this
may also have the side effect of causing media services

00:27:22.620 --> 00:27:27.810
that are in use by all of the entities
on the system to be reset.

00:27:27.810 --> 00:27:30.770
Don't let this happen to your app.

00:27:30.770 --> 00:27:32.260
What will this result in?

00:27:32.260 --> 00:27:37.300
Well, fewer stars in the app store that's for sure.

00:27:37.300 --> 00:27:39.560
How do you avoid this calamity?

00:27:39.560 --> 00:27:45.730
Come to www.C210 meet me in Presidio, and I will
tell you all about how to use the AV, oh, wait,

00:27:45.730 --> 00:27:48.590
I'm having one of those time shift things, right?

00:27:48.590 --> 00:27:57.630
Sorry. Use the protocol just described, load the
values for T's asynchronously and you will be good.

00:27:57.630 --> 00:28:00.440
Now, this duration thing, this troublesome duration thing.

00:28:00.440 --> 00:28:05.740
I told you how expensive it is sometimes
to calculate the duration of an MP3 file.

00:28:05.740 --> 00:28:07.510
Even if you do it right and you load

00:28:07.510 --> 00:28:13.600
that value asynchronously you're sitting there saying I
don't really need you to do all of that work on my behalf,

00:28:13.600 --> 00:28:19.670
I just want to play the darned thing, I don't need
you to find out the duration exactly in advance.

00:28:19.670 --> 00:28:21.140
Well, yes, that's actually true.

00:28:21.140 --> 00:28:28.580
It turns out that for duration, which is a special value
defined by AVAsset, it's usually sufficient particularly

00:28:28.580 --> 00:28:32.530
in playback scenarios to use an estimated value.

00:28:32.530 --> 00:28:37.610
You don't need to know exactly how long something
takes unless you're trying to coordinate something else

00:28:37.610 --> 00:28:41.140
with its playback, which is not typically the case.

00:28:41.140 --> 00:28:50.990
So, by default the behavior of AVURLAsset is to
provide enough accuracy for a playback scenario.

00:28:50.990 --> 00:28:58.180
Note that if the underlying format that stores the media
offers summary information about the timing and duration

00:28:58.180 --> 00:29:02.810
of the resource, the information that we
provide you will be completely accurate.

00:29:02.810 --> 00:29:08.310
For example, if the file is a QuickTime movie
file or an MPEG4 file, those things do contain

00:29:08.310 --> 00:29:13.730
that summary information, and we will give it to
you and you can find out if any particular instance

00:29:13.730 --> 00:29:20.820
of AVURLAsset provides precise duration
and timing by examining its eponymous key,

00:29:22.030 --> 00:29:28.820
but if you require precise duration and timing from every
asset that you're working with, if you're doing something

00:29:28.820 --> 00:29:35.960
that requires that degree of precision, you can request it
at initialization time by setting in the options dictionary

00:29:35.960 --> 00:29:42.750
when you initialize the AVURLAsset the key
AVURLAssetPreferPreciseDurationAndTimingKey

00:29:42.750 --> 00:29:48.040
and we will give you an instance of AVURLAsset
that will be accurate regardless of cost.

00:29:48.040 --> 00:29:55.950
Okay. So that is the fundamental interaction
that you will have with this framework,

00:29:55.950 --> 00:30:00.450
and you'll note as we discuss these classes
this afternoon that there are similar stages

00:30:00.450 --> 00:30:04.200
of operation with each of the chief classes.

00:30:04.200 --> 00:30:11.120
You initialize something, you prepare it for the purpose
that you want to use it for, you observe its status in order

00:30:11.120 --> 00:30:15.230
to determine whether it's ready
for that purpose then you move on.

00:30:15.230 --> 00:30:20.550
Playback not surprisingly is very
similar in behavior to inspection.

00:30:24.530 --> 00:30:29.100
I mentioned earlier that the chief class
for controlling playback is AVPlayer.

00:30:29.100 --> 00:30:35.600
It has the methods on it for controlling rate and
so forth that you would expect in such a class,

00:30:35.600 --> 00:30:41.870
but beyond control it's extremely rich in
the facilities that it provides to allow you

00:30:41.870 --> 00:30:45.450
to observe the presentation state, the playback state,

00:30:45.450 --> 00:30:50.110
as it changes so that you can synchronize
a UI to playback state, for example.

00:30:50.110 --> 00:30:57.180
I mentioned that the AVPlayer plays items, it has a
property known as its current item so you can find

00:30:57.180 --> 00:31:02.390
out what it's playing at any given
time, and the AVPlayer item

00:31:02.390 --> 00:31:07.670
as I mentioned earlier confers
presentation state upon an asset.

00:31:07.670 --> 00:31:15.130
It describes how an asset should be presented so it's
possible to play an asset with more than one player item.

00:31:15.130 --> 00:31:20.790
For example, in one context you may wish to play a
particular time segment of an asset with one instance

00:31:20.790 --> 00:31:26.770
of AVPlayer item and in another context play a
different AVPlayer item associated with the same asset

00:31:26.770 --> 00:31:29.180
to play a different time range of interest.

00:31:29.180 --> 00:31:37.550
You can initialize an AVPlayer item with an
existing asset that you have or directly from a URL.

00:31:37.550 --> 00:31:42.080
If you initialize it with a URL, the
AVPlayer item will prepare the instance

00:31:42.080 --> 00:31:45.870
of AVAsset for you that you can use for inspection.

00:31:47.480 --> 00:31:53.960
AVPlayer item is also the class that you use in
order to control time as playback progresses.

00:31:53.960 --> 00:31:59.110
This is what you'd use to seek, for example, or to step.

00:31:59.110 --> 00:32:04.650
In addition, AVPlayerItems have one or more
AVPlayerItem tracks that correspond with the tracks

00:32:04.650 --> 00:32:08.530
of the asset you're playing and those
have presentation state as well.

00:32:08.530 --> 00:32:13.950
In particular, whether a track
is enabled for playback or not.

00:32:13.950 --> 00:32:18.270
So, having given you the overview
of the player related classes,

00:32:18.270 --> 00:32:24.650
let's have a look at how you would code
up preparation for playback in your app.

00:32:24.650 --> 00:32:29.670
One of the challenges I mentioned right at
the start of our talk is that it's difficult

00:32:29.670 --> 00:32:35.620
or it may require a little extra work in
order for you to treat assets uniformly

00:32:35.620 --> 00:32:39.640
because of differences particularly in delivery protocol.

00:32:39.640 --> 00:32:44.950
There are two main classes of assets that
you need to be aware of for playback.

00:32:44.950 --> 00:32:47.610
The first one is a file-based asset.

00:32:47.610 --> 00:32:53.750
Essentially an asset that we have random access to
in order to read information out of its container.

00:32:53.750 --> 00:32:56.330
The second one is a stream-based asset.

00:32:56.330 --> 00:33:00.420
We have a lesser degree of control over
what we can read out of that asset.

00:33:00.420 --> 00:33:04.290
It's essentially being beamed to us by a server.

00:33:04.290 --> 00:33:08.980
There's a little bit of a difference in the way
that you set up playback of an asset depending

00:33:08.980 --> 00:33:12.170
on whether it's filed based or stream based.

00:33:12.170 --> 00:33:15.420
So, let's talk about the workflows for each and then talk

00:33:15.420 --> 00:33:18.840
about what you would do if you
don't know what type you have.

00:33:18.840 --> 00:33:25.880
To start with if you have a file-based asset, in other
words, something like a video from the camera role

00:33:25.880 --> 00:33:32.080
that was shot with the camera or an item from the iPod
library that you can access by the media library framework

00:33:32.080 --> 00:33:36.980
or even a file that resides in a remote HTTP server.

00:33:36.980 --> 00:33:41.730
Here's the workflow that you would use to
playback any one of those file-based assets.

00:33:41.730 --> 00:33:47.770
As I mentioned earlier, initialize an instance
of AVURLAsset with the asset of interest

00:33:47.770 --> 00:33:53.750
and then it is your responsibility in preparing
that asset for playback to load its tracks.

00:33:53.750 --> 00:33:56.500
The player is going to want to know
what media in there so go ahead

00:33:56.500 --> 00:34:03.310
and do the job using the AVAsychronousKeyValueLoading
protocol to load the tracks of that asset.

00:34:03.310 --> 00:34:12.780
If that succeeds, then you can go on to initialize an
AVPlayerItem with that asset and remember our correlary

00:34:12.780 --> 00:34:19.230
to our basic tenet, that Time Media takes time, that
initialization of an object does not guarantee readiness

00:34:19.230 --> 00:34:24.410
or suitability for any particular purpose,
I've just initialized an AVPlayerItem

00:34:24.410 --> 00:34:29.050
but when initialization completes,
it is not yet ready to play.

00:34:29.050 --> 00:34:37.330
What you'll want to do once you've created an AVPlayerItem
is observe its status key by a key value observing.

00:34:37.330 --> 00:34:44.590
Observing a status key you can be informed
of when the PlayerItem becomes ready to play

00:34:44.590 --> 00:34:52.010
and you initiate the process by which it becomes ready
to play by associating the AVPlayerItem with an AVPlayer.

00:34:52.010 --> 00:34:56.790
In this example, I'm initializing the
AVPlayer with the AVPlayerItem I created.

00:34:56.790 --> 00:35:01.390
That kicks off the process to prepare all of the
chutes and ladders to get the thing ready to play

00:35:01.390 --> 00:35:10.450
and via key value observing you'll soon discover that the
status of the player item has changed to ready to play.

00:35:10.450 --> 00:35:16.470
When that occurs, then it's possible for you to
survey the presentation state of the player item

00:35:16.470 --> 00:35:23.320
where tracks are enabled, for example, to choose a track
in this particular case to be disabled for playback,

00:35:23.320 --> 00:35:30.260
as an example, other customization of presentation state is
also possible, of course, but once you've prepared the item

00:35:30.260 --> 00:35:37.300
for playback with whatever customization that you
desire, then go ahead and tell the AVPlayer to play

00:35:37.300 --> 00:35:44.060
and that's essentially the workflow that
you follow to play a file-based asset.

00:35:46.310 --> 00:35:51.740
The second example as I mentioned
earlier would be for stream-based assets.

00:35:51.740 --> 00:35:57.700
As you know, iOS 4 supports the HTTP live stream protocol

00:35:57.700 --> 00:36:01.950
and HTTP live streams are essentially
a playback only technology.

00:36:01.950 --> 00:36:10.210
It is not possible for you to create an AVURLAsset
from an HTPP live stream URL from scratch.

00:36:10.210 --> 00:36:14.990
What you need to do if you fall into this category
you have an HTTP live stream that you wish

00:36:14.990 --> 00:36:22.350
to play is go directly to the player related classes.

00:36:22.350 --> 00:36:32.720
Start with an AVPlayerItem, initialize it with the URL for
your HTTP live stream, do not spill water on the hardware,

00:36:32.720 --> 00:36:38.840
you just weren't aware of the safety tips
you're going to receive at this session.

00:36:38.840 --> 00:36:42.000
I'm glad you're enjoying them.

00:36:42.000 --> 00:36:47.310
Associate the player item with an AVPlayer in order to
start the process of making that PlayerItem ready to play.

00:36:47.310 --> 00:36:49.780
Then a little magic happens.

00:36:49.780 --> 00:36:57.220
Once that PlayerItem becomes ready to play, the AVPlayerItem
will create on your behalf an AVAsset that you can use

00:36:57.220 --> 00:37:02.410
to inspect the contexts of that HTTP live stream
so you can find out what tracks are present.

00:37:02.410 --> 00:37:09.800
For example, and if necessary, you can customize the
presentation state as well, but presuming that you just want

00:37:09.800 --> 00:37:14.990
to play it then, of course, you can move on once
it's ready to play to tell the AVPlayer to play.

00:37:14.990 --> 00:37:25.280
As a side note, it is possible for you to take a shortcut
for HTTP live streams and simply initialize an instance

00:37:25.280 --> 00:37:31.360
of AVPlayer with a URL that you wish to play and the
AVPlayer will create on your behalf the AVPlayerItem

00:37:31.360 --> 00:37:33.780
and the whole chain of events will be kicked off for you.

00:37:33.780 --> 00:37:37.450
So, if you know that you're playing HTTP live streams,

00:37:37.450 --> 00:37:42.300
you can take this shortcut, but
here's how we put it all together.

00:37:42.300 --> 00:37:48.680
If you don't know in advance the type of resource
that you wish to play could be a file-based asset,

00:37:48.680 --> 00:37:52.920
it could be a stream-based one, here's what we recommend.

00:37:52.920 --> 00:37:58.800
Essentially, you have to concatenate the two workflows, and
we recommend that you start with the file-based workflow.

00:37:58.800 --> 00:38:01.830
Try the URL as a file-based asset.

00:38:01.830 --> 00:38:09.920
Create an AVURLAsset from that URL with that URL,
attempt to load its tracks key as described previously.

00:38:09.920 --> 00:38:14.160
If that succeeds, move on with
the file-based playback scenario.

00:38:14.160 --> 00:38:23.180
If it fails, it's possible that
URL is to a valid HTTP live stream.

00:38:23.180 --> 00:38:28.430
So, try it then by initializing an AVPlayerItem with the URL

00:38:28.430 --> 00:38:32.350
and move on with the stream-based
workflow as mentioned earlier.

00:38:32.350 --> 00:38:42.280
The two code paths converge, and you can treat them
uniformly once the Player Item becomes ready to play.

00:38:42.280 --> 00:38:47.270
All right so now you're preparing items for
playback, you've told the AVPlayerItem to play,

00:38:47.270 --> 00:38:51.790
how does your app stay in sync with time and control time?

00:38:51.790 --> 00:38:58.040
First of all AVPlayerItem as I mentioned earlier
is the class that provides control over time.

00:38:58.040 --> 00:38:59.620
Could I have a mic down for a second, please?

00:38:59.620 --> 00:39:06.370
I have a little housekeeping to take care of.

00:39:06.370 --> 00:39:08.750
Are we ready?

00:39:09.970 --> 00:39:11.660
Now? No. Okay.

00:39:11.660 --> 00:39:12.510
Sorry.

00:39:12.510 --> 00:39:16.770
[ Coughing ]

00:39:16.770 --> 00:39:21.030
[ Laughter ]

00:39:21.030 --> 00:39:23.110
>> Kevin Calhoun: Okay, thank you.

00:39:23.110 --> 00:39:29.350
That impromptu performance was
rehearsed endlessly for weeks at a time.

00:39:29.350 --> 00:39:33.200
[Laughter] Until it was perfected in San
Francisco, California, I'll skip that.

00:39:33.200 --> 00:39:34.920
So, control over time.

00:39:34.920 --> 00:39:41.560
Seek the time is the method that you use to move
in time within the time range of an AVPlayerItem.

00:39:41.560 --> 00:39:45.550
You should note that seek to time
is not necessarily precise.

00:39:45.550 --> 00:39:48.370
One of the things that you'll note
in the design of this framework is

00:39:48.370 --> 00:39:53.920
that we place a very high value
on responsiveness to the end user.

00:39:53.920 --> 00:40:01.370
So, seeking to time can be an extremely expensive
operation if you wish it to be for it to be precise.

00:40:01.370 --> 00:40:08.900
It can require to go to any specific time the decoding
of an arbitrary long sequence of dependent video frames.

00:40:08.900 --> 00:40:13.480
You don't necessarily need that work
to be done arriving at a time nearer

00:40:13.480 --> 00:40:18.160
to the time you wish is usually sufficient
and that's a behavior seek to time.

00:40:18.160 --> 00:40:24.100
It will give you good responsiveness and good
enough results for typical playback scenarios.

00:40:24.100 --> 00:40:30.170
However, if you need more precise control over
time as you seek around in an AVPlayerItem,

00:40:30.170 --> 00:40:37.600
you can use the variant method seek to time tolerance
before, tolerance after and these tolerances allow you

00:40:37.600 --> 00:40:43.430
to essentially to define the time range
within which you'll be satisfied for the time

00:40:43.430 --> 00:40:46.160
to arrive at when they seek operation is complete.

00:40:46.160 --> 00:40:51.430
You can set these tolerances to zero to
arrive at precisely the time that you desire,

00:40:51.430 --> 00:40:57.100
but you should note as I mentioned just earlier
this operation can be expensive and, in fact,

00:40:57.100 --> 00:41:02.770
it can be detrimental to the responsiveness of your
application to the end user so use with caution.

00:41:02.770 --> 00:41:07.260
Where does the media come from?

00:41:07.260 --> 00:41:10.000
The question I know we all ask each other.

00:41:10.000 --> 00:41:15.600
You can play file-based assets from
the camera roll as I mentioned earlier.

00:41:15.600 --> 00:41:16.700
How do you do that?

00:41:16.700 --> 00:41:21.700
The framework that you want to become familiar with in
order to play video that you can shoot with the camera

00:41:21.700 --> 00:41:25.810
on the device is the AssetsLibrary.framework.

00:41:25.810 --> 00:41:31.290
That framework has facilities that allow you to
survey the groups of assets that are available,

00:41:31.290 --> 00:41:37.000
essentially the camera rolls that have been
recorded, and within each group it allows you

00:41:37.000 --> 00:41:39.860
to enumerate the assets that are present.

00:41:39.860 --> 00:41:47.110
You can filter them by type such as just showing you the
videos, but once you have a specific instance of ALAsset

00:41:47.110 --> 00:41:53.710
from the AssetsLibrary.framework that you wish
to play, you obtain the URL from that ALAsset

00:41:53.710 --> 00:41:57.700
by asking it we missed this particular code
point up here so I'll tell you what it is.

00:41:57.700 --> 00:42:03.080
Use the method ALAsset default representation
to get us to fault representation

00:42:03.080 --> 00:42:07.820
and ask of its default representation for its URL.

00:42:07.820 --> 00:42:12.880
Once you have that URL in hand, you can
initialize an AVURLAsset and proceed

00:42:12.880 --> 00:42:17.220
with the file-based playback workflow as described earlier.

00:42:17.220 --> 00:42:24.980
Similarly, if you wish to play media from the iPod library,
the MediaPlayer framework has the facilities for you

00:42:24.980 --> 00:42:30.570
to allow you to query for any particular
piece of media of interest.

00:42:30.570 --> 00:42:38.150
Essentially you create an instance of MPMediaQuery and
resolve it against the MediaLibrary and it will give you one

00:42:38.150 --> 00:42:41.880
or more MPMediaItems that satisfy your query.

00:42:41.880 --> 00:42:46.870
Once you have the MPMediaItem in hand
that you wish to play via AVPlayer,

00:42:46.870 --> 00:42:53.750
you obtain its URL by requesting
its MPMediaItemPropertyAssetURL.

00:42:53.750 --> 00:42:57.510
That's the URL that you would use
to initialize an AVURLAsset

00:42:57.510 --> 00:43:01.750
and then you can proceed again with
the file-based playback workflow.

00:43:01.750 --> 00:43:09.230
So, now you have a source of media you know how to set up
playback and get it going, even the seek around and time,

00:43:09.230 --> 00:43:13.620
how do you keep your app in sync
with playback as it proceeds?

00:43:13.620 --> 00:43:18.400
Well, let's talk about the things that you can
observe and respond to while playback occurs.

00:43:18.400 --> 00:43:22.670
First of all you can track presentation state.

00:43:22.670 --> 00:43:26.950
A prime example here, for example, is the rate of playback.

00:43:26.950 --> 00:43:34.680
As it changes, you can use key-value observation in
order for your app to respond to changes to properties

00:43:34.680 --> 00:43:38.810
of playback both in AVPlayer and in AVPlayerItem.

00:43:38.810 --> 00:43:41.210
So key-value observing is your friend.

00:43:41.210 --> 00:43:48.580
Register for the observation of these keys and you'll
be able to discover not only when changes occur that you

00:43:48.580 --> 00:43:56.160
as the application initiates perhaps in response to
user input, but also and equally important changes

00:43:56.160 --> 00:44:02.080
that are initiated underneath you
by the framework or by the system.

00:44:02.080 --> 00:44:09.270
Well, what kinds of changes can occur in playback not
initiated by you the application in response to your user?

00:44:09.270 --> 00:44:16.820
For example, if you are playing visual media and
the user multitasks, which is out of your app,

00:44:16.820 --> 00:44:24.610
you'll observe a change in the rate because that playback
item, that AVPlayerItem will automatically be paused

00:44:24.610 --> 00:44:29.080
and you would observe that by key-value
observation of the rate key.

00:44:29.080 --> 00:44:36.090
Also, if you're playing remote media, you can observe
changes in the AVPlayerItemsProperties loaded time ranges

00:44:36.090 --> 00:44:44.450
and seekable time ranges, which will tell you what portions
of the timeline of the AVPlayerItem are currently available.

00:44:44.450 --> 00:44:50.120
Similarly if you are playing a HTTP live
stream that has alternate encodings,

00:44:50.120 --> 00:44:57.440
higher data rates for greater network bandwidth, lower
data rates for conditions that are not quite as promising,

00:44:57.440 --> 00:45:05.630
you can actually observe when the HTTP live stream
changes from one of the alternate encodings to another

00:45:05.630 --> 00:45:10.690
by observing the tracks key of AVPlayerItem
and as a switch occurs, you can see, ah-ha,

00:45:10.690 --> 00:45:14.660
now this AVPlayerItem is playing this encoding.

00:45:15.840 --> 00:45:21.730
One last thing you'll want to observe on AVPlayer and
AVPlayerItem already recommended as you set things

00:45:21.730 --> 00:45:25.130
up you wish to be observing their status keys.

00:45:25.130 --> 00:45:26.230
Are they ready to play?

00:45:26.230 --> 00:45:31.020
That's the first thing you'll need to know before you
kickoff playback, but it's possible for these objects

00:45:31.020 --> 00:45:38.130
to arrive at other interesting statuses as
well, in particular, if you or your neighbor

00:45:38.130 --> 00:45:44.300
or the guy three rows behind you writes one of those
applications that requests information synchronously

00:45:44.300 --> 00:45:52.780
of an AVAsset, that takes longer than the time out value and
his app is killed and media services are reset for everyone

00:45:52.780 --> 00:45:59.700
on the system after we have a moment to hang our heads
in shame, the first thing that you have to realize is

00:45:59.700 --> 00:46:05.350
that every other client of media services on
the system will be responsible for setting

00:46:05.350 --> 00:46:08.760
up their media operations all over again.

00:46:08.760 --> 00:46:10.180
How do you know that you have to do this?

00:46:10.180 --> 00:46:17.580
If someone has caused this calamity to occur, you
observe the status key on AVPlayer and AVPlayerItem.

00:46:17.580 --> 00:46:24.790
The status can change to a failure status and the
error key of either of those classes can report

00:46:24.790 --> 00:46:31.760
that media services were reset via the error code
of the NSError that's available from the error key.

00:46:31.760 --> 00:46:36.530
When you see that that has occurred, you know
oh, no, someone has done the wrong thing,

00:46:36.530 --> 00:46:43.460
media services have been reset, I need to create
new instances of my playback objects and so forth,

00:46:43.460 --> 00:46:47.610
put them into the state that I was
in before and then I can proceed.

00:46:51.040 --> 00:46:56.450
Now, if you are observing changes that you
initiate and you are also observing changes

00:46:56.450 --> 00:47:03.100
that the framework initiates underneath you, well, there
needs to be some way of serializing your registration

00:47:03.100 --> 00:47:08.740
and unregistration of interest and
notification with notifications in flight.

00:47:08.740 --> 00:47:13.830
We don't wish for you to have to deal with any
possible race conditions if you're trying to disengage

00:47:13.830 --> 00:47:18.560
from key-value observation while
there's a notification about to arrive.

00:47:18.560 --> 00:47:24.860
So, in order to avoid that, our recommendation for
iOS 4, for your key-value observation of AVPlayer

00:47:24.860 --> 00:47:31.310
and the other player related classes, is to register
for a key-value observation and unregister from it

00:47:31.310 --> 00:47:36.410
on the main thread and that will guarantee
a sensible serialization of registration

00:47:36.410 --> 00:47:39.930
and unregistration with notifications in flight.

00:47:39.930 --> 00:47:44.830
Now, note that we still very highly
value responsiveness to the end user.

00:47:44.830 --> 00:47:49.820
We're not actually going to perform any of the work
associated with these state changes on the main thread,

00:47:49.820 --> 00:47:53.460
it's only the notifications of those
changes that we deliver on that thread

00:47:53.460 --> 00:47:56.560
and that we recommend that you register and unregister on.

00:47:56.560 --> 00:48:04.550
You can also track the readiness
for visual display of a visual item.

00:48:04.550 --> 00:48:11.020
Perhaps you have an AVPlayer playing audio visual
content, and you want to know when the AVPlayer layer

00:48:11.020 --> 00:48:17.410
that you have setup to display the visual output of
the player is ready for display in your layer tree.

00:48:17.410 --> 00:48:23.510
You can observe the ready for display key
on AVPlayerLayer and when that becomes yes,

00:48:23.510 --> 00:48:28.910
you know that you can insert the AVPlayerLayer into
your layer tree and it has something ready to draw.

00:48:28.910 --> 00:48:35.260
This is particularly useful if you want to code a
Core Animation transition from the tree in a state

00:48:35.260 --> 00:48:39.700
in which it lacks the AVPlayerLayer to
display the visual output to one that does,

00:48:39.700 --> 00:48:45.020
and you can do quite a few interesting effects with this.

00:48:45.020 --> 00:48:48.650
You can also track the progression of time.

00:48:48.650 --> 00:48:53.870
Now because time progresses incrementally during playback,

00:48:53.870 --> 00:48:57.250
it's not something that you can
observe by a key value observing.

00:48:57.250 --> 00:48:58.700
It doesn't work for that model.

00:48:58.700 --> 00:49:03.540
So, we have a different model available to
you for tracking the progression of time.

00:49:03.540 --> 00:49:11.700
AVPlayer offers you the option of creating one or more
periodic time observers and you get your hands on one

00:49:11.700 --> 00:49:16.490
of these by using the
AVPlayerMethodAddPeriodicTimeObserverForInterval,

00:49:16.490 --> 00:49:23.440
you supply an interval at which you want to be invoked
as time progresses and the block that you supply

00:49:23.440 --> 00:49:28.250
to this method will be called at that interval
and that should allow you, for example,

00:49:28.250 --> 00:49:32.750
to keep a UI that's tracking the
current time in sync with playback.

00:49:32.750 --> 00:49:36.990
The block will also be called when time
jumps also when playback starts or stops.

00:49:36.990 --> 00:49:45.670
So, you'll have full information, full disclosure about the
progression of time if it's moving smoothly or if it jumps.

00:49:45.670 --> 00:49:51.420
Also if you're trying to perform some manual programmatic
synchronization of something going on in your app

00:49:51.420 --> 00:49:56.360
with playback, we offer a boundary time observer.

00:49:56.360 --> 00:50:04.290
You create one of these on AVPlayer, give it a list of
times of interest, an array of CMTimes stored in NS Values,

00:50:04.290 --> 00:50:10.380
and when any one of those times is
traversed, when it's crossed during playback,

00:50:10.380 --> 00:50:15.260
the block that you supply will be evoked
and you can respond appropriately according

00:50:15.260 --> 00:50:19.350
to the current time that has been reached at that point.

00:50:19.350 --> 00:50:25.970
Now note that if you do a lot of expensive operations
in one of these blocks in response to either a periodic

00:50:25.970 --> 00:50:32.360
or a boundary time observer, we don't guarantee
delivery of all of the invocations of the block.

00:50:32.360 --> 00:50:37.740
It's up to you, of course, to code your apps to fit in the
operations that you perform in connection with playback

00:50:37.740 --> 00:50:43.080
so that you don't swamp the CPU; that you can co-exist with
the Time Media operation that's going on at the same time.

00:50:43.080 --> 00:50:49.600
Finally, the last thing that you
can track as playback progresses is

00:50:49.600 --> 00:50:53.820
when an AVPlayerItem reaches its end time and stops.

00:50:53.820 --> 00:51:00.660
We offer a good old NS notification for that purpose
known as AVPlayerItemDidPlayToEndTimeNotification.

00:51:00.660 --> 00:51:08.730
You can listen for this, it will fire when the
AVPlayerItem has played all the way through to the end.

00:51:08.730 --> 00:51:11.090
All right so that's basically the playback classes.

00:51:11.090 --> 00:51:17.500
What you need to do in order to initiate playback, how you
can get media from the various sources available to you

00:51:17.500 --> 00:51:21.520
in order to play it and how you can
keep in sync with playback as it occurs.

00:51:21.520 --> 00:51:27.470
A couple of best practices to cover before we move on.

00:51:27.470 --> 00:51:33.200
How do you become a good citizen of the platform
now that you are taking control over Time Media?

00:51:33.200 --> 00:51:38.990
Well, use the AVAsynchronousKeyValueLoadingProtocol
described earlier, that's number one.

00:51:38.990 --> 00:51:44.700
Also, tell the audio subsystem the type of
audio processing that you're performing.

00:51:44.700 --> 00:51:51.510
To do that use AVAudioSession in AVFoundation, set the
category of audio processing that you are performing.

00:51:51.510 --> 00:51:55.040
If you're playing, tell it that your category is playback.

00:51:55.040 --> 00:52:01.410
This allows the audio subsystem on the device to
arbitrate resources, audio-related resources properly

00:52:01.410 --> 00:52:05.560
for the various applications that
are trying to make use of them.

00:52:05.560 --> 00:52:07.700
There's more that you can do with AVAudioSession.

00:52:07.700 --> 00:52:10.470
For example, you can use it to become aware of interruptions

00:52:10.470 --> 00:52:14.370
that may arise during playback
or other Time Media operations.

00:52:14.370 --> 00:52:22.970
More details about AVAudioSession are available in the core
audio sessions that I'll point to you in just a few minutes.

00:52:22.970 --> 00:52:26.700
Special multi-tasking note.

00:52:26.700 --> 00:52:31.960
You are already aware from other sessions that
you can create your application and register it

00:52:31.960 --> 00:52:37.380
to get processing time in the background
and even to play audio in the background.

00:52:37.380 --> 00:52:42.490
What I need to make clear to you is the specific
behavior that occurs when you're playing visual media

00:52:42.490 --> 00:52:47.000
and the user switches you from
the foreground to the background.

00:52:47.000 --> 00:52:51.680
When that happens with no intervention
necessary on your part,

00:52:51.680 --> 00:52:57.020
the playback of a visual item will
automatically be paused and its display

00:52:57.020 --> 00:53:03.410
in the layer tree will automatically be disengaged
and you need do nothing in order to accomplish this.

00:53:03.410 --> 00:53:08.070
That's they standard user interface for what should
happen when visual items are playing on the platform

00:53:08.070 --> 00:53:10.770
and the app is switched to the background.

00:53:10.770 --> 00:53:16.220
Now, if you setup your application to get processing
time in the background and play audio, you can,

00:53:16.220 --> 00:53:19.730
if it's appropriate for your content
and the workflow of your app,

00:53:19.730 --> 00:53:26.800
to continue playing the audio portion
of the AVPlayerItem in the background.

00:53:26.800 --> 00:53:32.540
One other note about multi-tasking referring
back to the earlier functionality of inspection,

00:53:32.540 --> 00:53:37.720
the loading the values of an asset in
order to inspect information about it,

00:53:37.720 --> 00:53:44.700
any loading that you have initiated will continue
to progress even if your app is in the background

00:53:44.700 --> 00:53:51.140
and if the loading has completed while your app is in the
background and you've set up your app to get processing time

00:53:51.140 --> 00:53:56.420
in the background, you will be notified at the completion
of that loading while you're still in the background

00:53:56.420 --> 00:54:03.650
so you can set yourself up on the basis of what an asset
contains even while you await return to the foreground.

00:54:03.650 --> 00:54:09.050
If your application is not setup to get processing time in
the background, then you'll be notified of the completion

00:54:09.050 --> 00:54:13.330
of any loading that has occurred in the
interim when you return to the foreground.

00:54:14.690 --> 00:54:19.930
Okay. Let's review what we've covered
during these 55 or so minutes so far.

00:54:19.930 --> 00:54:29.270
I've given you an overview of the AVFoundation framework
in iOS 4; told you the ways in which we've expanded it;

00:54:29.270 --> 00:54:32.960
and covered the main areas of functionality that it offers.

00:54:32.960 --> 00:54:39.790
I have also given you the flavor of the API and what
you need to do in your apps, to as I said earlier,

00:54:39.790 --> 00:54:47.510
code a little forbearance into your applications because
the processes that you apply to Time Media will take time.

00:54:47.510 --> 00:54:51.640
You want your apps to remain responsive
and good citizens of the platform.

00:54:51.640 --> 00:54:58.720
I've told you in detail how to use the
inspection-related classes; AVAsset, AVAssetTrack,

00:54:58.720 --> 00:55:02.570
and how to use the playback-related
classes AVPlayer, et cetera.

00:55:02.570 --> 00:55:09.210
Remember that an AVFoundation because most of the
operations that occur occur asynchronously together

00:55:09.210 --> 00:55:16.630
with other things going on in the platform we're making
full use of programming paradigms that permit this level

00:55:16.630 --> 00:55:20.080
of asynchronicity, this level of cooperation.

00:55:20.080 --> 00:55:27.050
In particular, we're extending key-value coding with
something we're calling asynchronous key-value loading

00:55:27.050 --> 00:55:32.470
in AVFoundation so that you can request
specifically the information you require

00:55:32.470 --> 00:55:39.240
and the framework can tailor the work that it does on
your behalf specifically to those things that you need,

00:55:39.240 --> 00:55:44.560
notify you of when the information is available
and allow you to proceed on with your tasks all

00:55:44.560 --> 00:55:47.640
without reducing responsiveness to the end user.

00:55:47.640 --> 00:55:53.480
Remember, even simple questions can take time to answer.

00:55:53.480 --> 00:56:00.400
We're using other very typical paradigms available in
objective C 2.0, our classes have declared properties,

00:56:00.400 --> 00:56:05.260
you call their getters after checking whether the
information is available in order to obtain values

00:56:05.260 --> 00:56:15.220
of interest to you, you use key value observing to note
changes that occur in state, you use blocks as call backs.

00:56:15.220 --> 00:56:18.140
Lots of information about blocks
available at the conference.

00:56:18.140 --> 00:56:23.320
I'll give you a couple of other sessions that you
can go to to learn more about block-base programming.

00:56:23.320 --> 00:56:28.960
In the context of AVFoundation, a block is simply
a piece of code that you wish to have evoked

00:56:28.960 --> 00:56:35.660
when a certain operation is complete or a certain state
is reached and the block is evoked at some time later

00:56:35.660 --> 00:56:41.250
when that occurs or perhaps in line if that state has
already been reached, and then that block is the code

00:56:41.250 --> 00:56:46.830
that you would supply that tells you what to do as
a result of the operation that you have completed.

00:56:46.830 --> 00:56:50.110
So, where else can you go for more information?

00:56:50.110 --> 00:56:54.710
Eryk Vershen is your Evangelist, your Media
Technologies Evangelist you can contact.

00:56:54.710 --> 00:56:59.620
You heard him talk about HTTP live streaming and
know that he's well informed on these topics.

00:56:59.620 --> 00:57:06.430
Documentation for the AVFoundation Framework is available
together with the other iPhone documentation for iOS 4.

00:57:06.430 --> 00:57:12.620
And, as usual, the Apple Developer Forums are
excellent sources of information and contacts.

00:57:12.620 --> 00:57:20.790
Other sessions for you to attend not just the sessions
that follow this one about editing and use of the camera.

00:57:20.790 --> 00:57:22.880
Also I mentioned a core audio session.

00:57:22.880 --> 00:57:28.420
Let's see tomorrow in Mission 10:15AM,
Fundamentals of Digital Audio.

00:57:28.420 --> 00:57:33.320
There are other audio-related sessions that you may wish to
attend to learn more about audio processing on the platform.

00:57:33.320 --> 00:57:40.470
There will be a repeat of this session hopefully
delivered with expanded lung power on Thursday.

00:57:40.470 --> 00:57:45.410
If you have a colleague who wished to attend this
session but needed to go to one of the others,

00:57:45.410 --> 00:57:49.600
you can tell him exactly what will be said there and,
in fact, you can let him in on all the jokes as well.

00:57:49.600 --> 00:57:51.650
I'll come up with new ones by then.

00:57:51.650 --> 00:57:57.410
A couple of sessions about block-based programming
are available to you as well to learn more

00:57:57.410 --> 00:58:01.030
about structuring your application around the use of blocks.

00:58:01.030 --> 00:58:02.390
Very powerful technology there.

00:58:02.390 --> 00:58:05.090
I recommend that you become familiar with it.

00:58:05.090 --> 00:58:11.140
So, to summarize, Time Media takes time.

00:58:11.140 --> 00:58:17.350
Initialization of an object does not guarantee
suitability or fitness for any particular purpose.

00:58:17.350 --> 00:58:25.000
Observe the status of those things that you're interested in
and as the status becomes ready, then you can move forward

00:58:25.000 --> 00:58:28.310
with the operation that you wish to undertake.

00:58:28.310 --> 00:58:31.950
So by following these best practices,
your apps can stay current

00:58:31.950 --> 00:58:35.930
and have the full media processing
power of the platform available to them.

00:58:35.930 --> 00:58:40.830
All of the things you've seen in the demos
in the last day and some including an iMovie

00:58:40.830 --> 00:58:46.810
and other applications you've seen demoed, you can
do in your applications as well with these APIs.

00:58:46.810 --> 00:58:51.490
Make sure your apps stay responsive
by using the asynchronous facilities.

00:58:51.490 --> 00:58:54.290
Stay in time by tracking time as it progresses

00:58:54.290 --> 00:59:00.210
and above all make sure your app stays
alive, don't let the watch dog get you.

00:59:00.210 --> 00:59:05.420
And with that please stay tuned for the remainder of
the AV's Foundation sessions later this afternoon.

00:59:05.420 --> 00:59:06.390
Thank you very much.

00:59:06.390 --> 00:59:08.390
[ Applause ]

