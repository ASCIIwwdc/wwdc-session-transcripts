WEBVTT

00:00:06.500 --> 00:00:14.490
>> Murray Jason: Good morning and welcome to the
third of 3 talks focusing on Audio here at WWDC 10.

00:00:14.490 --> 00:00:15.980
My name is Murray Jason.

00:00:15.980 --> 00:00:19.990
I am on the Developer Publications Team at Apple.

00:00:19.990 --> 00:00:27.670
And you folks are here today because your
applications have the most demanding audio needs.

00:00:27.670 --> 00:00:32.990
To satisfy those needs, you want to go
to the lowest layer of our audio stack.

00:00:32.990 --> 00:00:36.440
I'm here to help with that.

00:00:36.440 --> 00:00:41.260
So today I'll talk about 3 main things.

00:00:41.260 --> 00:00:43.440
First, I'll put Audio Units in context.

00:00:43.440 --> 00:00:48.460
There may be some of you who are not
entirely clear on when to use Audio Units,

00:00:48.460 --> 00:00:53.470
when to use one of our other audio
technologies, so I'll try to answer that for you.

00:00:53.470 --> 00:00:59.850
Second, we'll take a quick look at the audio
architecture of an iPhone app that uses audio units

00:00:59.850 --> 00:01:06.440
and that will give us a conceptual grounding
for looking at the code for building one.

00:01:06.440 --> 00:01:13.870
I'll spend most of my time today showing you how
to build 2 different types of audio unit apps.

00:01:13.870 --> 00:01:21.160
Now, these are simple prototype apps that I designed
to illustrate some important design principles

00:01:21.160 --> 00:01:25.710
and coding patterns that you can
use in your own applications.

00:01:27.270 --> 00:01:30.590
So, let's begin at the beginning and define audio units.

00:01:30.590 --> 00:01:33.260
An audio unit is an audio processing plug-in.

00:01:33.260 --> 00:01:38.550
It's the one type of audio plug-in available in iPhone OS.

00:01:38.550 --> 00:01:44.120
And the Audio Unit framework is the architecture,
the one architecture for audio plug-ins.

00:01:44.120 --> 00:01:49.460
One of its key features is that it provides
a flexible processing change facility

00:01:49.460 --> 00:01:54.560
that lets you string together audio units
in creative ways so you can do things

00:01:54.560 --> 00:01:57.380
that a single audio unit could not do on its own.

00:01:57.380 --> 00:02:03.750
One of the key value adds over
our other audio technologies is

00:02:03.750 --> 00:02:10.960
that Audio Units support real-time
input, output and simultaneous I/O.

00:02:10.960 --> 00:02:16.900
Being a low level API, they demand an informed
approach so you're in the right place.

00:02:16.900 --> 00:02:19.790
So, here's our audio stack.

00:02:19.790 --> 00:02:26.610
All audio technologies in iOS are
built on top of audio units

00:02:26.610 --> 00:02:30.340
so you're using them whether or
not you're using them directly.

00:02:30.340 --> 00:02:36.140
Most mobile application audio needs are
handled extremely well by the Objective-C layer

00:02:36.140 --> 00:02:39.220
of the stack, Media Player and AV Foundation.

00:02:39.220 --> 00:02:42.880
And if you were here for the earlier talks
today, you heard quite a bit about these.

00:02:42.880 --> 00:02:51.260
The Media Player framework gives you
access to the user's iPod Library

00:02:51.260 --> 00:02:58.450
and the AV Foundation framework provides a flexible and
powerful set of classes for playing and recording audio.

00:02:58.450 --> 00:03:04.890
And in iOS 4, it adds about 4 dozen
or so new classes focused on video

00:03:04.890 --> 00:03:10.050
but with a lot of very interesting audio capabilities.

00:03:10.050 --> 00:03:20.090
Now if you're doing a game and want to provide an immersive
3D sound environment, you'll use one of our C APIs, OpenAL.

00:03:20.090 --> 00:03:26.760
And if you want to work with audio
samples or do more advanced work,

00:03:26.760 --> 00:03:30.900
you can use one of the very powerful
opaque types in Audio Toolbox.

00:03:30.900 --> 00:03:35.120
The Audio Queue API which we've heard about
a little bit earlier connects to input

00:03:35.120 --> 00:03:38.210
or output hardware and gives you access to samples.

00:03:38.210 --> 00:03:43.460
Audio Converter let's you convert
to and from various formats.

00:03:43.460 --> 00:03:46.340
And Extended Audio File let's you write to and from disk.

00:03:46.340 --> 00:03:52.060
It's when you want to do more advanced work that you
don't want and don't want anything in between you

00:03:52.060 --> 00:03:55.890
and the audio units that you use them directly.

00:03:55.890 --> 00:04:02.990
So, with Audio Units, like I mentioned, you can
perform simultaneous I/O with very low latency.

00:04:02.990 --> 00:04:09.390
If you're doing a synthetic musical
instrument or an interactive musical game

00:04:09.390 --> 00:04:15.590
where responsiveness is very important, you can
use audio units as well and the third scenario

00:04:15.590 --> 00:04:21.100
where you'd pick them is if you want one
of their built-in features such as mixing

00:04:21.100 --> 00:04:23.910
or echo cancellation as Eric talked about.

00:04:23.910 --> 00:04:29.090
iOS gives you 4 sets of audio units listed here.

00:04:29.090 --> 00:04:35.550
We group them into effects, mixers,
I/O and format converters.

00:04:37.210 --> 00:04:42.360
Currently, in iOS, we provide 1 effect
unit and that's the iPod Equalizer.

00:04:42.360 --> 00:04:47.780
It's the same audio units that the iPod app itself uses.

00:04:47.780 --> 00:04:53.050
We provide 2 mixers that we also heard
about earlier if you were here this morning.

00:04:53.050 --> 00:04:58.810
The 3D Mixer is the audio unit upon which OpenAL is built.

00:04:58.810 --> 00:05:05.600
The Multichannel Mixer lets you combine any number of
mono or stereo streams into a single stereo output.

00:05:05.600 --> 00:05:07.890
There are 3 I/O units.

00:05:07.890 --> 00:05:16.450
The Remote I/O connects to input and output audio
hardware and provides format conversion for you.

00:05:16.450 --> 00:05:22.990
The Voice Processing I/O adds to
that acoustic echo cancellation.

00:05:22.990 --> 00:05:25.180
The Generic Output is a little bit different.

00:05:25.180 --> 00:05:32.170
It sends its output back to your application and all
of these I/O units make use of the Format Converter.

00:05:32.170 --> 00:05:36.370
The converter itself lets you convert
to and from linear PCM.

00:05:36.370 --> 00:05:41.260
Today I'm going to focus on these two.

00:05:41.260 --> 00:05:46.700
These are probably the most commonly used audio
units, I'll also say something about the equalizer.

00:05:46.700 --> 00:05:53.620
If most mobile application audio needs are handled well by
the Objective-C layer, where do we want to use audio units?

00:05:53.620 --> 00:05:59.360
Well, in a VoIP app, Voice over Internet
Protocol, you use our voice processing I/O unit.

00:05:59.360 --> 00:06:02.330
It is purpose built for that and it keeps getting better.

00:06:02.330 --> 00:06:10.640
In an interactive music app, for example, you may be
providing drum sounds and one or more melodic instruments

00:06:10.640 --> 00:06:17.010
and want to mix them together to a
stereo output, you'd use a mixer unit.

00:06:17.010 --> 00:06:24.690
For real-time audio I/O processing such as an app where
the users talks into the device and the voice comes

00:06:24.690 --> 00:06:27.370
out sounding different, you use a Remote I/O.

00:06:27.370 --> 00:06:31.980
So, that's a quick overview.

00:06:31.980 --> 00:06:38.000
Now, let's look at the architecture
of an app that uses audio units.

00:06:38.000 --> 00:06:46.760
In this part of my talk, we'll begin with a demo
of a "hello world" style app using an I/O unit.

00:06:46.760 --> 00:06:52.960
And then we'll look at the design of that app
starting with a black box and moving quickly

00:06:52.960 --> 00:06:58.270
through a functional description and
then the API pieces that make it work.

00:06:58.270 --> 00:07:01.020
So, I'd now like to invite up on to stage, Bill Stewart

00:07:01.020 --> 00:07:04.990
from the Core Audio Engineering Team
to show us the I/O host example.

00:07:04.990 --> 00:07:09.080
>> Bill Stewart: So, what I'm going to
show today is the first of 2 examples.

00:07:09.080 --> 00:07:14.620
I'll come back later and show you the second one and
then Murray's going to go through and look at the code

00:07:14.620 --> 00:07:18.860
that we have in order to write this audio unit.

00:07:18.860 --> 00:07:24.290
And what the program does is that
it's going to take a microphone input

00:07:24.290 --> 00:07:28.330
through this connector here which has a mic built in to it.

00:07:28.330 --> 00:07:37.370
Take it through the phone and then we're going to use a
mixer unit and we're not really using the mixer unit to mix

00:07:37.370 --> 00:07:46.420
because there's only one source here but we're going to use
a service on the mixer unit to pan the mono input from left

00:07:46.420 --> 00:07:51.220
to right and then we'll go out and
you'll hear the sound coming out.

00:07:51.220 --> 00:07:55.470
So, what I'm going to do now is launch
application and if I could have my mic turned off.

00:07:55.470 --> 00:08:00.840
So, here is me talking through the phone
with the feedback which is just great.

00:08:00.840 --> 00:08:08.970
And as you can see I have a pan control here and if I
pan this to the left of my finger works, here we go,

00:08:08.970 --> 00:08:11.730
then you'll hear my voice coming out the left speaker.

00:08:11.730 --> 00:08:17.620
Alternatively, if I go to right you'll hear my
voice coming out of the right speaker of course.

00:08:17.620 --> 00:08:21.830
[Whispering] Well, see, that's
got nothing to do with audio, so.

00:08:21.830 --> 00:08:23.560
[ Applause ]

00:08:23.560 --> 00:08:24.810
There you go.

00:08:24.810 --> 00:08:26.640
And then back into the middle.

00:08:26.640 --> 00:08:33.380
So, we'll get Murray to come back and we'll go through
the how to build this application and it's a good way

00:08:33.380 --> 00:08:36.130
to get yourself started with Audio Units.

00:08:36.130 --> 00:08:37.860
>> Murray Jason: Thanks, Bill.

00:08:37.860 --> 00:08:44.190
So, a black box sketch of what you
just saw looks something like this.

00:08:44.190 --> 00:08:51.610
Audio comes in from the microphone and goes out to output
hardware and in between, it goes through a stereo panner.

00:08:51.610 --> 00:08:55.470
So, what would a functional representation of this be?

00:08:55.470 --> 00:08:59.970
For the panner, we need something to perform the panning.

00:08:59.970 --> 00:09:03.970
We need something to handle input and
we need something to handle output.

00:09:03.970 --> 00:09:12.080
We also need or could at least use the help of an
object to let us manage and coordinate these 3 objects.

00:09:12.080 --> 00:09:20.200
So, as Bill mentioned, the panning feature
is handled by the Multichannel Mixer unit.

00:09:20.200 --> 00:09:28.300
The coordination feature that we need
is going to be handled by an opaque type

00:09:28.300 --> 00:09:35.690
from the Audio Toolbox layer called an AU
Graph and we call it an audio processing graph.

00:09:35.690 --> 00:09:39.100
So, what about input and output?

00:09:39.100 --> 00:09:46.190
Input and output have a special responsibility of
connecting to the input and output audio hardware.

00:09:46.190 --> 00:09:54.790
Whatever the user has selected for input, whatever they've
selected for output and conveying that to your application.

00:09:54.790 --> 00:10:03.990
Well, it turns out that the input and
output roles are handled by 2 parts

00:10:03.990 --> 00:10:09.520
of one object and that one object is the I/O Unit.

00:10:09.520 --> 00:10:17.660
The input element of the I/O Unit connects to input audio
hardware and sends it to your application, likewise,

00:10:17.660 --> 00:10:23.340
the output element takes audio from your
application and conveys it to the output hardware.

00:10:23.340 --> 00:10:27.570
So, before we get into the code, let's make
sure we're clear on just a few definitions.

00:10:27.570 --> 00:10:34.730
An audio unit as I've mentioned is an audio
processing plug-in that you find it at runtime.

00:10:34.730 --> 00:10:38.750
An audio unit node, now that's a term
that I haven't mentioned yet today,

00:10:38.750 --> 00:10:45.000
is an object that represents an audio unit
in the context of an audio processing graph.

00:10:45.000 --> 00:10:49.440
And the graph itself is the object
that manages the network of nodes.

00:10:49.440 --> 00:10:55.910
Now we'll look at the steps you take
to create the app that you just saw.

00:10:55.910 --> 00:10:58.290
We're going to use this checklist.

00:10:58.290 --> 00:11:01.810
It's a little bit long but we can refer
back to it to keep track of where we are.

00:11:01.810 --> 00:11:04.340
Let's just get into it.

00:11:04.340 --> 00:11:11.090
The first step in building this application is the same
as the first step in just about any audio application

00:11:11.090 --> 00:11:14.740
and that is to configure the audio
session, going through step by step.

00:11:14.740 --> 00:11:20.460
First we're going to declare the sample
rate that we want the hardware to use.

00:11:20.460 --> 00:11:25.950
This is because we want to have some
command over the audio quality in our app

00:11:25.950 --> 00:11:29.610
and we also want to avoid sample rate conversion.

00:11:29.610 --> 00:11:34.980
Sample rate conversion is quite CPU-intensive
especially if you're going for a high audio quality.

00:11:34.980 --> 00:11:43.050
So, we've just declared the value then we get
hold of a pointer to the audio session object

00:11:43.050 --> 00:11:46.150
and we use that in the rest of the calls.

00:11:46.150 --> 00:11:52.130
Here we call the setPreferredHardwareSampleRate
instance method

00:11:52.130 --> 00:11:56.350
of the audio session to let it know what we would like.

00:11:56.350 --> 00:12:01.410
The system may or may not be able to comply with
our request depending on what else is going on.

00:12:01.410 --> 00:12:06.070
We also set a category.

00:12:06.070 --> 00:12:11.290
This is a simultaneous I/O app so we
need the play and record category.

00:12:11.290 --> 00:12:16.680
We then asked the session to activate.

00:12:16.680 --> 00:12:22.930
At this point, it grants our request
for the sample rate if it can.

00:12:22.930 --> 00:12:31.210
In either case we ask the audio session object what the
actual hardware sample rate is after activation and we stash

00:12:31.210 --> 00:12:38.200
that away in an instance variable
so we can use it throughout our app.

00:12:38.200 --> 00:12:45.780
The next step is to specify the audio units that you want
from the system because remember your application's running

00:12:45.780 --> 00:12:47.820
but the audio units are not acquired yet.

00:12:47.820 --> 00:12:52.440
To do that, you make use of a struct
called AudioComponentDescription.

00:12:52.440 --> 00:13:02.650
You fill its fields with 3 codes and together these 3
codes uniquely identify the audio unit that you want.

00:13:02.650 --> 00:13:09.350
For the I/O unit, we're going to use output
as the type, Remote I/O as the subtype

00:13:09.350 --> 00:13:13.000
and all iPhone OS audio units are manufactured by Apple.

00:13:13.000 --> 00:13:19.790
On the Desktop, the story is somewhat different
where third party audio units are available as well.

00:13:19.790 --> 00:13:23.700
We do the same thing for our mixer unit.

00:13:23.700 --> 00:13:28.000
Declare the struct and then fill
its fields, mixer for the type,

00:13:28.000 --> 00:13:32.820
multichannel mixer for the subtype
and again Apple as the manufacturer.

00:13:32.820 --> 00:13:37.750
Now, we're ready to create the graph.

00:13:37.750 --> 00:13:50.320
Do that by declaring the graph and then instantiating it
by calling NewAUGraph, declare a couple of AU node types

00:13:50.320 --> 00:13:58.940
for the audio unit nodes and then the second parameter
in this call, the AUGraphAddNode call is a pointer

00:13:58.940 --> 00:14:02.500
to the description that you saw on the previous slide.

00:14:02.500 --> 00:14:07.020
This is our request to the system to
give us pointers to the audio units.

00:14:07.020 --> 00:14:15.440
Next, we're going to instantiate the audio units because we
can't work with them until we have real instances of them.

00:14:15.440 --> 00:14:22.580
Calling AUGraphOpen instantiates both the
graph and the audio units it contains.

00:14:22.580 --> 00:14:28.580
We then declare 2 audio unit types.

00:14:28.580 --> 00:14:38.800
One for the Remote I/O, one for the Multichannel
Mixer and then call AUGraphNodeInfo which is a call

00:14:38.800 --> 00:14:44.790
that lets us get pointers to our
instances of the I/O unit and the mixer.

00:14:44.790 --> 00:14:47.710
So, that was quite a lot of code.

00:14:47.710 --> 00:14:49.630
This is where we are.

00:14:49.630 --> 00:14:52.850
We've configured the audio session.

00:14:52.850 --> 00:14:57.500
In particular, we've established
the sample rate we're going to use.

00:14:57.500 --> 00:15:05.230
We specified the audio units we want and then
obtained references to instances of them.

00:15:05.230 --> 00:15:11.710
So, now we're ready to configure the audio
units and configuring means customizing them

00:15:11.710 --> 00:15:15.280
for the particular use we want in the app.

00:15:15.280 --> 00:15:19.250
To configure audio units, you need to
know about a particular characteristic

00:15:19.250 --> 00:15:22.060
of audio units and that is the audio unit property.

00:15:22.060 --> 00:15:31.800
An audio unit property is a key-value pair
and typically it does not change over time.

00:15:31.800 --> 00:15:37.660
Properties that you'll run into a lot when
working with audio units are stream format,

00:15:37.660 --> 00:15:41.050
the connection from one audio unit to another.

00:15:41.050 --> 00:15:45.160
And on a mixer unit, the number of it's input busses.

00:15:45.160 --> 00:15:54.630
In general, not always but in general, the time to set
properties is when an audio unit is not initialized,

00:15:54.630 --> 00:15:57.380
that means not in the state to play sound.

00:15:57.380 --> 00:16:03.360
A property key is a globally unique constant.

00:16:03.360 --> 00:16:06.850
A property value is a designated type.

00:16:06.850 --> 00:16:17.100
It can be just about anything with a particular read-write
access and a target scope or scopes, and by scope,

00:16:17.100 --> 00:16:20.080
I mean the part of the audio unit that it applies to.

00:16:20.080 --> 00:16:29.140
For example, here is the set input call back
properties description as you see it in our docs.

00:16:29.140 --> 00:16:34.510
And all of the properties are described
in, Audio Unit Properties Reference.

00:16:34.510 --> 00:16:41.770
So, now I want to focus on one particular property
and that is the property of stream formats.

00:16:41.770 --> 00:16:51.640
When you're working with audio at the individual sample
level, you need to do more than just specify the data type.

00:16:51.640 --> 00:16:58.850
A data type is not expressive enough to describe
what an audio sample value is and if you're here

00:16:58.850 --> 00:17:04.480
for the previous talk, you saw quite a
bit of information about why that's true.

00:17:04.480 --> 00:17:11.160
So, when working with audio units, you
need to be aware of some key things.

00:17:11.160 --> 00:17:19.910
The hardware itself has stream formats and it imposes those
stream formats on the outward facing sides of the I/O unit.

00:17:19.910 --> 00:17:24.170
You'll see a picture of that in a second.

00:17:24.170 --> 00:17:28.970
Your application specifies the stream format for itself.

00:17:28.970 --> 00:17:36.250
The stream format you're going to use and the I/O
units are capable of converting between those two.

00:17:36.250 --> 00:17:42.380
As James mentioned, you use the AudioStreamBasicDescription
to specify a stream format and it's a mouthful

00:17:42.380 --> 00:17:46.920
so we often call it, usually call it ASBD.

00:17:46.920 --> 00:17:53.210
And they're so ubiquitous, these structs, in the use of Core
Audio and in your work with audio units that it behooves you

00:17:53.210 --> 00:17:57.030
to become familiar with them and
even comfortable with using them.

00:17:57.030 --> 00:18:00.010
We have some resources for you there.

00:18:00.010 --> 00:18:07.030
First, you can take a look at Core Audio Data Types
Reference which describes all the fields of the struct.

00:18:07.030 --> 00:18:12.370
You can download and play with our
sample code that uses the ASBDs.

00:18:12.370 --> 00:18:19.980
And in particular, I recommend that you take a look at
a file that's included in your Xcode Tools Installation

00:18:19.980 --> 00:18:24.300
at this path, the CAStreamBasicDescription file.

00:18:24.300 --> 00:18:33.220
Now, this is a C++ file but it defines the gold
standard on the correct way to use an ASBD.

00:18:33.220 --> 00:18:36.630
So, let's look at where this happens in the app.

00:18:36.630 --> 00:18:41.030
As I mentioned the hardware imposes stream formats.

00:18:41.030 --> 00:18:48.520
The audio input hardware imposes a format on the
incoming side of the input element of the I/O unit.

00:18:48.520 --> 00:18:57.970
Likewise, the output audio hardware imposes its
stream format on the output of the output element.

00:18:57.970 --> 00:19:02.420
Now your application has some responsibilities as well.

00:19:02.420 --> 00:19:10.480
You specify a stream format on the application
side of the input element of the I/O unit

00:19:10.480 --> 00:19:14.360
and also wherever else is needed
and that's application dependent.

00:19:14.360 --> 00:19:18.090
In this case, we need to set it on the output of the mixer.

00:19:18.090 --> 00:19:24.530
So, this is the code you use to fill in the
fields of an audio stream basic description.

00:19:24.530 --> 00:19:32.490
You begin by specifying the data type
you'll use to represent each sample.

00:19:32.490 --> 00:19:38.190
The recommended type to use when working
with audio units is audio unit sample type.

00:19:38.190 --> 00:19:47.960
This is a defined type that's a cross platform
type on iOS devices that uses 8.24 format.

00:19:47.960 --> 00:19:51.560
On the Desktop it uses 32-bit float.

00:19:51.560 --> 00:19:56.880
And here we simply count the number of bytes in that sample,

00:19:56.880 --> 00:20:00.250
in that data type because we'll need
that to fill in the fields later.

00:20:00.250 --> 00:20:08.740
Second step is to declare your struct and to
explicitly initialize all its fields to zero.

00:20:08.740 --> 00:20:14.230
Now this is an important step and it ensures
that none of the fields contain garbage data,

00:20:14.230 --> 00:20:18.510
because if they contain garbage data then
the results will probably not be very happy.

00:20:18.510 --> 00:20:20.880
Then now we start filling in the struct.

00:20:20.880 --> 00:20:28.000
The first field that we fill in is the
FormatID and we're using linear PCM and why,

00:20:28.000 --> 00:20:34.430
because audio units use uncompressed
audio so linear PCM is the format to use.

00:20:34.430 --> 00:20:44.190
Next in the flags field, we refine that
format by setting a flag or set of flags.

00:20:44.190 --> 00:20:50.560
But what the flags do is specify the
particular layout of the bits in the sample.

00:20:50.560 --> 00:20:56.250
The choices that you need to make when filling
out an ASBD are is this integer or floating point,

00:20:56.250 --> 00:21:01.360
is this interleave data, non-interleave
data, is it big-endian or little-endian.

00:21:01.360 --> 00:21:06.450
So if you had to do that manually, it will be a
complicated process but in practice it's as simple

00:21:06.450 --> 00:21:12.760
as using this one meta flag AudioUnitCanonical
and it takes care of the work for you.

00:21:12.760 --> 00:21:23.500
The next 4 fields in the struct specify the organization
and meaning of the content of an individual value.

00:21:23.500 --> 00:21:30.010
These are the BytesPerPacket, BytesPerFrame,
FramesPerPacket and BitsPerChannel.

00:21:30.010 --> 00:21:32.530
For more detail you can look at our docs.

00:21:32.530 --> 00:21:41.040
If you're using mono audio which we are in this
example, you set the ChannelsPerFrame to 1,

00:21:41.040 --> 00:21:45.030
for stereo audio you set it to 2 and so on.

00:21:45.030 --> 00:21:49.970
And then finally, you specify a SampleRate for the stream.

00:21:49.970 --> 00:21:58.080
And we're using the graphSampleRate which is the variable
that's holding the hardware sample rate we obtained early

00:21:58.080 --> 00:21:59.860
on when setting up the audio session.

00:21:59.860 --> 00:22:03.080
Now we can configure the I/O unit by applying this format.

00:22:03.080 --> 00:22:06.670
We're going to use the InputElement of the audio unit,

00:22:06.670 --> 00:22:12.730
the one that connects to the audio input
hardware and that is element number 1.

00:22:12.730 --> 00:22:18.130
And a convenient mnemonic for that is to notice
that the letter I of the word input looks sort

00:22:18.130 --> 00:22:22.010
of like a 1 then we call AudioUnitSetProperty.

00:22:22.010 --> 00:22:26.750
This is the function you use to
set any property on any audio unit.

00:22:26.750 --> 00:22:32.410
We have the key and value highlighted
here, we're using the--

00:22:32.410 --> 00:22:35.940
we're applying the StreamFormat
property and using the inputStreamFormat

00:22:35.940 --> 00:22:39.530
that you saw defined on the previous slide.

00:22:39.530 --> 00:22:44.570
There's one more configuration we
need to do on the I/O unit and that is

00:22:44.570 --> 00:22:53.590
because by default I/O units have their
output enabled but their input disabled.

00:22:53.590 --> 00:22:58.420
We're doing simultaneous I/O so we need to enable input.

00:22:58.420 --> 00:23:05.490
Set a variable to a nonzero value and apply
it to the EnableIO property like this.

00:23:05.490 --> 00:23:08.510
Now we're ready to configure the mixer unit.

00:23:09.950 --> 00:23:15.230
We're only using one input bus because
we're not mixing multiple sounds together.

00:23:15.230 --> 00:23:17.890
We're just taking the sound from the microphone.

00:23:17.890 --> 00:23:23.560
So we specify the value of one and apply it
to the ElementCount property of the mixer.

00:23:23.560 --> 00:23:30.120
The second thing that we need to do for the
Multichannel Mixer is to set its output stream format.

00:23:30.120 --> 00:23:39.230
Now it turns out that the Multichannel
Mixer is preconfigured to use stereo output.

00:23:39.230 --> 00:23:42.080
All we really need to do is set the sample rate.

00:23:42.080 --> 00:23:49.580
So this is a bit of a convenience property by calling
AudioUnitSetProperty and specifying sample rate,

00:23:49.580 --> 00:23:55.200
we can apply the same sample rate that
we got from hardware and this ensures

00:23:55.200 --> 00:23:59.940
that the mixer has the same sample rate on input and output.

00:23:59.940 --> 00:24:04.360
That's very important because mixers
do not perform sample rate conversion.

00:24:04.360 --> 00:24:14.290
So the audio units are configured and the
next step is to connect them together.

00:24:14.290 --> 00:24:20.820
First, we need to connect the input side
of the I/O unit to the input of the mixer.

00:24:20.820 --> 00:24:23.090
We call AUGraphConnectNodeInput.

00:24:23.090 --> 00:24:28.400
And the semantic here is source to destination.

00:24:28.400 --> 00:24:32.540
The numbers indicate the bus number of the audio unit.

00:24:32.540 --> 00:24:41.810
So we're connecting element 1 or bus 1, those are synonyms
of the I/O node, that's the input part of the I/O unit

00:24:41.810 --> 00:24:44.160
to the input of the mixer with this call.

00:24:44.160 --> 00:24:54.150
Likewise, we call AUGraphConnectNodeInput again to connect
the output of the mixer to the output part of the I/O unit.

00:24:54.150 --> 00:24:57.230
Well, that's most of the code.

00:24:57.230 --> 00:25:02.210
All that's left is to provide the
user interface and to initialize

00:25:02.210 --> 00:25:06.180
and then start the processing graph
which starts audio moving.

00:25:06.180 --> 00:25:11.010
To provide the user interface we need one more--

00:25:11.010 --> 00:25:17.410
we need to understand one more characteristic of
audio units and that is the Audio Unit Parameter.

00:25:17.410 --> 00:25:27.570
So parameters like properties are key-value pairs but unlike
properties they're intended to be varied during processing.

00:25:27.570 --> 00:25:36.850
Parameters that you'll run into a lot and some of which
we'll use today are volume, muting, stereo panning position,

00:25:36.850 --> 00:25:42.860
and that way it works is that you create a user
interface to let the user control the parameters

00:25:42.860 --> 00:25:46.720
and then connect that user interface to the audio unit.

00:25:46.720 --> 00:25:56.140
A parameter key is an identifier
that is defined by the audio unit.

00:25:56.140 --> 00:26:02.530
The parameter value is always of
the same type, it's 32-bit float.

00:26:02.530 --> 00:26:09.210
And it's up to the audio unit to define the
meaning and permissible range for that value.

00:26:09.210 --> 00:26:12.750
Here's an example of what our documentation
looks like for a parameter

00:26:12.750 --> 00:26:16.990
and all of the parameters are described
in Audio Unit Parameters Reference.

00:26:16.990 --> 00:26:18.860
So let's build a user interface.

00:26:18.860 --> 00:26:25.760
We'll use a UI slider object from UIKit which is
a natural choice for doing something like panning.

00:26:25.760 --> 00:26:29.440
Here you see one with some labels around it.

00:26:29.440 --> 00:26:38.110
And we're going to apply the value of the slider thumb
position to the pan parameter of the Multichannel Mixer.

00:26:38.110 --> 00:26:45.730
I will just mention that this parameter, this pan parameter
for the Multichannel Mixer is a new feature of iOS 4.

00:26:45.730 --> 00:26:52.410
We'll save the value of the thumb into
a variable to apply to the audio unit.

00:26:52.410 --> 00:26:55.930
We'll call it here new pan position.

00:26:55.930 --> 00:27:02.090
And set it like this using the
AudioUnitSetParameter function call.

00:27:02.090 --> 00:27:07.010
Again it's a key-value semantic,
same way as with properties.

00:27:07.010 --> 00:27:14.810
Now to convey the position of a UI widget
on the screen into this C function,

00:27:14.810 --> 00:27:18.350
we wrap it in an IB action method like this.

00:27:18.350 --> 00:27:24.940
And that's all there is to creating a user
interface for an audio unit parameter.

00:27:24.940 --> 00:27:35.760
Next we initialize the graph and what that does is check
all of the connections and formats that you specified.

00:27:35.760 --> 00:27:38.120
Make sure that they're all valid.

00:27:38.120 --> 00:27:46.860
It conveys formats from source to destination in
some cases and if everything returns without error,

00:27:46.860 --> 00:27:50.640
you can start the audio processing
graph and audio starts moving.

00:27:50.640 --> 00:27:59.160
Sometime later you can-- when you're done with your
audio you can call AUGraphStop and audio stops.

00:27:59.160 --> 00:28:07.150
And that is most of the audio code that
you use to create the sample you saw.

00:28:07.150 --> 00:28:08.900
To see all of it, you can download it.

00:28:08.900 --> 00:28:14.480
It's available at the attendee site linked
from the detailed description for the session.

00:28:14.480 --> 00:28:22.780
Next we're going to look at a rather
different sort of audio unit application.

00:28:22.780 --> 00:28:30.310
And that is one that does not take audio from the microphone
but instead uses audio that the application generates.

00:28:30.310 --> 00:28:35.840
In this part of the talk, we'll again start with a demo
to see what we're aiming at and what this is about.

00:28:35.840 --> 00:28:43.100
We'll look at the architecture and then
we'll show you the code how to build it.

00:28:43.100 --> 00:28:47.620
So again, I would like to invite up on to
stage Bill Stewart from Core Audio Engineering.

00:28:47.620 --> 00:28:49.900
>> Bill Stewart: So I'm going to just launch this app.

00:28:49.900 --> 00:28:56.070
So what I'm going to show you here is the
application Murray will step through in a moment.

00:28:56.070 --> 00:29:05.850
What we're doing is sort of simulating a synthesizer so
if you've all seen a bunch of apps that are available

00:29:05.850 --> 00:29:11.590
that do synthesis, this is something like
the way that these apps are constructed.

00:29:11.590 --> 00:29:16.170
Now in this case we're going to
have 2 separate sources of sound.

00:29:16.170 --> 00:29:19.250
We're going to have a guitar sound and a beat sound.

00:29:19.250 --> 00:29:24.850
We don't provide in the example a guitar
synthesizer or drum machine or anything.

00:29:24.850 --> 00:29:27.880
So what we're doing is just using a very small file.

00:29:27.880 --> 00:29:36.180
We just read the file back into a buffer and that's a kind
of place holder for where your synthesizer code would be.

00:29:36.180 --> 00:29:41.060
So if I can just, [background music]
let's just start this playing.

00:29:41.060 --> 00:29:46.700
So I've got a global volume which
controls the volume of the entire mix here.

00:29:46.700 --> 00:29:54.160
I can mute different parts of the mix so I can
turn the guitar off or can turn the beats off.

00:29:54.160 --> 00:30:03.730
And these are just using audio unit parameters
that are defined on the input busses for the mixer.

00:30:03.730 --> 00:30:08.740
And then I can also control the relative
volumes of the 2 inputs that I have going

00:30:08.740 --> 00:30:12.180
into the mixer so the guitar, I can make it quieter.

00:30:12.180 --> 00:30:15.060
Or I can make the beat quieter.

00:30:15.060 --> 00:30:23.260
And that's basically using parameters on the
mixer to provide the mix into a controller.

00:30:23.260 --> 00:30:27.580
And then the Start and the Stop
button is just calling AUGraphStart

00:30:27.580 --> 00:30:31.750
and Stop in this case and that
stops the entire graph for you.

00:30:31.750 --> 00:30:34.990
So that's basically the demo and
then Murray is going to go through.

00:30:34.990 --> 00:30:40.700
He'll build on some of the knowledge that we covered in
the previous section and then go through the specific parts

00:30:40.700 --> 00:30:44.510
of this app and show you basically
how to build this kind of thing.

00:30:44.510 --> 00:30:48.450
So back to slides and back to Murray, thank you.

00:30:48.450 --> 00:30:48.770
>> Murray Jason: OK.

00:30:48.770 --> 00:30:53.480
So let's take a look at a picture of the app we just saw.

00:30:53.480 --> 00:31:01.350
So the first thing to notice here is that we're only using
the output piece of the I/O unit and the second thing

00:31:01.350 --> 00:31:07.920
to notice is that instead of taking audio from
a microphone, we're using callback functions.

00:31:07.920 --> 00:31:15.060
Those callback functions are attached to the
2 input busses of the multichannel mixer unit.

00:31:15.060 --> 00:31:21.040
So to build an app like this you begin in exactly
the same way as you would build the first demo

00:31:21.040 --> 00:31:25.830
that we saw, the I/O hosts simultaneous I/O app.

00:31:25.830 --> 00:31:33.010
You configure the audio session and in particular get hold
of the hardware sample rate and specify your category.

00:31:33.010 --> 00:31:39.730
Specify the audio units that you want
so you can ask the system for them.

00:31:39.730 --> 00:31:42.820
Construct your processing graph.

00:31:42.820 --> 00:31:46.450
Open it to instantiate everything.

00:31:46.450 --> 00:31:51.680
And that lets you obtain references to the
audio units that you want to configure.

00:31:51.680 --> 00:31:55.270
From here the story diverges a little bit.

00:31:55.270 --> 00:31:57.880
So let's look at that.

00:31:57.880 --> 00:32:02.890
In this case, we are actually mixing,
we have 2 different sounds.

00:32:02.890 --> 00:32:07.410
So the mixer needs 2 inputs and we need to set that.

00:32:07.410 --> 00:32:15.070
You may have noticed that in the drawing that the
beat sound is mono and the guitar sound is stereo.

00:32:15.070 --> 00:32:18.600
And that's to add a little interest to the story here.

00:32:18.600 --> 00:32:23.790
So we need to set a separate stream
format on each mixer input then we need

00:32:23.790 --> 00:32:26.110
to take responsibility for generating the audio.

00:32:26.110 --> 00:32:32.310
We do that by way of callback functions and need
to attach those callbacks to the mixer inputs.

00:32:32.310 --> 00:32:37.540
To set the mixer bus count to 2, we
use the same call AudioUnitSetProperty

00:32:37.540 --> 00:32:41.670
as before this time setting a value of 2 for the property.

00:32:41.670 --> 00:32:45.120
Now we need to set the stream formats.

00:32:45.120 --> 00:32:49.720
I'm not going to show you the audio
stream basic description setup for this.

00:32:49.720 --> 00:32:52.450
It's very similar to what you saw before.

00:32:52.450 --> 00:32:56.980
But we suppose that we have a stereo
format and a mono format defined.

00:32:56.980 --> 00:33:07.490
We're going to put the guitar sound on bus 0 of
the mixer and apply the stereo format to that bus.

00:33:07.490 --> 00:33:15.850
In the same way, we'll apply the-- we're
gonna send the beats sound to bus 1

00:33:15.850 --> 00:33:21.800
of the mixer and apply to it the mono stream format.

00:33:21.800 --> 00:33:28.380
We also need to ensure that the output
sample rate on the mixer is the same.

00:33:28.380 --> 00:33:33.530
That's a step that we also did in the previous app.

00:33:33.530 --> 00:33:38.260
There is one more property that's important
to set in this case and not in the other case.

00:33:38.260 --> 00:33:40.340
I'll try to explain that.

00:33:40.340 --> 00:33:43.760
This property is called MaximumFramesPerSlice.

00:33:43.760 --> 00:33:45.020
It's a got a bit of a funny name.

00:33:45.020 --> 00:33:47.790
So let's figure out what it means.

00:33:47.790 --> 00:33:54.060
Now the term slice in that name is a notion
we use to help understand what's going

00:33:54.060 --> 00:33:58.280
on when an audio unit is asked to provide audio.

00:33:58.280 --> 00:34:07.020
The system asks for audio in terms of render cycles
and the slice is the set of audio unit sample frames

00:34:07.020 --> 00:34:11.370
that is requested of an audio unit
in one of these render cycles.

00:34:11.370 --> 00:34:18.170
And a render cycle in turn is an
invocation of an audio units callback.

00:34:18.170 --> 00:34:25.010
Closely related to this idea is a hardware
property called I/O buffer duration.

00:34:25.010 --> 00:34:33.220
This is available both as a read and
write value through the audio session API

00:34:33.220 --> 00:34:37.390
and it has a default value but you can also set it.

00:34:37.390 --> 00:34:40.170
And it determines if set the slice size.

00:34:40.170 --> 00:34:51.220
If you do want to set it, you make a call like this using
the audio sessions set preferred I/O buffer duration call.

00:34:51.220 --> 00:34:56.380
Now there are a few slice sizes
that are very good to know about.

00:34:56.380 --> 00:35:03.990
First, the default size, the default
size is the size that is in play

00:35:03.990 --> 00:35:07.580
when your application is active and the screen is lit.

00:35:07.580 --> 00:35:11.890
And you have not set a specific I/O buffer duration.

00:35:11.890 --> 00:35:17.680
The system will ask for 1,024 frames of audio.

00:35:17.680 --> 00:35:23.600
That works out to about 0.02 seconds of
sound each time it calls the render callback.

00:35:23.600 --> 00:35:29.480
If the screen sleeps, the system knows
that there cannot be any user interaction.

00:35:29.480 --> 00:35:36.920
So to save power, it increases the frame count so
it has to call the render callback less frequently.

00:35:36.920 --> 00:35:42.280
And it uses a slice size of 4,096.

00:35:42.280 --> 00:35:44.450
That's about a tenth of a second.

00:35:44.450 --> 00:35:53.560
If you want to perform very low latency I/O, you
can set the frame count as low as about 200 frames

00:35:53.560 --> 00:35:57.960
by using the audio session property
that I showed you on the previous slide.

00:35:57.960 --> 00:36:02.280
Now that's a lot but there's a little bit more
about this property and that is when you need

00:36:02.280 --> 00:36:06.390
to set it and when you don't need to set it.

00:36:06.390 --> 00:36:11.620
You never need to set this property on an
I/O unit because I/O units are preconfigured

00:36:11.620 --> 00:36:15.840
to handle anything the system might request of them.

00:36:15.840 --> 00:36:21.760
All other audio units including mixer
units need this property explicitly set

00:36:21.760 --> 00:36:27.030
to handle the screen going dark
if there is not input active.

00:36:27.030 --> 00:36:35.100
If you're not using the input side of an I/O unit then
you do need to set the maximum frames per slice property,

00:36:35.100 --> 00:36:43.430
if you don't and the screen goes dark, the system will ask
for more samples than the audio unit is prepared to deliver.

00:36:43.430 --> 00:36:45.640
An error will be generated and your sound will stop.

00:36:45.640 --> 00:36:54.710
So to set it, this is as simple as using the
value of 4,096 again calling AudioUnitSetProperty,

00:36:54.710 --> 00:36:59.430
this time using the maximum frames
per slice key on the mixer.

00:36:59.430 --> 00:37:06.190
So the audio units are configured and now we need
to connect the sounds to the inputs of the mixer

00:37:06.190 --> 00:37:09.150
by attaching the render callback functions.

00:37:09.150 --> 00:37:17.660
Now audio unit render callback functions are
normal callbacks, they don't use the block syntax

00:37:17.660 --> 00:37:22.520
so they need a context connected
to them and the way we do that is

00:37:22.520 --> 00:37:26.870
by using a struct called the AURenderCallback struct.

00:37:26.870 --> 00:37:33.680
The struct includes a pointer to your callback
and a pointer to whatever context you want

00:37:33.680 --> 00:37:36.570
to give the callback for it to do its work.

00:37:36.570 --> 00:37:43.590
Here we set up one for the guitar sound
and then apply it to the guitar bus

00:37:43.590 --> 00:37:48.240
of the mixer input by calling AUGraphSetNodeInputCallback.

00:37:48.240 --> 00:37:54.350
We do the same thing for the beats sound, put
together a struct that points both to the callback

00:37:54.350 --> 00:38:00.640
and to the context it needs, maybe same or different
depending on how you want to write your code

00:38:00.640 --> 00:38:04.350
and attach it to the appropriate bus on the mixer.

00:38:04.350 --> 00:38:08.570
Now everything's hooked up but I haven't
said anything about the callbacks themselves.

00:38:08.570 --> 00:38:14.070
They're one of the most interesting
parts so let's look at them now.

00:38:14.070 --> 00:38:20.180
The role of a callback is to generate
or otherwise obtain the audio to play.

00:38:20.180 --> 00:38:23.800
In the demo that you saw, they
were simply grabbing some sound.

00:38:23.800 --> 00:38:30.130
They simply played some sounds out of a buffer
that took its sounds from some small files on disk.

00:38:30.130 --> 00:38:39.080
In your apps you can generate a synthetic
piano, farm animals, whatever you'd like to do.

00:38:39.080 --> 00:38:42.990
The callback then conveys that audio to an audio unit.

00:38:42.990 --> 00:38:48.980
The system invokes those callbacks as
needed when the output wants more audio.

00:38:48.980 --> 00:39:01.490
A key feature of callback functions that you must know from
the start is that they live on a real-time priority thread.

00:39:01.490 --> 00:39:05.750
That means all your work is done
in a time-constrained environment.

00:39:05.750 --> 00:39:13.080
Whatever happens inside the body of a render
callback must take this into consideration.

00:39:13.080 --> 00:39:18.150
You cannot take locks, you cannot allocate memory.

00:39:18.150 --> 00:39:24.930
If you miss the deadline for the next invocation you
get a gap in the sound, the trains left the station.

00:39:24.930 --> 00:39:28.920
This is what the callback prototype looks like.

00:39:28.920 --> 00:39:33.760
It's described in audio unit component services reference.

00:39:33.760 --> 00:39:36.120
Let's look at each of its parameters.

00:39:36.120 --> 00:39:43.150
The first parameter inRefCon is
the context that you associated

00:39:43.150 --> 00:39:47.340
with the callback when you attached it to the bus.

00:39:47.340 --> 00:39:52.640
It's whatever context the callback is
going to need to generate its sound.

00:39:52.640 --> 00:40:00.500
The second parameter ioActionFlags is
normally empty when your callback is invoked.

00:40:00.500 --> 00:40:09.810
However, if you're playing silence for example, if
you have a synthetic guitar and you're between notes,

00:40:09.810 --> 00:40:15.510
then you can give a hint to the audio
unit that there's no sound here.

00:40:15.510 --> 00:40:21.790
Nothing to process by oaring the value of this
parameter with the output is silenced flag.

00:40:21.790 --> 00:40:28.900
Now if you're doing this, you should
also, you must also memset the buffers

00:40:28.900 --> 00:40:34.520
in the last parameter the ioData parameter to 0.

00:40:34.520 --> 00:40:38.410
Some audio units need real silence
to do their work correctly.

00:40:38.410 --> 00:40:45.940
The next parameter inTimeStamp is the
time at which the callback was invoked.

00:40:45.940 --> 00:40:53.320
Now it has a field mSampleTime that is a sample counter.

00:40:53.320 --> 00:41:01.990
On every invocation, the value of that field increases by
the inNumberFrames parameter that we'll see in a moment.

00:41:01.990 --> 00:41:09.870
If you're doing a sequencer or a drum machine
you can use this for scheduling, this time stamp.

00:41:09.870 --> 00:41:19.280
BusNumber is simply the bus that called the
callback and each bus can have its own context.

00:41:19.280 --> 00:41:29.060
NumberFrames is the number of frames of sample data that
you are being requested to supply to the ioData parameter.

00:41:29.060 --> 00:41:35.130
And the ioData parameter is the centerpiece of the callback.

00:41:35.130 --> 00:41:41.290
It's what the callback needs to fill when called.

00:41:41.290 --> 00:41:48.900
ioData points to an audio buffer list struct,
you can read about that and how it's structured.

00:41:48.900 --> 00:41:54.230
We can take a quick look at how you might visualize it.

00:41:54.230 --> 00:42:02.050
If your callback is feeding a mono bus on a
mixer then you have a single buffer to fill.

00:42:02.050 --> 00:42:13.120
The size of that buffer will be inNumberFrames long and
the first sample will be at inTimeStamp.mSampleTime.

00:42:13.120 --> 00:42:17.760
That is-- that will be the frame
number for the first buffer.

00:42:17.760 --> 00:42:29.420
If you suppose that you're playing a piano sound and
the user just tapped the piano key then what you'll put

00:42:29.420 --> 00:42:36.040
into this buffer is the first .02 seconds
or so of the sound of the piano key.

00:42:36.040 --> 00:42:40.970
And the next time it's invoked,
the next .2 seconds and so on.

00:42:40.970 --> 00:42:49.460
If you're feeding a stereo bus you have 2 buffers
to fill and you can visualize that like this.

00:42:49.460 --> 00:42:58.590
So to create a user interface for this app, we do the
same thing, we use interface builder and use UIKit widgets

00:42:58.590 --> 00:43:03.580
and we connect them to the appropriate
parameters in the mixer unit.

00:43:03.580 --> 00:43:08.540
In this case, this sample uses the
volume parameter and applies it

00:43:08.540 --> 00:43:12.460
to 2 different places, the input scope and the output scope.

00:43:12.460 --> 00:43:21.120
The input scope for the input level on the
mixer, the output for the overall master volume.

00:43:21.120 --> 00:43:26.640
We're also making use of the enable
parameter to turn each channel on and off.

00:43:26.640 --> 00:43:35.940
The rest of the code is as we saw before, you initialize
the graph to set up all the connections and then call start.

00:43:35.940 --> 00:43:42.790
So at this point you've seen 2 different
applications, one that took audio from the microphone,

00:43:42.790 --> 00:43:45.720
one that took audio that your application generated.

00:43:45.720 --> 00:43:50.810
And we've used audio processing graphs but
we haven't really seen what they can do.

00:43:50.810 --> 00:43:52.400
So let's look at that now.

00:43:52.400 --> 00:43:59.320
First thing I'll talk about it is how audio processing
graphs add thread safety to the audio unit story.

00:43:59.320 --> 00:44:04.810
Then we'll look at the architecture
of a dynamic app and by dynamic,

00:44:04.810 --> 00:44:12.320
I mean one that the user can reconfigure while sound is
playing and then we'll see the code that makes that work.

00:44:12.320 --> 00:44:20.800
So starting with thread safety, audio
units on their own are not thread safe.

00:44:20.800 --> 00:44:27.440
While they are processing audio, you cannot do
any of these things, cannot reconfigure them,

00:44:27.440 --> 00:44:32.550
cannot play with connections, cannot
attach or remove callbacks.

00:44:32.550 --> 00:44:40.570
However, placed in the context of an audio
processing graph, you can specify the changes you want

00:44:40.570 --> 00:44:49.900
and then when you call AUGraphUpdate, all pending changes
are implemented in a thread-safe manner and sound continues.

00:44:49.900 --> 00:44:53.440
And there is no step 3.

00:44:53.440 --> 00:45:00.450
So AU graphs like many of our other APIs
use a sort of a to-do list metaphor.

00:45:00.450 --> 00:45:04.790
Now all audio unit graph calls can be called at anytime.

00:45:04.790 --> 00:45:15.740
But in typical use, things like connecting callbacks, adding
nodes to a graph and so on, are the ones that you'll do--

00:45:15.740 --> 00:45:18.840
the ones that you can do while audio is playing.

00:45:18.840 --> 00:45:29.040
And the semantic is that this task is added
to a pending list of things to implement.

00:45:29.040 --> 00:45:32.710
Audio continues without interruption.

00:45:32.710 --> 00:45:36.970
If you are not playing audio, if
the graph is not initialized

00:45:36.970 --> 00:45:41.440
and you call AUGraphInitialize then
all pending tasks are executed.

00:45:42.480 --> 00:45:51.670
If audio is playing and you call AUGraphUpdate
then any pending tasks are executed at that time.

00:45:51.670 --> 00:45:55.210
So here again is an architectural
diagram of the mixer host sample.

00:45:55.210 --> 00:46:02.660
Suppose here that the user is playing audio
and enjoying their guitar and beat sounds,

00:46:02.660 --> 00:46:07.750
but they want a little more punch in the
beat so they want to add an equalizer.

00:46:07.750 --> 00:46:12.890
The tasks to make that happen are the following.

00:46:12.890 --> 00:46:17.370
First you need to break the connection
between the beats and the mixer input.

00:46:17.370 --> 00:46:27.700
Then you need to add an EQ unit to the graph,
you need to configure it on both input and output

00:46:27.700 --> 00:46:35.540
and then make connections, all the
time without disrupting the audio.

00:46:35.540 --> 00:46:43.160
So to do that are these steps,
we'll just go through them quickly.

00:46:43.160 --> 00:46:49.370
To disconnect the beats callback
you call AUGraphDisconnectNodeInput.

00:46:49.370 --> 00:46:56.010
As I mentioned, that becomes a
pending task not executed yet.

00:46:56.010 --> 00:47:04.540
You then use an AudioComponentDescription
struct to specify the iPod EQ unit

00:47:04.540 --> 00:47:09.990
and add it to the graph by calling AUGraphAddNode.

00:47:09.990 --> 00:47:18.880
Now when a graph is already initialized, when you
call AUGraphAddNode, the node added to the graph,

00:47:18.880 --> 00:47:25.530
the action of adding the node to the
graph initializes its audio unit.

00:47:25.530 --> 00:47:31.050
So when this step is finished the
iPod EQ unit is initialized

00:47:31.050 --> 00:47:34.660
and you can obtain it by calling AUGraphNodeInfo.

00:47:34.660 --> 00:47:44.360
Next you're ready to configure and initialize
and if I said initialize I meant instantiate.

00:47:44.360 --> 00:47:51.060
So we have an instantiated iPod
EQ unit and a reference to it.

00:47:51.060 --> 00:47:54.130
We're now going to configure it and initialize it.

00:47:54.130 --> 00:47:56.450
That's a few steps so let's look at those.

00:47:56.450 --> 00:48:04.420
Now we need to set stream format but we have a
different scenario here and that is we're starting

00:48:04.420 --> 00:48:09.850
with a working application that
already has its stream format set.

00:48:09.850 --> 00:48:18.210
So rather than redo that work, we'll use the
AudioUnitGetProperty function call to get the stream format

00:48:18.210 --> 00:48:23.640
from the mixer input bus storing that
here in the beatsStreamFormat parameter.

00:48:25.010 --> 00:48:32.380
Then apply that format to both the input
and the output of the iPod EQ unit.

00:48:34.130 --> 00:48:39.910
Here sending it to the input scope and
here applying it to the output scope.

00:48:39.910 --> 00:48:47.110
Now we explicitly initialize the iPod EQ, that's
because this could be an expensive operation.

00:48:47.110 --> 00:48:51.380
The iPod EQ is not actually in line yet.

00:48:51.380 --> 00:48:57.160
So any work that it has to do can
be done before you do that.

00:48:57.160 --> 00:49:07.860
Call AudioUnitInitialize and we've now configured and
initialized the iPod EQ, it's ready to be connected.

00:49:07.860 --> 00:49:13.800
You call AUGraphConnectNodeInput to connect
the iPod EQ output to the mixer input

00:49:13.800 --> 00:49:19.730
and attach the beats callback using
AUGraphSetNodeInputCallback.

00:49:19.730 --> 00:49:26.960
So at this point we have a pending list of these 4 items.

00:49:26.960 --> 00:49:35.070
The highlighted ones you see on the screen and you
implement them in one fell swoop by calling AUGraphUpdate.

00:49:36.590 --> 00:49:43.640
From the users' perspective, they've tapped the button and
all of a sudden they have EQ available on the beats sound.

00:49:43.640 --> 00:49:57.630
So, to wrap up this part of the talk, audio
processing graphs always include exactly 1 audio unit.

00:49:57.630 --> 00:50:02.700
That's whether you're performing
input, output or simultaneous I/O.

00:50:03.710 --> 00:50:08.600
They add great value to the audio unit
story by adding thread safety and they do

00:50:08.600 --> 00:50:12.270
that by using a to-do list metaphor that we went through.

00:50:12.270 --> 00:50:19.810
Now for more information on anything I've talked
about or anything else about audio units or audio,

00:50:19.810 --> 00:50:25.390
you can please contact Allan Schaffer who is
our Graphics and Game Technologies Evangelist.

00:50:25.390 --> 00:50:30.090
Eryk Vershen who is our Media Technologies
Evangelist and take a look

00:50:30.090 --> 00:50:34.290
at the iPhone Dev Center for docs and sample code.

00:50:34.290 --> 00:50:41.800
In particular, please look at Audio Unit Hosting Guide
for iPhone OS which is a new book that you can link

00:50:41.800 --> 00:50:45.680
to from the detailed description of this session.

00:50:45.680 --> 00:50:52.130
It's in a preliminary state at the moment, we'll be
flushing it out and please use our developer forums.

00:50:52.130 --> 00:50:56.340
In addition to these, also please use bugreport.apple.com.

00:50:56.340 --> 00:51:00.400
Tell us where we can do better
in both our APIs and in our docs.

00:51:00.400 --> 00:51:10.030
So, in summary, use audio units when you
need real-time high performance audio.

00:51:10.030 --> 00:51:19.000
Use I/O units to gain access to hardware, use properties
to configure them and parameters to control them.

00:51:19.000 --> 00:51:25.740
And make sure you understand the lifecycle of an
audio unit which includes access, instantiation,

00:51:25.740 --> 00:51:29.490
configuration, initialization and then rendering.

00:51:29.490 --> 00:51:34.520
Render callbacks let you send your
own audio into an audio unit

00:51:34.520 --> 00:51:40.180
and audio processing graphs let you manage
audio units dynamically while sound is playing.

00:51:40.180 --> 00:51:41.630
Thank you very much for your attention.

