WEBVTT

00:00:06.340 --> 00:00:06.960
>> Hello, everyone.

00:00:06.960 --> 00:00:10.550
Welcome to the session on maximizing OpenCL performance.

00:00:10.550 --> 00:00:18.340
In the previous session, we talked about an overview of
OpenCL, how to use it, and sharing between CL and GL.

00:00:18.340 --> 00:00:24.100
In this session, we're going to talk about tuning
of your kernels to get the best possible performance

00:00:24.100 --> 00:00:27.290
on AMD NVIDIA GPUs and Intel CPUs.

00:00:27.290 --> 00:00:31.280
First up is a presentation by Benedict Gaster of AMD.

00:00:31.280 --> 00:00:35.960
Ben is going to talk about doing game
physics with OpenCL, in particular,

00:00:35.960 --> 00:00:41.800
he's going to talk about doing cloud simulation and
Bullet Physics using OpenCL and tuning for AMD GPUs.

00:00:41.800 --> 00:00:44.000
So let me please -- Ben.

00:00:44.000 --> 00:00:46.080
>> Thank you.

00:00:46.080 --> 00:00:49.230
[ Applause ]

00:00:49.230 --> 00:00:54.030
>> Benedict Gaster: Okay, so as Aaftab said I'm
going to talk about OpenCL and accelerating physics.

00:00:54.030 --> 00:00:56.910
And when I talk about physics here,
we're talking about games physics.

00:00:56.910 --> 00:01:01.320
I'm not trying to solve the world's problems looking
where the next black hole is or something like that.

00:01:01.320 --> 00:01:02.420
I'm going to look at games physics.

00:01:02.420 --> 00:01:03.830
So what does that mean?

00:01:03.830 --> 00:01:08.570
So that means there are a number of APIs out there
and the one in particular that we've been focusing

00:01:08.570 --> 00:01:16.700
on is an API called Bullet, which is primarily developed
and architected by a guy called Erwin Coumans at Sony.

00:01:16.700 --> 00:01:20.200
And it's used in many third-party games.

00:01:20.200 --> 00:01:24.420
You know, and also films and so forth.

00:01:24.420 --> 00:01:29.070
It has a Zlib license, so it's completely -- you can do
whatever you like with it, it's completely open source.

00:01:29.070 --> 00:01:31.910
And they're collaborating on accelerating it.

00:01:31.910 --> 00:01:37.490
Today the implementation is purely CPU and there's
been some experiments with SSE and some other things.

00:01:37.490 --> 00:01:39.290
But it's not really an accelerated API.

00:01:39.290 --> 00:01:42.490
So what we're looking to do is
how can we give it a real boost.

00:01:42.490 --> 00:01:45.650
So we looked at cloth, and I'm going
to talk about that today, or soft body.

00:01:45.650 --> 00:01:49.310
So you can think of if you've got your little yellow
duck, my kids are always playing with that in the bath,

00:01:49.310 --> 00:01:54.730
and you're going to squeeze it, how do you represent that,
it's kind of a soft body, rather than something like a rock.

00:01:54.730 --> 00:01:56.190
A cloth is another form.

00:01:56.190 --> 00:01:58.110
And the key thing is that it's fully open source.

00:01:58.110 --> 00:02:02.100
So all the contributions, all the work that
we do in OpenCL is going to go back into that,

00:02:02.100 --> 00:02:05.790
and you'll be able to pick it up
and run it on your Mac platforms.

00:02:05.790 --> 00:02:09.100
So what is Bullet in OpenCL, what does it do?

00:02:09.100 --> 00:02:12.900
So on the big picture here standing on the
left is something I'm not going to talk about today,

00:02:12.900 --> 00:02:15.640
but it's rigid bodies, they're very important in physics.

00:02:15.640 --> 00:02:17.240
They're basically Little rocks or whatever.

00:02:17.240 --> 00:02:23.100
Here we have a pachinko machine, and 5,000 little
things bouncing around hitting off each other,

00:02:23.100 --> 00:02:25.700
hitting off the pegs, bouncing off the back.

00:02:25.700 --> 00:02:27.890
And all being accelerated in OpenCL.

00:02:27.890 --> 00:02:28.880
This all runs in real time.

00:02:28.880 --> 00:02:31.410
Everything I'm going to show you today is in real time.

00:02:31.410 --> 00:02:33.120
You know, that's the key thing about games.

00:02:33.120 --> 00:02:36.550
It may not be exact physics, but it
better give a good response to the user

00:02:36.550 --> 00:02:39.650
because otherwise they're not going
to want to play the game and so forth.

00:02:39.650 --> 00:02:44.710
Another thing, fluids is becoming more and more popular,
you know, maybe you want to do a river or we have some water

00:02:44.710 --> 00:02:47.390
that splashes down on top of the character in the game.

00:02:47.390 --> 00:02:53.860
And at the bottom here the stuff that I'm going to talk
about today is the cloth simulation, and so everything --

00:02:53.860 --> 00:03:00.590
all the flags, you can see all the posts, every single
one of those are cloth -- real cloth being simulated --

00:03:00.590 --> 00:03:04.650
not real cloth, but you know, game
physics cloth being simulated in OpenCL.

00:03:04.650 --> 00:03:09.950
And the little guy if he was dancing around in the middle
there, you'd see that he'd have a cape out the back there,

00:03:09.950 --> 00:03:14.740
and that would all be simulated as a cloth
simulation, all done or running in real time.

00:03:14.740 --> 00:03:18.500
So what is cloth simulation, what does it mean.

00:03:18.500 --> 00:03:20.130
Really, it's fairly straight forward.

00:03:20.130 --> 00:03:23.820
Basically you have some masses, so
basically think of them as particles

00:03:23.820 --> 00:03:26.980
and they have some form of mass,
and you have some connections.

00:03:26.980 --> 00:03:31.220
You could have all those springs and
things, and they could all be separated.

00:03:31.220 --> 00:03:32.940
But that's not how fabric works, you know?

00:03:32.940 --> 00:03:35.820
You grab it and it doesn't all fall apart and fall to bits.

00:03:35.820 --> 00:03:38.170
So you need some sort of things connecting them together.

00:03:38.170 --> 00:03:41.480
So basically you have what's called a mass spring system.

00:03:41.480 --> 00:03:44.090
Turns out there's lots of these masses, really.

00:03:44.090 --> 00:03:48.260
You want to simulate a piece of cloth and make
it look real you want lots of them, particles.

00:03:48.260 --> 00:03:52.810
That's really good for parallelism, because lots
of things that you can operate on gives you an idea

00:03:52.810 --> 00:03:56.600
that maybe you can do something -- if there's
only two of them, probably not very good to do.

00:03:56.600 --> 00:03:59.260
So that's the first thing you want to do when
you're thinking about parallelizing them,

00:03:59.260 --> 00:04:01.750
you want to make sure that there's lots of work to be done.

00:04:01.750 --> 00:04:05.670
So we need to connect them together with some
constraints, and this is what the springs does.

00:04:05.670 --> 00:04:11.790
And I show you here you can see that there are three
types of springs that we are considering, at least.

00:04:11.790 --> 00:04:12.240
Maybe there are others.

00:04:12.240 --> 00:04:16.780
But the main ones -- so if you see the straight
lines around the outside, they're not in the inside,

00:04:16.780 --> 00:04:22.590
these are what we call structural
springs, and they kind of provide --

00:04:22.590 --> 00:04:26.590
give a body, give a kind of shape to the
thing so it all doesn't collapse down.

00:04:26.590 --> 00:04:30.950
And then you want to be able to at some point,
you're going to want to be able to rip that cloth.

00:04:30.950 --> 00:04:32.600
You're going to need to be able to rip that fabric.

00:04:32.600 --> 00:04:33.860
So you're going to want to have shearing.

00:04:33.860 --> 00:04:36.080
These are the diagonal ones that go across.

00:04:36.080 --> 00:04:41.580
And then of course a soft body wouldn't be a soft
body if it didn't bend and deform and things.

00:04:41.580 --> 00:04:45.410
So you have these bending constraints
that are all connected together.

00:04:45.410 --> 00:04:51.950
So obviously we want to start, you know, want to be able
to parallelize this, we want to be able to do something.

00:04:51.950 --> 00:04:54.560
And so what we need -- we have lots of these particles.

00:04:54.560 --> 00:04:59.570
And what we do, it's very straight forward, we
have -- so it's good for parallel programming.

00:04:59.570 --> 00:05:07.410
And what we need to do is basically at each step
through of the simulation we generate some forces,

00:05:07.410 --> 00:05:12.760
apply them to the different connections into thing --

00:05:12.760 --> 00:05:15.440
different springs, so they'll have
different -- slightly -- characteristics.

00:05:15.440 --> 00:05:17.020
But basically it's a set of rules.

00:05:17.020 --> 00:05:21.710
You do calculate some impulses, plot it to
the masses and calculate the new positions.

00:05:21.710 --> 00:05:26.250
You know, and the key thing here, if you imagine
that there's lots of these things connected together

00:05:26.250 --> 00:05:28.950
and we could do these somewhat in parallel.

00:05:28.950 --> 00:05:33.920
It's not quite as simple as that, because
the fact is if I have one little bit of cloth

00:05:33.920 --> 00:05:37.560
and there's another bit connected, if I pull it
this way, this one's going to go that way as well.

00:05:37.560 --> 00:05:41.050
So we need to have some core of
locality and connecting things together.

00:05:41.050 --> 00:05:42.840
So parallelism is not complete.

00:05:42.840 --> 00:05:47.310
You know, it's not -- everything is not done individually.

00:05:47.310 --> 00:05:53.690
So one of the things we wanted to look at is that you
know, cloth could interact with rigid bodies, for example,

00:05:53.690 --> 00:05:58.440
maybe a throw on a chair or a flag, or
you'll drop a piece of cloth on to a bottle,

00:05:58.440 --> 00:06:01.210
so for example things like that, pour some water on to it.

00:06:01.210 --> 00:06:06.630
But another thing very much independent of all those things
is you might want to have some flags flying in the wind.

00:06:06.630 --> 00:06:07.970
That's an obvious force.

00:06:07.970 --> 00:06:14.940
It turns out this can be paralleled really easily and really
nicely because this is computed per vertex on the cloth.

00:06:14.940 --> 00:06:17.610
You know, that's how we're representing
each particle is in some sense a vertex.

00:06:17.610 --> 00:06:22.470
Because all you really want to too is describe a
mesh that you're then going render at some point.

00:06:22.470 --> 00:06:25.210
That's the end goal is to render
something that looks like a flag.

00:06:25.210 --> 00:06:27.250
Doesn't really need to be a flag.

00:06:27.250 --> 00:06:34.100
And so here we can compute it per vertex, excellent
parallel computation, force and density of the air medium,

00:06:34.100 --> 00:06:38.420
and you against the mass and the normal of the vertex.

00:06:38.420 --> 00:06:40.630
You know, so it's a very simple physics calculation.

00:06:40.630 --> 00:06:44.040
There's nothing rocket science here.

00:06:44.040 --> 00:06:46.270
What does it look like if we were
going to do this on the CPU.

00:06:46.270 --> 00:06:48.600
So this would be the Bullet implementation today,

00:06:48.600 --> 00:06:52.900
other physics implementations would take a very
similar approach if you're doing it on the CPU.

00:06:52.900 --> 00:06:58.480
Well you do a iterative, you know, integration,
verlet is a common one to use over the vertex position,

00:06:58.480 --> 00:07:03.260
and for each spring you compute a force,
update the vertices with a new position

00:07:03.260 --> 00:07:07.180
and repeat N times, where N is basically configurable.

00:07:07.180 --> 00:07:11.830
The idea here is you know, if you integrate
just once, it's not accurate enough.

00:07:11.830 --> 00:07:18.930
So you want to run the simulation -- it turns out
in physics, game physics at least, 10 randomly --

00:07:18.930 --> 00:07:24.380
I couldn't tell you exactly why it does, helps it converge
to a reasonable point that gives you a nice simulation.

00:07:24.380 --> 00:07:31.570
And that turns out not just to be true for cloth, it
turns out to be true for fluid and also for rigid bodies.

00:07:31.570 --> 00:07:37.240
Just I think ten -- maybe just enough convergence
there that gives you the simulation without running

00:07:37.240 --> 00:07:40.110
so many CPU cycles that you never get an answer.

00:07:40.110 --> 00:07:48.760
So, the critical thing here is that I'm -- you know, this
kind of integration is great, but you're updating everything

00:07:48.760 --> 00:07:54.630
in place, you're going from -- say you got the results
from one iteration being fed back into the next iteration.

00:07:54.630 --> 00:07:57.350
This is a very sequential algorithm.

00:07:57.350 --> 00:08:00.840
Everything can be immediate, everything
moves through this over immediately.

00:08:00.840 --> 00:08:05.400
But it's not a trivial thing to parallelize
if you try to just take this as is.

00:08:05.400 --> 00:08:08.940
So here you can see what this would
look like for each iteration.

00:08:08.940 --> 00:08:12.020
So this iteration is the N that
we talked about, the integrate.

00:08:12.020 --> 00:08:15.510
You simply walk through of the links and do the integration.

00:08:15.510 --> 00:08:17.200
Really very much straight out of a physics book.

00:08:17.200 --> 00:08:20.620
It's nothing to be thought about.

00:08:20.620 --> 00:08:25.580
So, move into parallel execution,
and this is a critical thing,

00:08:25.580 --> 00:08:28.900
I think the CPU implementation has no atomicity issues.

00:08:28.900 --> 00:08:31.960
You know, there -- it doesn't matter
that you're updating everything in place.

00:08:31.960 --> 00:08:38.070
You know, I might want to update a single spring,
a single mass may be affected by two links,

00:08:38.070 --> 00:08:40.630
and I could just do one and then I could do the next.

00:08:40.630 --> 00:08:43.230
And I could update the same memory
location, none of that doesn't matter,

00:08:43.230 --> 00:08:46.290
because I'm implementing it sequentially,
there's no race conditions.

00:08:46.290 --> 00:08:47.630
You know, that's the key thing.

00:08:47.630 --> 00:08:53.040
The trouble is, I actually want to do these
updates in parallel when using the GPU.

00:08:53.040 --> 00:08:55.790
So this introduces races.

00:08:55.790 --> 00:08:59.630
So the critical thing is I can't let you do those
ones in parallel, otherwise it wouldn't be a race.

00:08:59.630 --> 00:09:04.250
They're going to be things -- but of course if it's a node
over here that really doesn't effect the one over here,

00:09:04.250 --> 00:09:06.270
I could probably update those in parallel.

00:09:06.270 --> 00:09:08.100
So what does that mean?

00:09:08.100 --> 00:09:11.110
That means we're going to introduce a nation of batching.

00:09:11.110 --> 00:09:16.050
And again, this is not something that I've invented or
anything, it's a kind of well-known technique that's used

00:09:16.050 --> 00:09:19.700
in physics simulations and generally
in parallel programming as a whole.

00:09:19.700 --> 00:09:23.390
And the idea is we're going to
go through using graph coloring.

00:09:23.390 --> 00:09:25.490
We're going to color the nodes.

00:09:25.490 --> 00:09:28.460
And so here we're going to come -- color the links, sorry.

00:09:28.460 --> 00:09:34.990
And the key thing is that for each -- each batch
which is the colored links of the same color,

00:09:34.990 --> 00:09:36.690
you can -- I can execute them in parallel.

00:09:36.690 --> 00:09:38.110
There's nothing there.

00:09:38.110 --> 00:09:43.170
But between ones where they're different colors,
that means I have to synchronize between them.

00:09:43.170 --> 00:09:49.310
So what that means is I'll execute one batch, do as much
as I can in parallel in that batch, execute the next batch,

00:09:49.310 --> 00:09:53.160
do as much as that in parallel, and
so I am serializing the algorithm.

00:09:53.160 --> 00:09:54.890
But I'm only doing it at certain points.

00:09:54.890 --> 00:09:59.950
And you can think of these as kernel
launches in your CL program, for example,

00:09:59.950 --> 00:10:05.370
because that's inherently a sequential serialization point
because you're coming back from the parallel to device

00:10:05.370 --> 00:10:08.060
to a single -- you know, a thread in your CPU or whatever.

00:10:08.060 --> 00:10:11.860
You know, your host program, depending
on how you've designed it.

00:10:11.860 --> 00:10:14.100
And so this technique is known as batching.

00:10:14.100 --> 00:10:19.220
So basically, deriving batches, we need to be able to
build up the batches, we go -- for each iteration --

00:10:19.220 --> 00:10:21.650
again, we're going to have to do this for each iteration.

00:10:21.650 --> 00:10:29.850
We're going to step through number -- the size of the
batches, and then we find the first and the second,

00:10:29.850 --> 00:10:31.230
this is a kind of structure that we've got.

00:10:31.230 --> 00:10:33.140
We pick up the start and the number.

00:10:33.140 --> 00:10:38.280
So the start is an offset into the batch that we're going
to do so we can parallelize it on multiple GPUs if we want to.

00:10:38.280 --> 00:10:42.480
This implementation that I'll show you in the
demo today is not parallelized multiple devices,

00:10:42.480 --> 00:10:45.210
but it's designed such that it could be.

00:10:45.210 --> 00:10:50.950
Okay, so the code I'm going to show you now
is this is it, this is all the OpenCL code

00:10:50.950 --> 00:10:54.280
for the solver. There's other kernels and
everything, and this is the host site code.

00:10:54.280 --> 00:10:58.000
I'm using the C++ API rather than the standard C API.

00:10:58.000 --> 00:10:59.430
So that's why it might look a little different.

00:10:59.430 --> 00:11:01.130
But all the names should be fairly straightforward.

00:11:01.130 --> 00:11:06.940
But the key thing is here is I get to the kernel
itself, get the command key that I want to run it on.

00:11:06.940 --> 00:11:09.540
We've abstracted that out so it can be an arbitrary device.

00:11:09.540 --> 00:11:15.240
I set some kernel arguments and then I
enqueue the kernel and execute it.

00:11:15.240 --> 00:11:19.560
And if we go to the solver on the next side you can see
if you check back on the previous slide you'll see

00:11:19.560 --> 00:11:26.010
that my arguments correspond exactly to the
set that I set there, and of course because of the --

00:11:26.010 --> 00:11:33.160
in OpenCL, the global work size must
be a multiple of the local work size.

00:11:33.160 --> 00:11:35.370
We check -- we sometimes have to do some rounding.

00:11:35.370 --> 00:11:38.230
So we check that we can execute
inside that with that little thing.

00:11:38.230 --> 00:11:42.680
So the number links is the actual ones
that we're actually parallelizing over.

00:11:42.680 --> 00:11:46.290
This over itself is almost identical
to the CPU code except we're looking

00:11:46.290 --> 00:11:48.940
to the global GPU buffers and everything like that.

00:11:48.940 --> 00:11:50.690
But it's basically just the integration step.

00:11:50.690 --> 00:11:55.300
You know, we read the vertexes, the masses,
and then we compute the new positions.

00:11:55.300 --> 00:11:58.430
There really isn't that much more to it.

00:11:58.430 --> 00:12:01.390
Okay, so what does that mean?

00:12:01.390 --> 00:12:03.260
That's pretty good, that turned out to be pretty good.

00:12:03.260 --> 00:12:08.540
We can already start to see a pretty
good performance speed up over the GPU.

00:12:08.540 --> 00:12:11.180
However, I talked about the batching.

00:12:11.180 --> 00:12:13.880
And originally I've shown you here just nine batches.

00:12:13.880 --> 00:12:15.940
That's what we did from our original coloring.

00:12:15.940 --> 00:12:17.240
That's great.

00:12:17.240 --> 00:12:20.380
Except it turns out that the batches are quite small.

00:12:20.380 --> 00:12:23.610
You know, and what I mean by that,
there's not that much work in the batches.

00:12:23.610 --> 00:12:26.600
So there's not high density per thread of work.

00:12:26.600 --> 00:12:28.820
So we execute all this and notice not much work.

00:12:28.820 --> 00:12:32.080
So we're not really efficiently using the GPU.

00:12:32.080 --> 00:12:36.220
So the idea is can we, can we, can
we, can we keep increasing the size

00:12:36.220 --> 00:12:39.170
of the batches so that we can get more done.

00:12:39.170 --> 00:12:43.390
and basically we reduce -- not only do we create
larger batches but of course we reduce the number

00:12:43.390 --> 00:12:45.880
of serialization points in the program as well.

00:12:45.880 --> 00:12:48.730
Because that's what different batches introduce.

00:12:48.730 --> 00:12:55.390
So you can see here, it actually turns out that if
we're very careful we can get that down to four batches.

00:12:55.390 --> 00:12:58.460
And we've doubled the amount of work that's going on.

00:12:58.460 --> 00:13:03.990
We're not now completely -- you know,
again, it's not real physics here.

00:13:03.990 --> 00:13:09.430
We've made these observations, we've done some analysis,
and realized that we can use these techniques to do that.

00:13:09.430 --> 00:13:13.870
And in particular, the way that we get
around this is that we remove determinism.

00:13:13.870 --> 00:13:21.020
So what we were saying before, only wanted to
update a single link at one time, you know a node.

00:13:21.020 --> 00:13:23.680
And of course that causes serialization bottleneck.

00:13:23.680 --> 00:13:27.920
It turns out that if you're careful
and execute per vertex data,

00:13:27.920 --> 00:13:32.390
so that means recompute some data at the point of execution.

00:13:32.390 --> 00:13:38.930
So you may compute the same value twice, then
actually you can update links in parallel.

00:13:38.930 --> 00:13:42.190
Now of course we've gotten millions of
our users here to access,

00:13:42.190 --> 00:13:44.970
so recomputing worked, what's wrong with that.

00:13:44.970 --> 00:13:50.920
If you're memory-bound then using extra ALUs
to recompute the value is not a big issue.

00:13:50.920 --> 00:13:55.490
So it turns out that this is how
we can start to remove determinism.

00:13:55.490 --> 00:14:01.870
Something that we're looking at moving forward, and this
is not a proven point yet we've implemented it and it seems

00:14:01.870 --> 00:14:05.280
to work but we haven't mathematically
verified it's correct yet,

00:14:05.280 --> 00:14:08.920
is that we can remove determinism
all together and have a single batch.

00:14:08.920 --> 00:14:11.750
And the issue seems to be that it's slower to converge.

00:14:11.750 --> 00:14:16.060
And so N that I talked about earlier
is now not the right value for that.

00:14:16.060 --> 00:14:22.690
The problem is if we increased N to 20 rather than
10, that would double the amount of work we're doing.

00:14:22.690 --> 00:14:26.590
So it's kind of a balance of is that going
to give us back still better performance,

00:14:26.590 --> 00:14:28.750
because we're more effectively using the GPU.

00:14:28.750 --> 00:14:31.160
And that's an interesting thing, and
we don't have the answer to that today.

00:14:31.160 --> 00:14:37.080
But you can see these are the kind of things that
you really want to think about when building a GPU.

00:14:37.080 --> 00:14:41.940
Okay, so that's kind of it, I've kind
of give a number of points and a number

00:14:41.940 --> 00:14:45.490
of techniques for parallelizing for the GPU.

00:14:45.490 --> 00:14:47.210
They work really work on our GPU.

00:14:47.210 --> 00:14:50.050
Actually, these work well on any kind
of parallel problem we kind of solve,

00:14:50.050 --> 00:14:54.350
and these are really techniques
that you could use for OpenCL.

00:14:54.350 --> 00:14:57.240
One thing that's really important,
I think, with parallel programming

00:14:57.240 --> 00:15:00.860
and particularly with GPUs, is these are SIMD machines.

00:15:00.860 --> 00:15:03.450
So what that means is they're very wide vector machines.

00:15:03.450 --> 00:15:09.530
You know, you are executing a single
instruction many times for the same, you know,

00:15:09.530 --> 00:15:12.280
for the same vector but different elements.

00:15:12.280 --> 00:15:18.210
That means if your threads diverge, so if
you say AMD, it's a 64 wide vector machine,

00:15:18.210 --> 00:15:22.990
if only one thread is executing out
that 64, you're using 1/64 of a GPU.

00:15:22.990 --> 00:15:31.490
You can easily do that by saying if get global ID equals
1, say for example, then you're restricted that to do that,

00:15:31.490 --> 00:15:35.920
or group ID say equals 1, then you're just
restricting it to that word group and that SIMD.

00:15:35.920 --> 00:15:37.800
But it's very easy to do that.

00:15:37.800 --> 00:15:44.430
There are some techniques that we'd used in
the cloth simulations itself to reduce this.

00:15:44.430 --> 00:15:49.790
It turns out that if we're willing to
not allow an arbitrary complex mess --

00:15:49.790 --> 00:15:53.510
so what I might mean about that is I have a piece of cloth
that's kind of wrapped up and rolled over and kind of coming

00:15:53.510 --> 00:15:57.770
through itself, if we say actually that's
not used that common in games, you know,

00:15:57.770 --> 00:16:02.610
we can reduce the mesh to say a nice kind of lace cloth
we'll have flapping in the sky like the flags I'll show you

00:16:02.610 --> 00:16:08.660
in a minute, then actually we can make them regular and
we know the shape, we can preprocess them statically,

00:16:08.660 --> 00:16:11.430
and we know that we don't have any
diversions when we're generating that.

00:16:11.430 --> 00:16:16.400
So techniques like that, you know, you're not solving
the whole world's problems, you're not doing everything,

00:16:16.400 --> 00:16:20.580
you're saying what is the problem that
I want to address in this application.

00:16:20.580 --> 00:16:23.810
Then you can make trade-offs that
allow you to get, you know,

00:16:23.810 --> 00:16:27.700
reduced divergency, better throughput, better efficiency.

00:16:27.700 --> 00:16:29.920
I think that's very important,
I think in parallel programming.

00:16:29.920 --> 00:16:33.190
You know, it's very difficult to solve the general problem.

00:16:33.190 --> 00:16:36.330
If you can reduce and restrict your problem
a little bit, you're much more likely

00:16:36.330 --> 00:16:41.480
to get the performance speed-ups that you hoped to get.

00:16:41.480 --> 00:16:50.460
Okay, hopefully I can switch to -- okay, that window.

00:16:53.630 --> 00:16:55.550
So this is that you just saw those two flags.

00:16:55.550 --> 00:16:58.340
One of them is very far out, flying.

00:16:58.340 --> 00:17:01.500
And so this is running purely on the GPU.

00:17:01.500 --> 00:17:05.670
And just two flags on here at the
moment, it's completely being simulated.

00:17:05.670 --> 00:17:07.240
Just a very simple GL renderer.

00:17:07.240 --> 00:17:12.780
And I can simply keep pressing
here and just keep adding flags.

00:17:12.780 --> 00:17:16.670
At this point I think I've got 10
flags, you can't see all of them.

00:17:16.670 --> 00:17:19.490
But you can spin around and see some are very close up.

00:17:19.490 --> 00:17:23.070
And you can see that we haven't seen
any performance degradation at all.

00:17:23.070 --> 00:17:26.370
And I've just added these ten flags and
each one's adding it, just loading it to OpenCL.

00:17:26.370 --> 00:17:33.220
And actually if you were to look at this application
you'd see that the CPU is basically doing hardly anything.

00:17:33.220 --> 00:17:35.210
Turns out there's no bottlenecks on here.

00:17:35.210 --> 00:17:39.590
The only main thing is, is that because I knocked this
out very quickly and didn't have a lot of time to spend

00:17:39.590 --> 00:17:42.380
on the rendering I'm actually copying the data --

00:17:42.380 --> 00:17:45.830
I'm not using the GL sharing that was
talked about in the previous talk.

00:17:45.830 --> 00:17:49.880
And if I did that I could probably just keep
adding flags, maybe I have 50 or 60 flags.

00:17:49.880 --> 00:17:53.920
And again, not only would the GPU be
running, these would be running in real time.

00:17:53.920 --> 00:17:58.350
You actually wouldn't see any of the
CPU performance going up hardly at all.

00:17:58.350 --> 00:18:01.440
So you can off load that and do this work.

00:18:01.440 --> 00:18:03.800
And yeah, okay, so that's it.

00:18:03.800 --> 00:18:08.620
Now I'm going to hand over to Intel for their presentation.

00:18:10.510 --> 00:18:15.700
[ Applause ]

00:18:15.700 --> 00:18:23.990
>> Vinay Awasthi: Hello, today I am going to talk
about how to develop complex kernels for Intel CPUs.

00:18:23.990 --> 00:18:31.970
So jumping right into it -- and the purpose of this
talk is that we want to develop kernels which are used

00:18:31.970 --> 00:18:41.640
for solving complex problems such as solvers,
or dynamic programming related issues and problems.

00:18:41.640 --> 00:18:46.410
And how do we utilize multiple
cores, SIMD instructions, and caches.

00:18:46.410 --> 00:18:52.940
So first let's go where simple dynamic
programming problem called longest common subsequence.

00:18:52.940 --> 00:18:55.160
And this problem is essentially pretty simple --

00:18:55.160 --> 00:18:59.180
you have two strands of DNA and you
want to find out how similar they are.

00:18:59.180 --> 00:19:10.170
And the way you solve it is you create a table and you align
these strands, top and left side, and then you fill in zeros

00:19:10.170 --> 00:19:15.680
so that you can process from the
corners, going from left to right.

00:19:15.680 --> 00:19:21.460
And the code that is used to analyze this is on
your right-hand side, where any of the data matches,

00:19:21.460 --> 00:19:26.730
you pick up an element that is diagonal
to the element that is being processed.

00:19:26.730 --> 00:19:35.480
And if it does not match, then you take the maximum of the
cells that has talked to it and that are on the left of it.

00:19:35.480 --> 00:19:38.100
So it's something like this.

00:19:38.100 --> 00:19:40.160
So you're processing let's say first row.

00:19:40.160 --> 00:19:44.770
And you went through and A and
A matched for last two columns.

00:19:44.770 --> 00:19:47.250
So you picked up 0 and added 1 to it.

00:19:47.250 --> 00:19:51.890
And if it doesn't match, then you
take the maximum for the top or left,

00:19:51.890 --> 00:19:55.620
whichever is the maximum you can
fill into that particular cell.

00:19:55.620 --> 00:20:00.130
And you kind of just go in and fill in this
particular table all the way to the bottom.

00:20:00.130 --> 00:20:06.820
And at the bottom right corner, the number
that you end up with is your edit distance.

00:20:06.820 --> 00:20:12.750
That is essentially telling you how
many edits, that is insertion, deletion,

00:20:12.750 --> 00:20:16.750
that you need in order to transform one strain to another.

00:20:16.750 --> 00:20:22.670
And the higher the distance, that shows
that these are not very similar DNAs.

00:20:22.670 --> 00:20:29.480
And usually scientists or NIH tends to process
hundreds of thousands of these strand-long DNAs.

00:20:29.480 --> 00:20:34.520
And essentially, you want to process
them as fast as possible,

00:20:34.520 --> 00:20:38.070
and sometimes you just want to know the edit distance.

00:20:38.070 --> 00:20:45.320
At other times you want to know what
is the longest common subsequence.

00:20:45.320 --> 00:20:48.630
So in this case, what you need to do
is you have to traverse backwards.

00:20:48.630 --> 00:20:56.520
And the way you do it, is essentially using the same
comparison algorithm and then wherever diagonals are,

00:20:56.520 --> 00:20:59.510
that's where you have your common element.

00:20:59.510 --> 00:21:02.200
So in this case, it's JCTA.

00:21:02.200 --> 00:21:08.900
So why is it interesting that dynamic programming is
used to solve hard problems, NP complete problems.

00:21:08.900 --> 00:21:17.340
A lot of times they're used in set solvers or areas wherein
you want to solve common optimization relative problems.

00:21:17.340 --> 00:21:19.480
And they're very different than divide and conquer.

00:21:19.480 --> 00:21:24.630
Because in divide and conquer you get almost
half the size of the problem when you divide it.

00:21:24.630 --> 00:21:33.640
In case of dynamic programming, your subproblem is not too
small from the original problem that you started out with.

00:21:33.640 --> 00:21:39.240
And in this case, as I said earlier we're
going to cover vector data type optimizations,

00:21:39.240 --> 00:21:42.740
cache optimizations, and multicore optimizations.

00:21:42.740 --> 00:21:46.360
So jumping right into vector data type optimizations.

00:21:46.360 --> 00:21:49.430
So say this is your table that you want to work with.

00:21:49.430 --> 00:21:56.300
So as I showed you earlier, in case of longest common
subsequence, you kind of set up your requirements or --

00:21:56.300 --> 00:22:01.860
you need diagonal element, you need top element,
and left element in order to start processing.

00:22:01.860 --> 00:22:04.620
So we just fill in the number first.

00:22:04.620 --> 00:22:09.440
So now we are ready to process at least the top left corner.

00:22:09.440 --> 00:22:14.250
And usually, if you look at any
programming task such as CL RS or any one

00:22:14.250 --> 00:22:18.470
of those standard algorithm test,
this is how they describe it.

00:22:18.470 --> 00:22:23.560
You can go top left corner and process
this row by row, one row at a time.

00:22:23.560 --> 00:22:25.150
Kind of like this.

00:22:25.150 --> 00:22:31.780
But if you look a little bit deeply, you would notice that
after the first element is filled in, you can go diagonally.

00:22:31.780 --> 00:22:34.330
You can cross next two in this way.

00:22:34.330 --> 00:22:36.950
Then you go next three in this form.

00:22:36.950 --> 00:22:44.730
And four, and then you can have any number of elements
being processed in parallel all the way up to the end.

00:22:44.730 --> 00:22:47.000
So it is up to you how you pick up your set.

00:22:47.000 --> 00:22:51.690
And then after that you kind of
process all the elements in parallel.

00:22:51.690 --> 00:22:53.410
But there is more to it.

00:22:53.410 --> 00:22:57.840
You can also shift your rows to your right.

00:22:57.840 --> 00:23:01.740
And once you do that, those two elements will line up.

00:23:01.740 --> 00:23:06.920
And if you organize your table in such
a way that they line up horizontally,

00:23:06.920 --> 00:23:10.010
then in that case you have cache locality working for you.

00:23:10.010 --> 00:23:17.000
You do not have to have this if statement, the
Max part would essentially become mass load.

00:23:17.000 --> 00:23:20.790
So you set up your mass based on the
matches and then you do a straight store.

00:23:20.790 --> 00:23:23.700
And that will write whatever element needs to be written.

00:23:23.700 --> 00:23:30.290
You will be doing two stores, one for more mass that
matches, and another one more for mass that it not match.

00:23:30.290 --> 00:23:34.890
And you can fill in the Max of the elements that you have.

00:23:34.890 --> 00:23:40.040
Okay, so how do we go about running it for re-efficiently.

00:23:40.040 --> 00:23:43.590
So as you notice that essentially you --
your threads were coming out to be first,

00:23:43.590 --> 00:23:46.170
second, third, til you hit the full length.

00:23:46.170 --> 00:23:50.720
And then the same way you end up with the
problem at the end wherein it's three, two, one.

00:23:50.720 --> 00:23:53.890
So you do not want to have if conditions in your code.

00:23:53.890 --> 00:23:56.680
So what you need to do is pad it more.

00:23:56.680 --> 00:24:02.630
So on both sides you pad it, now you can process four
elements starting from the beginning all the way to the end.

00:24:02.630 --> 00:24:04.570
And this will run through all the way.

00:24:04.570 --> 00:24:08.310
And you will not have just four rows.

00:24:08.310 --> 00:24:11.180
You can have 8 rows, 16 rows, as many as you like.

00:24:11.180 --> 00:24:19.170
Essentially you are using a lot of vector data types to
process as many elements as you can process in parallel.

00:24:19.170 --> 00:24:29.250
Okay, so this also allows you to organize your
problem in such a way so that you can take advantage

00:24:29.250 --> 00:24:33.220
of cache locality and mass loads and stores.

00:24:33.220 --> 00:24:35.070
And you can also do streaming stores.

00:24:35.070 --> 00:24:40.150
Because once you have your data organized, you
can -- you don't need to have regular store,

00:24:40.150 --> 00:24:42.940
you can do right combining
of stores using streaming stores.

00:24:42.940 --> 00:24:49.150
Anyway, so how would you go about
multicore or multithread execution?

00:24:49.150 --> 00:24:56.490
So for as you saw earlier, that more
any set to be processed you need top row

00:24:56.490 --> 00:24:59.910
and the left column available so that you can process it.

00:24:59.910 --> 00:25:03.590
And number of transition execute, they change.

00:25:03.590 --> 00:25:06.290
So first you can see, you can go with 7.

00:25:06.290 --> 00:25:10.140
Then you kind of traverse down
diagonally as I showed earlier.

00:25:10.140 --> 00:25:15.800
So next time you can just run one thread, then
two, then three, and the same way you kind of exit.

00:25:15.800 --> 00:25:18.500
Now let's look at it in context.

00:25:18.500 --> 00:25:22.970
This is our group of elements that we want to process.

00:25:22.970 --> 00:25:29.410
Each one is running SIMD instruction, they
are processing multiple elements in each set.

00:25:29.410 --> 00:25:35.250
And these groups, we can process them in
this fashion in parallel in the first shot.

00:25:35.250 --> 00:25:37.230
After that, we can go to the next one.

00:25:37.230 --> 00:25:44.590
Then you have to wait till that next one is over, then
you process next 2, next 3, next 2, and then last one.

00:25:44.590 --> 00:25:46.700
And that's how you will exit.

00:25:46.700 --> 00:25:51.590
And there are other schemes to do the same thing.

00:25:51.590 --> 00:25:55.790
And that allows you to take advantage
of the cache hierarchy.

00:25:55.790 --> 00:25:59.890
So you know how much of a work set can fit in your caches.

00:25:59.890 --> 00:26:06.640
And you divide your work in such a way so that the
problem that you're trying to work on fits in the cache.

00:26:06.640 --> 00:26:09.830
And these approaches are called cache-oblivious algorithm.

00:26:09.830 --> 00:26:17.740
There's a lot of research going on in academic areas
where they're trying to find ways to fine-tune algorithms

00:26:17.740 --> 00:26:19.830
in such a way that they can be cache oblivious.

00:26:19.830 --> 00:26:23.690
In this case, you kind of divide your
problem and do execution in this order.

00:26:23.690 --> 00:26:31.870
It's important to go from one set to the other to the
right so that you are again taking advantage of the caches.

00:26:31.870 --> 00:26:37.960
If you go columnwise, then you lose
prefetching and cache-related advantages

00:26:37.960 --> 00:26:42.310
that are there, if you go 0, 1, 2, 3, in that fashion.

00:26:42.310 --> 00:26:49.450
Okay, so what does the performance gain
that you get if you go and do all this work?

00:26:49.450 --> 00:26:52.690
So once I started out, I was getting about --

00:26:52.690 --> 00:27:03.330
let's say 1 performance, and then as I added all
this OpenCL execution went up to almost 7 times.

00:27:03.330 --> 00:27:08.460
And then I tried to do it using threading and hand-tune SSE.

00:27:08.460 --> 00:27:12.800
So that also changed quite a bit based
on the threaded model that you picked

00:27:12.800 --> 00:27:16.250
and the best numbers that I got was using GCD and SSE.

00:27:16.250 --> 00:27:19.870
And that was about 7 to 10% better than OpenCL.

00:27:19.870 --> 00:27:29.250
So as you can see OpenCL code approach pretty close
to the code that you will get using GCD and SSE.

00:27:29.250 --> 00:27:31.330
Okay, so what are the lessons learned.

00:27:31.330 --> 00:27:36.980
One is you have to issue large work items,
around 10,000 to 200,000 instructions.

00:27:36.980 --> 00:27:38.560
Utilize map and map buffers.

00:27:38.560 --> 00:27:43.930
And the advantage of using that is they just
lock the memory, there's no copying happening.

00:27:43.930 --> 00:27:47.710
You just map and then you get your data, and then you unmap.

00:27:47.710 --> 00:27:53.660
Use CL_ALLOC_HOST_PTR so
that framework is allocating memory for you.

00:27:53.660 --> 00:27:55.540
And it is aligning memory for you.

00:27:55.540 --> 00:27:58.190
You don't have to worry about alignments
and things like that.

00:27:58.190 --> 00:28:05.460
But if you do have a need for it, then you use
CLU's pointer and you align it the way you want it.

00:28:05.460 --> 00:28:15.260
And try to avoid -- if you can, enqueue buffers or wait for
barriers or have atomics, because they would tend to slow

00:28:15.260 --> 00:28:20.260
down your performance because each kernel
is executing hundreds of thousands of times.

00:28:20.260 --> 00:28:28.730
Now other areas that are there to consider is there is no
added advantage of using image-related functions on CPU

00:28:28.730 --> 00:28:35.770
because there is no specialized hardware
that is optimizing image processing in CPUs.

00:28:35.770 --> 00:28:40.990
And you also do not want to do a lot of
edge condition processing in your kernels.

00:28:40.990 --> 00:28:46.290
You want to process it either earlier or
you want to page your dataset in such a way

00:28:46.290 --> 00:28:48.160
so that the whole thing could be processed.

00:28:48.160 --> 00:28:53.000
Use 1D range because then you can process
everything in one shot, go linearly.

00:28:53.000 --> 00:29:01.200
Okay, so here there are a few kernels that we are
going to see how we should go about fixing them.

00:29:01.200 --> 00:29:06.800
So one is -- a lot of patterns you will
see in sample codes and all,

00:29:06.800 --> 00:29:12.570
they tend to compute your work group size using
this kind of pattern wherein they find the global_id

00:29:12.570 --> 00:29:19.650
and know what is the data size, divide it,
and this kind of pattern is doing per item work repeatedly.

00:29:19.650 --> 00:29:26.840
And you want to focus in your kernel in such a way so
that you can eliminate as many instructions as possible

00:29:26.840 --> 00:29:30.210
when you're trying to write it,
so it is as small as possible,

00:29:30.210 --> 00:29:34.670
because that particular code is
going to get executed a lot of times.

00:29:34.670 --> 00:29:43.400
So in this case, what you could do is you can either set
up a constant or pass that as a parameter to your kernel.

00:29:43.400 --> 00:29:48.240
Now here, you will see a lot more of morphing
algorithms and lot of other image-processing algorithms.

00:29:48.240 --> 00:29:50.970
They tend to find -- they tend to work towards --

00:29:50.970 --> 00:29:55.700
okay, here's my pixel and here's the
line that I want to morph towards.

00:29:55.700 --> 00:30:00.890
And I need to find my distance so that I know the
weight, let's say [Inaudible] algorithm tend to do it,

00:30:00.890 --> 00:30:04.010
and there are a lot of other image-related
processing algorithms that do it.

00:30:04.010 --> 00:30:09.160
And general code is something like this, you
just kind of do A squared plus B squared and do square root.

00:30:09.160 --> 00:30:13.480
So try to use built in functions because
they're already optimized for you.

00:30:13.480 --> 00:30:16.610
So in this case, you will use [Inaudible].

00:30:16.610 --> 00:30:23.530
In this particular code, we are
using int to get the global thread ID.

00:30:23.530 --> 00:30:27.870
And this particular code will have problem
when you're running it on 64-bit machine,

00:30:27.870 --> 00:30:31.270
because there will be downcasting
happening from 64-bit to 32-bit.

00:30:31.270 --> 00:30:34.290
You want to use size_t.

00:30:34.290 --> 00:30:36.780
Here what is happening is the exponentor

00:30:36.780 --> 00:30:42.510
that is being passed is stopping Compiler
to do the loop and rolling for you.

00:30:42.510 --> 00:30:47.120
So what you want to do is either pass it as
a build-time configuration, wherein you can --

00:30:47.120 --> 00:30:54.830
when you're running your configuration during the build
time you can specify it, or specify it as a constant.

00:30:54.830 --> 00:30:58.360
In this particular code, you will
see that float2s are being used.

00:30:58.360 --> 00:31:05.090
And what you would like to do is use the
full vector length because Intel CPUs do best

00:31:05.090 --> 00:31:07.770
when the full lengths are being utilized.

00:31:07.770 --> 00:31:15.190
In this case you want to do twice the work using
float4, and again, don't use uint, use size_t.

00:31:15.190 --> 00:31:20.690
And this is the way you should do more
work, use full-length [Inaudible] vectors

00:31:20.690 --> 00:31:24.170
or vector data types to get the best performance.

00:31:24.170 --> 00:31:28.400
Okay, so let's go with the demo.

00:31:32.250 --> 00:31:38.280
See in here, my interest was towards
using dynamic programming.

00:31:38.280 --> 00:31:40.860
And it's not so much so to morphing here.

00:31:40.860 --> 00:31:45.650
My goal is to see how dynamic programming
could be used to morph one image

00:31:45.650 --> 00:31:50.830
to the other dynamically, or without any user intervention.

00:31:50.830 --> 00:31:53.310
And the way we do it is something like this.

00:31:53.310 --> 00:31:59.510
So each picture is transferred into
a gray scale to find out its edges.

00:31:59.510 --> 00:32:02.880
And after the edges are detected we do black run analysis.

00:32:02.880 --> 00:32:09.000
So you find out how many black runs are
there and you try to do match, split,

00:32:09.000 --> 00:32:11.770
or merge for each one of them, for all the pictures.

00:32:11.770 --> 00:32:20.390
So think of it all the rows are being analyzed, and then
we are separating the separate black runs for each picture.

00:32:20.390 --> 00:32:27.540
And then we are analyzing those black runs for the next
picture across the rows to find out where is the best match.

00:32:27.540 --> 00:32:32.840
When we detect the best match that is over 1
point that we want to transform towards in terms

00:32:32.840 --> 00:32:37.420
of image to image, then we go and execute.

00:32:37.420 --> 00:32:39.530
So there's a lot of work that is happening.

00:32:39.530 --> 00:32:41.480
Usually you would not do this much work for morphing.

00:32:41.480 --> 00:32:46.950
You would just go and have either users set
up the points like [Inaudible]'s, and other algorithms do.

00:32:46.950 --> 00:32:51.700
There are some kind of work that is involved
from the Animator's side to allow you to do it.

00:32:51.700 --> 00:32:53.990
But here we are just using dynamic programming.

00:32:53.990 --> 00:32:58.630
And then what we also do, we are
creating about 60 frames in between.

00:32:58.630 --> 00:33:05.760
And so image -- that juice box to the football,
there are about 60 images that are created.

00:33:05.760 --> 00:33:10.480
And the same way that we are going from
soccer ball to bowling pin, again, 60 images.

00:33:10.480 --> 00:33:14.320
And all of that is just dense, so
let's see how do they look.

00:33:14.320 --> 00:33:19.000
So here is the transformation.

00:33:19.000 --> 00:33:28.500
And then if we look at the images, you
can see how transition is happening.

00:33:34.490 --> 00:33:36.260
All the way.

00:33:41.140 --> 00:33:41.250
Okay.

00:33:41.250 --> 00:33:41.700
[ Applause ]

00:33:41.700 --> 00:33:46.230
>> So here are the conclusions.

00:33:46.230 --> 00:33:48.980
It's very easy to use and write.

00:33:48.980 --> 00:33:53.280
I wrote so many kernels in the past few
weeks, and it was just straightforward.

00:33:53.280 --> 00:33:56.570
You develop your code on the C
side and pretty much transfer it.

00:33:56.570 --> 00:34:03.270
And if you're using intrinsic, they have
functions on OpenCL side which directly map.

00:34:03.270 --> 00:34:08.170
And performance is almost on par
with GCD and SSE performance.

00:34:08.170 --> 00:34:11.640
If you're trying to use POSIX thread,
then you will see that there is a problem.

00:34:11.640 --> 00:34:13.870
And GCD tends to outperform POSIX.

00:34:13.870 --> 00:34:15.880
So those are the lessons that we saw.

00:34:15.880 --> 00:34:23.900
It scales very well across course, and effectively utilizes
SSE, and you can write portable code that will work

00:34:23.900 --> 00:34:28.630
across devices, so you don't have
to worry about whether the same code

00:34:28.630 --> 00:34:31.260
that you have will have problem
with next generation of CPUs.

00:34:31.260 --> 00:34:35.020
They would automatically be handled
behind the scenes for you.

00:34:35.020 --> 00:34:38.980
And it works across device generations.

00:34:38.980 --> 00:34:47.400
All right, with that I will ask
James to come over from NVIDIA.

00:34:47.400 --> 00:34:48.670
>> James Fung: Thanks.

00:34:51.510 --> 00:34:56.310
[ Applause ]

00:34:56.310 --> 00:34:58.460
>> James Fung: So my name's --
sorry -- my name's James Fung,

00:34:58.460 --> 00:35:02.580
I'll be talking about optimizing for GPUs using OpenCL.

00:35:02.580 --> 00:35:09.190
So today we're going to talk about how to get the maximum
performance out of the GPU in your OpenCL program.

00:35:09.190 --> 00:35:11.010
This is the outline of the talk.

00:35:11.010 --> 00:35:14.080
We're going to start talking with
work group size heuristics.

00:35:14.080 --> 00:35:16.760
How big should you make your work
groups, how much should you launch?

00:35:16.760 --> 00:35:19.530
Then we're going to move on to
some instruction optimizations,

00:35:19.530 --> 00:35:21.540
in particular we're going to talk about divergence.

00:35:21.540 --> 00:35:23.560
So you're on a parallel machine.

00:35:23.560 --> 00:35:24.840
Branching's fine.

00:35:24.840 --> 00:35:27.120
But there's some things you want
to think about when you do branch.

00:35:27.120 --> 00:35:29.680
Then we're going to move on to memory optimizations.

00:35:29.680 --> 00:35:34.960
On a parallel processor keeping the data moving through
that processor and keeping it fed is really important.

00:35:34.960 --> 00:35:40.430
And we're going to talk about two key memory
optimizations that let you get the maximum throughput.

00:35:40.430 --> 00:35:43.620
In particular, coalescing and how to handle local memory.

00:35:43.620 --> 00:35:50.050
And finally, we're going to talk about using the GP texture
hardware, of course GPU is a graphics processor in addition

00:35:50.050 --> 00:35:55.620
to a compute processor now, but that graphics
functionality is still available to you in OpenCL

00:35:55.620 --> 00:35:56.940
and there's some good features you can use.

00:35:56.940 --> 00:36:01.780
And we're going to show a demo of optical
flow using some of the texturing hardware.

00:36:01.780 --> 00:36:03.340
Now just one note.

00:36:03.340 --> 00:36:08.130
In my talk in OpenCL, you'll be familiar with, it's
called work items and work groups.

00:36:08.130 --> 00:36:13.820
In CUDA, both on the architecture and sort of in the
programming language, we call them threads and blocks.

00:36:13.820 --> 00:36:15.700
I might be using the two interchangeably.

00:36:15.700 --> 00:36:17.880
But you should know they're interchangeable.

00:36:17.880 --> 00:36:21.610
So let's take a look at what's actually on the GPU.

00:36:21.610 --> 00:36:25.050
So this is a picture of a GTX 285.

00:36:25.050 --> 00:36:31.040
On the GPU, on this -- you have up to
240 thread processors available to you.

00:36:31.040 --> 00:36:38.580
These are grouped together into 30 streaming
multiprocessors, with up to 4 gigabytes of RAM.

00:36:38.580 --> 00:36:45.300
So on the entire card you have up to 1
teraflop of peak theoretical single precision.

00:36:45.300 --> 00:36:48.610
Now that is actually IEEE 754
precision, but it's not fully conformant

00:36:48.610 --> 00:36:51.410
with the spec in some degree -- in some ways.

00:36:51.410 --> 00:36:54.210
In addition to single precision,
you also have double precision.

00:36:54.210 --> 00:36:58.100
And the card provides 87 giga flops
in double precision to you.

00:36:58.100 --> 00:37:08.090
Now in each of these SMs, these are these logical groupings
of thread processors, each SM holds 8 thread processors,

00:37:08.090 --> 00:37:14.780
1 double-precision unit, and in addition has 16
kilobytes of local memory and 16K worth of registers.

00:37:14.780 --> 00:37:20.250
So with so many processors available to you, it's
really important to be able to keep them all busy.

00:37:20.250 --> 00:37:27.580
Now I think the note here is that your work items or your
threads are executed concurrently in warps of 32 threads.

00:37:27.580 --> 00:37:29.040
So a warp is 32 threads.

00:37:29.040 --> 00:37:30.580
Basically execute in lock step.

00:37:30.580 --> 00:37:37.090
So however big your work group is, 32 at a time will
execute in a lock step in actual physical hardware.

00:37:37.090 --> 00:37:39.800
Now obviously thread instructions are executed sequentially.

00:37:39.800 --> 00:37:43.530
So executing other warps is the only way
to hide latency and keep the hardware busy.

00:37:43.530 --> 00:37:49.620
What we mean by that is if one warp needs to do
a fetch from memory and is waiting on memory,

00:37:49.620 --> 00:37:53.750
then if you have other warps available to
run on that SM it will schedule another warp

00:37:53.750 --> 00:37:58.570
in that maybe can do some arithmetic
computation inside the ALUs.

00:37:58.570 --> 00:38:05.820
And you like to provide the scheduler with as many warps
for it to choose from in order to keep the machine busy.

00:38:05.820 --> 00:38:07.660
So we think of something here called occupancy.

00:38:07.660 --> 00:38:14.160
And that's what we define as a number of resident warps
divided by the maximum possible number of resident warps.

00:38:14.160 --> 00:38:20.870
Now obviously you like an occupancy of 1, which means that
you've put as many warps on that SM as you possibly can.

00:38:20.870 --> 00:38:27.510
In practice of course, that's limited by resource
usage, in particular, registers and local memory.

00:38:27.510 --> 00:38:29.210
Let's look at an example then.

00:38:29.210 --> 00:38:31.310
So we have a hardware SM or shader unit.

00:38:31.310 --> 00:38:36.460
And say the specs say we can support
up to eight work groups.

00:38:36.460 --> 00:38:43.330
Each SM has 43 kilobytes total local memory
and can support up to 1024 work item maximum.

00:38:43.330 --> 00:38:45.210
So that's the hardware.

00:38:45.210 --> 00:38:51.380
Now the second point there, let's say we write our
kernel and we define work group size of 128 work items.

00:38:51.380 --> 00:38:58.120
But we find inside our kernel we need -- for each work item
we decide we'd like to use 24 kilobytes of local memory.

00:38:58.120 --> 00:39:02.100
Well, you've only got 32 kilobytes
of local memory on that SM.

00:39:02.100 --> 00:39:09.390
That means that the -- it can only fit one of these work
groups on that SM because it's limited by the local memory.

00:39:09.390 --> 00:39:12.270
It would need 48 at least to fit two of those.

00:39:12.270 --> 00:39:15.620
So what happens now only one work group
is going to be available on the SM.

00:39:15.620 --> 00:39:18.530
And that's going to give you 128 threads on the SM.

00:39:18.530 --> 00:39:22.650
And that's out of a maximum possible 1024 work items.

00:39:22.650 --> 00:39:29.040
So you have a little occupancy. Check your GPU
documentation for specific details on your hardware.

00:39:29.040 --> 00:39:36.040
So what are some heuristics as to how
big you should make these work groups

00:39:36.040 --> 00:39:38.340
and how many work groups you should launch.

00:39:38.340 --> 00:39:42.810
Well firstly, the number of work groups should be
at least greater than your number of multiprocessors

00:39:42.810 --> 00:39:46.680
so that every multiprocessor has at
least one work group to operate on.

00:39:46.680 --> 00:39:50.820
Secondly, the number of work groups per
multiprocessor should be greater than say two,

00:39:50.820 --> 00:39:54.610
so you have multiple work groups on each multiprocessor.

00:39:54.610 --> 00:39:59.180
So multiple work groups can run concurrently in a
multiple processor, and then if you have more than one,

00:39:59.180 --> 00:40:03.810
work groups that aren't waiting at a barrier, say,
can be swapped in and keeps the hardware busy.

00:40:03.810 --> 00:40:05.640
And again, that happens all transparently to you.

00:40:05.640 --> 00:40:09.220
But as long as you load the machine
up it will give you good efficiency.

00:40:09.220 --> 00:40:14.740
And again, subject to resource availability such as
registers or local memory, which is the example we just saw.

00:40:14.740 --> 00:40:19.990
Your total number of work groups you'd like to launch say
would be -- you want to launch in the order of hundreds.

00:40:19.990 --> 00:40:24.730
And that's going to let you scale from
small devices across to larger devices.

00:40:24.730 --> 00:40:28.380
These work groups are in pipeline
fashion, kind of batched out to the SMs.

00:40:28.380 --> 00:40:36.180
If you can even launch thousands of work groups in a kernel,
that's going to let you scale across multiple generations.

00:40:36.180 --> 00:40:40.400
The number of work items in a work group
generally should be a multiple of the warp size.

00:40:40.400 --> 00:40:45.440
So again, a warp is executed physically
in a lock step, if -- and is 32.

00:40:45.440 --> 00:40:49.460
If only -- if you have some morph
that's only like 15 or 2 or 3 threads,

00:40:49.460 --> 00:40:52.680
then the other threads in the warp aren't used.

00:40:52.680 --> 00:40:54.950
So basically want to keep all the
threads in the warp active.

00:40:54.950 --> 00:40:59.220
For control flow, the main thing you
want to be thinking about is divergence.

00:40:59.220 --> 00:41:01.380
This happens when you branch.

00:41:01.380 --> 00:41:03.340
So branching it fine.

00:41:03.340 --> 00:41:07.710
But you get divergence when work items
in a single warp take different paths.

00:41:07.710 --> 00:41:11.320
And what happens is different execution
paths then need to get serialized.

00:41:11.320 --> 00:41:16.370
And what that means is the GPU, if you
branch within a warp, has to, say, evaluate the

00:41:16.370 --> 00:41:18.940
if statement or the if part of the branch.

00:41:18.940 --> 00:41:24.030
And then go back, evaluate the else part
of the branch before it can move on.

00:41:24.030 --> 00:41:26.810
And so in our first example down
below is an example where that's going

00:41:26.810 --> 00:41:29.640
to happen, is when you're branching within a warp.

00:41:29.640 --> 00:41:34.390
So if you thread -- say you have some control flow
and you're examining your particular thread ID.

00:41:34.390 --> 00:41:40.700
And in this case, thread 0, 1, and 2 would take the if
part, and maybe other ones would take the else part.

00:41:40.700 --> 00:41:46.240
In that case, your branch granularity
is less than your warp size.

00:41:46.240 --> 00:41:52.780
So the GPU has to evaluate the if part,
then the else part before it moves on.

00:41:52.780 --> 00:41:58.380
But you can still have -- sorry -- you can
still have branching but avoid divergence.

00:41:58.380 --> 00:42:02.570
If your branch clearing granularity
is a whole multiple of your warp size.

00:42:02.570 --> 00:42:10.220
So in the case below, we have the branch,
but it's occurring basically in whole warps.

00:42:10.220 --> 00:42:16.690
So if warp 0, 1, or 2, say, take one part
of the if branch and then the other warps

00:42:16.690 --> 00:42:22.040
in that work group take the other end of the
branch, the hardware will know that it doesn't need

00:42:22.040 --> 00:42:28.670
to reality the other end of the
branch for that work group because --

00:42:28.670 --> 00:42:35.570
sorry, for that warp because they've
all taken one -- one leg instead.

00:42:35.570 --> 00:42:37.770
Let's look at some examples.

00:42:37.770 --> 00:42:39.250
So here we have a parallel reduction.

00:42:39.250 --> 00:42:44.400
In this case all we're trying to
do is find the sum of the buffers.

00:42:44.400 --> 00:42:49.230
And we'd like to use a parallel logarithm
in order to use all the processors.

00:42:49.230 --> 00:42:55.050
So we could launch a kernel and
let's look at say one work item.

00:42:55.050 --> 00:42:57.740
And here we have the values, say stored in local memory.

00:42:57.740 --> 00:43:00.350
And we have the threads of the work item.

00:43:00.350 --> 00:43:02.280
And each thread is going it to add its neighbor.

00:43:02.280 --> 00:43:05.990
But we don't need to have all the
threads active all the time.

00:43:05.990 --> 00:43:08.490
But we iterate.

00:43:08.490 --> 00:43:13.560
So basically each thread adds its neighbors, then goes
to the next iteration, each thread adds a partial sum,

00:43:13.560 --> 00:43:16.170
and we proceed in that fashion
till finally in the last thread,

00:43:16.170 --> 00:43:20.490
it's going to add two numbers and
comes up with the sum of the buffer.

00:43:20.490 --> 00:43:22.880
As you can see in this case we're
going to have a lot of divergence.

00:43:22.880 --> 00:43:29.080
We have thread 0 doing one thing, thread 1 in this case
is not doing anything, thread 2 is doing something else.

00:43:29.080 --> 00:43:35.300
So we have this interleaved type of workflow
and that'd going to give us divergence.

00:43:35.300 --> 00:43:38.970
So let's look at the same problem but solve
it in a way that doesn't need divergence.

00:43:38.970 --> 00:43:41.020
So again we have our array in local memory.

00:43:41.020 --> 00:43:46.440
Now we know each thread needs to
add one neighbor or one partial sum.

00:43:46.440 --> 00:43:50.500
But there's no reason that it has to be the
immediate neighbor, we're simply looking for a sum.

00:43:50.500 --> 00:43:52.550
So here we've simply restructured the problem.

00:43:52.550 --> 00:43:56.590
But we've now created a continuous set of
threads that are all doing the same warp.

00:43:56.590 --> 00:44:00.730
Now this is a smaller problem shown
here than you would have on a GPU,

00:44:00.730 --> 00:44:05.540
warp size on a GPU is 32, but imagine
you had a warp size of 8.

00:44:05.540 --> 00:44:11.400
In this case, in the first step now we have
an entire warp doing the same set of work.

00:44:11.400 --> 00:44:13.730
And so we can avoid divergence.

00:44:13.730 --> 00:44:17.310
When you're accessing the main memory on the GPU
there's a couple of tricks that are available to you

00:44:17.310 --> 00:44:19.200
to get of the maximum performance out of it.

00:44:19.200 --> 00:44:24.260
So you can say the main memory or the global memory,
that's kind of the memory that's advertised when you go

00:44:24.260 --> 00:44:28.320
to the store and buy the card, you
know, 512 megabytes or 1 gigabyte card.

00:44:28.320 --> 00:44:31.280
So referring to the global memory there.

00:44:31.280 --> 00:44:36.510
And there's some rules that if you follow
lets you get the peak data throughputs.

00:44:36.510 --> 00:44:39.590
And these are rules that kind of vary by generation.

00:44:39.590 --> 00:44:45.210
So with -- up to Compute 1.1,
we have different compute levels

00:44:45.210 --> 00:44:47.960
that describe the capabilities of the hardware.

00:44:47.960 --> 00:44:57.330
But for a GeForce 8600, 8800 GT, 9400 M, 9600
GT, basically if you can have a half-warp,

00:44:57.330 --> 00:45:04.080
16 threads in this case, a half-warp can access a
contiguous and aligned and in order set of data.

00:45:04.080 --> 00:45:07.540
Instead of falling back, instead
of having 16 warps fall back

00:45:07.540 --> 00:45:14.290
to 17 different transactions it can simply do one
larger transaction as long as you follow some rules.

00:45:14.290 --> 00:45:17.660
So the rules are basically -- and they need to be --

00:45:17.660 --> 00:45:21.150
the starting address for a region
must be a multiple of region size.

00:45:21.150 --> 00:45:23.080
So there's an alignment issue there.

00:45:23.080 --> 00:45:26.470
The kth thread and half-warp needs to access
the kth element of the work group.

00:45:26.470 --> 00:45:28.250
So there is an in order access there.

00:45:28.250 --> 00:45:31.640
And there's also a contiguous access requirement.

00:45:31.640 --> 00:45:32.910
There's one exception.

00:45:32.910 --> 00:45:34.910
Not all threads need to be participating.

00:45:34.910 --> 00:45:37.350
But let's look at some real examples here.

00:45:37.350 --> 00:45:41.080
So in these two examples we achieve coalescing.

00:45:41.080 --> 00:45:45.230
So in the first upper case, we
have a half-warp of threads.

00:45:45.230 --> 00:45:51.760
And are accessing a contiguous region of memory, it's
properly aligned, and each one's accessing it in order.

00:45:51.760 --> 00:45:57.320
So instead of the 16 threads having to do 16
different memory accesses the GPU coalesces

00:45:57.320 --> 00:45:59.830
that into one access of larger width.

00:45:59.830 --> 00:46:02.660
And so you get that back in one memory transaction.

00:46:02.660 --> 00:46:05.510
And then in the lower case we see our one exception here.

00:46:05.510 --> 00:46:09.160
Not all threads need to participate, but as
long as the other rules are all followed,

00:46:09.160 --> 00:46:12.570
and again it can do this in one single transaction.

00:46:12.570 --> 00:46:16.560
Let's look at some areas, some
situations, where we break coalescing.

00:46:16.560 --> 00:46:23.010
So again we have our properly assigned contiguous region,
but now we've permuted the access here in threads 1 and 2.

00:46:23.010 --> 00:46:26.070
Now it's going to break our coalescing
on the older generation,

00:46:26.070 --> 00:46:29.070
and it's going to fall back to
16 different memory transactions.

00:46:29.070 --> 00:46:31.320
It's going to impactor performance quite a bit.

00:46:31.320 --> 00:46:36.550
Similarly in the lower case, we're contiguous,
we're in order, but we're miss assigned.

00:46:36.550 --> 00:46:41.080
And so again it's going to fall back to 16
transactions instead of a single coalesced transaction.

00:46:41.080 --> 00:46:44.380
On the newer GPUs the coalescing rules are much improved.

00:46:44.380 --> 00:46:49.150
This applies to GeForce GTX 285 and the GeForce 330 M.

00:46:49.150 --> 00:46:52.840
Basically what the hardware is going
to be doing is it combines addressing

00:46:52.840 --> 00:46:56.480
within a half-warp into one or more blind segments.

00:46:56.480 --> 00:47:01.370
And it figures out what's the best way
to transfer the number of segments.

00:47:01.370 --> 00:47:06.090
So all threads with addresses within a segment
are serviced by a single memory transaction,

00:47:06.090 --> 00:47:08.890
regardless now of ordering or alignment within the segment.

00:47:08.890 --> 00:47:12.080
So let's look at some examples.

00:47:12.080 --> 00:47:15.720
This is what happens on the GTX 285, for instance.

00:47:15.720 --> 00:47:17.830
When you go to access your global memory.

00:47:17.830 --> 00:47:25.870
In the left-most case, we have a contiguous segment
again, it's completely out of order but that's fine,

00:47:25.870 --> 00:47:28.160
it's all -- it's contiguous and it's aligned.

00:47:28.160 --> 00:47:33.790
And so the GPU knows that it can service those
requests with sun single 64-byte segment.

00:47:33.790 --> 00:47:39.100
So goes ahead and gets basically get your coalesced
access, despite being completely out of order.

00:47:39.100 --> 00:47:46.870
In the second case there's a misalignment problem, so
the GPU can't do a 64-byte aligned memory transaction.

00:47:46.870 --> 00:47:52.830
But it looks at what you're doing, and it says
well, we can do this with a larger 128-byte segment.

00:47:52.830 --> 00:47:53.660
And so it'll go ahead.

00:47:53.660 --> 00:47:59.820
It will do a larger transfer, and so there's still some
data transfer that maybe you're not going to be using.

00:47:59.820 --> 00:48:04.190
But again, it won't suddenly fall
back to 16 memory transactions.

00:48:04.190 --> 00:48:06.990
So you don't fall off a cliff.

00:48:06.990 --> 00:48:11.220
And similarly, the last case has
another misaligned transaction.

00:48:11.220 --> 00:48:17.420
But in this case, just because of the wait in line the GPU
says well, I can do a 32-byte segment and a 64-byte segment.

00:48:17.420 --> 00:48:21.030
And again you haven't fallen back
to 16 different transactions.

00:48:21.030 --> 00:48:22.720
What happens when you actually do coalescing.

00:48:22.720 --> 00:48:24.040
So here's a simple experiment.

00:48:24.040 --> 00:48:25.660
This is on the 8800 GTX.

00:48:25.660 --> 00:48:29.130
So it's a previous generation GPU
with the more stringent rules.

00:48:29.130 --> 00:48:35.810
But basically in our kernel all we do is read
a float, increment this value, write it back.

00:48:35.810 --> 00:48:40.040
We're processing 3 million floats, 12 megabytes of data.

00:48:40.040 --> 00:48:44.130
We average our time over 10,000 runs.

00:48:44.130 --> 00:48:49.510
So 12K blocks, 256 threads per block meaning the floats.

00:48:49.510 --> 00:48:55.770
If we achieved coalesced access, we could do
this type of operation in 350 microseconds.

00:48:55.770 --> 00:48:59.050
In the second case there, coalesce but
with some threads not participating.

00:48:59.050 --> 00:49:00.350
That one exception.

00:49:00.350 --> 00:49:05.580
Again, the timing is still basically of
the same as a fully coalesced access.

00:49:05.580 --> 00:49:08.120
If we break coalescing, we can see the penalty for that.

00:49:08.120 --> 00:49:13.630
We've gone from 350 microseconds
up to almost 3500 microseconds.

00:49:13.630 --> 00:49:16.450
And that happens if we permuted or
had some misaligned thread access.

00:49:16.450 --> 00:49:21.120
So it's almost a 10 times speed difference
if you're able to achieve coalescing.

00:49:21.120 --> 00:49:24.350
In the second case, we can see the same kind of effect.

00:49:24.350 --> 00:49:27.960
And in this case what if we were
reading say, float3's data.

00:49:27.960 --> 00:49:32.570
That's kind of a -- it's a 3-element data structure.

00:49:32.570 --> 00:49:34.550
Doesn't actually follow coalescing rules.

00:49:34.550 --> 00:49:37.680
But we can do some tricks where we can
actually coalesce that through local memory.

00:49:37.680 --> 00:49:42.980
So you can actually load that in, say -- imagine
that it was simply a set of single element floats,

00:49:42.980 --> 00:49:48.930
put into local memory, treat it as a 3-element
float, and then write it back out to global.

00:49:48.930 --> 00:49:51.470
The point here though is if you
can use your this local memory

00:49:51.470 --> 00:49:54.210
to achieve coalescing, again you get really good results.

00:49:54.210 --> 00:49:58.930
So uncoalesced cases give you 3 -- 3300 microseconds.

00:49:58.930 --> 00:50:03.020
But again, if you achieve coalescing 360 microseconds.

00:50:03.020 --> 00:50:08.520
So coalescing is probably one of the major
performance improvements if you're not achieving it.

00:50:08.520 --> 00:50:11.280
We talked about local memories, mentioned several times.

00:50:11.280 --> 00:50:12.260
What is local memory.

00:50:12.260 --> 00:50:17.720
Well local memory is basically on our GPU,
it's a high-speed, low-latency on chip memory.

00:50:17.720 --> 00:50:23.010
So this is opposed to global memory which is on
the card, but it's not on actually GPU chip itself.

00:50:23.010 --> 00:50:28.190
Local memory is actually on the GPU and in fact
you can see there, it's actually inside the SM.

00:50:28.190 --> 00:50:31.560
And the way you can think about
it is as a user-managed cache.

00:50:31.560 --> 00:50:39.150
If you're processing an image, say, a national way to use
it would be to load a tile of that image into shared memory.

00:50:39.150 --> 00:50:44.830
And then all the threads in your SM can actually read
and write into that tile data as much as it wants.

00:50:44.830 --> 00:50:48.230
When you get your final result in, you
can push that back out to main memory.

00:50:48.230 --> 00:50:55.080
So in that case you can get a lot of quick accesses
without paying the penalty of going to global memory.

00:50:55.080 --> 00:51:00.220
When you're using local memory, it's not coalescing you're
worried about, you can actually access it very fast.

00:51:00.220 --> 00:51:03.720
But there are still some rules that
will give you better performance.

00:51:03.720 --> 00:51:06.140
In particular, something called a bank conflict.

00:51:06.140 --> 00:51:10.940
So on the GPU local memory -- local
memory is divided into banks.

00:51:10.940 --> 00:51:14.950
And that's essential to achieve high
bandwidth in a parallel application.

00:51:14.950 --> 00:51:17.640
And so each bank can service one address per cycle.

00:51:17.640 --> 00:51:22.580
And in general, memory can service as many
simultaneous accesses as it has banks.

00:51:22.580 --> 00:51:28.600
If you have multiple simultaneous accesses to a single
bank, the same bank, then you get a bank conflict.

00:51:28.600 --> 00:51:31.180
And in that case, those accesses need to be serialized.

00:51:31.180 --> 00:51:33.820
So let's look at some examples here.

00:51:33.820 --> 00:51:38.680
So here we have our 16 banks of our
local memory on the H address, 32-bit.

00:51:38.680 --> 00:51:41.620
So imagine you had an array of integers saying local memory.

00:51:41.620 --> 00:51:49.120
Then the first 16 integers in your array would be a
service by banks 0-15 and it loops around again.

00:51:49.120 --> 00:51:56.220
Now if your threads go ahead and access it and then
the -- in the leftmost case, kind of nice, orderly,

00:51:56.220 --> 00:52:01.240
it's all in order and are accessing only
one bank, there's no bank conflicts.

00:52:01.240 --> 00:52:04.840
Each bank can service all the threads.

00:52:04.840 --> 00:52:07.680
Similarly, permeation is fine.

00:52:07.680 --> 00:52:16.450
The key thing here is as long as those banks don't have
multiple requests coming to them, there's no bank conflicts.

00:52:16.450 --> 00:52:18.860
So examples where you do get a bank conflict.

00:52:18.860 --> 00:52:21.320
So on the left we have a two-way bank conflict.

00:52:21.320 --> 00:52:23.860
So we have multiple threads, in this case, 0 and 8.

00:52:23.860 --> 00:52:25.240
Both accessing bank 0.

00:52:25.240 --> 00:52:30.010
We got a 2-way bank conflict, hardware has
to service thread 0, then service thread 8.

00:52:30.010 --> 00:52:31.530
So it's serialized.

00:52:31.530 --> 00:52:33.400
Then you can have high order bank conflicts.

00:52:33.400 --> 00:52:38.620
On the right side we have 8-way bank conflict,
where 8 threads were all accessing the same bank

00:52:38.620 --> 00:52:41.950
and so it's got to serialize all those 8 transactions.

00:52:41.950 --> 00:52:46.610
Finally, the texture functionality on the
GPU is still available to you through OpenCL.

00:52:46.610 --> 00:52:48.380
And we're going to show an example here.

00:52:48.380 --> 00:52:51.900
The example is optical flow.

00:52:51.900 --> 00:52:55.320
Basically, optical flow is calculating
the motion of points in an image pair.

00:52:55.320 --> 00:52:56.770
So here we have an image.

00:52:56.770 --> 00:52:59.510
It's actually have standard test dataset.

00:52:59.510 --> 00:53:02.170
It's a picture of a guy closing his trunk.

00:53:02.170 --> 00:53:04.920
As you can see him, he's kind of pushing down the trunk.

00:53:04.920 --> 00:53:09.250
And what optical flow is going to do is calculate
the motion between image one and image two.

00:53:09.250 --> 00:53:12.530
And then down below we have the kind
of the results of the optical flow.

00:53:12.530 --> 00:53:16.780
And those blue areas, if you kind of
zoom in, they're basically small arrows.

00:53:16.780 --> 00:53:21.320
That tells you the direction and magnitude
of the motion that we see in the image pair.

00:53:21.320 --> 00:53:26.740
This side of the computation is embarrassingly parallel,
and compute-intensive, it's a great candidate for the GPU.

00:53:26.740 --> 00:53:32.180
And it's also used in applications such as inner
stabilization, future tracking and computer vision.

00:53:32.180 --> 00:53:34.940
It's also used in things like video encoding.

00:53:34.940 --> 00:53:39.750
Optical flow makes use of texture cache
and hardware bilinear triplication.

00:53:39.750 --> 00:53:40.800
Let's talk about those.

00:53:40.800 --> 00:53:43.390
Let's look at how do we actually implement optical flow?

00:53:43.390 --> 00:53:45.440
This is very high-level description.

00:53:45.440 --> 00:53:49.400
There's lots of many, many different
implementations of optical flow.

00:53:49.400 --> 00:53:54.540
Here we examine the pyramidal Lucas-Kanade
optical flow, which is a pretty well known one.

00:53:54.540 --> 00:53:56.900
Basically, we create an image pyramid first.

00:53:56.900 --> 00:54:02.410
So we downsample the image, so we have the
same image downsampled to different sizes.

00:54:02.410 --> 00:54:05.880
We do some edge filtering on it to
get a vertical or horizontal edges,

00:54:05.880 --> 00:54:08.810
or derivatives, at each level of the pyramid.

00:54:08.810 --> 00:54:13.660
Then we basically put it into some
kernels that solve for the optical flow.

00:54:13.660 --> 00:54:20.860
Solving for the optical flow is basically as easy as
you know, computing this G inverse B matrix calculation.

00:54:20.860 --> 00:54:22.470
And G is 2 by 2.

00:54:22.470 --> 00:54:25.950
So we can simply precompute G at every point
in our image so we have those available

00:54:25.950 --> 00:54:28.120
to us when we need to do the flow calculation.

00:54:28.120 --> 00:54:29.980
And finally, we can go ahead and solve for the flow.

00:54:29.980 --> 00:54:37.390
And the optical flow processing is a -- is sequentially
goes from the smallest level of the pyramid,

00:54:37.390 --> 00:54:42.690
that kind of gives you course estimates, and you refine
it down to larger levels of the pyramid in putting

00:54:42.690 --> 00:54:47.490
in the flow results to each level
until you get the final base of level.

00:54:48.820 --> 00:54:51.320
So solving for flow is an iterative logarithm.

00:54:51.320 --> 00:54:56.760
You estimate the motion, calculate
the vector, V, but it's iterative.

00:54:56.760 --> 00:55:00.740
So you've got to guess for V, then
you update your current position,

00:55:00.740 --> 00:55:04.070
and then you iterate again to get a new guess for V.

00:55:04.070 --> 00:55:10.120
And so what you end up doing is you have a small window of
computation that's kind of slowly sliding across your image.

00:55:10.120 --> 00:55:13.920
The nice thing is on the GPU you have a texture cache.

00:55:13.920 --> 00:55:18.450
So if you keep these images in cache, even
though you're sliding variably along the image,

00:55:18.450 --> 00:55:21.280
and some areas of the image would
have large motions and some areas

00:55:21.280 --> 00:55:24.900
of the image would have only a small amount of motion.

00:55:24.900 --> 00:55:29.490
Basically you slide around the image,
the GPU will manage the cache for you.

00:55:29.490 --> 00:55:36.960
So you get cached coherent -- I'm sorry
-- cache data access, bodies and textures.

00:55:36.960 --> 00:55:41.100
Another nice thing about the texture pipeline
on the GPU is hardware interpolation.

00:55:41.100 --> 00:55:45.720
So in our algorithms, subpixel accuracy
and subpixel sampling is crucial.

00:55:45.720 --> 00:55:51.510
Between iterations as you're sliding around, your center
position generally is not going to land exactly on a pixel.

00:55:51.510 --> 00:55:53.730
So rather what you're going to need to do is interpolate.

00:55:53.730 --> 00:55:56.680
So if you take a closer look at the computations here,

00:55:56.680 --> 00:56:01.240
where I and J in the upper equation
are both the images.

00:56:01.240 --> 00:56:05.830
So we're looking at the I at X Y, but
we're looking at the second image, J,

00:56:05.830 --> 00:56:08.770
with some offset, which is our current guess for our flow.

00:56:08.770 --> 00:56:15.720
And so when we resample that, we need to get
interpolated samples, and we use that information in order

00:56:15.720 --> 00:56:19.560
to compute our B vector from the previous slide.

00:56:19.560 --> 00:56:26.220
The nice thing now though is that because we're using the
texture hardware, we can apply linear interpolation to it.

00:56:26.220 --> 00:56:34.030
So we can look up on floating-point coordinates, slightly
offset, and we retrieve a bilinear interpolate sample.

00:56:34.030 --> 00:56:37.920
And that bilinear interpolation
happens in hardware, in GP hardware.

00:56:37.920 --> 00:56:40.930
So that's really useful hardware for you to use.

00:56:40.930 --> 00:56:44.270
So we've looked at the pyramidal
Lucas-Kanade flow for this demo,

00:56:44.270 --> 00:56:48.970
and the additional thing here is the visualization
that you're going to see is actually done by the GPU

00:56:48.970 --> 00:56:51.830
by sharing data between OpenGL and OpenCL.

00:56:51.830 --> 00:56:53.890
So maybe we can switch over.

00:56:53.890 --> 00:56:54.400
Okay, great.

00:56:54.400 --> 00:56:58.790
So in the upper left there we see our optical flow.

00:56:58.790 --> 00:57:03.760
And if Eric kind of moves around here, you
can see he's calculating the motion vectors.

00:57:03.760 --> 00:57:08.430
So the green is the motion being calculated for each frame.

00:57:08.430 --> 00:57:10.490
And we're calculating dense optical flow in this case.

00:57:10.490 --> 00:57:13.290
So it's 640 by 480 input.

00:57:13.290 --> 00:57:18.580
With older methods they would usually only
try to compute to flow at particular points.

00:57:18.580 --> 00:57:24.460
But with the processing power in the GPU
we can compute optical flow at every point.

00:57:24.460 --> 00:57:26.570
So we're actually calculating at every point.

00:57:26.570 --> 00:57:32.300
We're only displaying kind of a subset, a
grid, in order to make the arrows visible.

00:57:32.300 --> 00:57:38.050
This is running on a GTX 285, and these arrows
are actually being drawn via buffer sharing.

00:57:38.050 --> 00:57:43.240
So I can simply compute the two end points
of the arrows that I'd like to draw.

00:57:43.240 --> 00:57:44.990
Send that right into OpenGL.

00:57:44.990 --> 00:57:51.910
And it can go ahead, we do a GL again, you know, GL lines,
basically, strong lines, and that all happens on the GPU.

00:57:51.910 --> 00:57:53.280
Okay, thanks a lot.

00:57:53.280 --> 00:57:57.140
Actually, just going to --

00:58:00.450 --> 00:58:02.890
[ Applause ]

00:58:02.890 --> 00:58:08.470
>> Switch it back to this -- okay, and so if we
look at our performance across different GPUs,

00:58:08.470 --> 00:58:13.890
different GPUs have different amount of
SMs, and certainly pays to have more SMs.

00:58:13.890 --> 00:58:19.240
So comparing 8800 GT versus GTX 285, running
the same code across two different GPUs

00:58:19.240 --> 00:58:22.340
with more SMs you can get better performance.

00:58:22.340 --> 00:58:28.390
Okay, thanks a lot, and I'm going
to hand it over to Aaftab.

00:58:28.390 --> 00:58:30.770
[ Applause ]

00:58:30.770 --> 00:58:36.530
>> So hope these two sessions gave you a deeper
insight into how OpenCL works, how to use GPUs,

00:58:36.530 --> 00:58:40.890
and some guidelines on writing performance
OpenCL programs for CPUs and GPUs.

00:58:40.890 --> 00:58:41.250
Thank you.

