1
00:00:06,440 --> 00:00:08,210
>> James McCartney: My name is James McCartney.

2
00:00:08,210 --> 00:00:11,940
After me, Eric Allamanche will be up to talk.

3
00:00:11,940 --> 00:00:14,080
There's going to be two parts to this talk.

4
00:00:14,080 --> 00:00:18,020
I'm going to talk about Audio Processing
Basics, and then Eric will talk

5
00:00:18,020 --> 00:00:23,920
about the Voice Processing Audio Unit and Audio Codecs.

6
00:00:23,920 --> 00:00:33,240
So, in many of the past WWDCs we've in Core Audio, we've
given a lot of talks about Core Audio and how to use it.

7
00:00:33,240 --> 00:00:38,710
And we've always assumed a certain knowledge
about audio that everyone is an audio programmer.

8
00:00:38,710 --> 00:00:43,350
But a lot of people just want to be able to play audio.

9
00:00:43,350 --> 00:00:50,550
So, I'm-- we're going to step back a bit and I'm
going to talk about well, what is digital audio

10
00:00:50,550 --> 00:00:53,770
and how does it, you know, how is it represented.

11
00:00:53,770 --> 00:00:56,520
So, there'll be three parts to my talk

12
00:00:56,520 --> 00:01:03,870
about audio representation formats,
converting audio, and processing audio.

13
00:01:03,870 --> 00:01:07,770
So, what is digital audio?

14
00:01:07,770 --> 00:01:10,450
Sound is a moving air molecules.

15
00:01:10,450 --> 00:01:16,770
And a microphone transduces that
into an electronic waveform.

16
00:01:16,770 --> 00:01:25,020
And you sample that waveform at a
periodic interval called sampling rate.

17
00:01:25,020 --> 00:01:27,140
And each sample is a number.

18
00:01:27,140 --> 00:01:35,680
And so, you need to have at least two numbers per cycle of
a sine wave in order to be able to reconstruct a sine wave.

19
00:01:35,680 --> 00:01:41,540
And therefore, the highest reproducible
frequency is half of the sampling rate.

20
00:01:41,540 --> 00:01:46,950
So, there's different ways -- once you
have sampled the waveform as numbers,

21
00:01:46,950 --> 00:01:50,440
there's different ways to represent
that, you know, in the computer.

22
00:01:50,440 --> 00:01:58,720
I'm going to talk about liner PCM, nonlinear
PCM, and packetized compressed formats.

23
00:01:58,720 --> 00:02:04,520
Linear PCM or LPCM is the most direct
way to represent the sampled audio.

24
00:02:04,520 --> 00:02:09,940
And you just store the sampled numbers in binary form.

25
00:02:09,940 --> 00:02:12,080
But there's a lot of ways to do that.

26
00:02:12,080 --> 00:02:15,240
Are the numbers you're storing integer or floating point?

27
00:02:15,240 --> 00:02:19,470
Are the integers, if you're storing
integers, are they signed or unsigned.

28
00:02:19,470 --> 00:02:20,930
How many bits are in each number?

29
00:02:20,930 --> 00:02:22,780
That's called the bit depth.

30
00:02:22,780 --> 00:02:26,450
What order are you storing the bytes in?

31
00:02:26,450 --> 00:02:29,960
If the most significant byte is coming
first, that's called big endian.

32
00:02:29,960 --> 00:02:33,800
If the least significant byte is coming
first, that's called little endian.

33
00:02:33,800 --> 00:02:36,990
How many channels of audio are there?

34
00:02:36,990 --> 00:02:43,000
And are you storing the channels of audio together, which
is called interleaved, or are you storing them separately?

35
00:02:43,000 --> 00:02:44,960
That's called non-interleaved.

36
00:02:44,960 --> 00:02:51,970
And are the bytes packed or is there padding?

37
00:02:51,970 --> 00:03:01,050
So, now, different groups in different
areas refer to these things differently.

38
00:03:01,050 --> 00:03:10,040
So, in Core Audio, we have a consistent way
of referring to samples, frames, and packets.

39
00:03:10,040 --> 00:03:14,000
Now, if you work in codecs, you might
call a frame what we call a packet.

40
00:03:14,000 --> 00:03:17,910
Or if you work in the music industry,
you might call things in another way.

41
00:03:17,910 --> 00:03:21,180
So, we've decided on a certain way of calling things.

42
00:03:21,180 --> 00:03:24,280
So, a sample is one sample of a waveform.

43
00:03:24,280 --> 00:03:29,060
A frame is a collection of samples
for each channel, you know,

44
00:03:29,060 --> 00:03:34,410
audio stream for the same vertically aligned moment in time.

45
00:03:34,410 --> 00:03:43,060
And then a packet is for a particular stream of audio
is the smallest cohesive unit of data for that format.

46
00:03:43,060 --> 00:03:44,990
It's what you pass around.

47
00:03:44,990 --> 00:03:49,080
For a linear PCM, one packet and one frame are synonymous.

48
00:03:49,080 --> 00:03:55,690
But for compressed formats, one packet is
some group of bytes that you can decompress

49
00:03:55,690 --> 00:04:00,470
into some number of frames of linear PCM.

50
00:04:00,470 --> 00:04:08,690
And so, this is going to show the difference between what
interleaved audio and non-interleaved audio looks like.

51
00:04:08,690 --> 00:04:14,080
In Core Audio, we have a universal container
for audio called the AudioBufferList

52
00:04:14,080 --> 00:04:18,060
which we use to pass around audio to all our APIs.

53
00:04:18,060 --> 00:04:19,920
That contains an array of buffers.

54
00:04:19,920 --> 00:04:25,850
So, in the non-- in the interleaved
case, you see that for stereo sound,

55
00:04:25,850 --> 00:04:28,830
you have the left and right channels are alternating.

56
00:04:28,830 --> 00:04:30,770
They're interleaved in a single buffer.

57
00:04:30,770 --> 00:04:37,970
And in the non-interleaved case,
each channel is in its own buffer.

58
00:04:37,970 --> 00:04:47,170
So, when we see what one frame
looks like in each of these formats,

59
00:04:47,170 --> 00:04:53,600
one frame of interleaved audio,
this is linear, stereo linear PCM.

60
00:04:53,600 --> 00:04:58,090
So, one frame of stereo would be the left
and right sample in the same buffer together.

61
00:04:58,090 --> 00:05:04,290
So, if you're talking about 2-byte
like 16-bit audio samples,

62
00:05:04,290 --> 00:05:09,280
the left and right sample together will be 4 bytes of audio.

63
00:05:09,280 --> 00:05:19,220
Whereas in the non-interleaved case, you have, in each
buffer, one sample, so there would be 2 bytes occupied

64
00:05:19,220 --> 00:05:23,920
by a frame in each buffer in a non-interleaved case.

65
00:05:23,920 --> 00:05:32,450
So, in linear PCM or in audio in general, there's a--
well, in linear PCM, there's two dimensions of quality,

66
00:05:32,450 --> 00:05:36,840
basically - there's sample rate and bit depth.

67
00:05:36,840 --> 00:05:42,510
In sample rate, as I mentioned earlier, determines
what the highest reproducible frequency is.

68
00:05:42,510 --> 00:05:45,290
It's the bandwidth of the audio you're listening to.

69
00:05:45,290 --> 00:05:47,570
So, we just halve the sampling rate.

70
00:05:47,570 --> 00:05:52,370
So, this is the list of common
sample rates and how they're used.

71
00:05:52,370 --> 00:05:56,270
Eight kHz is narrow-band speech.

72
00:05:56,270 --> 00:06:04,630
You can only represent frequencies up to
4 kHz, so it's not very good for music.

73
00:06:04,630 --> 00:06:07,220
Sixteen kHz is wide-band speech.

74
00:06:07,220 --> 00:06:19,230
You're able to represent other incidental sounds
in this format, and it just sounds better.

75
00:06:19,230 --> 00:06:26,120
Then 44 kHz, 44.1 kHz, that's CD quality audio.

76
00:06:26,120 --> 00:06:32,290
That contains basically the full human audible spectrum.

77
00:06:32,290 --> 00:06:39,810
And another common rate is 48 kHz, which is using
digital audio tape and a lot of audio hardware.

78
00:06:39,810 --> 00:06:47,070
Then you see other higher rates used in
pro equipment, and now it's showing up like

79
00:06:47,070 --> 00:06:54,050
in home theater equipment with 96
kHz and 192 kHz sampling rates.

80
00:06:54,050 --> 00:06:57,660
So, a human hearing extends up to 20 kHz.

81
00:06:57,660 --> 00:07:02,520
So, you don't really need sampling
rates above 48 kHz in order to be able

82
00:07:02,520 --> 00:07:10,320
to reproduce the entire human audible spectrum, but there's
technical reasons why you might want higher sampling rates

83
00:07:10,320 --> 00:07:18,390
than 48 kHz, and that has to do with being
able to simplify your audio processing.

84
00:07:18,390 --> 00:07:23,300
So, in a lot of pro situations or in
audio processing situations internally,

85
00:07:23,300 --> 00:07:29,050
you'll have rates like 96 kHz or 192 kHz.

86
00:07:29,050 --> 00:07:31,770
So, the other dimension is bit depths.

87
00:07:31,770 --> 00:07:34,400
And bit depth determines Signal to Noise Ratio.

88
00:07:34,400 --> 00:07:37,020
And so what is Signal to Noise Ratio?

89
00:07:37,020 --> 00:07:41,000
It's the amplitude of the signal, which is what
you're interested in, the music or the speech,

90
00:07:41,000 --> 00:07:46,110
divided by the amplitude of the noise, and
the noise in this case is quantization noise.

91
00:07:46,110 --> 00:07:55,160
When you converted this, the analog signal to a
number, if you converted to an integer, especially,

92
00:07:55,160 --> 00:08:04,270
there's only an integer number of steps that you have
available to you to represent the amplitude of the waveform.

93
00:08:04,270 --> 00:08:12,610
And the error between the value that you chose and the
actual value of the audio is called the quantization error,

94
00:08:12,610 --> 00:08:18,500
and that becomes a noise in your audio stream.

95
00:08:18,500 --> 00:08:28,430
And so, every 6 decibels, it's measured in decibels, and
every 6 decibels is roughly a factor of two in an amplitude.

96
00:08:28,430 --> 00:08:35,960
So, and then every bit you add to the audio, gives
you 6 more decibels of Signal to Noise Ratio.

97
00:08:35,960 --> 00:08:40,800
So, in the integer format, Signal to
Noise Ratio is amplitude dependent.

98
00:08:40,800 --> 00:08:46,140
So, you see a quoting of what the Signal
to Noise Ratio is for an integer format,

99
00:08:46,140 --> 00:08:50,250
that's referring to what you get
if you got a full amplitude signal.

100
00:08:50,250 --> 00:08:54,100
But the quieter signals have a worse Signal to Noise Ratio.

101
00:08:54,100 --> 00:09:06,350
And so, if you got a signal that's at -20 dB, and you've
only got 48 dB of-- or you've got an 8-bit audio signal,

102
00:09:06,350 --> 00:09:12,100
then you're going to have a 20
dB worse Signal to Noise Ratio.

103
00:09:12,100 --> 00:09:19,180
But in floating point, Signal to Noise
Ratio is independent of the amplitude.

104
00:09:19,180 --> 00:09:24,890
So, this is some common bit depths and how they're used.

105
00:09:24,890 --> 00:09:33,320
8 bit integer is sort of the format that was used
when it was very expensive to store and process audio.

106
00:09:33,320 --> 00:09:38,610
It's used in the old games and
a lot of gear made in the 1980s.

107
00:09:38,610 --> 00:09:43,260
It's got 48 decibels of Signal to
Noise Ratio for a full scale signal.

108
00:09:43,260 --> 00:09:52,250
So, that's not very good especially if you've got things
that are not at full scale, which is quite often the case.

109
00:09:52,250 --> 00:09:55,040
Then 16 bit integer is CD quality sound.

110
00:09:55,040 --> 00:10:02,880
It's 96 decibels of Signal to Noise Ratio, that's
quite good except if you're applying a quite sound

111
00:10:02,880 --> 00:10:08,360
and you're turning the volume way up then you can
start to hear the limitations of 16 bit integers.

112
00:10:08,360 --> 00:10:12,600
So, 24 bit integers gives you 144
decibels of Signal Noise Ratio.

113
00:10:12,600 --> 00:10:13,870
That's quite good.

114
00:10:13,870 --> 00:10:22,460
A lot of-- actually I think almost all hard work
can't actually even reproduce that much quality.

115
00:10:22,460 --> 00:10:33,070
So, then for internal processing, you
have-- or in OS X, we use 32 bit values.

116
00:10:33,070 --> 00:10:41,830
The AudioUnitSampleType on iPhone OS or
iOS is a 32 bit integer, which is 8.24,

117
00:10:41,830 --> 00:10:45,900
which is 8 bits of integers in
sine bit and 24 bits of fraction.

118
00:10:45,900 --> 00:10:52,030
That gives you 144 decibels of Signal to Noise Ratio,
but then you've also got 42 decibels of headroom,

119
00:10:52,030 --> 00:10:57,180
so you can go over unit you gain and sort of not
worry about some kinds of processing problems

120
00:10:57,180 --> 00:11:01,300
when you're dealing with internal processing.

121
00:11:01,300 --> 00:11:07,260
Then on the desktop, we'll use 32 bit floating point
which has 144 decibels of Signal to Noise Ratio

122
00:11:07,260 --> 00:11:09,200
for any amplitude even very quiet signals.

123
00:11:09,200 --> 00:11:14,370
And it's got an essentially unlimited dynamic range.

124
00:11:14,370 --> 00:11:19,660
So, one point about quality, once you've
lost quality, you can't add it back.

125
00:11:19,660 --> 00:11:26,800
So, sometimes, you hear about people converting their
audio to a higher sample rate or to a higher bit depth.

126
00:11:26,800 --> 00:11:29,950
That's not going to gain you any quality.

127
00:11:29,950 --> 00:11:39,390
Similarly, re-encoding compressed audio to a higher bit
rate or re-encoding it with a better codec is not going

128
00:11:39,390 --> 00:11:42,460
to give you quality back for your signal.

129
00:11:42,460 --> 00:11:50,430
It's only if you have the original uncompressed source
at a higher quality will you be able to then re-encode

130
00:11:50,430 --> 00:11:54,960
to a better format than you previously encoded it to.

131
00:11:54,960 --> 00:12:04,170
So, then, the next step past linear
PCM was kind of historical.

132
00:12:04,170 --> 00:12:07,290
But it's called nonlinear PCM.

133
00:12:07,290 --> 00:12:12,390
And that's instead of storing the number, you store the
logarithm of the number, and that increases the Signal

134
00:12:12,390 --> 00:12:17,530
to Noise Ratio of quiet signals at
the expense of the loud signals.

135
00:12:17,530 --> 00:12:24,090
And there are two common algorithms, and they differ in
how they quantize or proximate the logarithm function.

136
00:12:24,090 --> 00:12:25,760
It's mu-law and A-law.

137
00:12:25,760 --> 00:12:29,900
And those both encode audio in 8 bits per sample.

138
00:12:29,900 --> 00:12:32,990
Then you get to packetized compressed formats.

139
00:12:32,990 --> 00:12:38,080
And that's when a group of frames is
compressed into a packet of bytes.

140
00:12:38,080 --> 00:12:43,090
One thing to note is that packets often
have dependencies on preceding packets.

141
00:12:43,090 --> 00:12:49,670
So, when you're decoding audio, you're putting
the codec into a particular state, and that--

142
00:12:49,670 --> 00:12:53,650
the next packet will assume that the codec is in that state.

143
00:12:53,650 --> 00:13:01,640
So, if you take chunks of compressed audio from
different streams and you splice them together just

144
00:13:01,640 --> 00:13:07,080
by appending the packets to each other, the codec
is not going to be in the state that the packet,

145
00:13:07,080 --> 00:13:12,560
the next packet after the splice is going to think
that it's in, and you're going to get a glitch.

146
00:13:12,560 --> 00:13:25,400
So, the way you really have to do that is to edit
non-compressed data together and then compress that,

147
00:13:25,400 --> 00:13:32,660
or you do something more sophisticated about
how you splice your packetized audio together

148
00:13:32,660 --> 00:13:36,170
by overlapping and decompressing or re-compressing.

149
00:13:36,170 --> 00:13:39,550
So, but there's going to be more about
compressed formats in the next section.

150
00:13:39,550 --> 00:13:44,980
Now, formats are represented in Core Audio by a
structure called the AudioStreamBasicDescription.

151
00:13:44,980 --> 00:13:49,550
It's used in nearly every API in Core Audio.

152
00:13:49,550 --> 00:13:53,220
It's been covered extensively in previous WWDC talks.

153
00:13:53,220 --> 00:13:57,610
There's information about how to fill one
out, how you get one from the various APIs

154
00:13:57,610 --> 00:14:01,380
like AudioFormat, AudioFile, and AudioConverter.

155
00:14:01,380 --> 00:14:08,470
And one thing to note is that if you use the
AVAudioFoundation classes which play a file

156
00:14:08,470 --> 00:14:13,700
to the audio hardware, then you can avoid the
audio stream basic descriptions altogether.

157
00:14:13,700 --> 00:14:16,710
So then, converting audio.

158
00:14:16,710 --> 00:14:21,840
When you've got audio in all these formats,
how do you get from one format to another?

159
00:14:21,840 --> 00:14:25,160
So, we have an API called the AudioConverter
which does this.

160
00:14:25,160 --> 00:14:30,710
And there are three main conversions you can do which
is linear PCM to linear PCM, which handles all kinds

161
00:14:30,710 --> 00:14:36,290
of transformations like sample rate conversion, bit
depth changes, converting integer to floating point,

162
00:14:36,290 --> 00:14:41,090
interleaved to non-interleaved or any combination
of these, or removing numbers of channels.

163
00:14:41,090 --> 00:14:47,210
And then you have encoding and decoding which is
taking, encoding is taking linear PCM and converting it

164
00:14:47,210 --> 00:14:54,620
to a compressed format, and decoding is taking the
compressed format and turning it back to the linear PCM.

165
00:14:54,620 --> 00:14:58,340
Some of our APIs have AudioConverters built within them.

166
00:14:58,340 --> 00:15:04,570
So, if you use these APIs, then you can avoid having
to deal with the complexity of the AudioConverter

167
00:15:04,570 --> 00:15:08,130
and sort of take advantage of some
of the work that's been done for you.

168
00:15:08,130 --> 00:15:14,710
So, if you're on one of these scenarios like if you
are playing or recording buffers of audio from memory

169
00:15:14,710 --> 00:15:20,400
and you want to play it out to the hardware or record
from the hardware, then you can use the AudioQueue API

170
00:15:20,400 --> 00:15:26,230
and that will take care of converting
between your format and the hardware format.

171
00:15:26,230 --> 00:15:36,610
If you want to read and write audio files into memory or
to and from memory, then you can use the ExtendedAudio API,

172
00:15:36,610 --> 00:15:40,900
ExtendedAudioFile API, and that will
handle the conversions between your--

173
00:15:40,900 --> 00:15:44,470
the format you want and the format that the file is in.

174
00:15:44,470 --> 00:15:50,420
Then if you want to just play a file out to the audio
hardware or record a file from the audio hardware,

175
00:15:50,420 --> 00:15:56,600
then AVAudioPlayer will handle the conversion between
the file format and the hardware format for you

176
00:15:56,600 --> 00:16:02,330
so you don't have to deal with format change at all.

177
00:16:02,330 --> 00:16:10,360
And in this case, you don't have to even name
the formats using audio stream basic description.

178
00:16:10,360 --> 00:16:16,210
OK. So, when you're doing a sample rate
conversion using the AudioConverter,

179
00:16:16,210 --> 00:16:19,370
you have a number of ways to set the quality.

180
00:16:19,370 --> 00:16:26,500
It's a-- sample rate conversion is a relatively
expensive operation depending on what quality you want.

181
00:16:26,500 --> 00:16:34,970
There's a property called the AudioConverterSample
RateConverterComplexity property which is,

182
00:16:34,970 --> 00:16:42,820
allows you to choose several different algorithms
which choose different levels of quality.

183
00:16:42,820 --> 00:16:48,610
So, there's linear, normal, and on the
desktop, you have mastering quality.

184
00:16:48,610 --> 00:16:52,320
Linear is just a linear interpolation between samples.

185
00:16:52,320 --> 00:16:55,440
It's fast, but it's not very good quality.

186
00:16:55,440 --> 00:17:06,740
And then normal is a-- does a better
job doing more sophisticated algorithm.

187
00:17:06,740 --> 00:17:15,890
So, within normal and mastering complexities,
you have several bands of quality which are

188
00:17:15,890 --> 00:17:18,730
from minimum to maximum that you can set.

189
00:17:18,730 --> 00:17:23,540
Linear is just linear, there's
no quality setting for linear.

190
00:17:23,540 --> 00:17:26,620
A higher quality costs more CPU.

191
00:17:26,620 --> 00:17:29,790
The other thing is on the desktop
where you have normal and mastering,

192
00:17:29,790 --> 00:17:33,430
the lowest quality of mastering is better
than the highest quality of normal.

193
00:17:33,430 --> 00:17:41,310
So, they're completely disjoint bands of quality, but
mastering is quite a lot more expensive especially

194
00:17:41,310 --> 00:17:44,860
if you're doing-- if you choose
maximum of mastering quality.

195
00:17:47,490 --> 00:17:56,060
So, processing audio, in Core Audio, we have AudioUnits
which are used to process audio, there are components.

196
00:17:56,060 --> 00:18:02,470
And the main attributes of them is they have inputs
and outputs, so you have ways to get audio in and out,

197
00:18:02,470 --> 00:18:05,430
and then you have parameters that
you can adjust in real time.

198
00:18:05,430 --> 00:18:09,260
And there's a lot of different kinds of AudioUnits.

199
00:18:09,260 --> 00:18:16,820
There's I/O which talk to the hardware so you
can read audio from the hardware or play it out.

200
00:18:16,820 --> 00:18:25,480
And then there are effects which is the most
numerous category which gives you filters,

201
00:18:25,480 --> 00:18:29,890
compressors, delays, reverbs, time/pitch changes.

202
00:18:29,890 --> 00:18:32,440
And then there are panners and mixers.

203
00:18:32,440 --> 00:18:39,780
For the I/O AudioUnits on iOS or the
iPhone OS, you have the remote I/O unit

204
00:18:39,780 --> 00:18:44,570
which is your most direct access to the audio hardware.

205
00:18:44,570 --> 00:18:51,180
And then on the desktop, you have AUHAL
which fulfills basically the same role.

206
00:18:51,180 --> 00:18:53,540
It's an AudioUnit that talks to the hardware.

207
00:18:53,540 --> 00:19:00,320
On the desktop, the AUHAL is built on top of the
HAL which is a sensor hardware abstraction layer,

208
00:19:00,320 --> 00:19:04,080
and that's the low level access to the hardware.

209
00:19:04,080 --> 00:19:12,450
If you use the AUHAL AudioUnit, you're going to
benefit from having a lot of the details of dealing

210
00:19:12,450 --> 00:19:18,830
with the low level handled for your
including audio conversion, format conversion.

211
00:19:18,830 --> 00:19:22,740
But there's no cost of latency for using that AudioUnit.

212
00:19:22,740 --> 00:19:31,260
Just an example, on the desktop, you have various
filter AudioUnits which these images are from the UI

213
00:19:31,260 --> 00:19:37,920
of the AudioUnit that show a graph of frequency
versus the gain for the various filters.

214
00:19:37,920 --> 00:19:47,150
There's a Parametric EQ, Graphic EQ, Lowpass,
Highpass, Bandpass, Low and High Shelf Filters.

215
00:19:47,150 --> 00:19:50,790
And then you also have Compressors.

216
00:19:50,790 --> 00:19:54,270
And there's Delay unit, Reverb unit.

217
00:19:54,270 --> 00:19:57,730
There's also Panner units.

218
00:19:57,730 --> 00:20:00,190
We have Mixers.

219
00:20:00,190 --> 00:20:04,080
On the iPhone, you have a Mutichannel
Mixer which actually does mono

220
00:20:04,080 --> 00:20:08,210
and stereo, and then there's the Embedded 3D Mixer.

221
00:20:08,210 --> 00:20:15,330
And then on the desktop, you have Multichannel
Mixers, 3D Mixers, Stereo Mixer and Matrix Mixer.

222
00:20:15,330 --> 00:20:23,330
So, I'm going to talk about the Embedded 3D Mixer.

223
00:20:23,330 --> 00:20:28,710
It gives you two basic algorithms which
is equal power panning for stereo,

224
00:20:28,710 --> 00:20:38,780
and then you have now spherical head algorithm which gives
you interaural time to delay cues, intensity difference,

225
00:20:38,780 --> 00:20:42,400
and filtering due to head, and distance filtering.

226
00:20:42,400 --> 00:20:50,040
So, the 3D Mixer uses Azimuth, Elevation, and Distance

227
00:20:50,040 --> 00:20:58,280
as its parameter digital listener-centric
parameterization of the source of the audio.

228
00:20:58,280 --> 00:21:04,160
So, Azimuth is the angle from directly
forward for the listener.

229
00:21:04,160 --> 00:21:09,520
So, positive is around to the listener's right,
and negative is around to the listener's left.

230
00:21:09,520 --> 00:21:12,540
And 180 is in the rear.

231
00:21:12,540 --> 00:21:21,520
So as this just illustrates these parameters'
distances, some distance from the listener.

232
00:21:21,520 --> 00:21:30,080
And then on some of the desktop panners
and 3D Mixers, you have also Elevation.

233
00:21:30,080 --> 00:21:37,220
So, then there's also a property on the
3D Mixer which is Distance attenuation.

234
00:21:37,220 --> 00:21:43,270
There's a reference distance which below which
there is no change in the amplitude of the audio,

235
00:21:43,270 --> 00:21:46,860
and there's a maximum distance above
which there is no change in the amplitude.

236
00:21:46,860 --> 00:21:55,890
But between that, there's a distance curve, and there's
several different distance curves you can choose.

237
00:21:55,890 --> 00:22:01,680
Another way to access 3D spatialization is through OpenAL.

238
00:22:01,680 --> 00:22:07,280
And OpenAL is a OpenGL-like library for 3D audio.

239
00:22:07,280 --> 00:22:11,400
It's cross-platform, and allows
3D spatialized source positioning.

240
00:22:11,400 --> 00:22:16,910
It's built on top of the 3D Mixer, so you're using the 3D
Mixer underneath, but it allows you to use world coordinates

241
00:22:16,910 --> 00:22:21,980
which are in x, y, z, so the listener can be anywhere
and the source can be anywhere in space rather

242
00:22:21,980 --> 00:22:27,600
than using listener-centric coordinates
like the 3D Mixer uses.

243
00:22:27,600 --> 00:22:34,510
OK. So, now, it's time for Eric Allamanche to talk
about the Voice Processing Unit and Audio Codec.

244
00:22:34,510 --> 00:22:37,410
[ Applause ]

245
00:22:37,410 --> 00:22:38,550
>> Eric Allamanche: Thank you, James.

246
00:22:38,550 --> 00:22:39,950
And welcome again.

247
00:22:39,950 --> 00:22:44,340
My name is Eric Allamanche, and I'm going to
walk you through the Voice Processing Audio Unit

248
00:22:44,340 --> 00:22:49,360
and the Audio Codecs we provide on the iPhone.

249
00:22:49,360 --> 00:22:53,980
So, let's start with the Voice Processing Audio Unit.

250
00:22:53,980 --> 00:22:59,640
The Voice Processing Audio Unit was
added to iPhone OS 3.0 last year.

251
00:22:59,640 --> 00:23:05,730
And it is basically a dedicated RemoteIO
unit with a built-in Acoustic Echo Canceler.

252
00:23:05,730 --> 00:23:11,120
So, from a programmer's perspective, this RemoteIO--

253
00:23:11,120 --> 00:23:18,120
this Echo Canceler can be accessed exactly the
same way as you would access the RemoteIO unit.

254
00:23:18,120 --> 00:23:26,340
So, basically, setting up, creating the
instance, setting parameters, and so on.

255
00:23:26,340 --> 00:23:33,180
But this year, in iOS 4, we added a new algorithm
which provides significantly better quality,

256
00:23:33,180 --> 00:23:36,860
of course, at the cost of a heavier CPU load.

257
00:23:36,860 --> 00:23:39,490
And this Echo Canceler was specifically designed

258
00:23:39,490 --> 00:23:46,100
to allow extremely high quality audio
chat like in the FaceTime application.

259
00:23:46,100 --> 00:23:57,000
So, now, we provide two, we offer two algorithms in this
RemoteIO unit, and this allows you to make a tradeoff

260
00:23:57,000 --> 00:24:03,870
between the quality and the CPU you want
to spend for this kind of application.

261
00:24:05,190 --> 00:24:10,270
Let me just recall the functionality
of an Acoustic Echo Canceler.

262
00:24:10,270 --> 00:24:15,390
So, on the left-hand side, you have
what we call the far end speaker,

263
00:24:15,390 --> 00:24:19,040
which has the device with a microphone and a loud speaker.

264
00:24:19,040 --> 00:24:24,600
And on the right-hand side, there is the
near end speaker with the same appliance.

265
00:24:24,600 --> 00:24:32,980
So, the far end speaker starts to talk, and this is
visualized with the blue arrows, so the speech signal goes

266
00:24:32,980 --> 00:24:34,750
to the microphone on the far end speaker's end.

267
00:24:34,750 --> 00:24:43,130
And then it's encoded and propagated over the internet
and comes to our device to the near end speaker device.

268
00:24:43,130 --> 00:24:49,030
And so, this signal is then rendered
through the loud speaker.

269
00:24:49,030 --> 00:24:59,480
But because most devices, the microphone is really in the
vicinity of the loud speaker, there is a certain amount

270
00:24:59,480 --> 00:25:04,560
of acoustic energy which goes back
to the near end speaker's microphone.

271
00:25:04,560 --> 00:25:11,700
And if we don't take any measures at this level, well,
this signal is then propagated back to the far end speaker.

272
00:25:11,700 --> 00:25:18,820
And because of all the delays in the chain being
from the encoding, network delays and et cetera,

273
00:25:18,820 --> 00:25:26,260
the far end speaker signal comes back after
a certain amount of time typically around 100

274
00:25:26,260 --> 00:25:30,960
or 200 milliseconds; and this is perceived as an echo.

275
00:25:30,960 --> 00:25:34,970
So, what happens now if the near end speaker talks on top

276
00:25:34,970 --> 00:25:42,060
of the far end speaker while the signals
are mixed together acoustically in the air,

277
00:25:42,060 --> 00:25:47,490
and so this mixed signal is then captured by
the microphone and send back to far end speaker.

278
00:25:47,490 --> 00:25:52,870
So, this is now where the Echo Canceler
comes in to play because we want

279
00:25:52,870 --> 00:25:58,900
to eliminate the amount of blue
signal here from the lower path.

280
00:25:58,900 --> 00:26:06,690
And this is done with several algorithms which
have been developed over the last decades.

281
00:26:06,690 --> 00:26:14,220
And basically, the purpose of the echo-- of
the Echo Canceler is to analyze both signals,

282
00:26:14,220 --> 00:26:20,390
so the signal which is about to be sent to the loud
speaker and the signal which is captured by the microphone.

283
00:26:20,390 --> 00:26:24,770
And from these two signals, the Echo
Canceler tries to make an estimate

284
00:26:24,770 --> 00:26:29,000
of the amount of echo included in the lower path.

285
00:26:29,000 --> 00:26:36,030
And this amount-- this echo is then
removed from the-- from the path.

286
00:26:36,030 --> 00:26:44,090
This is visualized at the-- usually-- so the
subtraction sign on the lower part of this diagram.

287
00:26:44,090 --> 00:26:49,630
And what basically-- and in either case, what should
go back to the far end speaker is actually a red signal

288
00:26:49,630 --> 00:26:54,980
which is the speech of the near end speaker only.

289
00:26:54,980 --> 00:27:05,810
So, now, what happens if other app sounds are playing
or you get a notification, e-mail or whatever,

290
00:27:05,810 --> 00:27:16,700
then this sound gets mixed in with the far end
speaker's signal, and then rendered through the speaker.

291
00:27:16,700 --> 00:27:25,290
But what we want to do here is we want
to eliminate this sound, this loud sound,

292
00:27:25,290 --> 00:27:33,390
or it could be some background audio playing in the case of
a game, for example, and we don't want this signal to go,

293
00:27:33,390 --> 00:27:35,440
to be echoed back to the far end speaker.

294
00:27:35,440 --> 00:27:43,220
And in order to do this, we have to put the
summation point of the signals on the left,

295
00:27:43,220 --> 00:27:47,580
on the farthest most side of this-- of the system.

296
00:27:47,580 --> 00:27:53,980
And this is why the-- this mixing happens
before the Echo Canceler is invoked.

297
00:27:53,980 --> 00:28:04,330
One important thing to note here is that your application
has, you will see all the signals that you have

298
00:28:04,330 --> 00:28:07,430
to deal with, but you won't see any other signals coming

299
00:28:07,430 --> 00:28:14,560
in like an e-mail notification or
sounds played by other applications.

300
00:28:14,560 --> 00:28:20,290
So, because you won't see these signals here,
in this case, the mixing will happen further

301
00:28:20,290 --> 00:28:23,470
on the right side, and those won't be eliminated.

302
00:28:23,470 --> 00:28:30,400
So that's why this RemoteIO with
built-in Canceler is an ideal solution

303
00:28:30,400 --> 00:28:36,550
to eliminate all ancillary sounds
coming from other applications.

304
00:28:37,600 --> 00:28:44,980
The way you open and interact with this processing
unit is exactly the same as with the RemoteIO units,

305
00:28:44,980 --> 00:28:49,110
and this will be explained in much more
detail in the next session by Murray.

306
00:28:49,110 --> 00:28:52,110
I just wanted to point out here how-- some basic setups.

307
00:28:52,110 --> 00:28:56,230
So, you create the audio component description.

308
00:28:56,230 --> 00:29:02,540
And the difference here is that you just provide the
componentSubType to be the VoiceProcessingIO instead

309
00:29:02,540 --> 00:29:04,880
of the RemoteIO, and that's basically it.

310
00:29:04,880 --> 00:29:08,940
So, once you find the component and create a new instance,

311
00:29:08,940 --> 00:29:17,070
you have your RemoteIO with a built-in
Canceler ready to be used.

312
00:29:18,390 --> 00:29:25,390
Now, with this voice processing
unit, we provide a few parameters.

313
00:29:25,390 --> 00:29:30,130
The first one is the Bypass parameter which
allows you to bypass the whole process,

314
00:29:30,130 --> 00:29:35,990
which means that basically nothing happens,
everything is mixed together and nothing is removed.

315
00:29:35,990 --> 00:29:38,720
This can be useful in certain circumstances.

316
00:29:38,720 --> 00:29:49,510
The voice processing unit also has built-in automatic
gain control unit to boost the resulting signal coming

317
00:29:49,510 --> 00:29:56,430
out from the Echo Canceler, and this can be
controlled by this property and it is on by default.

318
00:29:56,430 --> 00:30:02,680
And another property is for ducking the
NonVoiceAudio, as I mentioned in the diagram.

319
00:30:02,680 --> 00:30:06,080
So, all the other app sounds is
what we call the NonVoiceAudio.

320
00:30:06,080 --> 00:30:11,330
And there's a property to duck
this audio to a certain extent.

321
00:30:12,450 --> 00:30:17,010
Now, in iOS 4, we've added two more properties.

322
00:30:17,010 --> 00:30:24,500
And as I mentioned, on the first slide, we added a
new algorithm which provides much better quality,

323
00:30:24,500 --> 00:30:31,430
and this quality, this algorithm is now
controlled by this VoiceProcessingQuality property.

324
00:30:31,430 --> 00:30:37,880
So, with this property, you can select either the
old echo suppressor we had from which way it is,

325
00:30:37,880 --> 00:30:44,360
either in iPhone OS 3.0 or the
new better one available in iOS 4.

326
00:30:44,360 --> 00:30:46,970
And the last property we added is a MuteOutput,

327
00:30:46,970 --> 00:30:50,710
which basically zeroes out the signal
coming out of the Echo Canceler.

328
00:30:50,710 --> 00:30:57,030
So, this is for the muting of a conversation.

329
00:30:57,030 --> 00:31:01,510
So that was about the Voice Processing Unit.

330
00:31:01,510 --> 00:31:04,460
Now, let's dive into the Audio Codecs.

331
00:31:04,460 --> 00:31:12,310
The term CODEC is a contraction from encoder and
decoder, and this is not specific to audio only,

332
00:31:12,310 --> 00:31:17,100
but applies to any kind of codecs
like video codecs and so forth.

333
00:31:17,100 --> 00:31:23,550
And the main purpose of this-- of a codec is
to compress and decompress PCM audio signals.

334
00:31:23,550 --> 00:31:28,660
So, because we're talking about audio
codecs, we only deal with PCM audio signals.

335
00:31:28,660 --> 00:31:33,680
And in general, we differentiate two
different, two big categories of codecs.

336
00:31:33,680 --> 00:31:42,230
One being the lossy codecs which are associated with loss
of information, and on the other hand, lossless codecs.

337
00:31:42,230 --> 00:31:51,730
And of course, codecs, audio codecs nowadays
are core technology in digital audio I mean,

338
00:31:51,730 --> 00:31:59,620
it is the basically the backbone of the iPod and any
media player application, and also for the iTunes Store.

339
00:31:59,620 --> 00:32:05,620
Now, let's talk about lossy versus lossless audio codecs.

340
00:32:05,620 --> 00:32:09,280
In the case of lossless codecs,
there is no loss of information.

341
00:32:09,280 --> 00:32:16,860
So, after one encoding and decoding cycle, the resulting
signal should be bit identical to the input signal,

342
00:32:16,860 --> 00:32:22,350
and this is regardless of the bit depth,
be it 16, 24 or 32 bits integer or float.

343
00:32:22,350 --> 00:32:25,550
So, no loss of information.

344
00:32:25,550 --> 00:32:32,100
But because there is, this can be compared
to the Unix zip command, for example.

345
00:32:32,100 --> 00:32:38,400
But Unix zip is a general tool, and it is not, it
doesn't provide good compression ratios for audio signals

346
00:32:38,400 --> 00:32:44,010
in general, so that's why it's better
to have dedicated audio lossless codecs.

347
00:32:44,010 --> 00:32:53,160
And typical compression factors for state of the art
lossless codecs nowadays are in the range from 1.5 to 2

348
00:32:53,160 --> 00:32:57,330
and 2 being a very good compression ratio already.

349
00:32:57,330 --> 00:33:04,080
On the other hand, the lossy codecs, which
are the most widely used one like MP3 and AAC.

350
00:33:04,080 --> 00:33:11,500
And this typically rely on a perceptual
model of the human auditory system.

351
00:33:11,500 --> 00:33:19,640
What this means is that as James mentioned
before, we can only hear signals up to 20 kHz,

352
00:33:19,640 --> 00:33:25,670
but this is already a very optimistic
case because growing older,

353
00:33:25,670 --> 00:33:31,300
this frequency shifts towards more 15 kHz and even lower.

354
00:33:31,300 --> 00:33:40,630
And by taking advantage of many properties of what's going
on in our auditory system and especially the masking effect

355
00:33:40,630 --> 00:33:47,290
which basically is you have a tone playing back at a certain
magnitude and you have another tone coming in somehow

356
00:33:47,290 --> 00:33:52,050
in the same frequency range but with a much lower magnitude.

357
00:33:52,050 --> 00:33:55,700
What will happen is that this second
tone won't be perceived at all.

358
00:33:55,700 --> 00:34:00,050
So, because it is not perceived,
there is no reason to encode it.

359
00:34:00,050 --> 00:34:05,140
And this is what the lossy codecs try to achieve

360
00:34:05,140 --> 00:34:13,090
by evaluating first what information
can be discarded and what should remain.

361
00:34:13,090 --> 00:34:18,970
So, this is the irrelevant part of the
information that we're going to try to remove,

362
00:34:18,970 --> 00:34:23,630
and the other part is the redundant information,
but this is more of a mathematical nature.

363
00:34:23,630 --> 00:34:28,680
This is basically the predictable part of the signal.

364
00:34:28,680 --> 00:34:33,580
The lossy codec is basically controlled by the bit rate.

365
00:34:33,580 --> 00:34:36,660
So, it is obvious the higher the
bit rate, the higher the quality.

366
00:34:36,660 --> 00:34:40,450
And conversely, the lower the bit
rate, the worse the quality.

367
00:34:40,450 --> 00:34:46,830
So, it is extremely important to make good decisions
regarding the bit rate because we want to make sure

368
00:34:46,830 --> 00:34:52,460
that we don't degrade the signal or
that degradations won't be perceived.

369
00:34:52,460 --> 00:35:00,870
And in contrast to lossless codecs, lossy codecs have
typical compression factors ranging between 6 and 24,

370
00:35:00,870 --> 00:35:07,840
which is a very big range, and this is
achieved with very sophisticated algorithms.

371
00:35:07,840 --> 00:35:14,610
Now, I'm just going to talk about the audio
decoders, which are available on the iPhone.

372
00:35:14,610 --> 00:35:20,180
So, we first have the Adaptive Data Pulse
Code Modulation Codecs like IMA, IMA4, DVI,

373
00:35:20,180 --> 00:35:23,710
and MS-ADPCM which are very simple codecs.

374
00:35:23,710 --> 00:35:28,740
They don't provide very good audio quality,
but they were, because they were simple,

375
00:35:28,740 --> 00:35:35,540
they were widely used historically, and they
are still for example on Voice-over IP providers

376
00:35:35,540 --> 00:35:39,200
when they send you an e-mail, if
you can't pick up the phone,

377
00:35:39,200 --> 00:35:43,850
then voice messages are typically
encoded in one of these formats.

378
00:35:43,850 --> 00:35:50,760
Then we have the QDesign version 1 and 2, which
is actually the old audio codec which was used

379
00:35:50,760 --> 00:35:58,070
by QuickTime before we moved over to AAC,
so it's just there for historical reasons.

380
00:35:58,070 --> 00:36:06,290
Then of course, there's GSM, the GSM Full
Rate Codec used on the mobile networks.

381
00:36:06,290 --> 00:36:09,460
Then we have the Internet Low Bit rate Codec which is a free

382
00:36:09,460 --> 00:36:17,280
and open codec providing decent quality
at decent-- with pretty low bit rates.

383
00:36:17,280 --> 00:36:25,140
Then of course, MP3 which is MPEG-1/2 Layer 3.

384
00:36:25,140 --> 00:36:31,710
Apple Lossless which is the only
lossless codec we provide on the iPhone.

385
00:36:31,710 --> 00:36:39,660
And then the MPEG-4 AAC family of codecs which I
will discuss in more details in the next slides.

386
00:36:39,660 --> 00:36:44,900
Now, regarding encoders, we don't
provide an encoder for every decoder.

387
00:36:44,900 --> 00:36:50,420
There is no-- there is not always a need
for having an encoder for every formats.

388
00:36:50,420 --> 00:36:54,150
Therefore, the choices of encoders
is much more restricted here.

389
00:36:54,150 --> 00:36:57,940
So, for the ADPCM, we have IMA4.

390
00:36:57,940 --> 00:37:05,680
We have the iLBC codec which will allow you to use this as
a Voice-over-- for Voice over IP applications, for example.

391
00:37:05,680 --> 00:37:11,860
The Apple Lossless, which in certain
cases in used for the voice recording.

392
00:37:11,860 --> 00:37:18,400
And then for the MPEG-4 AAC, we provided three
different codecs the Low Complexity Codec,

393
00:37:18,400 --> 00:37:22,650
the Low Delay Codec, and the Enhanced Low Delay Codec.

394
00:37:22,650 --> 00:37:27,930
Now, regarding MP3, we don't provide any MP3
encoder, and this is also true on the desktop.

395
00:37:27,930 --> 00:37:29,970
Core Audio doesn't provide an MP3 encoder.

396
00:37:29,970 --> 00:37:34,260
So, if you want to encode to MP3 on
the desktop, you need to use iTunes.

397
00:37:34,260 --> 00:37:39,010
The audio converter doesn't-- can't encode to MP3.

398
00:37:39,010 --> 00:37:45,040
If we put this together all the codecs with their
characteristics, we get the following table.

399
00:37:45,040 --> 00:37:48,540
So, let's first start with iLBC.

400
00:37:48,540 --> 00:37:51,020
iLBC is 8 kHz.

401
00:37:51,020 --> 00:37:52,990
It is optimized for speech.

402
00:37:52,990 --> 00:37:59,390
It is a speech codec and has two, offers 2 bit
rates in ballpark of 15 kilobits per second.

403
00:37:59,390 --> 00:38:11,990
Then of course, we have MP3 which has a sampling rate range
of 16 to 48 kHz, and can provide mono-stereophonic signals.

404
00:38:11,990 --> 00:38:14,850
An MP3 is what we call a general audio codec.

405
00:38:14,850 --> 00:38:24,870
What this means it doesn't-- MP3 hasn't been designed
to encode a certain class of signals, but it is,

406
00:38:24,870 --> 00:38:29,620
it can be used for any kind of
signals including speech of course.

407
00:38:29,620 --> 00:38:33,610
ALAC has the particularity that the bit rate can't be set.

408
00:38:33,610 --> 00:38:37,330
And the reason is obvious because it is a lossless codec.

409
00:38:37,330 --> 00:38:42,360
So, the content itself will actually
determine what the final bit rate will be.

410
00:38:42,360 --> 00:38:46,490
And it is also a general audio codec.

411
00:38:46,490 --> 00:38:58,510
Then the AAC Low Complexity encoder, codec sorry, provides
a very broad sample rate range first going from 8 to 48 kHz,

412
00:38:58,510 --> 00:39:04,160
and it is also a general audio codec as same as MP3.

413
00:39:04,160 --> 00:39:11,760
And in iOS 4, we've added two more channel
configurations which are the 5.1 and the 7.1.

414
00:39:11,760 --> 00:39:18,540
What this means if you have an AAC file encoded as
with this 5.1 or 7.1, you will be able to decode it,

415
00:39:18,540 --> 00:39:22,070
however, it will only be rendered in stereo.

416
00:39:22,070 --> 00:39:25,840
We do a downmixing at the end of the decoding process.

417
00:39:25,840 --> 00:39:36,180
Now, for the High Efficiency AAC Codec, we have
also provide also a mono and stereo channels.

418
00:39:36,180 --> 00:39:43,390
In this codec as I will go into more details in the
next slide, this has been optimized for streaming audio.

419
00:39:43,390 --> 00:39:50,020
And the AAC Enhanced Low Delay or Enhanced
Low Delay has been specifically optimized

420
00:39:50,020 --> 00:39:54,400
for AV chat kind of applications.

421
00:39:54,400 --> 00:39:59,800
Now, let's go talk about the AAC Codecs into more details.

422
00:39:59,800 --> 00:40:09,050
Now, why do we even have to bother about AAC, I mean
isn't MP3 now is ubiquitous and why isn't MP3 good enough?

423
00:40:09,050 --> 00:40:15,200
Advanced Audio Codec, AAC, and I just wanted to
point out that none of the AAC stands for Apple,

424
00:40:15,200 --> 00:40:20,500
it's all about advanced audio, and it's
a standout-- it's an MPEG standout.

425
00:40:20,500 --> 00:40:22,380
MP3 is almost 20 years old now.

426
00:40:22,380 --> 00:40:24,660
It was standardized in 1991.

427
00:40:24,660 --> 00:40:29,780
And at that time, the requirements for audio codecs
were completely different than they are nowadays.

428
00:40:29,780 --> 00:40:36,830
So, and even at that time, it was a challenge
to decode an MP3 stream in real time.

429
00:40:36,830 --> 00:40:41,140
This could only be done with expensive DSP boards.

430
00:40:41,140 --> 00:40:46,320
Therefore, MP3 had serious limitation in its design.

431
00:40:46,320 --> 00:40:51,570
And most specifically, it was limited
in the bit rate it would support,

432
00:40:51,570 --> 00:40:54,000
the sampling rates and the channel configurations.

433
00:40:54,000 --> 00:40:58,710
So, right off the bat, MP3 can only handle stereo signals.

434
00:40:58,710 --> 00:41:04,140
And there were some other mathematical underpinnings
in the codec design itself which wouldn't allow it

435
00:41:04,140 --> 00:41:07,030
to be transparent for certain signal classes.

436
00:41:07,030 --> 00:41:13,670
So, all these together, led the
MPEG consortium to start a new--

437
00:41:13,670 --> 00:41:17,970
to a new work group which was focused
in designing a much better codec,

438
00:41:17,970 --> 00:41:23,410
non-backwards compatible codec,
and this is where AAC came to life.

439
00:41:23,410 --> 00:41:29,480
And the first version was standardized
in the course of MPEG-2 in 1997.

440
00:41:29,480 --> 00:41:37,560
But since that time, AAC was adopted as the basic
codec in MPEG-4, and since that time, it has seen many,

441
00:41:37,560 --> 00:41:43,920
many additions and new variations coming
in which makes it a very versatile codec.

442
00:41:43,920 --> 00:41:49,970
So, about the AAC Codecs, so first,
there is the Low-Complexity Codec.

443
00:41:49,970 --> 00:41:56,200
The Low-Complexity Codec is actually the codec you
use for any kind of media playback application.

444
00:41:56,200 --> 00:41:59,200
So, this is what the iPod uses.

445
00:41:59,200 --> 00:42:06,860
This is what the iPod application
uses on iPhone or iPod touches.

446
00:42:06,860 --> 00:42:13,010
Then we have the High Efficiency and High
Efficiency v2 Codecs which have the advantage

447
00:42:13,010 --> 00:42:20,690
that they provide similar quality to some
extent, but at significantly lower bit rate.

448
00:42:20,690 --> 00:42:25,240
And this makes it really interesting
for internet radio stations.

449
00:42:25,240 --> 00:42:33,540
And if you look up some-- even the iTunes radio station's
library, you will notice that every low bit rate,

450
00:42:33,540 --> 00:42:37,290
something around 64 kilobits and below is--

451
00:42:37,290 --> 00:42:42,510
most of the time, only encoded using
High-Efficiency or High-Efficiency v2.

452
00:42:42,510 --> 00:42:48,850
And on the other side, we have the Low Delay Codec
which was first introduced in Mac OS X Leopard.

453
00:42:48,850 --> 00:42:55,490
And it is the default codec for the iChat AV application.

454
00:42:55,490 --> 00:43:03,070
And today, in iOS 4, we've added a new codec which
is called the Enhanced Low Delay Codec, and it's--

455
00:43:03,070 --> 00:43:08,060
this is the codec which is used for
the-- for the FaceTime application.

456
00:43:08,060 --> 00:43:13,050
Now, Low Complexity and High Efficiency.

457
00:43:13,050 --> 00:43:18,160
Low Complexity provides the highest
audio quality multi-channel support.

458
00:43:18,160 --> 00:43:25,550
And High Efficiency uses some tricks in
order to significantly reduce the bit rate.

459
00:43:25,550 --> 00:43:30,650
And what High Efficiency is basically
doing is that during the decoding process,

460
00:43:30,650 --> 00:43:35,370
it synthesizes the other frequency
bands rather than encoding them.

461
00:43:35,370 --> 00:43:41,280
And so, this results in some significant bit rate savings.

462
00:43:41,280 --> 00:43:48,060
And as an extension to the High Efficiency,
the High Efficiency v2 Codec expands from mono

463
00:43:48,060 --> 00:43:52,480
to stereo signals using some parametric stereo techniques.

464
00:43:52,480 --> 00:43:59,190
And this v2 version can even provide lower
bit rates down to 20 kilobits per second.

465
00:43:59,190 --> 00:44:07,050
So, to summarize this, the highest quality will
always be achieved using the Low-Complexity Codec

466
00:44:07,050 --> 00:44:15,130
and the lowest bit rate with degraded quality will
be-- can be provided using the High Efficiency v2.

467
00:44:15,130 --> 00:44:20,780
Now, just to give you an idea--
I hit the wrong button, sorry.

468
00:44:20,780 --> 00:44:22,560
I have a few sound examples here.

469
00:44:22,560 --> 00:44:26,920
So, the first sound example is a low complexity
as encoded using the Low-Complexity Codec

470
00:44:26,920 --> 00:44:31,510
at 120 kilobits per second which is a very popular bit rate.

471
00:44:31,510 --> 00:44:39,480
[ Music ]

472
00:44:39,480 --> 00:44:43,770
The next example is going-- is encoded
using the High-Efficiency Codec

473
00:44:43,770 --> 00:44:47,510
at 64 kilobits per second, so half the bit rate.

474
00:44:47,510 --> 00:44:55,330
[ Music ]

475
00:44:55,330 --> 00:44:58,700
And the next one is going to be High-Efficiency v2 encoded

476
00:44:58,700 --> 00:45:03,510
at 32 kilobits per second, so even
half of what we just heard.

477
00:45:03,510 --> 00:45:11,060
[ Music ]

478
00:45:11,060 --> 00:45:15,000
Now, in order to put this in contrast
to what Low Complexity to--

479
00:45:15,000 --> 00:45:22,300
Low Complexity, the next example is actually the same item
encoded with Low Complexity but at 32 kilobits per second.

480
00:45:22,300 --> 00:45:27,510
So, exactly the same rate-- bit rate as High Efficiency v2.

481
00:45:27,510 --> 00:45:34,040
[ Music ]

482
00:45:34,040 --> 00:45:38,830
So, the-- even in this-- with this
acoustic, the difference is obvious.

483
00:45:38,830 --> 00:45:45,140
But just to be clear, the message here is not--
I don't want you to rush back to your offices

484
00:45:45,140 --> 00:45:50,340
and re-encode all your assets using a High Efficiency v2 at
the lowest possible bit rate, that's not the message here.

485
00:45:50,340 --> 00:45:55,920
So, I just want to say how efficient this technique is.

486
00:45:55,920 --> 00:46:01,290
And as I mentioned before, the high efficiency just--

487
00:46:01,290 --> 00:46:05,550
there's some clever synthesization
of the upper frequency bands,

488
00:46:05,550 --> 00:46:09,940
but they do not really reflect
the-- what the original content was.

489
00:46:09,940 --> 00:46:13,980
So, you should always be aware of this.

490
00:46:13,980 --> 00:46:20,460
So, the way this is working with the High Efficiency
is because when High Efficiency was introduced,

491
00:46:20,460 --> 00:46:27,420
many systems were already using AAC decoders, and
they wanted to preserve backwards compatibility.

492
00:46:27,420 --> 00:46:34,140
So, what the MPEG consortium did
was to use a layer approach.

493
00:46:34,140 --> 00:46:39,810
So, we start with a-- with the Low
Complexity base layer which is either mono

494
00:46:39,810 --> 00:46:46,610
or stereo and typically at 22 kHz sampling rate.

495
00:46:46,610 --> 00:46:52,190
And on top of this Low Complexity layer, we add
the High Efficiency which is also mono or stereo.

496
00:46:52,190 --> 00:46:58,700
But the High Efficiency layer operates at
double the frequency of the Low Complexity one.

497
00:46:58,700 --> 00:47:06,010
What this means is that-- I mean as I said
before, we synthesize the other frequency bands.

498
00:47:06,010 --> 00:47:13,730
And therefore, only the lower frequency portion
will be encoded using the Low Complexity.

499
00:47:13,730 --> 00:47:19,940
And similarly for the-- for the High-Efficiency
v2, we started with a low complexity layer,

500
00:47:19,940 --> 00:47:22,900
but this time, this layer is a mono-only layer.

501
00:47:22,900 --> 00:47:29,910
And then we add High Efficiency layer twice
the sampling rate, but will also be mono.

502
00:47:29,910 --> 00:47:36,540
And then on top of this, comes the High-Efficiency v2
layer which will then expand the mono to stereo signal.

503
00:47:36,540 --> 00:47:44,710
And this-- the discovery mechanism and how you deal
with this format is described in the tech note 22.3.6,

504
00:47:44,710 --> 00:47:50,360
so I recommend having a closer look at this document.

505
00:47:50,360 --> 00:47:54,320
Now, given that Low-Complexity provides such high qualities

506
00:47:54,320 --> 00:47:59,660
and we have very good efficiency using the High Efficiency
Codecs, what's the problem now with the Low Delay?

507
00:47:59,660 --> 00:48:01,400
Why do we need Low Delay?

508
00:48:01,400 --> 00:48:05,970
Well, if we look at the waveform
as it would come out directly

509
00:48:05,970 --> 00:48:09,870
from a Low Complexity decoder,
we would see something like this.

510
00:48:09,870 --> 00:48:14,690
The input signal is actually the same
signal, but really left-justified.

511
00:48:14,690 --> 00:48:24,200
So, we see here that before the signal onset, there is a
huge lag-- there is a huge region which contains only zeros.

512
00:48:24,200 --> 00:48:35,800
And this lag is obviously too much for a full
duplex type of-- for chat-like applications.

513
00:48:35,800 --> 00:48:43,320
This problem was somehow addressed with the Low
Delay AAC Codec, and you see that the output

514
00:48:43,320 --> 00:48:51,710
of AAC Low Delay Codec has substantially less-- the lag
is substantially less than for the Low Complexity one.

515
00:48:51,710 --> 00:48:57,750
And in the case of the Enhanced Low Delays, the
lag is even-- is only half of the Low Delay one.

516
00:48:57,750 --> 00:49:06,520
So, to summarize, sorry-- the-- and if we put this in terms
of numbers, then we see that the low complexity has a lag

517
00:49:06,520 --> 00:49:12,890
of 2112 samples whereas the Enhanced
Low Delay has only 240 samples lag.

518
00:49:12,890 --> 00:49:19,360
So, this shows that this codec is
well suited for AV chat applications.

519
00:49:19,360 --> 00:49:27,920
The Low Delay Codecs share the same foundations as AAC,
so it's actually an extension of the ACC standards,

520
00:49:27,920 --> 00:49:33,860
but they provide much smaller delays typically 15 to 40
milliseconds, and these have been specifically design

521
00:49:33,860 --> 00:49:38,150
for full-duplex type of communication applications.

522
00:49:38,150 --> 00:49:47,650
Low Delay, which is-- can be created using the format
ID, the format ID constant kAudioFormatMPEG4AAC_LD.

523
00:49:47,650 --> 00:49:50,890
It has a minimum delay of 20 milliseconds.

524
00:49:50,890 --> 00:49:55,510
The Enhanced Low Delay provides 15
milliseconds as the minimum delay.

525
00:49:55,510 --> 00:50:00,000
And because that they are part of AAC
codecs, they have a large bit rate range

526
00:50:00,000 --> 00:50:03,450
and they allow for even transparent quality.

527
00:50:03,450 --> 00:50:11,480
One thing I just wanted to point out is because the
windows-- the block sizes affect for all the codecs,

528
00:50:11,480 --> 00:50:16,040
the delay is actually proportional to the sampling rate.

529
00:50:16,040 --> 00:50:20,110
So, the higher the sampling rate, the
lower the delay will be, and conversely,

530
00:50:20,110 --> 00:50:25,260
the delay will increase if you
go down with the sampling rates.

531
00:50:25,260 --> 00:50:32,580
I just want to briefly give you an
overview of where those codec lives.

532
00:50:32,580 --> 00:50:39,230
Allan, in the previous session, did
explain when a software codec comes

533
00:50:39,230 --> 00:50:42,370
into play whenever-- how do a codec comes into play.

534
00:50:42,370 --> 00:50:46,250
This table just summarizes in which world which codec live.

535
00:50:46,250 --> 00:50:54,530
And we see that the High Efficiency and High Efficiency
v2 Codecs aren't yet available as software codecs.

536
00:50:54,530 --> 00:51:00,970
And this is important for your application because
if you want to use High Efficiency in codec material,

537
00:51:00,970 --> 00:51:04,780
you have to be aware of this that you
may not be able to decode it at its--

538
00:51:04,780 --> 00:51:08,280
at its full quality if the hardware
codec is already in use either

539
00:51:08,280 --> 00:51:14,890
by another application or by another AV player instance.

540
00:51:14,890 --> 00:51:21,270
And-- but the Low Delay Codecs are software only,
so they can have multiple instances on them.

541
00:51:21,270 --> 00:51:27,900
Now, I just wanted to go over some key
parameters for the encoding process.

542
00:51:27,900 --> 00:51:33,780
First, we have the sampling rate, of course, the number
of channels, and the bit rate, the bit rate modes,

543
00:51:33,780 --> 00:51:39,150
and all this leads to the subjective
quality, which we want to maintain.

544
00:51:39,150 --> 00:51:42,590
The bit rate determines the compression ratio.

545
00:51:42,590 --> 00:51:48,350
The higher the bit rate, the bigger the
resulting file, but the better the quality.

546
00:51:48,350 --> 00:51:52,300
And the bit rate is typically accounts for all channels.

547
00:51:52,300 --> 00:51:58,510
So we don't specify on a per channel basis because
sometimes it doesn't make sense like for 5.1 material

548
00:51:58,510 --> 00:52:04,610
or 7.1 because the Low Frequency effect
channel doesn't require many resources.

549
00:52:04,610 --> 00:52:11,590
And the bit rate typically also grows with the number of--
with the number of channels and also the sampling rate.

550
00:52:11,590 --> 00:52:18,510
And one thing to be aware of is that the AAC encoder--
the software AAC encoder has a sample rate converter--

551
00:52:18,510 --> 00:52:24,360
an internal sample rate converter, and it made you a
sample rate conversion if you specify your bit rate

552
00:52:24,360 --> 00:52:28,820
which is too low for the sampling rate of the input signal.

553
00:52:28,820 --> 00:52:35,720
So, like in the example I just showed
before, the 32-kilobit Low Complexity one,

554
00:52:35,720 --> 00:52:42,010
the sampling rate was actually down to 16 kHz.

555
00:52:42,010 --> 00:52:48,000
Bit rate modes is another knob
you can turn to do some tradeoffs.

556
00:52:48,000 --> 00:52:55,480
The most simple mode is the Constant Bit Rate Mode which
allocates a fixed amount of bytes for every packet.

557
00:52:55,480 --> 00:52:57,190
But therefore, it is not flexible at all.

558
00:52:57,190 --> 00:52:58,720
It doesn't accommodate to the content.

559
00:52:58,720 --> 00:53:06,970
So, encoding-- one second of silence takes us many
resources as encoding complete symphony orchestra.

560
00:53:06,970 --> 00:53:14,930
We recommend the use and the default is actually the Average
Bit Rate Mode, which has much more flexibility in the sense

561
00:53:14,930 --> 00:53:21,690
that it dynamically allocates the resources to
the-- for every packets according to its content,

562
00:53:21,690 --> 00:53:29,520
but with the constraint of trying to maintain
the average bit rate as provided by the user.

563
00:53:29,520 --> 00:53:35,080
And the most flexible mode which is also known
from MP3 is the so-called Variable Bit Rate Mode

564
00:53:35,080 --> 00:53:40,310
where there is basically no limitations to the bit rate.

565
00:53:40,310 --> 00:53:47,770
And VBR is expressed in terms of quality
in terms of-- instead of bit rate.

566
00:53:47,770 --> 00:53:53,300
Just to wrap up about the encoder, I just want to give a
few recommendations and hopefully you will follow them.

567
00:53:53,300 --> 00:53:59,700
The thing is that you should choose the codec
according to the use case and the limitations.

568
00:53:59,700 --> 00:54:06,210
If you want high-quality audio like media playback, then
there is no doubt you should always use Low Complexity.

569
00:54:06,210 --> 00:54:12,910
For streaming kind of applications like streaming
radios, High Efficiency is the best choice obviously

570
00:54:12,910 --> 00:54:19,350
because of the lower band, the
much significantly lower bit rates.

571
00:54:19,350 --> 00:54:26,360
But if you want high-quality voice chats, then
you should reuse the Enhanced Low Delay Codec.

572
00:54:26,360 --> 00:54:29,670
And whenever possible, you should
favor the highest possible--

573
00:54:29,670 --> 00:54:35,580
about highest possible quality by choosing the right codec,
the best encoding mode, and the highest possible bit rate.

574
00:54:35,580 --> 00:54:41,820
And also as James stated in the previous section
is that lost information can be recorded.

575
00:54:41,820 --> 00:54:50,000
What this means if you convert an MP3 to an AAC, even
at the higher bit rate, the quality will be degraded,

576
00:54:50,000 --> 00:54:52,630
even though the high-- the bit rate has been higher.

577
00:54:52,630 --> 00:54:55,380
So, this is something you should really avoid.

578
00:54:55,380 --> 00:55:02,230
If you don't have the source material, try--
avoid transcoding from one format to the next.

579
00:55:02,230 --> 00:55:09,230
Now, the way this AAC streams are
packaged, basically, you will--

580
00:55:09,230 --> 00:55:14,000
well you know these .mp4 endings which
is the MPEG-4 native file format.

581
00:55:14,000 --> 00:55:19,750
There's also the .m4a which is MPEG-4 compatible
which adds iTunes-specific data chunks,

582
00:55:19,750 --> 00:55:24,970
and this can also be used to encode--
to embed ALAC material.

583
00:55:24,970 --> 00:55:32,250
And there's the preferred format which is the
Core Audio file format with the ending .caf.

584
00:55:32,250 --> 00:55:41,350
For streaming, you have the ending .adts or .aac which is a
self-framing format, and this is what is used in SHOUTcast,

585
00:55:41,350 --> 00:55:44,400
internet broadcast, and HTTP live streaming.

586
00:55:44,400 --> 00:55:46,030
So, this concludes my talk.

587
00:55:46,030 --> 00:55:52,850
I just wanted to point out to the next session about Audio
Development for iPhone OS Part 2 by Murray who will go

588
00:55:52,850 --> 00:56:07,530
into much more details about the
RemoteIO, the AudioUnits, excuse me.

