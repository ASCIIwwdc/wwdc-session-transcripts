WEBVTT
X-TIMESTAMP-MAP=MPEGTS:181083,LOCAL:00:00:00.000

00:00:09.956 --> 00:00:10.956 A:middle
&gt;&gt; Thank you all for coming

00:00:10.956 --> 00:00:12.596 A:middle
to on the last sessions
of this week.

00:00:12.596 --> 00:00:14.196 A:middle
I hope we've all had
a great week here.

00:00:14.586 --> 00:00:16.106 A:middle
Today, in our session,
we're going to be talking

00:00:16.106 --> 00:00:20.236 A:middle
about Core Image Effects and
Techniques on iOS and Mac OS.

00:00:21.496 --> 00:00:23.356 A:middle
So, Core Image.

00:00:24.346 --> 00:00:27.406 A:middle
In a nutshell, Core Image is a
foundational image processing

00:00:27.406 --> 00:00:31.176 A:middle
framework on both iOS and Mac
OS and it's used in a variety

00:00:31.176 --> 00:00:36.306 A:middle
of great applications from Photo
Booth to iPhoto on both desktop

00:00:36.306 --> 00:00:37.266 A:middle
and embedded products.

00:00:37.646 --> 00:00:40.036 A:middle
It's also used in the
new photos app for some

00:00:40.036 --> 00:00:42.536 A:middle
of their image effects,
the new filter effects

00:00:42.536 --> 00:00:44.806 A:middle
that are available in iOS 7.

00:00:44.806 --> 00:00:47.226 A:middle
And it's also in a wide variety

00:00:47.226 --> 00:00:50.706 A:middle
of very successful App
Store apps as well.

00:00:51.096 --> 00:00:52.846 A:middle
So, it's a great
foundational technology.

00:00:52.846 --> 00:00:55.126 A:middle
You spend a lot of time making
sure to get the best performance

00:00:55.126 --> 00:00:57.156 A:middle
out of it and we want to
tell you all about it today.

00:00:58.676 --> 00:01:00.556 A:middle
So, the key concepts
we're going to be talking

WEBVTT
X-TIMESTAMP-MAP=MPEGTS:181083,LOCAL:00:00:00.000

00:01:00.556 --> 00:01:03.226 A:middle
about today are going to be how

00:01:03.226 --> 00:01:04.906 A:middle
to get started using
the Core Image API?

00:01:05.126 --> 00:01:07.506 A:middle
How to leverage the built-in
filters that we provide?

00:01:07.506 --> 00:01:11.106 A:middle
How to provide input
images into these filters?

00:01:11.566 --> 00:01:13.506 A:middle
How to render the
output of those effects?

00:01:14.026 --> 00:01:18.526 A:middle
And lastly, we have a great
demo of how to bridge Core Image

00:01:18.526 --> 00:01:20.156 A:middle
in OpenCL technologies together.

00:01:20.826 --> 00:01:23.896 A:middle
So, the key concepts
of Core Image.

00:01:23.996 --> 00:01:25.526 A:middle
It's actually a very
simple concept

00:01:25.526 --> 00:01:26.546 A:middle
and it's very simple to code.

00:01:27.196 --> 00:01:29.286 A:middle
The idea is you have
filters that allow you

00:01:29.286 --> 00:01:31.956 A:middle
to perform per pixel
operations on an image.

00:01:32.346 --> 00:01:35.236 A:middle
So, in a very simple example,
we have an input image

00:01:35.236 --> 00:01:37.026 A:middle
and original image,
pictures of boats,

00:01:37.686 --> 00:01:40.196 A:middle
and we want to apply a
Sepia Tone Filter to that

00:01:40.196 --> 00:01:42.836 A:middle
and the results after
that is a new image.

00:01:44.066 --> 00:01:46.016 A:middle
But we actually have
lots of filters

00:01:46.016 --> 00:01:49.286 A:middle
and you can chain them together
into either chains or graphs.

00:01:49.376 --> 00:01:52.486 A:middle
And this-- by combining
these multiple filters,

00:01:52.486 --> 00:01:54.136 A:middle
you can create very
complex effects.

00:01:54.276 --> 00:01:58.226 A:middle
In this slightly more complex
example, we're taking an image,

00:01:58.696 --> 00:02:00.586 A:middle
running it through Sepia
Tone then running it

WEBVTT
X-TIMESTAMP-MAP=MPEGTS:181083,LOCAL:00:00:00.000

00:02:00.586 --> 00:02:03.716 A:middle
to a Hue Adjustment Filter to
make it into a blue tone image

00:02:04.276 --> 00:02:06.086 A:middle
and then we're adding
some contrast

00:02:06.086 --> 00:02:08.216 A:middle
by using the color
controls filter.

00:02:09.675 --> 00:02:11.386 A:middle
Now, while you can
conceptually think of their--

00:02:11.386 --> 00:02:14.926 A:middle
being an intermediate image
between every filter internally

00:02:15.106 --> 00:02:16.236 A:middle
to improve performance,

00:02:16.756 --> 00:02:18.966 A:middle
Core Image will concatenate
these filters

00:02:18.966 --> 00:02:22.356 A:middle
into a combined program in order

00:02:22.356 --> 00:02:25.756 A:middle
to get the best possible
performance and this is achieved

00:02:25.756 --> 00:02:28.516 A:middle
by eliminating intermediate
buffers which is a big benefit.

00:02:29.036 --> 00:02:33.666 A:middle
And then we also do
additional runtime optimizations

00:02:33.666 --> 00:02:34.686 A:middle
on the filter graph.

00:02:34.766 --> 00:02:37.806 A:middle
For example, both the
hue adjustment an example

00:02:37.806 --> 00:02:41.276 A:middle
and the contrast or
both matrix operations

00:02:41.596 --> 00:02:43.466 A:middle
and if you have sequential
matrix operations

00:02:43.466 --> 00:02:45.696 A:middle
in your filter graph then
Core Image will combine those

00:02:45.696 --> 00:02:47.326 A:middle
into a combined matrix

00:02:47.446 --> 00:02:49.516 A:middle
which will actually further
improve both performance

00:02:49.516 --> 00:02:50.116 A:middle
and quality.

00:02:51.776 --> 00:02:54.256 A:middle
So, let me give you
a real quick example

00:02:54.256 --> 00:02:56.036 A:middle
of this working in action.

00:02:56.446 --> 00:02:59.786 A:middle
So, if I bring this up
here, we have an application

00:02:59.976 --> 00:03:04.476 A:middle
which we first used a
little bit last year at WWDC

WEBVTT
X-TIMESTAMP-MAP=MPEGTS:181083,LOCAL:00:00:00.000

00:03:04.476 --> 00:03:09.616 A:middle
and now we actually
have fully vague version

00:03:09.616 --> 00:03:10.356 A:middle
of the application.

00:03:10.776 --> 00:03:14.616 A:middle
So, the idea here is you
bring up the filters pop-up

00:03:15.066 --> 00:03:19.496 A:middle
and that's allows you to add
either input sources or filters

00:03:19.496 --> 00:03:21.136 A:middle
to your rendering graph.

00:03:21.136 --> 00:03:22.936 A:middle
In this case, I just want
to bring in the video

00:03:22.936 --> 00:03:25.446 A:middle
from the video feed and
hopefully you can see that OK.

00:03:26.226 --> 00:03:28.296 A:middle
Once we have that
effect, we can then add

00:03:28.296 --> 00:03:31.136 A:middle
on additional adjustments
like for example we can go

00:03:31.136 --> 00:03:38.746 A:middle
and find another effect in
here and with color controls,

00:03:38.746 --> 00:03:41.996 A:middle
we can increase saturation
or contrast

00:03:42.646 --> 00:03:45.506 A:middle
and we can do these
effects live.

00:03:45.506 --> 00:03:46.866 A:middle
We can also delete them.

00:03:46.866 --> 00:03:49.026 A:middle
If you want to do a
slightly more complex effect,

00:03:49.026 --> 00:03:52.296 A:middle
we can do a pattern here.

00:03:52.296 --> 00:03:53.646 A:middle
We can go to dot screen.

00:03:54.646 --> 00:03:56.646 A:middle
And dot screen, hopefully
you can see this turns your--

00:03:56.966 --> 00:04:02.796 A:middle
turns the video into a-- like
a newsprint type dot pattern,

WEBVTT
X-TIMESTAMP-MAP=MPEGTS:181083,LOCAL:00:00:00.000

00:04:02.796 --> 00:04:04.476 A:middle
we can adjust the
size of the dot

00:04:04.476 --> 00:04:05.806 A:middle
and the angle of
the dot pattern.

00:04:06.846 --> 00:04:09.976 A:middle
Now, let's say this doesn't
quite suit our desires

00:04:09.976 --> 00:04:10.326 A:middle
right now.

00:04:10.326 --> 00:04:11.626 A:middle
This is a black and
white pattern.

00:04:11.626 --> 00:04:14.866 A:middle
We'd like to kind of
combine this halftone pattern

00:04:14.866 --> 00:04:16.386 A:middle
with the original
color of the image.

00:04:16.886 --> 00:04:20.125 A:middle
We can actually represent graphs
in this list for you here.

00:04:20.255 --> 00:04:23.136 A:middle
We can-- what we can do is
we can add another instance

00:04:23.886 --> 00:04:24.946 A:middle
of the input video.

00:04:25.756 --> 00:04:29.536 A:middle
So, now we've got two operations
on this stack of filters

00:04:29.916 --> 00:04:32.466 A:middle
and then we can then
combine those

00:04:32.696 --> 00:04:34.466 A:middle
with another combining filter.

00:04:34.876 --> 00:04:36.286 A:middle
Yeah, here we go, [inaudible].

00:04:36.666 --> 00:04:38.906 A:middle
So, now, hopefully you can
see it on the projector

00:04:38.906 --> 00:04:41.716 A:middle
but we've got both the
halftone pattern and the color

00:04:41.716 --> 00:04:45.406 A:middle
from the original image
shining through, all right.

00:04:45.986 --> 00:04:49.006 A:middle
So, let me pop this off, delete.

00:04:49.386 --> 00:04:53.676 A:middle
That was the first demo of
the Funhouse Application.

00:04:53.676 --> 00:04:54.806 A:middle
Let me go back to my slides

00:04:55.126 --> 00:04:57.656 A:middle
and the great news
is the source code

00:04:57.656 --> 00:04:59.056 A:middle
for this app is now available.

00:04:59.306 --> 00:05:01.786 A:middle
So, this has been a
much requested feature.

WEBVTT
X-TIMESTAMP-MAP=MPEGTS:181083,LOCAL:00:00:00.000

00:05:01.786 --> 00:05:04.716 A:middle
We showed this last year a
little bit and it should--

00:05:04.716 --> 00:05:06.096 A:middle
[ Applause ]

00:05:06.096 --> 00:05:08.526 A:middle
really great shape for you guys
to look at this application

00:05:08.526 --> 00:05:11.446 A:middle
and see how we did
all this fun stuff.

00:05:11.666 --> 00:05:15.426 A:middle
So, once you've look at the
code, you can see very quickly

00:05:15.426 --> 00:05:18.276 A:middle
that there's really three
basic classes that you need

00:05:18.486 --> 00:05:19.896 A:middle
to understand to use Core Image.

00:05:20.706 --> 00:05:24.706 A:middle
The first class is the CIFilter
class and this is mutable object

00:05:24.706 --> 00:05:26.536 A:middle
that represents an effect
that you want to apply

00:05:27.306 --> 00:05:29.676 A:middle
and a filter has
either an image input

00:05:29.676 --> 00:05:31.346 A:middle
or numeric input parameters

00:05:31.826 --> 00:05:34.826 A:middle
and also it has an output
image parameter as well.

00:05:35.346 --> 00:05:38.276 A:middle
And at the time you ask for
the output image parameter,

00:05:38.536 --> 00:05:40.126 A:middle
it will return in an object

00:05:40.126 --> 00:05:41.746 A:middle
that represents the
current state based

00:05:41.976 --> 00:05:43.836 A:middle
on the current state
of input parameters.

00:05:45.316 --> 00:05:47.266 A:middle
The second key object
type that you need

00:05:47.266 --> 00:05:49.746 A:middle
to understand is
the CIImage object

00:05:50.036 --> 00:05:51.606 A:middle
and this is an immutable object

00:05:51.656 --> 00:05:53.816 A:middle
that represents the
recipe for an image.

00:05:54.256 --> 00:05:56.216 A:middle
And there're basically
two types of images.

00:05:56.216 --> 00:05:59.356 A:middle
There's an image that's come
directly from a file or an input

00:05:59.756 --> 00:06:02.996 A:middle
of some sort and-- or you can
also have a CIImage that comes

WEBVTT
X-TIMESTAMP-MAP=MPEGTS:181083,LOCAL:00:00:00.000

00:06:02.996 --> 00:06:04.316 A:middle
from the output of a CIFilter.

00:06:04.736 --> 00:06:07.086 A:middle
The third key data type
that you needed to be aware

00:06:07.086 --> 00:06:10.346 A:middle
of is a CIContext and a
CIContext is the object

00:06:10.346 --> 00:06:12.436 A:middle
through which Core Image
will render its result.

00:06:12.906 --> 00:06:16.946 A:middle
It can be either based on a
CPU renderer or GPU renderer

00:06:17.306 --> 00:06:19.916 A:middle
and that's really important to
distinguish between those two

00:06:19.916 --> 00:06:21.366 A:middle
and I'll talk about
that a little bit later

00:06:21.366 --> 00:06:22.116 A:middle
in the presentation.

00:06:24.336 --> 00:06:27.316 A:middle
So, as I mentioned in the intro,
Core Image is available both

00:06:27.316 --> 00:06:30.686 A:middle
on iOS and Mac OS and for
the most part, they're very,

00:06:30.686 --> 00:06:32.186 A:middle
very similar between
the two platforms

00:06:32.216 --> 00:06:34.686 A:middle
but there are a few
platforms specifics

00:06:34.686 --> 00:06:35.806 A:middle
that you might want
to be aware off.

00:06:37.046 --> 00:06:38.926 A:middle
First of all, in terms
of built-in filters,

00:06:39.536 --> 00:06:43.186 A:middle
on iOS Core Image has about over
a hundred built-in filters now

00:06:43.186 --> 00:06:45.156 A:middle
and we've added some
more in iOS 7 as well.

00:06:46.036 --> 00:06:50.656 A:middle
And on Mac OS X, we have
over a 150 built-in filters

00:06:50.656 --> 00:06:52.586 A:middle
and we also have the
ability for your application

00:06:52.586 --> 00:06:54.046 A:middle
to provide its own filters.

00:06:55.596 --> 00:06:59.886 A:middle
The core API is very similar
between the two platforms.

WEBVTT
X-TIMESTAMP-MAP=MPEGTS:181083,LOCAL:00:00:00.000

00:07:00.496 --> 00:07:04.106 A:middle
The key classes I mentioned
earlier are CIFilters, CIImage

00:07:04.106 --> 00:07:06.896 A:middle
and CIContext and
they're available on both

00:07:06.896 --> 00:07:08.506 A:middle
and they're largely
identical APIs.

00:07:09.856 --> 00:07:13.016 A:middle
On OS X, there are few
other additional classes

00:07:13.016 --> 00:07:15.726 A:middle
such as CIKernel and CIFilter
shape which are useful

00:07:15.726 --> 00:07:17.276 A:middle
if you're creating your
own custom filters.

00:07:19.336 --> 00:07:23.296 A:middle
On both platforms, we have
render-time optimizations

00:07:23.376 --> 00:07:25.786 A:middle
and while the optimizations
are slightly different due

00:07:25.786 --> 00:07:28.996 A:middle
to the differing natures of the
platforms, the idea is the same

00:07:28.996 --> 00:07:30.196 A:middle
and that Core Image
will take care

00:07:30.196 --> 00:07:33.896 A:middle
of doing the best render-time
optimizations that are possible

00:07:34.186 --> 00:07:35.866 A:middle
to render your requested graph.

00:07:36.296 --> 00:07:39.246 A:middle
There're a few similarities

00:07:39.246 --> 00:07:41.286 A:middle
and differences regarding
color management

00:07:41.286 --> 00:07:44.826 A:middle
which is also something
to be aware of.

00:07:44.826 --> 00:07:49.496 A:middle
On iOS, Core Image
supports either sRGB content

00:07:49.966 --> 00:07:51.806 A:middle
or a non-color managed workflow

00:07:51.806 --> 00:07:54.706 A:middle
if you decide that's what's
best for your application.

00:07:55.466 --> 00:07:58.816 A:middle
On OS X, the-- you can either
have a non-color managed

00:07:58.816 --> 00:08:03.836 A:middle
workflow or you can support any
ICC based colored profile using

WEBVTT
X-TIMESTAMP-MAP=MPEGTS:181083,LOCAL:00:00:00.000

00:08:04.216 --> 00:08:06.076 A:middle
the CG color space graph object.

00:08:07.196 --> 00:08:11.126 A:middle
In both cases, both
in iOS and Mac OS,

00:08:11.126 --> 00:08:13.646 A:middle
the internal working
space that Core Image uses

00:08:13.646 --> 00:08:17.686 A:middle
for its filters are unclamped
linear data and this is useful

00:08:17.786 --> 00:08:20.726 A:middle
to produce high quality
results and predictable results

00:08:20.726 --> 00:08:22.766 A:middle
across a variety of
different color spaces.

00:08:24.956 --> 00:08:27.616 A:middle
Lastly, there're some
important differences in terms

00:08:27.616 --> 00:08:29.236 A:middle
of the rendering
architecture that's used.

00:08:29.906 --> 00:08:33.015 A:middle
On iOS, we have a
CPU rendering path

00:08:33.306 --> 00:08:35.926 A:middle
and we also have our GPU based
rendering path that's based

00:08:35.926 --> 00:08:37.385 A:middle
on OpenGL ES 2.0.

00:08:38.515 --> 00:08:42.866 A:middle
And on OS X, we have also a CPU
and a GPU based rendering path.

00:08:42.866 --> 00:08:48.186 A:middle
Our CPU rendering path is built
up on top of OpenCL on it's--

00:08:48.216 --> 00:08:49.606 A:middle
using its CPU rendering.

00:08:50.236 --> 00:08:56.186 A:middle
And also, new on Mavericks,
Core Image will also use OpenCL

00:08:56.186 --> 00:08:58.046 A:middle
in the GPU and I'd like
to give you a little demo

00:08:58.046 --> 00:09:00.066 A:middle
of that today 'cause we got
some great benefits out of that.

WEBVTT
X-TIMESTAMP-MAP=MPEGTS:181083,LOCAL:00:00:00.000

00:09:00.696 --> 00:09:03.446 A:middle
For a wide variety of operations
in Core Image, we get very,

00:09:03.446 --> 00:09:06.566 A:middle
very, very high performance
due to the fact

00:09:06.566 --> 00:09:07.636 A:middle
that we leverage the GPU.

00:09:08.056 --> 00:09:10.976 A:middle
For example, we can be
adjusting the slide or real time

00:09:10.976 --> 00:09:16.756 A:middle
on this 3K image or I think
it's 3.5K by something image

00:09:17.206 --> 00:09:20.066 A:middle
and we're getting very,
very fluid results

00:09:20.066 --> 00:09:21.346 A:middle
on the sliderand that's--

00:09:21.346 --> 00:09:23.836 A:middle
'cause these are
relatively simple operations.

00:09:25.036 --> 00:09:27.286 A:middle
One way we like to think
about this, however,

00:09:27.286 --> 00:09:29.546 A:middle
is how does this
performance change as we start

00:09:29.546 --> 00:09:33.426 A:middle
to do more complex operations
and how do we make sure

00:09:33.426 --> 00:09:35.306 A:middle
that the interactive behavior

00:09:35.306 --> 00:09:37.246 A:middle
of Core Images is
fluid as possible.

00:09:37.966 --> 00:09:41.776 A:middle
So, we've been spending a lot
of time on this in Mavericks

00:09:42.056 --> 00:09:44.556 A:middle
and we came up with
this demo application

00:09:44.556 --> 00:09:46.226 A:middle
to help demonstrate performance.

00:09:46.656 --> 00:09:48.496 A:middle
One thing that really
makes it easier

00:09:48.496 --> 00:09:50.066 A:middle
to see the performance
instead of trying

00:09:50.066 --> 00:09:54.536 A:middle
to subjectively judge a slider
is we have this little test mode

00:09:54.956 --> 00:09:57.636 A:middle
where it will 50 renders
in rapid succession

00:09:57.636 --> 00:09:58.646 A:middle
as quickly as possible.

00:09:59.276 --> 00:10:01.596 A:middle
And what it will do is it will
take these filter operations

WEBVTT
X-TIMESTAMP-MAP=MPEGTS:181083,LOCAL:00:00:00.000

00:10:01.596 --> 00:10:04.316 A:middle
that we've done and it
will prepend to beginning

00:10:04.316 --> 00:10:06.426 A:middle
of that filter operation
in exposure adjustment

00:10:06.736 --> 00:10:09.656 A:middle
and it will adjust that
exposure and render it 50 times

00:10:09.656 --> 00:10:10.956 A:middle
with 50 different exposures.

00:10:11.216 --> 00:10:13.406 A:middle
And that will force Core Image
to have to render everything

00:10:13.406 --> 00:10:14.866 A:middle
after it in the filter
graph again.

00:10:15.946 --> 00:10:18.426 A:middle
So, if we go through here,
it will do a quick sweep

00:10:18.426 --> 00:10:21.826 A:middle
of the image and you can see
we're getting 0.83 seconds

00:10:21.896 --> 00:10:24.006 A:middle
and that's an interesting
number that turns

00:10:24.006 --> 00:10:26.806 A:middle
out that that's how long it
takes to render 50 frames

00:10:26.806 --> 00:10:30.256 A:middle
if you're limited by 60 frames
per second display time.

00:10:30.516 --> 00:10:32.896 A:middle
So, that's good, that means
we're hitting 60 frames per

00:10:32.896 --> 00:10:35.006 A:middle
second or maybe we're
actually even faster

00:10:35.006 --> 00:10:36.296 A:middle
but we're limited
by the frame, right.

00:10:37.296 --> 00:10:38.286 A:middle
So, the question is, however,

00:10:38.286 --> 00:10:39.816 A:middle
what starts to happen
is we start

00:10:39.816 --> 00:10:42.046 A:middle
to do more complex
operations and obviously

00:10:42.046 --> 00:10:44.646 A:middle
if we start throwing in
very complex operations

00:10:45.116 --> 00:10:46.716 A:middle
like highlights and
shadows adjustments

00:10:47.036 --> 00:10:49.156 A:middle
and more importantly
very large blurs.

00:10:49.626 --> 00:10:54.136 A:middle
This blur is actually even
more than the 50 pixels,

00:10:54.246 --> 00:10:56.726 A:middle
50-50 value that you're
seeing in that slider.

00:10:56.976 --> 00:11:00.736 A:middle
It's actually hundreds of pixels
wide but hundred of pixels tall

WEBVTT
X-TIMESTAMP-MAP=MPEGTS:181083,LOCAL:00:00:00.000

00:11:01.056 --> 00:11:03.866 A:middle
and that requires a lot
of fetching from an image.

00:11:04.186 --> 00:11:07.036 A:middle
So, obviously in this
case, when we do a sweep,

00:11:07.346 --> 00:11:10.046 A:middle
we're getting not quite
real time performance.

00:11:11.896 --> 00:11:14.316 A:middle
And while it's, you know,
impressive, you know,

00:11:14.316 --> 00:11:15.756 A:middle
we could do better
and this is one

00:11:15.756 --> 00:11:17.116 A:middle
of the reasons we
spend a lot of time

00:11:17.446 --> 00:11:20.786 A:middle
in Mavericks changing the
internals of Core Image

00:11:21.016 --> 00:11:23.856 A:middle
so that it would use OpenCL
instead and as you see

00:11:23.856 --> 00:11:27.866 A:middle
as we turn on OpenCL on the
GPU path, we're now back

00:11:27.866 --> 00:11:30.326 A:middle
down to 60 frames per second

00:11:30.426 --> 00:11:32.236 A:middle
on this complex rendering
operation.

00:11:32.586 --> 00:11:34.356 A:middle
So, we're really pleased
with these results.

00:11:34.836 --> 00:11:36.626 A:middle
The great thing also
about this performance is

00:11:36.626 --> 00:11:40.096 A:middle
that it particularly benefits
operations that were complex.

00:11:40.776 --> 00:11:44.116 A:middle
We were doing large
complex render graphs.

00:11:44.686 --> 00:11:48.766 A:middle
So that was again the
demonstration of OpenCL

00:11:48.766 --> 00:11:54.406 A:middle
on the GPU on OS X Mavericks.

00:11:54.476 --> 00:11:56.836 A:middle
So, as I've talked about
today, we've had a lot

00:11:56.836 --> 00:11:58.016 A:middle
of built-in filters and I'd

00:11:58.016 --> 00:12:00.316 A:middle
like to give you a
little bit more detail

WEBVTT
X-TIMESTAMP-MAP=MPEGTS:181083,LOCAL:00:00:00.000

00:12:00.316 --> 00:12:02.416 A:middle
on those built-on filters
and some we've added

00:12:02.846 --> 00:12:04.956 A:middle
and give you some more
information on how

00:12:04.956 --> 00:12:06.386 A:middle
to use filters in
your application.

00:12:07.406 --> 00:12:09.546 A:middle
So, we have a ton
of useful filters

00:12:09.976 --> 00:12:11.806 A:middle
and it's probably
barely even readable

00:12:11.806 --> 00:12:14.356 A:middle
to see them all here so, I just
want to highlight some today.

00:12:15.286 --> 00:12:18.236 A:middle
So, first of all the filters
fall under different categories.

00:12:18.236 --> 00:12:20.556 A:middle
We have whole bunch of
filters for doing color effects

00:12:20.556 --> 00:12:21.546 A:middle
and color adjustments.

00:12:22.546 --> 00:12:25.606 A:middle
In my slides earlier, I called
out three as an example,

00:12:25.606 --> 00:12:27.986 A:middle
color controls, hue
adjustment, and sepia tone.

00:12:28.366 --> 00:12:29.506 A:middle
The other ones work similarly.

00:12:29.506 --> 00:12:31.306 A:middle
They take input image
and have parameters

00:12:31.306 --> 00:12:32.416 A:middle
and produce an output image.

00:12:33.826 --> 00:12:38.346 A:middle
We've also added some new ones
in both of iOS 7 in Mavericks

00:12:38.826 --> 00:12:41.296 A:middle
that we think will be
useful for different--

00:12:41.576 --> 00:12:42.726 A:middle
a variety of different uses.

00:12:43.096 --> 00:12:45.526 A:middle
We have, for example,
color polynomial

00:12:45.526 --> 00:12:47.416 A:middle
and color cross polynomial
that allows you

00:12:47.416 --> 00:12:49.896 A:middle
to do polynomial operations
that combine the red,

00:12:49.946 --> 00:12:51.716 A:middle
green and blue channels
in interesting ways.

00:12:51.716 --> 00:12:53.566 A:middle
You can actually do some really
interesting color effects

00:12:53.566 --> 00:12:53.916 A:middle
with this.

00:12:54.186 --> 00:12:57.236 A:middle
We also have a class
of filters which fall

00:12:57.236 --> 00:13:00.396 A:middle
into either geometry adjustments
or distortion effects.

WEBVTT
X-TIMESTAMP-MAP=MPEGTS:181083,LOCAL:00:00:00.000

00:13:01.366 --> 00:13:04.766 A:middle
And for example, one of these
is a fun effect called twirl

00:13:04.766 --> 00:13:07.756 A:middle
distortion and we can actually
demo that real quickly here.

00:13:09.236 --> 00:13:12.356 A:middle
And you can see this
adjusting a twirl on an image

00:13:12.576 --> 00:13:13.776 A:middle
and this actually running

00:13:13.776 --> 00:13:16.086 A:middle
in that presentation right
now using Core Image.

00:13:16.816 --> 00:13:18.476 A:middle
It's kind of recorded movie.

00:13:19.296 --> 00:13:22.106 A:middle
We also have several
blur and sharpen effects

00:13:22.436 --> 00:13:25.366 A:middle
and I mentioned blur and sharpen
because blurs in particular one

00:13:25.366 --> 00:13:27.026 A:middle
of the most foundational types

00:13:27.026 --> 00:13:30.116 A:middle
of image processing you
can perform, Gaussian blur

00:13:30.116 --> 00:13:33.156 A:middle
for example is used as the
basis of a whole variety

00:13:33.156 --> 00:13:35.046 A:middle
of different effects
such as sharpening

00:13:35.046 --> 00:13:36.286 A:middle
and edge detection and the like.

00:13:37.076 --> 00:13:42.796 A:middle
We've also added some new blur
or convolution effects to iOS 7

00:13:42.796 --> 00:13:45.586 A:middle
in Mavericks and we've
picked some that were--

00:13:45.676 --> 00:13:48.276 A:middle
be particularly general so that
they can be used in a variety

00:13:48.276 --> 00:13:52.296 A:middle
of applications, very, very
common to use either 3X3

00:13:52.296 --> 00:13:55.426 A:middle
or 5X5 convolutions and
we've implemented those

00:13:55.426 --> 00:13:56.716 A:middle
and optimized the
heck out of them

00:13:56.716 --> 00:13:58.236 A:middle
so they'll get really
good performance.

00:13:58.856 --> 00:14:02.586 A:middle
We've also added a horizontal
and vertical convolution

WEBVTT
X-TIMESTAMP-MAP=MPEGTS:181083,LOCAL:00:00:00.000

00:14:02.586 --> 00:14:04.686 A:middle
which is useful if your
convolution is a separable

00:14:04.686 --> 00:14:10.416 A:middle
operation and, again, we've
optimized the heck out of these.

00:14:10.636 --> 00:14:12.936 A:middle
We also have a class of
filters called generators

00:14:12.996 --> 00:14:15.866 A:middle
and these are filters that
don't take an input image

00:14:15.926 --> 00:14:19.446 A:middle
but will produce an output image
and these are things for effects

00:14:19.446 --> 00:14:21.406 A:middle
like starburst and
random textures

00:14:21.406 --> 00:14:25.136 A:middle
and checkerboard patterns
but we've added a new one

00:14:25.166 --> 00:14:30.006 A:middle
in both iOS 7 in Mavericks
called QR code generator

00:14:30.586 --> 00:14:33.666 A:middle
and this is a filter that takes
a string as an input parameter

00:14:34.186 --> 00:14:40.096 A:middle
and also a quality setting
and then also will produce

00:14:40.126 --> 00:14:43.546 A:middle
as its output a chart
image, bar chart image.

00:14:44.656 --> 00:14:45.806 A:middle
So that can be useful on a lot

00:14:45.806 --> 00:14:47.176 A:middle
of interesting applications
as well.

00:14:48.766 --> 00:14:52.236 A:middle
We also, have a class
called face detector

00:14:52.906 --> 00:14:56.086 A:middle
and this not exactly a
filter per se but it's--

00:14:56.086 --> 00:14:57.616 A:middle
you can think of it as
a filter in the sense

00:14:57.616 --> 00:15:00.026 A:middle
that it takes input images
and produces output image--

WEBVTT
X-TIMESTAMP-MAP=MPEGTS:181083,LOCAL:00:00:00.000

00:15:00.026 --> 00:15:04.416 A:middle
out data and we've had this
for a couple of releases now.

00:15:04.756 --> 00:15:10.946 A:middle
The great thing is starting
now in OS 7 in Mavericks,

00:15:10.946 --> 00:15:12.456 A:middle
we've made some enhancement
to that.

00:15:12.776 --> 00:15:14.716 A:middle
In the past, you
could take a face

00:15:14.716 --> 00:15:16.516 A:middle
and it will return
the bounding informa--

00:15:16.516 --> 00:15:18.386 A:middle
bounding rect for the face

00:15:18.656 --> 00:15:20.266 A:middle
and it will also
return the coordinates

00:15:20.266 --> 00:15:21.116 A:middle
for the eyes and mouth.

00:15:21.666 --> 00:15:25.776 A:middle
But starting in Mavericks in iOS
7, there's a flag you can pass

00:15:25.776 --> 00:15:27.876 A:middle
in that will also
return information

00:15:27.876 --> 00:15:30.896 A:middle
like whether a smile is present
or whether the eye is blinking,

00:15:31.026 --> 00:15:32.566 A:middle
so that's another
nice new enhancement.

00:15:32.886 --> 00:15:36.176 A:middle
So there's a brief overview
of our 100 plus filters.

00:15:36.736 --> 00:15:39.376 A:middle
One question we're commonly
asked is how do we choose what

00:15:39.376 --> 00:15:41.546 A:middle
filters we use or can
we add this filter or--

00:15:41.886 --> 00:15:43.446 A:middle
and I wanted to just
talk a moment

00:15:43.446 --> 00:15:44.666 A:middle
about our process on that.

00:15:45.176 --> 00:15:47.516 A:middle
So the key thing we
want to consider,

00:15:47.516 --> 00:15:49.576 A:middle
these two key criterion,

00:15:49.886 --> 00:15:52.226 A:middle
one is that a filter
must be broadly usable.

00:15:52.226 --> 00:15:55.746 A:middle
We want to make sure that we
add filters like convolutions

00:15:55.746 --> 00:15:57.846 A:middle
which are useful on a
wide variety of usages

00:15:58.146 --> 00:16:00.226 A:middle
so that we can implement
them in a robust way

WEBVTT
X-TIMESTAMP-MAP=MPEGTS:181083,LOCAL:00:00:00.000

00:16:00.446 --> 00:16:03.736 A:middle
and have them be useful to a
wide variety of client needs.

00:16:04.356 --> 00:16:06.886 A:middle
And also we want to make sure
we choose a type of operations

00:16:06.886 --> 00:16:08.666 A:middle
that can be well
implemented and performant

00:16:08.666 --> 00:16:10.006 A:middle
on our target platform.

00:16:10.496 --> 00:16:15.256 A:middle
So, as I mentioned in
my brief introduction

00:16:15.256 --> 00:16:16.556 A:middle
at the very beginning
of the presentation,

00:16:16.556 --> 00:16:18.696 A:middle
you can chain together
multiple filters and I wanted

00:16:18.696 --> 00:16:21.016 A:middle
to give you an idea in code
of how easy this is to do.

00:16:21.016 --> 00:16:22.996 A:middle
You start out with
an input image,

00:16:23.376 --> 00:16:25.646 A:middle
you create a filter object
saying, "I'd the filter

00:16:25.646 --> 00:16:28.206 A:middle
with a name," and you
specify a filter with the name

00:16:28.206 --> 00:16:31.936 A:middle
like CISepiaTone and at the same
time, you specify the parameters

00:16:31.936 --> 00:16:34.636 A:middle
such as the input image
and the intensity amount

00:16:35.126 --> 00:16:36.436 A:middle
and once you have the filter,

00:16:36.436 --> 00:16:37.976 A:middle
you can ask it for
its output image.

00:16:38.336 --> 00:16:40.246 A:middle
And that's basically
one line of code

00:16:40.246 --> 00:16:42.106 A:middle
that will apply a
filter to an image.

00:16:42.106 --> 00:16:43.916 A:middle
If we want to apply
a second filter,

00:16:43.916 --> 00:16:46.766 A:middle
it's just same idea,
slightly different.

00:16:46.766 --> 00:16:48.176 A:middle
What we're going to be
doing here is we're going

00:16:48.176 --> 00:16:49.196 A:middle
to be picking a different
filter.

00:16:49.196 --> 00:16:50.926 A:middle
We'll pick hue adjustment
in this case.

00:16:51.536 --> 00:16:53.976 A:middle
And the key difference is the
input image, in this case,

00:16:53.976 --> 00:16:55.746 A:middle
is the output image in
the previous filter.

00:16:56.616 --> 00:16:58.576 A:middle
So it's very, very
simple, two lines of code

00:16:58.966 --> 00:17:01.056 A:middle
and we've applied multiple
filters to an image.

WEBVTT
X-TIMESTAMP-MAP=MPEGTS:181083,LOCAL:00:00:00.000

00:17:01.956 --> 00:17:03.766 A:middle
The other things that's
important and great to keep

00:17:03.766 --> 00:17:05.226 A:middle
in mind is that these--

00:17:05.455 --> 00:17:07.856 A:middle
at the time you're building
up the render graph here,

00:17:08.205 --> 00:17:11.496 A:middle
the filter graph, there's no
actual work being performed.

00:17:11.685 --> 00:17:13.576 A:middle
This is all very fast and
could be done very quickly.

00:17:14.236 --> 00:17:17.205 A:middle
The actual work of a
rendering image is deferred

00:17:17.205 --> 00:17:20.876 A:middle
until we actually get a request
to render it and at that time

00:17:20.876 --> 00:17:23.066 A:middle
that we can make out
render-time optimizations

00:17:23.066 --> 00:17:24.945 A:middle
to make the best
possible performance

00:17:24.945 --> 00:17:26.766 A:middle
on the destination context.

00:17:29.436 --> 00:17:32.976 A:middle
So another thing is that you can
create your own custom filters.

00:17:33.296 --> 00:17:36.906 A:middle
I'm going to actually do some
of these on both iOS and OS X.

00:17:37.646 --> 00:17:41.516 A:middle
We have over a hundred
built-in filters and on iOS 7,

00:17:42.266 --> 00:17:44.996 A:middle
while you cannot create
your own custom kernels,

00:17:44.996 --> 00:17:48.116 A:middle
you can create your own custom
filters by building up filters

00:17:48.116 --> 00:17:49.496 A:middle
out of other built-in filters.

00:17:50.146 --> 00:17:53.106 A:middle
And this is a very
effective way to create new

00:17:53.106 --> 00:17:54.116 A:middle
and interesting effects.

00:17:54.776 --> 00:17:57.046 A:middle
And again, we've chosen some
of the new filters we've added

00:17:57.046 --> 00:18:00.266 A:middle
in iOS 7 to be particularly
useful for this goal.

WEBVTT
X-TIMESTAMP-MAP=MPEGTS:181083,LOCAL:00:00:00.000

00:18:01.736 --> 00:18:02.536 A:middle
So how does this work?

00:18:03.236 --> 00:18:07.726 A:middle
So the idea is you want to
create a CIFilter subclass

00:18:08.026 --> 00:18:11.896 A:middle
and in that filter, you want
to wrap a set of other filters.

00:18:12.396 --> 00:18:14.716 A:middle
So there're set of things
that you need to do.

00:18:14.716 --> 00:18:16.336 A:middle
One is you need to
declare properties

00:18:16.336 --> 00:18:18.296 A:middle
for your filter subclass

00:18:18.296 --> 00:18:20.116 A:middle
that declares what its
input parameters is.

00:18:20.506 --> 00:18:22.306 A:middle
For example, you might
have an input image

00:18:22.306 --> 00:18:23.846 A:middle
or other numeric parameters.

00:18:24.356 --> 00:18:26.126 A:middle
You want to override
set defaults

00:18:26.226 --> 00:18:27.756 A:middle
so that the default values are--

00:18:28.026 --> 00:18:29.556 A:middle
for your filter are
setup appropriately

00:18:29.556 --> 00:18:32.266 A:middle
if the calling code doesn't
specify anything else.

00:18:33.256 --> 00:18:35.156 A:middle
And lastly and most
importantly, you're going

00:18:35.156 --> 00:18:36.396 A:middle
to override output image.

00:18:37.136 --> 00:18:39.916 A:middle
And it's in this method that you
will return your filter graph.

00:18:40.666 --> 00:18:42.796 A:middle
And internally, Core Image
actually uses this technique

00:18:42.796 --> 00:18:44.266 A:middle
on some of its own
built-in filters.

00:18:44.836 --> 00:18:48.276 A:middle
As an example, there's a
built-in filter called CIColor

00:18:48.276 --> 00:18:50.446 A:middle
invert which inverts all
the colors in an image.

00:18:50.766 --> 00:18:51.796 A:middle
And if you think
about it, really,

00:18:51.796 --> 00:18:54.896 A:middle
that's just a special case
of a color matrix operation.

00:18:55.276 --> 00:18:58.366 A:middle
So, if you look at our
source code for color invert,

00:18:58.366 --> 00:19:01.776 A:middle
all it does in its output image
method is create an instance

WEBVTT
X-TIMESTAMP-MAP=MPEGTS:181083,LOCAL:00:00:00.000

00:19:01.776 --> 00:19:03.776 A:middle
of the filter for CIColor matrix

00:19:04.166 --> 00:19:07.306 A:middle
and passing the appropriate
parameters for the red, green,

00:19:07.306 --> 00:19:09.866 A:middle
and blue, and bias vectors.

00:19:10.196 --> 00:19:14.376 A:middle
You can also do this
kind of thing to build

00:19:14.376 --> 00:19:16.206 A:middle
up really other interesting
image effects.

00:19:16.206 --> 00:19:17.396 A:middle
For example, let's
say you wanted

00:19:17.396 --> 00:19:20.406 A:middle
to do a Sobel Edge Detector
in your application.

00:19:21.036 --> 00:19:24.066 A:middle
Well, a Sobel Edge Detector
is really just a special case

00:19:24.066 --> 00:19:25.466 A:middle
of a 3X3 convolution.

00:19:26.076 --> 00:19:28.636 A:middle
In fact, it's very simple
convolution, all it is,

00:19:28.636 --> 00:19:31.456 A:middle
is depending on whether you're
doing a horizontal Sobel

00:19:31.456 --> 00:19:34.936 A:middle
or a vertical Sobel, you're
going to have some pattern

00:19:34.936 --> 00:19:38.476 A:middle
of ones and twos and zeros
in your 3X3 convolution.

00:19:39.546 --> 00:19:42.326 A:middle
One thing to keep in mind
is, especially on iOS,

00:19:42.576 --> 00:19:46.466 A:middle
because we don't want to add a
bias term to this convolution.

00:19:47.036 --> 00:19:50.196 A:middle
And the idea here is that we
want to produce an output image

00:19:50.226 --> 00:19:54.016 A:middle
that is grey where the image
is flat and black and white

00:19:54.016 --> 00:19:55.276 A:middle
where there are edges.

00:19:56.086 --> 00:19:59.166 A:middle
And that's particularly
important, because on iOS,

00:19:59.166 --> 00:20:02.636 A:middle
our intermediate buffers are
8-bit buffers for these type

WEBVTT
X-TIMESTAMP-MAP=MPEGTS:181083,LOCAL:00:00:00.000

00:20:02.636 --> 00:20:04.996 A:middle
of operations when they-- and
they can only represent values

00:20:04.996 --> 00:20:07.066 A:middle
between black and white.

00:20:08.486 --> 00:20:10.436 A:middle
One thing to keep
in mind, however,

00:20:10.436 --> 00:20:11.896 A:middle
is that by adding a bias,

00:20:11.896 --> 00:20:13.546 A:middle
you are actually
producing an infinite image,

00:20:13.546 --> 00:20:17.126 A:middle
because outside the image where
the image is flat and clear,

00:20:17.816 --> 00:20:20.496 A:middle
you're going to have grey as the
output of this Sobel Detector.

00:20:20.496 --> 00:20:25.396 A:middle
So let me just give a little
demo of that in action

00:20:25.876 --> 00:20:28.286 A:middle
and I've actually-- in
this particular demo,

00:20:28.286 --> 00:20:32.256 A:middle
so it's a little bit more
interesting to look at on stage,

00:20:33.026 --> 00:20:35.946 A:middle
I'm recoloring the
image so it looks--

00:20:36.206 --> 00:20:39.296 A:middle
so the flat areas look black
and the edges look colorful.

00:20:39.766 --> 00:20:43.416 A:middle
So again, I've got an input
video source and I can go

00:20:43.416 --> 00:20:47.966 A:middle
and add a filter to it and I'm
going to add a custom filter

00:20:49.116 --> 00:20:53.846 A:middle
that we've implemented
called Sobel Edge Detector.

00:20:54.596 --> 00:20:56.826 A:middle
So as we can see here,
hopefully that shows

00:20:56.826 --> 00:20:57.786 A:middle
up on the display, OK.

00:20:57.786 --> 00:21:02.696 A:middle
You can see my glasses and as I
tilt my head, you can see the--

WEBVTT
X-TIMESTAMP-MAP=MPEGTS:181083,LOCAL:00:00:00.000

00:21:02.696 --> 00:21:03.916 A:middle
or you can see the
stripes in my shirt.

00:21:04.936 --> 00:21:07.426 A:middle
And in this case, the
images are being recolored

00:21:07.726 --> 00:21:09.396 A:middle
so that the flat
areas look black

00:21:09.396 --> 00:21:11.636 A:middle
and the edges look colorful.

00:21:11.636 --> 00:21:15.686 A:middle
And one thing you'll see is
that these circles at the--

00:21:15.816 --> 00:21:17.786 A:middle
above my head are actually
colorful and that's

00:21:17.786 --> 00:21:19.666 A:middle
because the Sobel Edge
Detector is working

00:21:19.666 --> 00:21:21.206 A:middle
on each color plane separately.

00:21:21.556 --> 00:21:23.666 A:middle
And there's-- if there's a
color fringe in the image,

00:21:23.666 --> 00:21:26.356 A:middle
it will show up as a colorful
edge in the Sobel Edge Detector.

00:21:26.436 --> 00:21:28.516 A:middle
If we wanted to get rid
of those colorful frames,

00:21:28.516 --> 00:21:30.236 A:middle
all we have to do is
append another filter.

00:21:30.726 --> 00:21:40.206 A:middle
We can add in color controls
and then we can desaturate that.

00:21:40.206 --> 00:21:41.276 A:middle
And now we've got [inaudible]

00:21:41.276 --> 00:21:46.166 A:middle
of monochrome edge
detector, all right.

00:21:47.716 --> 00:21:50.006 A:middle
All right, so back to slides.

00:21:50.076 --> 00:21:53.086 A:middle
And again, as I mentioned
before, the source code

00:21:53.086 --> 00:21:54.546 A:middle
for that filter and the--

00:21:54.726 --> 00:21:58.016 A:middle
is all available in the Core
Image Fun House application.

00:21:58.496 --> 00:22:00.486 A:middle
All right, so there's
another great use

WEBVTT
X-TIMESTAMP-MAP=MPEGTS:181083,LOCAL:00:00:00.000

00:22:00.486 --> 00:22:03.516 A:middle
for creating your own
CIFilter subclasses and that's

00:22:03.556 --> 00:22:06.066 A:middle
if you want to use Core
Image in combination

00:22:06.066 --> 00:22:07.286 A:middle
with the new Sprite Kit API.

00:22:08.126 --> 00:22:10.246 A:middle
So it's a great new
API, the Sprite Kit API,

00:22:10.646 --> 00:22:13.076 A:middle
and one of the things that
supports is the ability

00:22:13.076 --> 00:22:15.696 A:middle
to associate a CIFilter
with several objects

00:22:15.696 --> 00:22:18.606 A:middle
in your Sprite Kit application.

00:22:18.726 --> 00:22:22.186 A:middle
For example, you can set a
filter on an effect node,

00:22:22.186 --> 00:22:23.996 A:middle
you can set a filter
on a texture,

00:22:24.646 --> 00:22:26.626 A:middle
or you can set a
filter on a transition.

00:22:27.746 --> 00:22:29.296 A:middle
And it's a great API but one

00:22:29.296 --> 00:22:32.086 A:middle
of the caveats is it you only
can associate one filter.

00:22:32.606 --> 00:22:35.166 A:middle
So if you actually want to have
a more complicated render graph

00:22:35.286 --> 00:22:38.416 A:middle
associated with either a
transition or an object

00:22:38.416 --> 00:22:40.836 A:middle
in your Spite Kit world,

00:22:41.336 --> 00:22:43.396 A:middle
then you can create
a CIFilter subclass.

00:22:44.216 --> 00:22:47.886 A:middle
And what you need to do in
that subclass is you need

00:22:47.886 --> 00:22:50.496 A:middle
to make sure that your filter
has an input image parameter

00:22:51.276 --> 00:22:53.416 A:middle
and you need-- if you're
running a transition effect,

00:22:53.416 --> 00:22:55.276 A:middle
you want to make sure it
has an input time parameter.

00:22:56.466 --> 00:23:00.236 A:middle
And you can have other inputs
but you want to specify them

WEBVTT
X-TIMESTAMP-MAP=MPEGTS:181083,LOCAL:00:00:00.000

00:23:00.716 --> 00:23:06.796 A:middle
at setup time before you
pass it in the Sprite Kit.

00:23:07.016 --> 00:23:10.376 A:middle
So let me go on to the next
section in my presentation today

00:23:10.376 --> 00:23:11.536 A:middle
to talk about input images.

00:23:12.736 --> 00:23:16.596 A:middle
So, the input in the
filters is input images

00:23:16.596 --> 00:23:18.286 A:middle
and we have a wide
variety of different ways

00:23:18.286 --> 00:23:19.846 A:middle
of getting images
into your filters.

00:23:20.496 --> 00:23:24.526 A:middle
One of the most commonly
requested is to use images

00:23:24.526 --> 00:23:27.146 A:middle
from a file and that's very,
very easy to do, it's one line

00:23:27.236 --> 00:23:29.336 A:middle
of code, create a
CIImage from a URL.

00:23:30.816 --> 00:23:34.846 A:middle
Another common form--
source is bringing data

00:23:34.846 --> 00:23:38.096 A:middle
in from your photo
library and you can do

00:23:38.096 --> 00:23:42.166 A:middle
that by just asking the
ALAssetsLibrary class

00:23:42.216 --> 00:23:44.426 A:middle
for a default representation.

00:23:44.686 --> 00:23:45.686 A:middle
And then, once you have that,

00:23:45.686 --> 00:23:47.056 A:middle
you can ask for the
full screen image

00:23:47.056 --> 00:23:50.796 A:middle
and that will return a CGImage
and once you have a CGImage,

00:23:50.796 --> 00:23:53.846 A:middle
you can create a CIImage
from that CGImage.

00:23:54.996 --> 00:23:58.736 A:middle
Another example is bringing in
data from a live video stream

00:23:59.026 --> 00:24:02.106 A:middle
and this is the case that
we use inside the Fun

WEBVTT
X-TIMESTAMP-MAP=MPEGTS:181083,LOCAL:00:00:00.000

00:24:02.106 --> 00:24:03.136 A:middle
House application.

00:24:03.936 --> 00:24:06.406 A:middle
And in this case, you'll
get a call back method

00:24:06.406 --> 00:24:08.646 A:middle
to produce a process
of frame of video

00:24:09.176 --> 00:24:11.306 A:middle
and that will give you
a sample buffer object.

00:24:11.816 --> 00:24:13.296 A:middle
Once you have the
sample buffer object,

00:24:13.296 --> 00:24:18.446 A:middle
you can ask for
CMSampleBufferGetImageBuffer

00:24:18.516 --> 00:24:20.876 A:middle
and that will return a
CVImage buffer object.

00:24:21.276 --> 00:24:23.236 A:middle
And a CVImage buffer
object is really--

00:24:23.606 --> 00:24:26.316 A:middle
it's just a subclass of
the CVPixel buffer object

00:24:26.596 --> 00:24:28.546 A:middle
which you can use to
create a CIImage from.

00:24:29.056 --> 00:24:35.166 A:middle
At the same time I'm talking
about creating CIImages,

00:24:35.166 --> 00:24:37.316 A:middle
we should also talk a little
bit about image metadata

00:24:37.936 --> 00:24:40.366 A:middle
which is a very important
thing about images these days.

00:24:40.366 --> 00:24:43.336 A:middle
You can ask an image
for its properties

00:24:43.586 --> 00:24:45.406 A:middle
and that will return
a dictionary

00:24:45.936 --> 00:24:47.786 A:middle
of metadata associated
with that image.

00:24:48.226 --> 00:24:51.016 A:middle
And it will turn the dictionary
containing the same key value

00:24:51.016 --> 00:24:52.226 A:middle
pairs that would be present

00:24:52.226 --> 00:24:55.636 A:middle
if you would call the API
CGImageSourceCopyProperties

00:24:55.636 --> 00:24:56.176 A:middle
AtIndex.

00:24:56.526 --> 00:25:00.196 A:middle
It contains, for some image,
hundreds of properties.

WEBVTT
X-TIMESTAMP-MAP=MPEGTS:181083,LOCAL:00:00:00.000

00:25:00.646 --> 00:25:01.646 A:middle
The one that I want to call

00:25:01.646 --> 00:25:04.556 A:middle
out today is the
orientation property,

00:25:04.726 --> 00:25:06.686 A:middle
kCGImagePropertyOrientation.

00:25:07.166 --> 00:25:09.116 A:middle
And this really important
because we all know

00:25:09.116 --> 00:25:12.096 A:middle
with our cameras
today, you image--

00:25:12.096 --> 00:25:14.786 A:middle
the camera can be held in any
orientation and the images

00:25:14.786 --> 00:25:18.106 A:middle
that saved into the camera
roll has metadata associated

00:25:18.106 --> 00:25:19.686 A:middle
that says what orientation
it was in.

00:25:20.196 --> 00:25:22.906 A:middle
So, if you want to present
that image to your user

00:25:22.906 --> 00:25:25.766 A:middle
in the correct way, you need to
read the orientation property

00:25:25.966 --> 00:25:27.666 A:middle
and apply appropriate
transform to it.

00:25:28.106 --> 00:25:30.026 A:middle
The great thing is that
metadata is all set

00:25:30.026 --> 00:25:32.516 A:middle
up for you automatically if
you use the image with URL

00:25:32.516 --> 00:25:34.126 A:middle
or image with data APIs.

00:25:34.396 --> 00:25:37.026 A:middle
If you're using other methods
to instantiate an image,

00:25:37.376 --> 00:25:39.766 A:middle
you can specify the
metadata using the

00:25:39.766 --> 00:25:41.266 A:middle
kCIImageProperties option.

00:25:41.686 --> 00:25:45.196 A:middle
Another thing we've
added on both Mavericks

00:25:45.196 --> 00:25:49.306 A:middle
in iOS 7 is much more robust
support for YCbCr images.

00:25:50.096 --> 00:25:54.306 A:middle
A CIImage can be based on
a bi-planar YCC 420 data

00:25:54.796 --> 00:25:57.626 A:middle
and this is a great way to get
good performance out of video.

00:25:58.276 --> 00:26:01.806 A:middle
On OS X, you want to
use an IOSurface object

WEBVTT
X-TIMESTAMP-MAP=MPEGTS:181083,LOCAL:00:00:00.000

00:26:01.806 --> 00:26:04.176 A:middle
to represent this
data and on iOS,

00:26:04.176 --> 00:26:06.266 A:middle
you want to use a
CVPixelBuffer to represent this.

00:26:06.856 --> 00:26:09.086 A:middle
The great thing is Core Image
takes care of all the hard work

00:26:09.086 --> 00:26:12.536 A:middle
for you, it will take
this bi-planar data

00:26:12.536 --> 00:26:16.706 A:middle
and it will combine the full-res
Y channel and the subsampled

00:26:16.706 --> 00:26:18.446 A:middle
to CbCr planes into a full image

00:26:18.446 --> 00:26:22.796 A:middle
and it will also apply the
appropriate 3x4 color matrix

00:26:22.796 --> 00:26:26.636 A:middle
to convert the YCC
values into RGB values.

00:26:27.156 --> 00:26:29.026 A:middle
If you are curious about all
the math involved in this,

00:26:29.026 --> 00:26:32.206 A:middle
I highly recommend the book
by Poynton, "Digital Video

00:26:32.206 --> 00:26:33.576 A:middle
and HD Algorithms" which goes

00:26:33.576 --> 00:26:36.646 A:middle
over in great detail all the
matrix math that you need

00:26:36.646 --> 00:26:39.766 A:middle
to understand to
correctly process YCC data.

00:26:41.036 --> 00:26:43.046 A:middle
The other thing you might
want to keep in mind is

00:26:43.046 --> 00:26:46.036 A:middle
if you're working on 420
data, you might be working

00:26:46.036 --> 00:26:49.326 A:middle
on a video type workflow and
in that case, you might want

00:26:49.326 --> 00:26:53.396 A:middle
to tell on Mac OS, you might
want to talk Core Image

00:26:53.396 --> 00:26:57.716 A:middle
to use the rec709 linear working
space rather than its default

00:26:57.716 --> 00:27:01.116 A:middle
which is generic RGB and this
can prevent some clipping errors

WEBVTT
X-TIMESTAMP-MAP=MPEGTS:181083,LOCAL:00:00:00.000

00:27:01.286 --> 00:27:04.106 A:middle
to the color matrix operations.

00:27:04.536 --> 00:27:05.746 A:middle
The third section I want to talk

00:27:05.746 --> 00:27:07.446 A:middle
about today is rendering
Core Image output.

00:27:08.856 --> 00:27:11.066 A:middle
If you have an image and
you've applied a filter,

00:27:11.066 --> 00:27:13.516 A:middle
there are several ways to render
the output using Core Image.

00:27:14.076 --> 00:27:16.896 A:middle
One of the most common
is rendering an image

00:27:16.896 --> 00:27:17.836 A:middle
to your photo library.

00:27:19.206 --> 00:27:20.656 A:middle
And again this is
very easy to do.

00:27:20.656 --> 00:27:22.306 A:middle
There's one thing you
want to be aware of is

00:27:22.306 --> 00:27:24.076 A:middle
when you're saving images
to your phot library,

00:27:24.196 --> 00:27:26.016 A:middle
you could quite easily
be working

00:27:26.016 --> 00:27:30.226 A:middle
on a very high resolution
image, 5 megapixels for example

00:27:30.826 --> 00:27:33.686 A:middle
and resolutions of this
size are actually bigger

00:27:33.686 --> 00:27:35.306 A:middle
that a GPU limits
that are supported

00:27:35.306 --> 00:27:36.326 A:middle
on some of our devices.

00:27:37.056 --> 00:27:39.656 A:middle
So, in order to render
this image with Core Image,

00:27:39.656 --> 00:27:41.636 A:middle
you want to tell Core Image
to use a software renderer.

00:27:42.606 --> 00:27:47.066 A:middle
And this also has the advantage
that if you're doing a bunch

00:27:47.066 --> 00:27:48.316 A:middle
of exports in the background,

00:27:48.756 --> 00:27:51.466 A:middle
you can do this while your
app is suspended rather

00:27:51.466 --> 00:27:54.806 A:middle
than if you're-- we try
to use our GPU renderer.

00:27:55.946 --> 00:27:57.656 A:middle
And once you've created
the CIContext,

00:27:58.096 --> 00:28:02.876 A:middle
there's an assets library
method which we can use

WEBVTT
X-TIMESTAMP-MAP=MPEGTS:181083,LOCAL:00:00:00.000

00:28:02.876 --> 00:28:06.056 A:middle
to write the JPEG into
that library roll.

00:28:06.246 --> 00:28:09.766 A:middle
The key image from key API
to call is for Core Image

00:28:10.126 --> 00:28:15.126 A:middle
to create a CGImage
from a CIImage.

00:28:15.256 --> 00:28:18.956 A:middle
Another common way of rendering
an image is to render it

00:28:18.956 --> 00:28:22.336 A:middle
into a UIIimage view
using a UIImage.

00:28:22.766 --> 00:28:25.186 A:middle
And this code for this is
actually very, very simple.

00:28:26.076 --> 00:28:29.626 A:middle
All you do is you
UIImage support CIImage

00:28:29.626 --> 00:28:32.726 A:middle
so you can create a UIImage
from an output of a filter

00:28:32.726 --> 00:28:36.546 A:middle
and then you can just tell an
image view to use that UIImage.

00:28:37.056 --> 00:28:40.346 A:middle
And this very, very easy to code
but it's actually not the best

00:28:40.766 --> 00:28:41.856 A:middle
from a performance perspective

00:28:41.936 --> 00:28:43.436 A:middle
and let me talk a
little bit about that.

00:28:43.656 --> 00:28:47.056 A:middle
Internally, what UIImage is
doing is asking Core Image

00:28:47.056 --> 00:28:52.416 A:middle
to render it and turn it into a
CGImage but what happens here is

00:28:52.416 --> 00:28:54.216 A:middle
that when Core Image
is rendering,

00:28:55.436 --> 00:28:58.396 A:middle
it will upload the
image to the GPU,

00:28:58.836 --> 00:29:01.296 A:middle
it will perform the filter
effect that's desired

WEBVTT
X-TIMESTAMP-MAP=MPEGTS:181083,LOCAL:00:00:00.000

00:29:01.716 --> 00:29:04.236 A:middle
and because the image is being
read back into a CGImage,

00:29:04.236 --> 00:29:05.996 A:middle
it's being read back
into CPU memory.

00:29:07.126 --> 00:29:09.036 A:middle
And then when it comes
time to actually display it

00:29:09.036 --> 00:29:11.696 A:middle
in the UIView, it's then
going back being rendered

00:29:11.696 --> 00:29:13.696 A:middle
on the GPU using core animation.

00:29:15.126 --> 00:29:16.916 A:middle
And while this is
effective and simple,

00:29:17.276 --> 00:29:18.756 A:middle
it means that we're
making several trips

00:29:19.206 --> 00:29:21.606 A:middle
across the boundary
between CPU and the GPU

00:29:21.676 --> 00:29:24.666 A:middle
and that's not ideal, so
we'd like to avoid that.

00:29:25.206 --> 00:29:28.816 A:middle
Much better approach is to
take an image, upload it once

00:29:28.816 --> 00:29:34.296 A:middle
to the GPU and have CI do
all the rendering directly

00:29:34.296 --> 00:29:34.886 A:middle
to the display.

00:29:35.316 --> 00:29:37.816 A:middle
And that's actually quite
easy to do in your application

00:29:38.206 --> 00:29:42.186 A:middle
if you have a CAEAGLLayer
for example, you can--

00:29:42.186 --> 00:29:44.966 A:middle
at the time that you're
instantiating your object,

00:29:45.076 --> 00:29:47.686 A:middle
you want to creat a
CIContext at the same time.

00:29:48.276 --> 00:29:51.556 A:middle
We created an EAGLContext
with the--

00:29:51.586 --> 00:29:56.336 A:middle
of type OpenGL ES2 and then
we tell CI to create a context

00:29:57.056 --> 00:29:59.976 A:middle
from that EAGLContext.

WEBVTT
X-TIMESTAMP-MAP=MPEGTS:181083,LOCAL:00:00:00.000

00:30:00.126 --> 00:30:01.956 A:middle
Then when it comes time
to update the display

00:30:01.956 --> 00:30:03.286 A:middle
in your update screen method,

00:30:03.756 --> 00:30:05.116 A:middle
we're going to do a
couple things here.

00:30:05.166 --> 00:30:06.676 A:middle
We're going to ask the--

00:30:06.676 --> 00:30:10.296 A:middle
our model object to
create a CIImage to render.

00:30:10.966 --> 00:30:14.656 A:middle
We're then going to set up
the GL blend mode to be--

00:30:15.056 --> 00:30:16.556 A:middle
let's say in this
case, source over.

00:30:17.096 --> 00:30:18.766 A:middle
This is actually a little
subtle thing to change

00:30:18.766 --> 00:30:20.776 A:middle
between iOS 6 and iOS 7.

00:30:20.936 --> 00:30:24.786 A:middle
On iOS 6, we would always blend
with source over blend mode.

00:30:25.136 --> 00:30:27.656 A:middle
But there are a lot
of interesting cases

00:30:27.656 --> 00:30:29.036 A:middle
where you might want to
use a different blend mode.

00:30:29.436 --> 00:30:33.426 A:middle
So now if your app is
linked on or after iOS 7,

00:30:33.846 --> 00:30:36.246 A:middle
you have the ability to
specify your own blend mode.

00:30:36.836 --> 00:30:41.756 A:middle
And then once we set up the
blend mode, we tell Core Image

00:30:41.756 --> 00:30:44.556 A:middle
to draw the image into
the context which is based

00:30:44.556 --> 00:30:46.786 A:middle
on the EAGLContext,
and then lastly

00:30:46.786 --> 00:30:49.036 A:middle
to actually present
the images to the user,

00:30:49.256 --> 00:30:52.476 A:middle
we're going to bind the render
buffer and present that to the--

00:30:52.876 --> 00:30:53.846 A:middle
present that render buffer.

00:30:53.946 --> 00:30:57.786 A:middle
The next thing I'd like
to talk about is rendering

00:30:57.786 --> 00:31:00.116 A:middle
to a CVPixelBufferFef, and this
is another interesting thing

WEBVTT
X-TIMESTAMP-MAP=MPEGTS:181083,LOCAL:00:00:00.000

00:31:00.116 --> 00:31:03.076 A:middle
that we talked about and show
in the Fun House application

00:31:03.366 --> 00:31:06.606 A:middle
where you may want to be
applying a filter to a video

00:31:07.046 --> 00:31:08.146 A:middle
and saving that to disk.

00:31:09.116 --> 00:31:11.196 A:middle
Now, if you want to make

00:31:11.196 --> 00:31:12.476 A:middle
that a little bit more
interesting a problem,

00:31:12.476 --> 00:31:15.566 A:middle
you may also-- while you're
saving the video to disk,

00:31:15.566 --> 00:31:19.096 A:middle
you may also want to present
it to the user as a view

00:31:19.096 --> 00:31:20.696 A:middle
so they can see what's
being recorded.

00:31:21.346 --> 00:31:24.446 A:middle
So this is actually an
interesting example and I want

00:31:24.446 --> 00:31:26.766 A:middle
to talk a little bit
about that in a few--

00:31:27.246 --> 00:31:28.986 A:middle
a little bit of code
we have here.

00:31:29.876 --> 00:31:32.266 A:middle
All right, so again all
this code is available

00:31:32.266 --> 00:31:33.406 A:middle
in the Core Image Fun House.

00:31:33.906 --> 00:31:38.156 A:middle
And what we have here is a
scenario of where we want

00:31:38.156 --> 00:31:40.976 A:middle
to record video and also display
it to the user at the same time.

00:31:41.696 --> 00:31:43.956 A:middle
And in our view object when we--

00:31:44.116 --> 00:31:45.826 A:middle
when our view object
gets instantiated

00:31:45.826 --> 00:31:46.756 A:middle
when the app launches,

00:31:47.096 --> 00:31:48.986 A:middle
we're going to be
creating an EAGLContext

00:31:49.146 --> 00:31:51.236 A:middle
as I demonstrated in the slide.

00:31:51.816 --> 00:31:53.896 A:middle
And then were going to be
creating at the same time,

00:31:53.896 --> 00:31:56.016 A:middle
we'll also create a
CIContext at that same time

00:31:56.116 --> 00:31:57.286 A:middle
with that EAGLContext.

00:31:58.216 --> 00:32:00.366 A:middle
We look later on it-- when
it comes time to render,

WEBVTT
X-TIMESTAMP-MAP=MPEGTS:181083,LOCAL:00:00:00.000

00:32:00.626 --> 00:32:02.986 A:middle
we have a callback
method to capture output

00:32:03.626 --> 00:32:05.776 A:middle
and again were going to be
given a sample buffer here.

00:32:05.966 --> 00:32:08.006 A:middle
If we look down further
in this code,

00:32:08.616 --> 00:32:12.466 A:middle
we are doing some
basic rectangle math

00:32:12.466 --> 00:32:14.156 A:middle
to make sure we render
it in the correct place.

00:32:14.156 --> 00:32:17.976 A:middle
We're going to take the
source image that we get

00:32:17.976 --> 00:32:20.826 A:middle
from the sample buffer
and we're going

00:32:20.826 --> 00:32:22.016 A:middle
to apply our filters to it.

00:32:22.806 --> 00:32:25.586 A:middle
Now we have this output image
and we want to render that.

00:32:25.586 --> 00:32:27.276 A:middle
Now, there're two
scenarios here.

00:32:27.276 --> 00:32:28.606 A:middle
One is when the app is running

00:32:28.606 --> 00:32:30.476 A:middle
and it's just displaying
live preview.

00:32:30.886 --> 00:32:34.066 A:middle
In that case in this code here,
all we're doing is setting

00:32:34.066 --> 00:32:38.036 A:middle
up our blend mode and rendering
the filtered image directly

00:32:38.036 --> 00:32:41.266 A:middle
to the context of the display
with the appropriate rectangle.

00:32:41.756 --> 00:32:44.816 A:middle
In the case when
we're recording,

00:32:45.256 --> 00:32:46.406 A:middle
we want to do two things.

00:32:46.486 --> 00:32:48.826 A:middle
First of all, we
[inaudible] start

00:32:48.826 --> 00:32:50.876 A:middle
up our video writing object.

00:32:51.636 --> 00:32:56.446 A:middle
And then we're going to ask
core video for a pixel buffer

00:32:56.446 --> 00:32:59.436 A:middle
to render into out of
it's as-- out of its pool.

WEBVTT
X-TIMESTAMP-MAP=MPEGTS:181083,LOCAL:00:00:00.000

00:33:00.206 --> 00:33:04.006 A:middle
Then we will ask Core Image
to render the filtered image

00:33:04.006 --> 00:33:08.206 A:middle
into that buffer and that will
apply our filters into that.

00:33:09.106 --> 00:33:11.236 A:middle
Now that we have
that rendered buffer,

00:33:11.236 --> 00:33:13.096 A:middle
we're going to do two things
with it, one is we're going

00:33:13.096 --> 00:33:14.926 A:middle
to draw that image
to the display

00:33:15.936 --> 00:33:18.536 A:middle
with the appropriate rectangle
and then we're also going

00:33:18.536 --> 00:33:21.356 A:middle
to tell the writer
object that we want

00:33:21.356 --> 00:33:23.556 A:middle
to append this rendered buffer

00:33:24.096 --> 00:33:26.176 A:middle
with the appropriate time
stamp into the stream.

00:33:27.016 --> 00:33:29.056 A:middle
And that's pretty much all there
is to it and that's how we--

00:33:29.196 --> 00:33:32.476 A:middle
when I run the application
to Fun House if you try it

00:33:32.476 --> 00:33:34.936 A:middle
after the presentation,
you can both record

00:33:34.936 --> 00:33:39.066 A:middle
into your camera roll and
preview at the same time.

00:33:39.736 --> 00:33:43.116 A:middle
So just a few last minute
tips for best performance,

00:33:43.286 --> 00:33:44.846 A:middle
you keep in mind that Core Image

00:33:45.426 --> 00:33:49.556 A:middle
and filter objects are
autoreleased objects.

00:33:49.556 --> 00:33:51.806 A:middle
So if you're not using
ARC in your application,

00:33:51.806 --> 00:33:55.346 A:middle
you'll want to use autorelease
pools to reduce memory pressure.

00:33:56.266 --> 00:33:58.476 A:middle
Also, it's a very good idea not

00:33:58.476 --> 00:34:00.466 A:middle
to creat a CIContext
every time you render it,

WEBVTT
X-TIMESTAMP-MAP=MPEGTS:181083,LOCAL:00:00:00.000

00:34:00.516 --> 00:34:02.556 A:middle
much better to create
it once and reuse it.

00:34:03.356 --> 00:34:05.226 A:middle
And also be aware that
both Core Animation

00:34:05.226 --> 00:34:08.556 A:middle
and Core Image both make
extensive use if the GPU.

00:34:08.766 --> 00:34:11.766 A:middle
So if you want your Core
Animations to be smooth,

00:34:11.966 --> 00:34:15.306 A:middle
you want to either stagger
your Core Image operations

00:34:15.306 --> 00:34:20.136 A:middle
or use a CPU CIContext so
that they don't put pressure

00:34:20.136 --> 00:34:23.306 A:middle
on the GPU at the same time.

00:34:23.306 --> 00:34:25.806 A:middle
Another couple of other tips
and practices is to be aware

00:34:25.806 --> 00:34:29.835 A:middle
that the GPU Context on
iOS has limited dimensions

00:34:30.235 --> 00:34:32.196 A:middle
and there's an
inputMaximumImageSize

00:34:32.196 --> 00:34:34.656 A:middle
and an outputMaximumImageSize
and there're APIs

00:34:34.656 --> 00:34:36.306 A:middle
that you can call to query that.

00:34:37.636 --> 00:34:39.766 A:middle
It's always a good idea
from performance effecting

00:34:39.766 --> 00:34:41.746 A:middle
to use small an image
as possible.

00:34:41.996 --> 00:34:44.065 A:middle
The performance of Core
Image is largely dictated

00:34:44.065 --> 00:34:48.246 A:middle
by the complexity of your
filter graph and by the number

00:34:48.246 --> 00:34:50.766 A:middle
of pixels in your output image,
so there's some great APIs

00:34:50.766 --> 00:34:52.886 A:middle
that allow you to
reduce your image size.

00:34:53.216 --> 00:34:57.416 A:middle
One that we used in this example
for Fun House is when you ask

00:34:57.416 --> 00:35:01.716 A:middle
for a library from your
asset library, you can say,

WEBVTT
X-TIMESTAMP-MAP=MPEGTS:181083,LOCAL:00:00:00.000

00:35:01.716 --> 00:35:04.056 A:middle
I want a full screen image
rather than a full size image

00:35:05.016 --> 00:35:06.116 A:middle
and that will return

00:35:06.116 --> 00:35:09.096 A:middle
in appropriately sized
image for your display.

00:35:09.676 --> 00:35:15.536 A:middle
So, the last section of our talk
today is Bridging Core Image

00:35:15.536 --> 00:35:16.166 A:middle
in OpenCL.

00:35:16.706 --> 00:35:18.776 A:middle
And early on in the
presentation, I was talking

00:35:18.776 --> 00:35:20.886 A:middle
about the great performance
whence we're getting

00:35:20.886 --> 00:35:24.216 A:middle
out of Core Image on
Mavericks by using OpenCL.

00:35:24.926 --> 00:35:27.466 A:middle
And the great thing is we
get improved performance

00:35:27.466 --> 00:35:31.606 A:middle
to do advances in the OpenCL
Compiler, and also the fact

00:35:31.606 --> 00:35:33.786 A:middle
that OpenCL has less
state management.

00:35:33.786 --> 00:35:35.306 A:middle
So, we got some great
performance whence.

00:35:35.806 --> 00:35:36.696 A:middle
And the other great thing is

00:35:36.696 --> 00:35:38.436 A:middle
so there's nothing your
application needs to do

00:35:38.436 --> 00:35:40.256 A:middle
to change to accept
this to reach us,

00:35:40.406 --> 00:35:41.546 A:middle
to do this automatically.

00:35:41.616 --> 00:35:44.376 A:middle
And there's actually some great
technology behind the hood.

00:35:44.856 --> 00:35:47.326 A:middle
All of the built-in kernels
and your custom kernels

00:35:47.326 --> 00:35:49.806 A:middle
that are all written in
CI's Kernel language are all

00:35:49.806 --> 00:35:51.876 A:middle
automatically converted
into OpenCL code.

00:35:52.366 --> 00:35:53.966 A:middle
So, it's really some
great stuff that we have

00:35:53.966 --> 00:35:55.276 A:middle
to make all this work
behind the scene.

00:35:55.836 --> 00:35:59.146 A:middle
If we think about
it a little bit,

00:35:59.146 --> 00:36:01.686 A:middle
the Core Image kernel
language has some really great

WEBVTT
X-TIMESTAMP-MAP=MPEGTS:181083,LOCAL:00:00:00.000

00:36:01.686 --> 00:36:02.796 A:middle
advantages though.

00:36:03.216 --> 00:36:06.806 A:middle
With its CIKernel language,
you can write kernel once

00:36:06.806 --> 00:36:09.376 A:middle
and it'll work across
device, classes and also

00:36:09.376 --> 00:36:11.346 A:middle
across different image formats.

00:36:11.506 --> 00:36:13.406 A:middle
And it also automatically
supports great things

00:36:13.406 --> 00:36:17.166 A:middle
like tiling of large images and
concatenation of complex graphs.

00:36:17.786 --> 00:36:19.766 A:middle
However, there are some very
interesting image-processing

00:36:19.766 --> 00:36:22.176 A:middle
operations out there that
cannot be well-expressed

00:36:22.176 --> 00:36:23.956 A:middle
in Core Images kernel
language due

00:36:23.956 --> 00:36:25.206 A:middle
to the nature of the algorithm.

00:36:26.116 --> 00:36:28.066 A:middle
But some of those
algorithms can be represented

00:36:28.066 --> 00:36:29.346 A:middle
in OpenCL's language.

00:36:30.106 --> 00:36:32.396 A:middle
And the question is, how
can you bridge the best

00:36:32.396 --> 00:36:33.306 A:middle
of both to these together?

00:36:33.756 --> 00:36:36.686 A:middle
How can you bridge both Core
Image and OpenCL together

00:36:36.726 --> 00:36:40.896 A:middle
to get some really great
image-processing on Mavericks?

00:36:41.276 --> 00:36:45.086 A:middle
So, to talk about that, I'm
going to bring up Alexander

00:36:45.426 --> 00:36:49.386 A:middle
to talk about bridging
Core Image in OpenCL,

00:36:49.386 --> 00:36:52.116 A:middle
and have some great
stuff to talk about.

00:36:52.526 --> 00:36:53.126 A:middle
&gt;&gt; Thank you, David.

00:36:53.576 --> 00:36:54.946 A:middle
So, my name is Alexander
Neiman [phonetic]

00:36:54.946 --> 00:36:56.086 A:middle
and today I'm going
talk to you a little bit

00:36:56.086 --> 00:36:59.426 A:middle
about how we can use both
Core Image and OpenCL together

00:36:59.426 --> 00:37:01.006 A:middle
in order to create new
and interesting effects

WEBVTT
X-TIMESTAMP-MAP=MPEGTS:181083,LOCAL:00:00:00.000

00:37:01.006 --> 00:37:02.186 A:middle
which we wouldn't be able

00:37:02.186 --> 00:37:04.576 A:middle
to do using just Core
Image on its own.

00:37:05.786 --> 00:37:09.596 A:middle
So, we're going to start with
an image that's pretty hazy

00:37:09.596 --> 00:37:11.746 A:middle
that David took a little
bit more than a year ago

00:37:12.046 --> 00:37:15.096 A:middle
from an airship, and he
said, "This picture suck,

00:37:15.096 --> 00:37:15.936 A:middle
how can we make them better?

00:37:15.936 --> 00:37:16.786 A:middle
Can we get rid of the haze?"

00:37:17.076 --> 00:37:18.676 A:middle
And for the sake of
the demo, we did.

00:37:19.136 --> 00:37:22.836 A:middle
So, if we look closely
here, we're just going

00:37:22.836 --> 00:37:25.506 A:middle
to get a little animation of
the desired result we're going

00:37:25.506 --> 00:37:27.316 A:middle
to try to get here is
we're literally going

00:37:27.316 --> 00:37:28.756 A:middle
to peal the haze off this image.

00:37:30.346 --> 00:37:31.896 A:middle
So, how are we going to do this?

00:37:32.996 --> 00:37:35.476 A:middle
Well, the basic idea is
that haze is accumulated

00:37:35.476 --> 00:37:36.406 A:middle
as a function of distance.

00:37:36.626 --> 00:37:38.486 A:middle
And further away, you get
the more haze there is.

00:37:38.936 --> 00:37:41.466 A:middle
But if we were to look at
any part of this image,

00:37:41.466 --> 00:37:43.136 A:middle
so if we zoom in this
little section here,

00:37:44.636 --> 00:37:47.796 A:middle
there should be something that's
blocking this image which is

00:37:47.796 --> 00:37:52.136 A:middle
to say that if we were to look
at this area under the arches,

00:37:52.176 --> 00:37:54.406 A:middle
there should be either
an object that's black

00:37:54.406 --> 00:37:56.226 A:middle
or a really dark shadow.

00:37:56.666 --> 00:37:59.086 A:middle
But because of the atmospheric
haze that's accumulated,

00:37:59.956 --> 00:38:00.846 A:middle
it's no longer black.

WEBVTT
X-TIMESTAMP-MAP=MPEGTS:181083,LOCAL:00:00:00.000

00:38:00.846 --> 00:38:02.106 A:middle
It has-- it's colorful.

00:38:02.196 --> 00:38:04.406 A:middle
And what we want to do is we
want to remove that color.

00:38:04.736 --> 00:38:07.656 A:middle
So, the question is, how are we
going to find out what's black

00:38:07.846 --> 00:38:09.956 A:middle
and apply that to a greater area

00:38:10.126 --> 00:38:13.396 A:middle
so that we can eventually
get a dehazed image.

00:38:13.396 --> 00:38:16.946 A:middle
So, if we were to look at the
pixel somewhere in the middle

00:38:16.946 --> 00:38:21.116 A:middle
of this little area, and then
search for a certain area in X,

00:38:21.206 --> 00:38:25.646 A:middle
and other area in Y, we
get a search rectangle.

00:38:27.006 --> 00:38:28.576 A:middle
Now, if we look at
the search rectangle,

00:38:29.106 --> 00:38:30.126 A:middle
we can see that there is going

00:38:30.126 --> 00:38:32.376 A:middle
to be a local minimum
that we can use.

00:38:32.376 --> 00:38:34.756 A:middle
And once we know that that
should have been black,

00:38:34.986 --> 00:38:38.956 A:middle
we can apply that amount of--

00:38:39.176 --> 00:38:42.516 A:middle
we know that how much haze has
been applied because we know

00:38:42.516 --> 00:38:44.796 A:middle
that that should have been black
originally, and that amount

00:38:44.796 --> 00:38:46.786 A:middle
of haze is probably
going to be uniformed

00:38:46.786 --> 00:38:48.816 A:middle
over the entire rectangle.

00:38:50.216 --> 00:38:51.476 A:middle
So, if we look at this visually,

00:38:51.476 --> 00:38:54.376 A:middle
we're going to compute a
morphological mean operation

00:38:54.376 --> 00:38:55.566 A:middle
in the X direction first.

00:38:55.566 --> 00:38:56.856 A:middle
So, we're going to have our--

00:38:56.856 --> 00:38:59.486 A:middle
we're going to search for the
small value in the X direction,

00:38:59.486 --> 00:39:01.366 A:middle
and then we're going to
do the exact same thing

WEBVTT
X-TIMESTAMP-MAP=MPEGTS:181083,LOCAL:00:00:00.000

00:39:01.396 --> 00:39:02.256 A:middle
in the Y direction.

00:39:02.256 --> 00:39:04.936 A:middle
And then we get this
kind of blacky pattern.

00:39:05.276 --> 00:39:08.296 A:middle
We're going to blur this
result to some degree,

00:39:09.246 --> 00:39:11.156 A:middle
and now we have a really
good representation

00:39:11.156 --> 00:39:14.126 A:middle
of how much haze there is,
and we can subtract this

00:39:14.126 --> 00:39:16.326 A:middle
from our original image using
a Difference Blend Mode.

00:39:16.576 --> 00:39:19.346 A:middle
And if we do that, we get a
beautifully dehazed image.

00:39:19.896 --> 00:39:21.336 A:middle
In terms of workflow,
the way we're going

00:39:21.336 --> 00:39:23.456 A:middle
to do this is we're going to
start with our input image,

00:39:24.516 --> 00:39:28.346 A:middle
and we're going to perform a
morphological mean operation

00:39:28.636 --> 00:39:32.256 A:middle
in the X direction first where
we search for a minimum value,

00:39:32.696 --> 00:39:34.366 A:middle
and the values just
kind of come together.

00:39:35.606 --> 00:39:37.336 A:middle
The next thing we're
going to do is we're going

00:39:37.336 --> 00:39:40.306 A:middle
to perform a morphological mean
operation in the Y direction,

00:39:40.306 --> 00:39:43.176 A:middle
and these, they're
exaggerated a little bit

00:39:43.176 --> 00:39:45.396 A:middle
like your search is a little bit
larger than we would actually do

00:39:45.396 --> 00:39:47.336 A:middle
in real life to get this
effect but just for the--

00:39:47.386 --> 00:39:48.426 A:middle
so you can actually
see something

00:39:48.426 --> 00:39:50.436 A:middle
on these slides where,
exaggerated it.

00:39:50.526 --> 00:39:52.686 A:middle
Once we've got a morphological
mean operation performed,

00:39:52.936 --> 00:39:54.306 A:middle
we're then going to
blur that result,

00:39:55.056 --> 00:39:58.586 A:middle
and then we get a nice
gradient which we can then use

00:39:59.176 --> 00:40:00.856 A:middle
to subtract from
our original image,

WEBVTT
X-TIMESTAMP-MAP=MPEGTS:181083,LOCAL:00:00:00.000

00:40:00.916 --> 00:40:03.596 A:middle
and we'll get our dehazed image.

00:40:03.596 --> 00:40:06.846 A:middle
So if we take our input image
and our Gaussian blur image,

00:40:08.226 --> 00:40:11.916 A:middle
and perform Difference Blending,
we'll get our desired result.

00:40:12.286 --> 00:40:13.776 A:middle
The generation of
the input image

00:40:13.776 --> 00:40:16.336 A:middle
and the Gaussian blur can
be done using Core Image.

00:40:16.886 --> 00:40:21.436 A:middle
But we want to use OpenCL to
perform the morphological mean

00:40:21.436 --> 00:40:23.956 A:middle
in X and Y operations
because those are operations

00:40:23.956 --> 00:40:25.656 A:middle
which would be traditionally
difficult to do

00:40:26.036 --> 00:40:28.386 A:middle
in Core Image's kernel language.

00:40:29.226 --> 00:40:33.506 A:middle
The problem is that Core Image
doesn't know about clMemObject

00:40:34.276 --> 00:40:38.466 A:middle
and OpenCL doesn't
know about CIImages.

00:40:38.806 --> 00:40:39.696 A:middle
So, how are we going to do this?

00:40:40.046 --> 00:40:41.806 A:middle
Well we're going
to use IOSurface.

00:40:41.986 --> 00:40:45.046 A:middle
And in OS X Mavericks,
we've done a lot of work

00:40:45.046 --> 00:40:49.376 A:middle
to improve how we use IOSurface
and make sure that we stay

00:40:49.376 --> 00:40:50.756 A:middle
on the GPU the whole time.

00:40:51.226 --> 00:40:54.646 A:middle
And so we're going to go
through all the steps today

00:40:54.646 --> 00:40:57.656 A:middle
to show how we can do this
and get maximum performance

00:40:57.656 --> 00:40:59.376 A:middle
and combine all these
APIs together.

WEBVTT
X-TIMESTAMP-MAP=MPEGTS:181083,LOCAL:00:00:00.000

00:41:00.016 --> 00:41:03.196 A:middle
So, let's take a look at our
workflow to process this image.

00:41:04.006 --> 00:41:07.446 A:middle
So, we're going to start
by asking Core Image

00:41:07.446 --> 00:41:10.246 A:middle
to down-sample the image
because we don't need--

00:41:10.386 --> 00:41:11.816 A:middle
in order to generate a
gradient which we're going

00:41:11.816 --> 00:41:14.166 A:middle
to be then using to perform
the subtraction, we don't need

00:41:14.476 --> 00:41:15.536 A:middle
to run at full resolution.

00:41:15.536 --> 00:41:19.636 A:middle
The next thing we're going to do
is we're going to ask Core Image

00:41:19.636 --> 00:41:20.746 A:middle
to render into an IOSurface.

00:41:20.746 --> 00:41:24.656 A:middle
Traditionally speaking, most of
the time you render to a buffer

00:41:24.656 --> 00:41:27.426 A:middle
as in like writing the file
to disk or directly displaying

00:41:27.426 --> 00:41:29.076 A:middle
on screen, but you can
also render to IOSurface

00:41:29.076 --> 00:41:31.276 A:middle
which as I mentioned
something we've really improved

00:41:31.276 --> 00:41:32.446 A:middle
in Mavericks.

00:41:32.856 --> 00:41:35.026 A:middle
We're then going to use OpenCL

00:41:35.096 --> 00:41:36.976 A:middle
to compute the minimum
using some kernels

00:41:36.976 --> 00:41:38.046 A:middle
that we're going to
go over in detail.

00:41:39.746 --> 00:41:42.576 A:middle
Once we've got the output from
OpenCL, we're then going to take

00:41:42.576 --> 00:41:46.656 A:middle
that IOSurface that was tied to
the clMemObject, and we're going

00:41:46.656 --> 00:41:47.756 A:middle
to create a new CIImage.

00:41:48.496 --> 00:41:51.556 A:middle
We're going to blur that result,
perform a Difference Blending,

00:41:52.336 --> 00:41:54.296 A:middle
and then we just
render and we're done.

00:41:55.156 --> 00:41:56.366 A:middle
So, let's take a look
at all these steps

00:41:56.366 --> 00:41:57.056 A:middle
in little more detail.

00:41:58.556 --> 00:42:00.446 A:middle
So, first thing is first,
we're going to import an image

WEBVTT
X-TIMESTAMP-MAP=MPEGTS:181083,LOCAL:00:00:00.000

00:42:00.446 --> 00:42:03.476 A:middle
with the URL, so we just create,

00:42:03.476 --> 00:42:05.866 A:middle
we just call CIImage,
image with URL.

00:42:05.866 --> 00:42:08.236 A:middle
We then are going
to down-sample it

00:42:08.236 --> 00:42:09.876 A:middle
so that we have fewer
pixels to process.

00:42:09.876 --> 00:42:11.666 A:middle
Again, in order to compute this
gradient as I mentioned earlier,

00:42:11.666 --> 00:42:13.036 A:middle
we don't need the
full resolution.

00:42:14.016 --> 00:42:16.996 A:middle
And then we're going to inset
our rectangle a little bit

00:42:16.996 --> 00:42:19.636 A:middle
such that if we were to
have generated an image

00:42:19.916 --> 00:42:24.686 A:middle
that wasn't integral,
we would end up with--

00:42:24.686 --> 00:42:27.346 A:middle
on the boarder of the image, we
might end up with some pixels

00:42:27.346 --> 00:42:28.456 A:middle
that have a little
bit of alpha value.

00:42:28.456 --> 00:42:30.556 A:middle
And we don't want to make
our kernel more complicated

00:42:30.556 --> 00:42:32.276 A:middle
than it needs to be so
we're just going to get rid

00:42:32.276 --> 00:42:33.186 A:middle
of one pixel on the EDGE.

00:42:33.826 --> 00:42:38.736 A:middle
And then we're going to down--
and then we're going to crop

00:42:38.736 --> 00:42:41.306 A:middle
that image to get rid of
that one pixel of boarder.

00:42:41.566 --> 00:42:44.126 A:middle
Also, I should mention before
I forget that the sample code

00:42:44.126 --> 00:42:47.356 A:middle
for this be also
available for download

00:42:47.356 --> 00:42:49.846 A:middle
on the Session Breakout page
at some point later on today.

00:42:50.226 --> 00:42:52.826 A:middle
So, don't worry if you don't
follow everything here.

00:42:53.976 --> 00:42:55.696 A:middle
But let's get to the next
step of this process.

00:42:55.696 --> 00:42:57.636 A:middle
First thing we're going
to do is we're going

00:42:57.636 --> 00:42:58.906 A:middle
to create an IOSurface.

WEBVTT
X-TIMESTAMP-MAP=MPEGTS:181083,LOCAL:00:00:00.000

00:43:00.066 --> 00:43:02.176 A:middle
So in order to that, we're
going to specify a bunch

00:43:02.176 --> 00:43:04.506 A:middle
of properties including the,
you know, bytes per row,

00:43:04.666 --> 00:43:07.816 A:middle
bytes per element, width, height
and whatever pixel OS type,

00:43:07.876 --> 00:43:10.106 A:middle
the pixel format that we'll be
using for our input surface.

00:43:10.636 --> 00:43:13.376 A:middle
We then create an IOSurface
using IOSurface Create.

00:43:14.646 --> 00:43:16.026 A:middle
Once we've done that,
we're going to want

00:43:16.026 --> 00:43:17.956 A:middle
to create a CIContext
which again as mentioned--

00:43:17.956 --> 00:43:20.716 A:middle
as David mentioned earlier,
we're going to want to hold

00:43:20.716 --> 00:43:23.036 A:middle
on to 'cause if we perform
this effect multiple times,

00:43:23.846 --> 00:43:25.866 A:middle
it's good to hold on to
all the resources tied

00:43:25.866 --> 00:43:26.546 A:middle
with that context.

00:43:27.486 --> 00:43:30.016 A:middle
And we're going to
make sure that we use--

00:43:30.016 --> 00:43:32.576 A:middle
that we initialize our CIContext
with the OpenGL context

00:43:32.576 --> 00:43:33.796 A:middle
that we eventually
planned on rendering with.

00:43:34.326 --> 00:43:37.476 A:middle
And then we're going to
actually ask Core Image

00:43:37.476 --> 00:43:40.106 A:middle
to render our scaled image
down into the IOSurface,

00:43:40.656 --> 00:43:42.286 A:middle
and we're going to
make sure that we--

00:43:42.286 --> 00:43:45.616 A:middle
our output color space is
equal to the input color space

00:43:45.616 --> 00:43:46.416 A:middle
of our original image.

00:43:49.076 --> 00:43:51.126 A:middle
So now, let's get
into the nitty-gritty

00:43:51.126 --> 00:43:52.466 A:middle
of what we'll be
doing with OpenCL.

00:43:53.316 --> 00:43:56.326 A:middle
So, first things first,
we're going to want

00:43:56.326 --> 00:43:59.536 A:middle
to create a CL context
which is going to be able

00:43:59.536 --> 00:44:02.566 A:middle
to share all the data
from the IOSurfaces

WEBVTT
X-TIMESTAMP-MAP=MPEGTS:181083,LOCAL:00:00:00.000

00:44:02.566 --> 00:44:06.466 A:middle
that we created with OpenGL.

00:44:06.886 --> 00:44:09.336 A:middle
In order to do that, we're
going to get a shared group

00:44:09.336 --> 00:44:11.766 A:middle
for the GL context that we
plan on using and we're going

00:44:11.766 --> 00:44:15.666 A:middle
to create a context
using that shared group.

00:44:16.016 --> 00:44:17.316 A:middle
Once we've done that,
we're then going

00:44:17.376 --> 00:44:21.526 A:middle
to use a function called CL
Create Image from IOSurface

00:44:21.576 --> 00:44:25.726 A:middle
to the Apple which allows
us to take in IOSurface

00:44:25.806 --> 00:44:27.106 A:middle
and create a clMemObject.

00:44:28.426 --> 00:44:30.526 A:middle
Now, this is going to
correspond to our input image

00:44:31.036 --> 00:44:33.866 A:middle
which was the result from what
we generated that we asked CI

00:44:33.896 --> 00:44:36.546 A:middle
to render in initially,
the down-sampled image.

00:44:36.886 --> 00:44:39.646 A:middle
But we also need an
image to write out to,

00:44:40.156 --> 00:44:42.796 A:middle
and our algorithm is going
to be a 2-pass approach,

00:44:42.796 --> 00:44:43.706 A:middle
a separable approach,

00:44:43.706 --> 00:44:45.596 A:middle
we're going to perform
our morphological mean

00:44:45.596 --> 00:44:48.116 A:middle
in the X direction and
morphological mean Y direction.

00:44:48.166 --> 00:44:48.876 A:middle
So, for the first pass,

00:44:48.876 --> 00:44:51.106 A:middle
we're going to create
an intermediate image

00:44:51.106 --> 00:44:53.606 A:middle
which is the output of the first
kernel which will then be using

00:44:53.606 --> 00:44:55.106 A:middle
as the input for
the second kernel.

00:44:55.456 --> 00:44:56.986 A:middle
So, we've got our
intermediate image here.

00:44:56.986 --> 00:45:00.556 A:middle
And then we're going to need
one more IOSurface based

WEBVTT
X-TIMESTAMP-MAP=MPEGTS:181083,LOCAL:00:00:00.000

00:45:00.946 --> 00:45:03.526 A:middle
ClMemObjectfor our final output
which is what we're going

00:45:03.526 --> 00:45:05.826 A:middle
to hand back to Core Image
to do the final rendering.

00:45:06.886 --> 00:45:08.266 A:middle
Let's take a little
bit, let's take a look

00:45:08.266 --> 00:45:09.746 A:middle
at like conceptually
what we want to do.

00:45:09.746 --> 00:45:14.136 A:middle
So, we've got zoomed in area
of an image, and we're going

00:45:14.136 --> 00:45:16.436 A:middle
to take a look at how the
search is going to happen,

00:45:16.436 --> 00:45:18.066 A:middle
and then eventually, we're
going to go over each line

00:45:18.066 --> 00:45:19.866 A:middle
of code that's involved
in doing this.

00:45:20.856 --> 00:45:24.136 A:middle
So, basically we're looking for
a minimum value, and we're going

00:45:24.136 --> 00:45:28.076 A:middle
to initialize it to a very large
value which is to say 1,1,1,1,

00:45:28.256 --> 00:45:29.736 A:middle
so all white and opaque.

00:45:31.086 --> 00:45:32.486 A:middle
And we're going to look
for a minimum value,

00:45:32.486 --> 00:45:35.826 A:middle
and we're basically just going
to start searching from our left

00:45:35.826 --> 00:45:39.016 A:middle
and go to our right, and we're
going to keep updating the value

00:45:39.346 --> 00:45:42.116 A:middle
of that mean V until we
get the lowest value.

00:45:42.836 --> 00:45:46.096 A:middle
And right now, we're looking
at how this would work

00:45:46.266 --> 00:45:48.256 A:middle
if we were operating
in the X direction,

00:45:48.796 --> 00:45:51.076 A:middle
and eventually we would
do the exact same thing

00:45:51.676 --> 00:45:53.746 A:middle
in the Y direction, and that's
going to keep animating,

00:45:53.746 --> 00:45:54.836 A:middle
and I'm going to start
talking a little bit

00:45:54.836 --> 00:45:57.226 A:middle
about the source code.

00:45:57.356 --> 00:46:00.846 A:middle
So, that's the entire
source code for the filter

WEBVTT
X-TIMESTAMP-MAP=MPEGTS:181083,LOCAL:00:00:00.000

00:46:00.846 --> 00:46:01.966 A:middle
that we're going
to want to create,

00:46:01.966 --> 00:46:03.666 A:middle
it does the morphological
mean operation.

00:46:04.456 --> 00:46:05.546 A:middle
So first things first,

00:46:05.546 --> 00:46:07.056 A:middle
we're going to have three
parameters to this function.

00:46:07.056 --> 00:46:09.236 A:middle
The first parameter is
going to be our input image,

00:46:10.136 --> 00:46:11.886 A:middle
second parameter is going
to be our output image,

00:46:13.226 --> 00:46:14.146 A:middle
and the third parameter is going

00:46:14.146 --> 00:46:15.676 A:middle
to tell us how far
we need to search.

00:46:16.236 --> 00:46:20.056 A:middle
We're then going to create
a sampler, and we're going

00:46:20.056 --> 00:46:24.236 A:middle
to use unnormalized
coordinates, and clamp to EDGE

00:46:24.266 --> 00:46:26.096 A:middle
because the way this
algorithm is designed,

00:46:26.096 --> 00:46:28.176 A:middle
we will eventually search
outside of the bounds

00:46:28.176 --> 00:46:29.516 A:middle
of the image, and
we want to make sure

00:46:29.516 --> 00:46:32.256 A:middle
that we don't read black,
but that we reach the value

00:46:32.256 --> 00:46:33.566 A:middle
of the pixel on the
EDGE of the image

00:46:33.566 --> 00:46:35.416 A:middle
such that are we
don't bleed in black

00:46:35.416 --> 00:46:37.646 A:middle
and then get an incorrect
results.

00:46:38.016 --> 00:46:39.926 A:middle
The next thing we're going to
do is we're going to ask CL

00:46:39.926 --> 00:46:43.176 A:middle
for the global ID in zero
and one, and that's going

00:46:43.176 --> 00:46:45.166 A:middle
to tell us basically
effectively where we want

00:46:45.166 --> 00:46:49.476 A:middle
to write our result out to and
we're also going to use this

00:46:49.506 --> 00:46:51.796 A:middle
to determine where we should be
reading from in our input image.

00:46:52.366 --> 00:46:58.276 A:middle
So, the next thing we do is we
initialize our minimum value

00:46:58.276 --> 00:47:02.986 A:middle
to opaque white, and then
we perform our For loop

WEBVTT
X-TIMESTAMP-MAP=MPEGTS:181083,LOCAL:00:00:00.000

00:47:02.986 --> 00:47:04.466 A:middle
which searches in this
case from left to right.

00:47:05.446 --> 00:47:07.526 A:middle
So, if we're going
to compute our new--

00:47:08.036 --> 00:47:10.496 A:middle
the new location where
we're reading from for,

00:47:10.496 --> 00:47:11.756 A:middle
and we're going to do
this for, you know,

00:47:12.116 --> 00:47:15.526 A:middle
span [phonetic] times
2, and so we are going

00:47:15.596 --> 00:47:18.706 A:middle
to offsite all these,
the location by 0.5.5

00:47:18.706 --> 00:47:20.146 A:middle
such that we're reading from
the center of the pixel.

00:47:20.446 --> 00:47:22.286 A:middle
And we're also going to
offsite the X location

00:47:22.576 --> 00:47:23.586 A:middle
by the value of I.

00:47:23.616 --> 00:47:27.316 A:middle
And if we do that and then read
from the image of that location,

00:47:28.146 --> 00:47:30.366 A:middle
we'll get a new value,
and we can compare

00:47:30.366 --> 00:47:31.956 A:middle
that with our current minimum.

00:47:32.196 --> 00:47:34.676 A:middle
And we just keep updating
that, and when we're done,

00:47:35.156 --> 00:47:39.566 A:middle
we write that value out to our
output image, and we're done.

00:47:39.566 --> 00:47:40.956 A:middle
And this is going to
get ran on every kernel

00:47:40.996 --> 00:47:42.636 A:middle
in a very similar
fashion that you would do

00:47:42.636 --> 00:47:45.046 A:middle
if you're writing a CIKernel.

00:47:45.426 --> 00:47:48.646 A:middle
And although this may look like
a relatively naive approach due

00:47:48.646 --> 00:47:55.466 A:middle
to texture caching on GPUs, this
is about its optimal as it gets.

00:47:55.556 --> 00:47:58.066 A:middle
So, you can actually perform
this at really high speed.

00:47:58.876 --> 00:48:00.886 A:middle
And we've tried a bunch of
other approaches, and this ended

WEBVTT
X-TIMESTAMP-MAP=MPEGTS:181083,LOCAL:00:00:00.000

00:48:00.886 --> 00:48:02.276 A:middle
up being as fast it gets.

00:48:02.756 --> 00:48:03.816 A:middle
And if you were going
to do this,

00:48:04.206 --> 00:48:06.296 A:middle
you would also create a kernel
that was very similar to this

00:48:06.626 --> 00:48:08.836 A:middle
for the Y direction, and
all you need to do is

00:48:08.836 --> 00:48:10.996 A:middle
to change the relocation.

00:48:10.996 --> 00:48:14.006 A:middle
Instead of incrementing I for
X, you would increment I for Y,

00:48:14.796 --> 00:48:17.166 A:middle
and the rest would
remain the same.

00:48:17.276 --> 00:48:19.146 A:middle
So let's take a look
at what we need to do

00:48:19.486 --> 00:48:22.296 A:middle
to actually run the CIKernels,
I'm sorry, CL kernels.

00:48:23.396 --> 00:48:25.266 A:middle
First things first, we're going
to give it a character string

00:48:25.266 --> 00:48:26.646 A:middle
with our code that
we looked at earlier.

00:48:27.216 --> 00:48:32.096 A:middle
We're then going to create
a CL program from that code

00:48:33.006 --> 00:48:34.866 A:middle
with the context that we
have-- we created earlier.

00:48:35.966 --> 00:48:37.256 A:middle
We're then going to
build that program

00:48:37.396 --> 00:48:38.716 A:middle
for some number of devices.

00:48:39.286 --> 00:48:43.176 A:middle
And then we're going to
create 2 kernels by looking

00:48:43.176 --> 00:48:45.546 A:middle
into that program
and asking for--

00:48:45.546 --> 00:48:47.936 A:middle
to look up the morphological
mean X

00:48:47.936 --> 00:48:52.406 A:middle
and morphological
mean Y kernels.

00:48:52.836 --> 00:48:55.686 A:middle
Once we have that, we're
pretty much ready to go.

00:48:55.686 --> 00:48:58.286 A:middle
All we need to do now is
set up some parameters,

00:48:58.286 --> 00:49:00.916 A:middle
and ask OpenCL to run.

WEBVTT
X-TIMESTAMP-MAP=MPEGTS:181083,LOCAL:00:00:00.000

00:49:01.416 --> 00:49:02.656 A:middle
So, as we saw earlier,

00:49:02.656 --> 00:49:06.346 A:middle
the CL kernel took 3
parameters to the function.

00:49:06.796 --> 00:49:08.846 A:middle
The first parameter
is the input image.

00:49:09.576 --> 00:49:11.356 A:middle
The second parameter
is our output image.

00:49:11.356 --> 00:49:13.956 A:middle
In this case, when we're doing
the morphological mean operation

00:49:13.956 --> 00:49:16.576 A:middle
in the X direction, it's going
to be the intermediate image.

00:49:17.156 --> 00:49:18.966 A:middle
And our third parameter
is going to be the value

00:49:18.966 --> 00:49:20.486 A:middle
of how far we want to
search in the X direction.

00:49:22.116 --> 00:49:24.416 A:middle
Once we've done that, all
we need to do is ask OpenCL

00:49:24.456 --> 00:49:26.846 A:middle
to enqueue that kernel and run.

00:49:26.926 --> 00:49:30.016 A:middle
And so here, we're gong to
say, run the minimum X kernel,

00:49:30.016 --> 00:49:31.876 A:middle
and we're going to give
it some workgroup sizes,

00:49:32.206 --> 00:49:35.826 A:middle
and the map for figuring out how
the optimal workgroup size is

00:49:35.826 --> 00:49:40.586 A:middle
on the source code that will
be available later on today.

00:49:40.746 --> 00:49:42.256 A:middle
So once we've done our
pass in the X direction,

00:49:42.256 --> 00:49:43.526 A:middle
we're going to do
the exact same thing.

00:49:44.036 --> 00:49:45.836 A:middle
But instead of searching in
X, we're going to search in Y

00:49:46.916 --> 00:49:48.996 A:middle
and we're going to run that,
and we just need to set our--

00:49:48.996 --> 00:49:51.216 A:middle
our input image is going to
be the intermediate image

00:49:51.216 --> 00:49:53.596 A:middle
and the output image is going to
be the output image and we need

00:49:53.596 --> 00:49:57.666 A:middle
to get a new span Y, we call
clEnqueueNDRange once again

00:49:57.816 --> 00:49:59.946 A:middle
with the minimum Y
kernel and we're done.

WEBVTT
X-TIMESTAMP-MAP=MPEGTS:181083,LOCAL:00:00:00.000

00:50:00.996 --> 00:50:03.686 A:middle
The last thing we need to do
before we hand this off back

00:50:03.736 --> 00:50:06.836 A:middle
to Core Image is we
need to call clFlush,

00:50:06.836 --> 00:50:09.116 A:middle
and the reason we do this
is because we want to make

00:50:09.116 --> 00:50:11.506 A:middle
that all the work from OpenCL
has been submitted to the GPU

00:50:11.506 --> 00:50:14.166 A:middle
with no additional work
get submitted such that

00:50:14.166 --> 00:50:15.816 A:middle
when we start using
the IOSurface inside

00:50:15.816 --> 00:50:17.616 A:middle
of Core Image, the
data is valid.

00:50:18.846 --> 00:50:20.036 A:middle
And so this is a
really important step,

00:50:20.036 --> 00:50:21.656 A:middle
otherwise you're going
to see image corruption.

00:50:22.826 --> 00:50:25.556 A:middle
And that's all we need
to do with OpenCL.

00:50:26.996 --> 00:50:28.346 A:middle
The next thing we're
going to do is

00:50:28.346 --> 00:50:31.086 A:middle
when we've got our input
image from OpenCL that was--

00:50:31.276 --> 00:50:33.316 A:middle
that corresponds
to an IOSurface,

00:50:33.896 --> 00:50:38.026 A:middle
we then create a new
CIImage from that IOSurface,

00:50:38.446 --> 00:50:41.136 A:middle
and we specify the color
space which is identical

00:50:41.136 --> 00:50:45.406 A:middle
to the color space that we used
at the very beginning to ask CI

00:50:45.506 --> 00:50:47.946 A:middle
to render that down-sampled
image.

00:50:48.376 --> 00:50:49.296 A:middle
So, we're almost done.

00:50:50.886 --> 00:50:52.036 A:middle
The next thing we're
going to do is we're going

00:50:52.036 --> 00:50:54.896 A:middle
to blur the image, and in
order to blur the image,

00:50:54.896 --> 00:50:56.786 A:middle
the first thing we're going to
do is we're going to perform

00:50:56.786 --> 00:50:58.336 A:middle
and to find clamp which is going

00:50:58.336 --> 00:51:01.606 A:middle
to basically give us a very
similar effect to what we did

WEBVTT
X-TIMESTAMP-MAP=MPEGTS:181083,LOCAL:00:00:00.000

00:51:01.606 --> 00:51:03.996 A:middle
when we asked for clamp to
edge 'cause we don't want

00:51:03.996 --> 00:51:06.526 A:middle
to be reading in black pixels
when we perform our blur.

00:51:06.636 --> 00:51:09.986 A:middle
So, we're going to do and to
find clamp, we're then going

00:51:09.986 --> 00:51:14.156 A:middle
to call CI Gaussian blur,
specify a radius and ask

00:51:14.406 --> 00:51:18.466 A:middle
for the output image, and then
we're going to crop that back

00:51:18.466 --> 00:51:21.356 A:middle
to the original size of
what the scaled image was.

00:51:21.486 --> 00:51:24.066 A:middle
So now we have the blurred
image that we were looking

00:51:24.066 --> 00:51:26.846 A:middle
for which we can then use to
the final Difference Blending.

00:51:27.496 --> 00:51:31.216 A:middle
So, in order to do the
Difference Blending,

00:51:31.666 --> 00:51:32.546 A:middle
it's very simple.

00:51:32.546 --> 00:51:35.146 A:middle
We just create a filter called
CI Difference Blend Mode.

00:51:36.256 --> 00:51:38.396 A:middle
We set the input image to
our original input image

00:51:38.396 --> 00:51:42.096 A:middle
which was scaled
down in this case.

00:51:42.296 --> 00:51:45.226 A:middle
We use the blurred image that we
just created from the IOSurface

00:51:45.286 --> 00:51:47.236 A:middle
as our background
image, and in order

00:51:47.236 --> 00:51:50.496 A:middle
to generate the final image,
we just call value for key

00:51:50.766 --> 00:51:51.796 A:middle
and ask for the output image.

00:51:52.276 --> 00:51:56.506 A:middle
Once we've done that,
the next thing we need

00:51:56.506 --> 00:52:00.646 A:middle
to do is call context
or CIContext draw image,

WEBVTT
X-TIMESTAMP-MAP=MPEGTS:181083,LOCAL:00:00:00.000

00:52:00.646 --> 00:52:04.346 A:middle
our final image at a certain
location and give it the balance

00:52:04.346 --> 00:52:05.986 A:middle
of what we would like
to render which is then

00:52:05.986 --> 00:52:08.406 A:middle
in this case be equal
to final image extent.

00:52:08.836 --> 00:52:11.016 A:middle
Now, this can get
kind of complicated

00:52:11.016 --> 00:52:15.286 A:middle
when you start generating a
lot of effects, and in Xcode 5

00:52:15.416 --> 00:52:19.956 A:middle
and on Mavericks, you can now
hover over things in Xcode

00:52:19.956 --> 00:52:20.876 A:middle
and get a description.

00:52:21.166 --> 00:52:23.926 A:middle
And so in this case, you can see
as I'm hovering over an image,

00:52:23.926 --> 00:52:28.926 A:middle
I can see the IO-- the CIImage
is based on an IOSurface

00:52:29.136 --> 00:52:30.416 A:middle
and that it's got
a certain size.

00:52:31.776 --> 00:52:37.466 A:middle
But we've also added something
now on OS X Mavericks,

00:52:37.716 --> 00:52:40.076 A:middle
such that if you were to
click on the Quick Looks,

00:52:40.466 --> 00:52:42.846 A:middle
you'll actually get a preview
of what your CIImage looks like,

00:52:43.256 --> 00:52:45.376 A:middle
and we're hoping to
have this for iOS 7

00:52:45.376 --> 00:52:46.286 A:middle
as well in the near future.

00:52:46.756 --> 00:52:49.986 A:middle
So, this can really help when
you're debugging your apps.

00:52:51.076 --> 00:52:54.376 A:middle
So, let's take a look
at the effect once more

00:52:54.376 --> 00:52:57.186 A:middle
in action 'cause it was
a lot of blood and sweat.

00:52:57.186 --> 00:52:59.146 A:middle
So, it's worth one
more animation I think.

WEBVTT
X-TIMESTAMP-MAP=MPEGTS:181083,LOCAL:00:00:00.000

00:53:00.736 --> 00:53:02.246 A:middle
And in the meanwhile,
we're going to talk

00:53:02.246 --> 00:53:03.846 A:middle
about a few little caveats.

00:53:03.846 --> 00:53:05.266 A:middle
One thing worth noting is

00:53:05.676 --> 00:53:07.186 A:middle
that this algorithm
performs really well

00:53:07.186 --> 00:53:08.916 A:middle
in removing atmospheric
haze to the extent

00:53:08.916 --> 00:53:12.086 A:middle
where if you were actually
to run this on an image

00:53:12.086 --> 00:53:14.516 A:middle
that had a lot of sky and
didn't have any dark data

00:53:14.516 --> 00:53:16.326 A:middle
or anything black, no
shadows, no nothing.

00:53:16.746 --> 00:53:18.546 A:middle
It would actually
get rid of the sky.

00:53:19.126 --> 00:53:22.696 A:middle
So, it would look black and
that's not terribly interesting.

00:53:22.696 --> 00:53:23.716 A:middle
So, you don't want

00:53:23.716 --> 00:53:25.496 A:middle
to necessarily use
this one wholesale

00:53:25.966 --> 00:53:28.216 A:middle
but is a fair amount
literature out there

00:53:28.216 --> 00:53:29.926 A:middle
about how this is implemented

00:53:29.926 --> 00:53:31.176 A:middle
and ours is actually
pretty quick.

00:53:31.246 --> 00:53:33.106 A:middle
You can get really
good frame rates

00:53:33.106 --> 00:53:34.476 A:middle
and were quite pleased
with the results.

00:53:34.646 --> 00:53:36.216 A:middle
The other thing is
you can also--

00:53:36.676 --> 00:53:37.776 A:middle
because this is a function,

00:53:37.856 --> 00:53:40.876 A:middle
atmosphere case basically
accumulates exponentially,

00:53:41.106 --> 00:53:43.216 A:middle
if you take the logarithm
of those values,

00:53:44.506 --> 00:53:46.906 A:middle
you get effectively what
corresponds to a depth map.

00:53:47.286 --> 00:53:48.786 A:middle
So once you have the depth map,

00:53:48.786 --> 00:53:50.596 A:middle
you could do really
interesting effects

00:53:51.116 --> 00:53:53.466 A:middle
such as refocusing an image
afterwards which is the kind

00:53:53.466 --> 00:53:56.546 A:middle
of thing where you can do like
a fake tilt shift effects,

00:53:56.546 --> 00:53:59.826 A:middle
et cetera, and we talked about
that in WWDC a few years ago

00:53:59.826 --> 00:54:01.166 A:middle
about how you could do that
with Core Image as well.

WEBVTT
X-TIMESTAMP-MAP=MPEGTS:181083,LOCAL:00:00:00.000

00:54:02.666 --> 00:54:06.156 A:middle
So, some additional information,
Allan Schaffer is our graphics

00:54:06.156 --> 00:54:07.956 A:middle
and game technology's
evangelist,

00:54:07.956 --> 00:54:09.636 A:middle
and you could reach him
at aschaffer@apple.com.

00:54:10.166 --> 00:54:12.446 A:middle
There's documentation
at developer.apple.com,

00:54:12.446 --> 00:54:15.756 A:middle
and then of course you can
always go to devforums.apple.com

00:54:15.826 --> 00:54:19.786 A:middle
to talk to other developers
and get in touch with us

00:54:19.936 --> 00:54:20.886 A:middle
if you have any questions.

00:54:21.296 --> 00:54:24.026 A:middle
Related sessions and labs, there
are few additional sessions

00:54:24.026 --> 00:54:26.356 A:middle
which you may also want to
go back and look at later

00:54:26.356 --> 00:54:28.356 A:middle
if you have additional--

00:54:28.506 --> 00:54:29.656 A:middle
if you're curious
about the technologies

00:54:29.656 --> 00:54:31.986 A:middle
that we talked a little
bit about-- earlier today.

00:54:32.766 --> 00:54:35.406 A:middle
And on that note, I would like
to thank you all once again

00:54:35.406 --> 00:54:38.386 A:middle
for coming, and I hope you
enjoy the rest of WWDC.

00:54:38.636 --> 00:54:38.916 A:middle
Thank you.

00:54:39.576 --> 00:54:42.470 A:middle
[ Applause ]

