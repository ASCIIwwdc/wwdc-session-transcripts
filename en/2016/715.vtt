WEBVTT

00:00:06.516 --> 00:00:18.500 A:middle
[ Music ]

00:00:23.716 --> 00:00:26.626 A:middle
&gt;&gt; Good afternoon, and welcome
to the Accelerate Session.

00:00:27.456 --> 00:00:31.226 A:middle
My name is Eric Bainville,
and I'm with the Core OS,

00:00:31.226 --> 00:00:32.336 A:middle
Vector and Numerics group.

00:00:34.246 --> 00:00:38.116 A:middle
Our group provides
performance libraries for CPU.

00:00:38.986 --> 00:00:42.646 A:middle
Performance libraries means
we usually are [inaudible] can

00:00:42.646 --> 00:00:46.366 A:middle
provide very compute intense
functions like matrix products,

00:00:46.686 --> 00:00:48.196 A:middle
[inaudible] transform,
these kind of things.

00:00:49.586 --> 00:00:51.616 A:middle
Most, most of these
functions are inside the

00:00:51.616 --> 00:00:52.656 A:middle
Accelerate framework.

00:00:52.896 --> 00:00:54.536 A:middle
We find for example, image,

00:00:54.536 --> 00:00:57.006 A:middle
which is an image
processing library.

00:00:57.766 --> 00:00:59.976 A:middle
It will transform between types

00:00:59.976 --> 00:01:02.916 A:middle
and also do geometric
transformations on images.

00:00:59.976 --> 00:01:02.916 A:middle
and also do geometric
transformations on images.

00:01:03.836 --> 00:01:06.706 A:middle
Next to vImage you will
find vDSP, which is more

00:01:06.706 --> 00:01:08.856 A:middle
for signal processing,
[inaudible] transforms,

00:01:09.526 --> 00:01:11.386 A:middle
and other signal
processing functions.

00:01:13.426 --> 00:01:18.736 A:middle
Then we have the BLAS, which
is linear algebra library.

00:01:18.736 --> 00:01:20.076 A:middle
It's very old.

00:01:20.146 --> 00:01:22.286 A:middle
It comes straight from the 70's.

00:01:22.286 --> 00:01:24.266 A:middle
Few years ago we added
Sparse, SparseBLAS,

00:01:24.266 --> 00:01:27.026 A:middle
SparseBLAS computation on
the vector and matrices.

00:01:27.616 --> 00:01:30.376 A:middle
We also provided LAPACK
in the LinearAlgebra,

00:01:30.516 --> 00:01:32.796 A:middle
which is higher level
linear algebra library.

00:01:34.196 --> 00:01:39.166 A:middle
Outside of Accelerate, we also
have a few libraries, like simd,

00:01:39.166 --> 00:01:42.766 A:middle
which is a set of headers
providing you direct access

00:01:42.796 --> 00:01:45.366 A:middle
to vector instructions
and vector types,

00:01:45.366 --> 00:01:47.376 A:middle
and they are not
directed to vector units

00:01:47.376 --> 00:01:50.346 A:middle
in the CPU's [inaudible]
without writing [inaudible]

00:01:50.556 --> 00:01:53.276 A:middle
or assembly code.

00:01:54.356 --> 00:01:56.726 A:middle
We also have compression,
which we introduced last year

00:01:56.866 --> 00:01:57.946 A:middle
for lossless compression,

00:01:58.876 --> 00:02:01.776 A:middle
and everything we
provide is optimized

00:01:58.876 --> 00:02:01.776 A:middle
and everything we
provide is optimized

00:02:01.776 --> 00:02:03.226 A:middle
for all the CPU's we support.

00:02:03.706 --> 00:02:06.296 A:middle
So when you get a new phone,
we optimize a code for it

00:02:06.916 --> 00:02:08.636 A:middle
so you don't have to do it.

00:02:09.045 --> 00:02:11.996 A:middle
Alright. Today, first, we start

00:02:11.996 --> 00:02:13.826 A:middle
with a quick refresh
on compression.

00:02:14.866 --> 00:02:18.476 A:middle
Then we'll introduce two new
libraries inside Accelerate;

00:02:19.106 --> 00:02:22.266 A:middle
BNNS, which is a set of
low level compute functions

00:02:22.266 --> 00:02:26.856 A:middle
for neural networks;
and the Quadrature,

00:02:27.466 --> 00:02:29.686 A:middle
which is a small
library dedicated

00:02:29.686 --> 00:02:31.736 A:middle
to numerical integration
of functions;

00:02:32.146 --> 00:02:34.166 A:middle
and then my colleague,
Steve, will come on stage

00:02:34.286 --> 00:02:36.356 A:middle
and introduce new
additions to simd.

00:02:37.756 --> 00:02:39.366 A:middle
Well, first, before we start,

00:02:39.566 --> 00:02:42.036 A:middle
let me tell you quickly
how to use Accelerate.

00:02:42.946 --> 00:02:46.866 A:middle
So depending on your language of
choice, you will have to import

00:02:46.866 --> 00:02:49.746 A:middle
or include the, the
declarations of the functions,

00:02:50.546 --> 00:02:53.786 A:middle
and then you need to link
with the Accelerate framework.

00:02:54.066 --> 00:02:57.656 A:middle
So from Xcode in your project
settings you will navigate

00:02:57.826 --> 00:03:01.986 A:middle
on your target, and then you
click on the, on build phases,

00:02:57.826 --> 00:03:01.986 A:middle
on your target, and then you
click on the, on build phases,

00:03:02.876 --> 00:03:04.056 A:middle
and as a window appears,

00:03:04.326 --> 00:03:06.826 A:middle
you click on link
binary with libraries.

00:03:06.826 --> 00:03:07.456 A:middle
You open that.

00:03:08.646 --> 00:03:10.856 A:middle
Alright. And those
are the ones appear

00:03:10.886 --> 00:03:14.566 A:middle
with the little plus here,
and you will get a list

00:03:14.566 --> 00:03:17.016 A:middle
of all the libraries and
frameworks you can link with,

00:03:17.016 --> 00:03:18.386 A:middle
and Accelerate is the first one.

00:03:20.516 --> 00:03:24.546 A:middle
[ Applause ]

00:03:25.046 --> 00:03:27.466 A:middle
Yes. Alright.

00:03:27.496 --> 00:03:28.846 A:middle
Well, if it's not the first one,

00:03:28.846 --> 00:03:30.596 A:middle
there's a search
bar on top of it.

00:03:30.596 --> 00:03:32.856 A:middle
So if you look for compression,
you type compression,

00:03:32.856 --> 00:03:33.836 A:middle
I knew we'd get compression.

00:03:34.516 --> 00:03:36.516 A:middle
Alright. So that's, now
you're using Accelerate.

00:03:36.866 --> 00:03:41.266 A:middle
OK. Yes. Then compression,
remember last year

00:03:41.436 --> 00:03:45.726 A:middle
when Sebastian introduced the
LZFSE's the platform state

00:03:45.726 --> 00:03:48.306 A:middle
of the union where we still
don't know who these guys are,

00:03:48.826 --> 00:03:53.396 A:middle
but today we're announcing
that LZFSE is open source.

00:03:53.936 --> 00:03:55.396 A:middle
So you will find
it on the github,

00:03:55.836 --> 00:03:58.026 A:middle
and we publish it
under BSD license.

00:03:59.206 --> 00:04:02.976 A:middle
Just let me remind you why
you will want to use that.

00:03:59.206 --> 00:04:02.976 A:middle
Just let me remind you why
you will want to use that.

00:04:03.726 --> 00:04:07.006 A:middle
So these are the platform
comparison between LZFSE

00:04:07.006 --> 00:04:09.446 A:middle
and zlib compared
to the same options.

00:04:10.026 --> 00:04:14.416 A:middle
So we are 1.4x faster for encode
and 2.6x faster for decode.

00:04:14.866 --> 00:04:17.906 A:middle
Alright. That was
our compression.

00:04:18.315 --> 00:04:19.805 A:middle
Now let's jump to BNNS.

00:04:20.286 --> 00:04:21.896 A:middle
Basic neural networks
subroutines.

00:04:22.886 --> 00:04:24.636 A:middle
So the name looks
really like BLAS.

00:04:24.676 --> 00:04:26.906 A:middle
BLAS means basic linear
algebra subroutines.

00:04:27.196 --> 00:04:29.506 A:middle
As I said, it comes
straight from the 70's,

00:04:30.726 --> 00:04:33.866 A:middle
and BNNS provides a, a set

00:04:33.866 --> 00:04:35.946 A:middle
of very low-level
computation routines.

00:04:36.106 --> 00:04:37.636 A:middle
So I will [inaudible] later.

00:04:37.726 --> 00:04:42.186 A:middle
We do only low level
computations

00:04:42.186 --> 00:04:44.726 A:middle
like matrix products but
dedicated to neural networks.

00:04:46.046 --> 00:04:48.626 A:middle
Before I enter into
details of what it does

00:04:49.106 --> 00:04:50.346 A:middle
and what the API is,

00:04:50.346 --> 00:04:54.606 A:middle
let me remind you how
neural network works.

00:04:55.366 --> 00:04:58.746 A:middle
Let's say we have this
network, which is trained

00:04:58.746 --> 00:05:00.756 A:middle
to recognize animals, OK.

00:04:58.746 --> 00:05:00.756 A:middle
to recognize animals, OK.

00:05:00.756 --> 00:05:02.546 A:middle
So on input you will
have an image,

00:05:02.676 --> 00:05:04.436 A:middle
and then you have
this big orange box.

00:05:04.896 --> 00:05:08.706 A:middle
Inside the orange box, the red
box represents the, the weights.

00:05:09.216 --> 00:05:12.486 A:middle
These are the parameters of,
of the network, and on output,

00:05:12.486 --> 00:05:14.386 A:middle
this thing we output
four values,

00:05:14.716 --> 00:05:17.016 A:middle
which are the probability
of having a cat, a dog,

00:05:17.116 --> 00:05:18.686 A:middle
a giraffe, or a snake.

00:05:19.496 --> 00:05:21.146 A:middle
OK. So first you
need to train it.

00:05:21.396 --> 00:05:22.816 A:middle
Let's say you have a cat image.

00:05:23.746 --> 00:05:27.526 A:middle
You process the cat image to the
network, and you get an answer.

00:05:28.486 --> 00:05:29.816 A:middle
So that's the highest
probability.

00:05:29.816 --> 00:05:30.516 A:middle
It says dog.

00:05:31.556 --> 00:05:33.256 A:middle
Well, that's what you
need to train them.

00:05:33.886 --> 00:05:37.016 A:middle
So it was a cat,
and what you do is

00:05:37.516 --> 00:05:39.326 A:middle
that you [inaudible]
the good answer,

00:05:39.606 --> 00:05:41.706 A:middle
and what if I slightly
modify the weights.

00:05:42.476 --> 00:05:47.186 A:middle
So the networks now will go
slightly in the cat direction,

00:05:47.826 --> 00:05:53.586 A:middle
and you need to do this millions
of times with a lot of images,

00:05:53.916 --> 00:05:58.276 A:middle
and at some point, you will
have a trained network answering

00:05:58.276 --> 00:06:00.736 A:middle
correctly for, for
on every request.

00:05:58.276 --> 00:06:00.736 A:middle
correctly for, for
on every request.

00:06:01.036 --> 00:06:05.886 A:middle
So if it was a, a giraffe,
and it says giraffe

00:06:05.886 --> 00:06:07.036 A:middle
because we trained it properly.

00:06:08.066 --> 00:06:10.936 A:middle
Alright. So that's
what the network does,

00:06:11.046 --> 00:06:13.626 A:middle
and notice the difference
between training and inference.

00:06:13.626 --> 00:06:15.276 A:middle
During inference, we
didn't change the weight.

00:06:16.086 --> 00:06:17.986 A:middle
So usually let's say
you have this as an app.

00:06:19.016 --> 00:06:21.586 A:middle
What will happen is that you
will do the training offline

00:06:22.076 --> 00:06:23.876 A:middle
when you build your app,
you will do your training,

00:06:23.876 --> 00:06:25.966 A:middle
and then when you shift the
app, you will shift the weights

00:06:26.356 --> 00:06:29.156 A:middle
and the, the network
apology, and what happens

00:06:29.156 --> 00:06:31.706 A:middle
on the device is on
this inference part.

00:06:32.616 --> 00:06:35.646 A:middle
OK. What's inside this
orange box, though?

00:06:35.646 --> 00:06:36.856 A:middle
So let me show you an example.

00:06:36.856 --> 00:06:39.326 A:middle
If you have been to the
What's New in Metal,

00:06:39.976 --> 00:06:42.936 A:middle
Part II speech yesterday, you
have seen a bigger example of,

00:06:43.286 --> 00:06:45.356 A:middle
that was a scene
recognition network,

00:06:45.746 --> 00:06:47.896 A:middle
and lots of demo [inaudible]
recognition network,

00:06:48.186 --> 00:06:50.686 A:middle
these are more advanced networks
with hundreds of layers.

00:06:51.236 --> 00:06:54.896 A:middle
This is state of the art from
five years ago, and this,

00:06:54.936 --> 00:06:58.986 A:middle
this describes a, a
[inaudible] recognition network.

00:06:59.616 --> 00:07:03.316 A:middle
So on input you have the small
image, which is a capture

00:06:59.616 --> 00:07:03.316 A:middle
So on input you have the small
image, which is a capture

00:07:03.316 --> 00:07:06.776 A:middle
of something written, and then
it goes to different layers.

00:07:07.526 --> 00:07:09.516 A:middle
Here you have a 5
x 5 convolution,

00:07:10.136 --> 00:07:12.326 A:middle
and the output will be
a stack of five images.

00:07:13.436 --> 00:07:15.386 A:middle
That's an output of five
different convolutions

00:07:15.386 --> 00:07:19.806 A:middle
with different weights, and
then you add another layer.

00:07:19.846 --> 00:07:23.376 A:middle
You take these five images,
apply a lot of convolutions

00:07:23.376 --> 00:07:25.496 A:middle
to them, and you will
get another bigger stack

00:07:26.206 --> 00:07:28.956 A:middle
of fifty images with
5 x 5 pixels.

00:07:29.346 --> 00:07:33.866 A:middle
So what's happening here is
you go from the image space

00:07:33.926 --> 00:07:36.876 A:middle
into the feature space
one layer at a time.

00:07:37.516 --> 00:07:40.006 A:middle
So the content of this
small images becomes more

00:07:40.006 --> 00:07:42.846 A:middle
and more abstract, and at the
end what you have is the feature

00:07:43.336 --> 00:07:45.306 A:middle
which will tell you that's
a zero, that's the one.

00:07:45.536 --> 00:07:46.866 A:middle
This is what you want on output.

00:07:47.536 --> 00:07:50.816 A:middle
OK. And so with a very
small number of layers,

00:07:51.356 --> 00:07:53.986 A:middle
you can really do that,
and it works really well.

00:07:54.656 --> 00:07:59.086 A:middle
So after these convolutions,
we take this 5 x 5 x 50 values,

00:07:59.976 --> 00:08:02.976 A:middle
take them as a single big vector

00:07:59.976 --> 00:08:02.976 A:middle
take them as a single big vector

00:08:03.036 --> 00:08:05.156 A:middle
and apply a fully
connected layer,

00:08:05.156 --> 00:08:07.146 A:middle
which is just a big
metrics product,

00:08:07.146 --> 00:08:09.736 A:middle
and it will mix all
these values together

00:08:09.736 --> 00:08:11.666 A:middle
and produce a set of 100 values.

00:08:12.446 --> 00:08:15.206 A:middle
That's called the hidden
layer in this specific model,

00:08:16.446 --> 00:08:21.516 A:middle
and then you need a last one,
which will make this 100 values,

00:08:22.006 --> 00:08:24.696 A:middle
mix them together, and produce
the ten outputs you want.

00:08:24.916 --> 00:08:27.936 A:middle
So at this point you are
computing future space each

00:08:27.936 --> 00:08:31.036 A:middle
value here is a probability
of being one specific digit.

00:08:32.015 --> 00:08:34.645 A:middle
Alright. So that's what
is inside the network,

00:08:34.645 --> 00:08:37.466 A:middle
and as you have seen here,
we have two different kinds

00:08:37.466 --> 00:08:40.206 A:middle
of layers, and this is
what we do inside of BNNS.

00:08:40.206 --> 00:08:42.606 A:middle
We provide the compute
part of these layers.

00:08:43.216 --> 00:08:47.286 A:middle
OK. Before we start about
what we really compute

00:08:47.286 --> 00:08:49.406 A:middle
and what the API is, let
me show you some numbers.

00:08:49.476 --> 00:08:52.306 A:middle
So this, this, here
[inaudible] , Caffe,

00:08:52.306 --> 00:08:54.316 A:middle
which is a well-known package

00:08:54.316 --> 00:08:56.786 A:middle
for [inaudible] network
computation,

00:08:57.416 --> 00:08:59.326 A:middle
and this is a convolution
part of Caffe.

00:08:59.586 --> 00:09:02.026 A:middle
So we have 14 different
convolution sizes.

00:08:59.586 --> 00:09:02.026 A:middle
So we have 14 different
convolution sizes.

00:09:02.576 --> 00:09:03.396 A:middle
Here, you can read them.

00:09:04.586 --> 00:09:10.776 A:middle
And this is the time Caffe takes
to, actually, that's the speed.

00:09:10.776 --> 00:09:11.676 A:middle
So higher is better.

00:09:11.676 --> 00:09:13.706 A:middle
This is Caffe on
this convolutions,

00:09:14.086 --> 00:09:15.676 A:middle
and this is what
you get with BNNS.

00:09:16.776 --> 00:09:18.696 A:middle
So on average, it's 2.1x faster,

00:09:18.696 --> 00:09:21.086 A:middle
and if you have even
bigger convolutions,

00:09:21.086 --> 00:09:23.756 A:middle
you can even reach
almost four times faster.

00:09:25.436 --> 00:09:27.976 A:middle
Alright. So that
was all the numbers.

00:09:28.106 --> 00:09:30.726 A:middle
Now let me tell you
what is inside BNNS.

00:09:32.106 --> 00:09:34.146 A:middle
So that's a set of low
level compute functions.

00:09:34.146 --> 00:09:35.246 A:middle
Very similar to BLAS.

00:09:35.246 --> 00:09:37.076 A:middle
That's why we named it BNNS.

00:09:37.746 --> 00:09:41.036 A:middle
It doesn't really know what's,
what is a neural network.

00:09:41.316 --> 00:09:43.646 A:middle
That's, that's your
side of the equation.

00:09:43.786 --> 00:09:48.506 A:middle
What it does is provide really
only the compute part of it.

00:09:48.746 --> 00:09:50.856 A:middle
OK. And it only,
only does inference.

00:09:51.856 --> 00:09:54.356 A:middle
Actually I'm not sure
it would make sense to,

00:09:54.356 --> 00:09:55.696 A:middle
to run training on the device.

00:09:55.696 --> 00:09:56.786 A:middle
That's, that's too expensive.

00:09:56.786 --> 00:09:58.976 A:middle
You need millions of images
and million of computations.

00:09:59.686 --> 00:10:00.406 A:middle
That wouldn't fit.

00:09:59.686 --> 00:10:00.406 A:middle
That wouldn't fit.

00:10:00.956 --> 00:10:04.106 A:middle
So usually inference would be
done offline, and as I told,

00:10:04.456 --> 00:10:07.866 A:middle
you will just do the inference
part on your, in your app.

00:10:08.156 --> 00:10:09.846 A:middle
And we provide three
different types of layers.

00:10:10.446 --> 00:10:12.116 A:middle
Convolution layers,
pooling layers,

00:10:12.326 --> 00:10:13.896 A:middle
and the fully-connected layers.

00:10:14.386 --> 00:10:18.296 A:middle
Why? Well, actually in
the, in the modern network,

00:10:18.296 --> 00:10:20.046 A:middle
you spend more than 75 percent

00:10:20.046 --> 00:10:21.826 A:middle
of the time computing
convolutions,

00:10:22.476 --> 00:10:24.786 A:middle
and then the next one on
the list of pooling layers

00:10:24.786 --> 00:10:26.486 A:middle
with something like 15 percent,

00:10:27.156 --> 00:10:29.566 A:middle
and fully-connected layers
also take a lot of time

00:10:29.566 --> 00:10:32.726 A:middle
but usually you find them only
at the end of the network, like,

00:10:33.296 --> 00:10:35.436 A:middle
like we have seen in the
example where we only two,

00:10:35.816 --> 00:10:37.646 A:middle
two fully-connected
layers at the end,

00:10:38.236 --> 00:10:40.706 A:middle
but this did take a lot of time.

00:10:40.816 --> 00:10:44.826 A:middle
OK. Now that we know what
is inside, I will enter

00:10:44.826 --> 00:10:47.296 A:middle
into the details of the three
different kind of layers

00:10:47.536 --> 00:10:51.316 A:middle
and what we compute and how to
create them through the API.

00:10:51.316 --> 00:10:54.856 A:middle
Let's start with the
convolution layers.

00:10:55.316 --> 00:10:56.586 A:middle
So this is a convolution.

00:10:56.586 --> 00:11:01.156 A:middle
It takes an input image, a block
of weight that the orange matrix

00:10:56.586 --> 00:11:01.156 A:middle
It takes an input image, a block
of weight that the orange matrix

00:11:01.156 --> 00:11:05.346 A:middle
on the middle, and each pixel
in the output image is computed

00:11:05.346 --> 00:11:08.346 A:middle
by taking a block of
input pixels, multiply,

00:11:08.576 --> 00:11:10.306 A:middle
multiplying them by
the, the weights,

00:11:10.686 --> 00:11:11.846 A:middle
and then you take
the sum of that,

00:11:11.846 --> 00:11:13.146 A:middle
and you get your upper pixel.

00:11:13.776 --> 00:11:16.226 A:middle
And you need to do that
for every output pixel.

00:11:16.866 --> 00:11:19.236 A:middle
So if you count, that's
a four dimensional loop

00:11:19.236 --> 00:11:21.166 A:middle
because you need
to loop on x, y,

00:11:21.446 --> 00:11:23.066 A:middle
and then on the kernel
dimensions.

00:11:24.096 --> 00:11:26.586 A:middle
Actually what they do is
slightly more complex than that

00:11:26.586 --> 00:11:29.386 A:middle
because what we have on input
is just, is not one image.

00:11:29.386 --> 00:11:30.596 A:middle
We have a stack of images.

00:11:31.186 --> 00:11:32.966 A:middle
So we need to duplicate
the weights

00:11:33.866 --> 00:11:37.776 A:middle
and know what we compute is
this convolution on each layer,

00:11:38.356 --> 00:11:40.786 A:middle
and then we take the sum of
all these guys, and what we,

00:11:40.786 --> 00:11:42.346 A:middle
and we get, to get
our output pixel.

00:11:42.706 --> 00:11:45.056 A:middle
So now the, the loop
is five dimensional.

00:11:45.756 --> 00:11:48.076 A:middle
I added the IC [inaudible]
of the equation,

00:11:48.996 --> 00:11:50.866 A:middle
and really this is
not what we compute

00:11:51.376 --> 00:11:53.196 A:middle
because we also have
a stack in output.

00:11:53.766 --> 00:11:56.356 A:middle
So what we really compute
in this convolution is this.

00:11:56.806 --> 00:12:00.546 A:middle
We do this many times, one
time for each output layer,

00:11:56.806 --> 00:12:00.546 A:middle
We do this many times, one
time for each output layer,

00:12:01.276 --> 00:12:03.896 A:middle
and now we have a
six-dimensional loop.

00:12:04.126 --> 00:12:06.826 A:middle
That means even if the
dimensions are very small,

00:12:06.936 --> 00:12:10.746 A:middle
like in this example, we
are nothing bigger than 264.

00:12:11.196 --> 00:12:13.426 A:middle
It's very small, but when
you multiply all these stuff

00:12:13.426 --> 00:12:16.116 A:middle
together, what you get is
billions of operations.

00:12:16.116 --> 00:12:18.676 A:middle
That means tens of
milliseconds of compute,

00:12:19.416 --> 00:12:21.976 A:middle
and in an entire network,
which is much bigger,

00:12:22.586 --> 00:12:25.306 A:middle
you can have trillions of,
of floating point operations.

00:12:25.306 --> 00:12:28.556 A:middle
That means seconds of CPU time.

00:12:28.716 --> 00:12:30.786 A:middle
OK. A compute in the
convolution layer,

00:12:30.786 --> 00:12:32.936 A:middle
now how do you create
that with BNNS?

00:12:33.446 --> 00:12:34.786 A:middle
Well, first thing
you need to do is

00:12:34.826 --> 00:12:36.716 A:middle
to describe your input stacks.

00:12:37.596 --> 00:12:40.956 A:middle
So you need to specify the
dimensions of the image,

00:12:41.366 --> 00:12:46.096 A:middle
the number of channels, and
also all the layered in memory.

00:12:46.316 --> 00:12:49.796 A:middle
So the increment between
two rows and the increment

00:12:49.796 --> 00:12:53.056 A:middle
between two layers, and
also a very important thing,

00:12:53.406 --> 00:12:57.306 A:middle
what type is used to store them.

00:12:57.436 --> 00:13:01.916 A:middle
For example, we, we work
on float -- 64-bit float.

00:12:57.436 --> 00:13:01.916 A:middle
For example, we, we work
on float -- 64-bit float.

00:13:02.586 --> 00:13:04.766 A:middle
On neural networks, we don't
need all this precision,

00:13:04.976 --> 00:13:08.766 A:middle
and usually people will use
64-bit float, and that's good

00:13:08.766 --> 00:13:10.196 A:middle
because the storage
is half of it.

00:13:10.266 --> 00:13:13.376 A:middle
So instead of having 20
megabytes, we have only 10,

00:13:13.706 --> 00:13:16.196 A:middle
and if you can go on
integer, eight-bit integer,

00:13:16.196 --> 00:13:17.676 A:middle
you will have only
five megabytes.

00:13:18.366 --> 00:13:21.356 A:middle
And you usually, usually don't
use any precision in the output.

00:13:22.836 --> 00:13:25.876 A:middle
So you can specify the type
used to store the input.

00:13:26.226 --> 00:13:29.696 A:middle
You need to do the same for the
output stack, and then you need

00:13:29.696 --> 00:13:31.326 A:middle
to describe the convolution
itself.

00:13:31.816 --> 00:13:33.076 A:middle
That's the kernel dimensions,

00:13:33.816 --> 00:13:35.816 A:middle
the padding which is
a [inaudible] band of,

00:13:35.896 --> 00:13:39.816 A:middle
of 0 added to the input, the
stride, which is the increment

00:13:39.816 --> 00:13:44.556 A:middle
of the loop in x and y, and the,
again, you need to repeat the,

00:13:44.556 --> 00:13:47.806 A:middle
the channel counts for input and
output and specify the weights.

00:13:48.086 --> 00:13:51.966 A:middle
That's what the orange block in
the middle, specify the weights,

00:13:51.966 --> 00:13:54.666 A:middle
and, again, you can have a,
a different storage type of,

00:13:54.666 --> 00:13:58.366 A:middle
of the weights, and
usually you want 16- or,

00:13:58.366 --> 00:14:00.286 A:middle
or 8-bit storage
for the weights.

00:13:58.366 --> 00:14:00.286 A:middle
or 8-bit storage
for the weights.

00:14:00.836 --> 00:14:03.696 A:middle
Again, because it will
lower the memory usage

00:14:03.696 --> 00:14:06.796 A:middle
and the storage needed
to store them.

00:14:08.076 --> 00:14:09.636 A:middle
Then once you have done that,

00:14:09.806 --> 00:14:12.276 A:middle
you can create a convolution
filter with this function.

00:14:13.276 --> 00:14:15.056 A:middle
You tell it, OK, this
is my input stack,

00:14:15.056 --> 00:14:18.056 A:middle
that's my output stack, that's
my convolution create a filter,

00:14:18.056 --> 00:14:20.886 A:middle
and you will get a filter
object which then you can use

00:14:20.996 --> 00:14:23.776 A:middle
to apply the convolution
on your, on your data,

00:14:23.966 --> 00:14:25.226 A:middle
and once you are done with it,

00:14:25.226 --> 00:14:28.006 A:middle
you will call a [inaudible]
filter to get rid of it and,

00:14:28.006 --> 00:14:30.066 A:middle
and really the resource is used.

00:14:30.696 --> 00:14:32.346 A:middle
This was for the
convolution layers.

00:14:32.346 --> 00:14:35.076 A:middle
Now, let's go and with
the pooling layers.

00:14:35.356 --> 00:14:37.546 A:middle
Pooling is slightly
simpler than convolutions.

00:14:38.076 --> 00:14:40.966 A:middle
So what to compute one output
pixel what you do is you take a

00:14:40.966 --> 00:14:45.046 A:middle
block of input pixels and take
the max value of the average,

00:14:45.446 --> 00:14:47.336 A:middle
and that's your result,
and you do this

00:14:47.336 --> 00:14:49.686 A:middle
for all pixels in all channels.

00:14:50.726 --> 00:14:52.006 A:middle
That's what the formula says.

00:14:52.996 --> 00:14:55.926 A:middle
Well, again, to create a, a
filter for the pooling layer,

00:14:56.046 --> 00:15:00.386 A:middle
you need to describe the input
and output stacks the same way

00:14:56.046 --> 00:15:00.386 A:middle
you need to describe the input
and output stacks the same way

00:15:00.386 --> 00:15:03.446 A:middle
as before, and you also
need to describe the,

00:15:03.446 --> 00:15:04.706 A:middle
the pooling layer itself.

00:15:04.706 --> 00:15:07.546 A:middle
Again, with the kernel
dimensions, padding, stride,

00:15:07.616 --> 00:15:10.336 A:middle
and this time you don't have
weights, but which function

00:15:10.336 --> 00:15:12.226 A:middle
to use to compute the output.

00:15:12.226 --> 00:15:14.276 A:middle
That will be max average.

00:15:15.616 --> 00:15:17.686 A:middle
And then once you have done
that, you can create the filter,

00:15:18.306 --> 00:15:19.806 A:middle
and you get a filter
object similar

00:15:19.806 --> 00:15:20.836 A:middle
to the one we had before.

00:15:21.726 --> 00:15:24.006 A:middle
The last kind of layers
we support is the fully

00:15:24.006 --> 00:15:24.706 A:middle
connected layers.

00:15:26.486 --> 00:15:28.076 A:middle
Well, it's called
fully connected

00:15:28.076 --> 00:15:29.876 A:middle
but that's a matrix
product hidden here.

00:15:30.126 --> 00:15:33.796 A:middle
So on input you have a vector,
and then you will multiply it

00:15:33.866 --> 00:15:37.216 A:middle
by the matrix, add some,
add a vector of bias,

00:15:37.216 --> 00:15:38.356 A:middle
and then you get the output.

00:15:39.186 --> 00:15:40.576 A:middle
So just a, a matrix product.

00:15:41.846 --> 00:15:44.326 A:middle
So this time, you don't have
images in it for your vector.

00:15:44.326 --> 00:15:47.386 A:middle
So you need to describe the
vector, that's its size.

00:15:47.916 --> 00:15:50.166 A:middle
Point out to the data
and also the type you use

00:15:50.166 --> 00:15:55.136 A:middle
to store these values, and,
again, you can use 32, 16 bits,

00:15:55.206 --> 00:15:56.386 A:middle
floating point, and integer.

00:15:59.266 --> 00:16:02.596 A:middle
And the you need to
describe the layer itself

00:15:59.266 --> 00:16:02.596 A:middle
And the you need to
describe the layer itself

00:16:02.646 --> 00:16:04.506 A:middle
through the dimensions
of the matrix

00:16:04.916 --> 00:16:06.186 A:middle
and the matrix co-efficients.

00:16:06.676 --> 00:16:10.556 A:middle
And also the bias is not on
the slide, but you have a bias.

00:16:11.186 --> 00:16:16.026 A:middle
And then you can create the
convolution filter, and you get,

00:16:16.026 --> 00:16:17.386 A:middle
again, if [inaudible]
to the others.

00:16:17.386 --> 00:16:20.606 A:middle
Now what we have on this
filters is how do we apply them.

00:16:21.306 --> 00:16:23.526 A:middle
So you have your input
data as your output data

00:16:24.276 --> 00:16:25.756 A:middle
and your filter,
and you will have,

00:16:25.796 --> 00:16:29.476 A:middle
you have two functions
to apply them.

00:16:29.546 --> 00:16:32.436 A:middle
So it's called filter apply if
you have only one pair of input

00:16:32.436 --> 00:16:35.006 A:middle
and output, and you have,
if you have several of them,

00:16:35.336 --> 00:16:39.746 A:middle
you will call filter apply
batch, and you, you tell,

00:16:39.966 --> 00:16:42.586 A:middle
we tell you the number of
pairs you hvee and how to get

00:16:42.586 --> 00:16:43.806 A:middle
from one pair to the next one.

00:16:43.806 --> 00:16:45.016 A:middle
That's a stride in memory.

00:16:45.016 --> 00:16:48.416 A:middle
Alright. And that's it for BNNS.

00:16:48.416 --> 00:16:50.396 A:middle
Let me wrap up.

00:16:50.666 --> 00:16:54.306 A:middle
So BNNS is a set of, of
low-level compute functions

00:16:54.306 --> 00:16:55.416 A:middle
for our neural networks.

00:16:56.406 --> 00:16:57.726 A:middle
Really low level.

00:16:57.726 --> 00:16:58.686 A:middle
We do the compute.

00:16:58.686 --> 00:16:59.286 A:middle
We do it well.

00:16:59.286 --> 00:17:03.286 A:middle
We do it fast, but it doesn't
know what a neural network is.

00:16:59.286 --> 00:17:03.286 A:middle
We do it fast, but it doesn't
know what a neural network is.

00:17:03.896 --> 00:17:07.415 A:middle
It just does the compute.

00:17:07.656 --> 00:17:10.136 A:middle
So we optimize it to be
fast and energy efficient,

00:17:10.756 --> 00:17:17.776 A:middle
and important thing, it will
under multiple data types.

00:17:18.425 --> 00:17:19.826 A:middle
OK, that was it for BNNS.

00:17:20.006 --> 00:17:21.066 A:middle
Now Quadrature.

00:17:21.556 --> 00:17:27.156 A:middle
We have a request, will, people
who are asking us about library

00:17:27.156 --> 00:17:29.186 A:middle
to do a numerical
integration of functions.

00:17:29.316 --> 00:17:30.046 A:middle
So here it is.

00:17:30.876 --> 00:17:33.696 A:middle
Yeah, remember your,
your school days.

00:17:34.166 --> 00:17:35.576 A:middle
So this is computing
the integral

00:17:35.576 --> 00:17:37.306 A:middle
of a function between a and b.

00:17:37.866 --> 00:17:42.586 A:middle
So that's a green area
between the curve and the axis.

00:17:42.586 --> 00:17:45.506 A:middle
Well, so to do that, you first
need to describe your function.

00:17:45.916 --> 00:17:47.246 A:middle
So you need to provide
a callback.

00:17:47.566 --> 00:17:50.806 A:middle
Once thing we did, and which
is different from your,

00:17:50.806 --> 00:17:52.616 A:middle
the old library's
doing the same thing is

00:17:52.616 --> 00:17:55.946 A:middle
that the callbacks takes a
number of points to evaluate.

00:17:57.006 --> 00:17:59.206 A:middle
Because usually when you compute
the integral, you will have

00:17:59.256 --> 00:18:01.336 A:middle
to evaluate the function
in, at many points,

00:17:59.256 --> 00:18:01.336 A:middle
to evaluate the function
in, at many points,

00:18:01.336 --> 00:18:03.646 A:middle
and if you have a callback,
a vectorized callback doing

00:18:03.646 --> 00:18:05.516 A:middle
that faster, it's all good.

00:18:05.516 --> 00:18:10.326 A:middle
You can do much, much faster
with this multiple x callback.

00:18:10.326 --> 00:18:13.766 A:middle
So it will, it will pass
you a number of values in x,

00:18:13.766 --> 00:18:15.726 A:middle
and you will have
to fill it with a f

00:18:15.726 --> 00:18:17.936 A:middle
of xi inside each value of y.

00:18:19.516 --> 00:18:20.986 A:middle
So that's your function,
and then you need

00:18:20.986 --> 00:18:22.826 A:middle
to tell it how to integrate it.

00:18:22.876 --> 00:18:25.406 A:middle
So we provide a set of three
different integration methods

00:18:26.166 --> 00:18:29.986 A:middle
with different complexity
and, and time, and also some

00:18:29.986 --> 00:18:32.816 A:middle
of them are able to integrate
to infinity and, note,

00:18:32.816 --> 00:18:34.986 A:middle
you will find details in
the Quadrature header.

00:18:36.516 --> 00:18:39.166 A:middle
And you also need to specify
what is the error you want

00:18:39.166 --> 00:18:41.666 A:middle
on the output, and
the max number

00:18:41.666 --> 00:18:43.826 A:middle
of intervals we can
subdivide maybe to,

00:18:43.936 --> 00:18:44.916 A:middle
to complete the output.

00:18:45.546 --> 00:18:47.546 A:middle
And then you pass that to
the integrate function,

00:18:49.426 --> 00:18:51.476 A:middle
and you also tell it a, b,

00:18:51.586 --> 00:18:55.416 A:middle
and you pass a point
to receive the error.

00:18:55.416 --> 00:18:59.226 A:middle
It's called estimated
error is here,

00:18:59.886 --> 00:19:01.996 A:middle
and it will return you the
estimated error on the output

00:18:59.886 --> 00:19:01.996 A:middle
and it will return you the
estimated error on the output

00:19:01.996 --> 00:19:04.796 A:middle
and also status, we receive
the status of the computation

00:19:04.796 --> 00:19:07.226 A:middle
because if you ask a
very, very low error,

00:19:07.226 --> 00:19:08.676 A:middle
sometimes it's not
able to convert,

00:19:08.676 --> 00:19:11.406 A:middle
and we will get that
in a status.

00:19:11.786 --> 00:19:12.896 A:middle
And that's it for Quadrature.

00:19:13.606 --> 00:19:16.316 A:middle
And now let me call
Steve on stage.

00:19:16.636 --> 00:19:18.716 A:middle
He will tell you about
new additions to simd.

00:19:19.956 --> 00:19:20.756 A:middle
&gt;&gt; Thanks very much, Eric.

00:19:21.306 --> 00:19:22.246 A:middle
My name's Steven Canon.

00:19:22.246 --> 00:19:24.066 A:middle
I work in the Vector,
Numerics group with Eric.

00:19:24.586 --> 00:19:27.806 A:middle
Eric took you back to
calculus just then.

00:19:27.806 --> 00:19:29.846 A:middle
I'm going to take you
ahead again a little bit

00:19:29.846 --> 00:19:32.206 A:middle
to linear algebra right now.

00:19:32.496 --> 00:19:34.926 A:middle
We have this nice module, simd,

00:19:35.686 --> 00:19:39.506 A:middle
which provides geometric
operations and vector operations

00:19:39.986 --> 00:19:43.506 A:middle
for C, Objective-C,
C ++, and Swift.

00:19:44.596 --> 00:19:47.656 A:middle
And it really closely mirrors
the Metal shading language.

00:19:48.486 --> 00:19:51.956 A:middle
It ties in closely with
SceneKit and Model I/O

00:19:51.956 --> 00:19:53.466 A:middle
and all those graphics
libraries.

00:19:53.876 --> 00:19:57.196 A:middle
If you find yourself writing
vector code to do, you know,

00:19:57.196 --> 00:19:59.446 A:middle
small, you know, 3 x 3,

00:19:59.446 --> 00:20:01.526 A:middle
4 x 4 kind of linear
algebra operations,

00:19:59.446 --> 00:20:01.526 A:middle
4 x 4 kind of linear
algebra operations,

00:20:01.806 --> 00:20:03.496 A:middle
this is the library you probably
want to be using instead

00:20:03.496 --> 00:20:03.976 A:middle
of writing that by hand.

00:20:04.596 --> 00:20:06.446 A:middle
We have most of what
you want available.

00:20:06.696 --> 00:20:08.416 A:middle
If something's not there,
ask us to provide it.

00:20:08.796 --> 00:20:10.486 A:middle
It's all really fast
and there it is.

00:20:10.776 --> 00:20:11.646 A:middle
So what's there now?

00:20:12.486 --> 00:20:13.996 A:middle
We've have a bunch of types.

00:20:14.606 --> 00:20:18.916 A:middle
There's vectors of floats and of
doubles, and we also have signed

00:20:18.916 --> 00:20:20.766 A:middle
and unsigned integers
like 2, 3, and 4.

00:20:22.336 --> 00:20:25.236 A:middle
And we have matrices of floats
and doubles of those same sizes.

00:20:26.686 --> 00:20:29.406 A:middle
And this is just what's
available in every language,

00:20:30.236 --> 00:20:31.976 A:middle
in C and C++ and Objective-C.

00:20:31.976 --> 00:20:34.156 A:middle
There's a bunch of other types
as well which are really useful

00:20:34.156 --> 00:20:36.616 A:middle
for writing your own generic
vector code, but I'm going

00:20:36.616 --> 00:20:39.376 A:middle
to focus on sort of the common
subset of what's available

00:20:39.376 --> 00:20:42.226 A:middle
on all the languages on all the
platforms in our talk today.

00:20:42.556 --> 00:20:45.186 A:middle
We have operations on those
types, too, obviously.

00:20:45.186 --> 00:20:48.836 A:middle
There's all the usual
arithmetic operations

00:20:48.866 --> 00:20:50.716 A:middle
for vectors and for matrices.

00:20:51.656 --> 00:20:54.186 A:middle
And we have all the familiar
geometry and shader functions

00:20:54.186 --> 00:20:55.826 A:middle
if you've done any
shader programming before.

00:20:55.826 --> 00:20:57.186 A:middle
Most of what you
would want to use is,

00:20:57.186 --> 00:20:58.376 A:middle
is going to be available here.

00:20:58.916 --> 00:21:00.876 A:middle
I'll show you a little
example now.

00:20:58.916 --> 00:21:00.876 A:middle
I'll show you a little
example now.

00:21:01.766 --> 00:21:04.766 A:middle
So this is the same
function written three times

00:21:04.766 --> 00:21:05.736 A:middle
in three different languages.

00:21:06.346 --> 00:21:09.386 A:middle
We have it in Objective-C
up top, we have it in C ++

00:21:09.386 --> 00:21:11.306 A:middle
in the middle, and we have
it in Swift on the bottom,

00:21:11.756 --> 00:21:14.506 A:middle
and you can see that the
boilerplate is a little bit

00:21:14.506 --> 00:21:15.696 A:middle
different between
the languages just

00:21:15.696 --> 00:21:17.316 A:middle
because function
declarations work differently

00:21:17.316 --> 00:21:19.486 A:middle
in these languages,
but if we focus

00:21:19.486 --> 00:21:23.126 A:middle
in on the actual computation
part, it's essentially the same

00:21:23.126 --> 00:21:26.906 A:middle
in all the languages, and it
also really closely mirrors the,

00:21:27.046 --> 00:21:28.896 A:middle
the way you would just write
this naturally in math.

00:21:29.576 --> 00:21:31.046 A:middle
So you don't have a lot
of weird function calls.

00:21:31.046 --> 00:21:33.166 A:middle
You don't have to write
four loops, all that stuff.

00:21:33.596 --> 00:21:35.966 A:middle
You just write your code in
this, this natural fluent style,

00:21:36.276 --> 00:21:38.716 A:middle
and we translate
all that for you.

00:21:38.996 --> 00:21:41.776 A:middle
So it's, it's nice and easy
to write, and this looks just

00:21:41.776 --> 00:21:43.406 A:middle
like the Metal code you would
write to do this as well.

00:21:43.866 --> 00:21:46.636 A:middle
Now it happens that the reflect
function is already available

00:21:46.636 --> 00:21:47.926 A:middle
built into the library already.

00:21:47.926 --> 00:21:49.396 A:middle
So you don't need to
write this yourself.

00:21:50.866 --> 00:21:52.756 A:middle
Calling functions
between languages

00:21:52.756 --> 00:21:54.696 A:middle
like there's a whole bunch
of stuff in model I/O

00:21:54.696 --> 00:21:58.506 A:middle
that exposes Objective-C
API's that take these types.

00:21:58.506 --> 00:21:59.506 A:middle
Situation is pretty good.

00:21:59.626 --> 00:22:02.506 A:middle
The vector types are
compiler extensions in C,

00:21:59.626 --> 00:22:02.506 A:middle
The vector types are
compiler extensions in C,

00:22:02.506 --> 00:22:04.286 A:middle
Objective-C, and in C++.

00:22:05.176 --> 00:22:07.576 A:middle
And in Swift, they're
defined as structs,

00:22:07.626 --> 00:22:10.896 A:middle
but the compiler knows how
to map between them for you.

00:22:11.606 --> 00:22:13.376 A:middle
So you don't really
need to do anything.

00:22:13.846 --> 00:22:14.856 A:middle
Here's a simple example.

00:22:14.966 --> 00:22:16.876 A:middle
If I have an Objective-C
function,

00:22:16.876 --> 00:22:19.596 A:middle
I call some function here
that operates on vector types,

00:22:20.356 --> 00:22:21.806 A:middle
and I want to call that function

00:22:21.956 --> 00:22:23.976 A:middle
from Swift using the
Swift vector types,

00:22:24.016 --> 00:22:25.796 A:middle
I can just do that,
and it just works.

00:22:25.796 --> 00:22:27.086 A:middle
I don't need to do
anything fancy.

00:22:27.626 --> 00:22:28.856 A:middle
These types have
the same layout.

00:22:28.856 --> 00:22:31.206 A:middle
So there's no cost to converting
or anything like that.

00:22:32.266 --> 00:22:33.976 A:middle
Similarly, for matrices,

00:22:34.246 --> 00:22:37.456 A:middle
the Swift matrix types are
layout compatible with the C,

00:22:37.456 --> 00:22:39.046 A:middle
Objective-C, and C++ types.

00:22:39.666 --> 00:22:42.516 A:middle
So if I need to work with
them, here I have a Swift,

00:22:42.906 --> 00:22:45.356 A:middle
I'm going to create a
Swift type from the C type.

00:22:46.376 --> 00:22:47.906 A:middle
All I need to do is
use the init function.

00:22:48.426 --> 00:22:49.056 A:middle
Works fine.

00:22:49.206 --> 00:22:50.646 A:middle
Doesn't have any
computational cost.

00:22:50.646 --> 00:22:53.416 A:middle
It just sort of changes the
types silently for me, and,

00:22:53.416 --> 00:22:56.616 A:middle
similarly, I can use the C
matrix property if I need

00:22:56.616 --> 00:23:00.856 A:middle
to get a C type to call C or
Objective-C or C++ function.

00:22:56.616 --> 00:23:00.856 A:middle
to get a C type to call C or
Objective-C or C++ function.

00:23:01.576 --> 00:23:03.086 A:middle
So we have a few things
that are new this year

00:23:03.286 --> 00:23:05.886 A:middle
that I want to show to you.

00:23:05.886 --> 00:23:08.976 A:middle
We have three new functions
- simd orient, simd incircle,

00:23:08.976 --> 00:23:12.066 A:middle
simd insphere - and these are
overloaded to support a bunch

00:23:12.066 --> 00:23:14.236 A:middle
of different types and different
lengths and things like that.

00:23:14.696 --> 00:23:16.856 A:middle
Basically everything in the
simd library works that way.

00:23:16.856 --> 00:23:18.636 A:middle
So even though we have
just three new functions,

00:23:18.636 --> 00:23:20.376 A:middle
it's actually a lot
of new stuff.

00:23:21.096 --> 00:23:22.746 A:middle
So I'm going to start
with orient.

00:23:23.766 --> 00:23:26.156 A:middle
What orient does is it lets us
answer the question is a set

00:23:26.156 --> 00:23:27.406 A:middle
of vectors positively oriented?

00:23:28.196 --> 00:23:29.356 A:middle
And what that means,

00:23:29.356 --> 00:23:31.056 A:middle
if you don't remember
your linear algebra,

00:23:31.446 --> 00:23:33.056 A:middle
is do they obey the
right-hand rule.

00:23:33.436 --> 00:23:35.916 A:middle
You might remember that from
physics, or, equivalently,

00:23:36.246 --> 00:23:37.466 A:middle
is there determinate positive.

00:23:37.466 --> 00:23:39.846 A:middle
If there's any math
majors in the audience,

00:23:39.846 --> 00:23:41.306 A:middle
you're objecting
right now that a set

00:23:41.456 --> 00:23:43.656 A:middle
of vectors does not
have a determinate.

00:23:43.756 --> 00:23:45.636 A:middle
What I mean here is just
you, you kind of take them,

00:23:45.636 --> 00:23:47.736 A:middle
and you slam them
together to make a matrix.

00:23:47.736 --> 00:23:49.516 A:middle
Take the determinate
of that matrix.

00:23:49.806 --> 00:23:51.666 A:middle
Is that positive or not?

00:23:51.856 --> 00:23:53.556 A:middle
So why do we care about this?

00:23:53.656 --> 00:23:55.516 A:middle
This is kind of simple
and stupid.

00:23:55.796 --> 00:23:58.556 A:middle
You can use this to answer a lot

00:23:58.556 --> 00:24:01.206 A:middle
of computational geometry
questions that are very useful.

00:23:58.556 --> 00:24:01.206 A:middle
of computational geometry
questions that are very useful.

00:24:01.576 --> 00:24:04.246 A:middle
Like, is the triangle facing
towards me or away from me?

00:24:04.276 --> 00:24:07.506 A:middle
If you think about a
tetrahedron, there are two faces

00:24:07.506 --> 00:24:09.496 A:middle
that are pointing towards you,
and there are also two faces

00:24:09.496 --> 00:24:11.486 A:middle
on the backside that
face away from you.

00:24:11.486 --> 00:24:14.016 A:middle
And if you're doing graphics
operations, it's useful to know

00:24:14.016 --> 00:24:15.836 A:middle
which ones are facing
towards you

00:24:15.836 --> 00:24:17.126 A:middle
because those are the ones
you want to operate on.

00:24:18.206 --> 00:24:20.596 A:middle
Similarly, if I have
a line and a point,

00:24:20.596 --> 00:24:22.316 A:middle
and I want to answer the
question is the point

00:24:22.316 --> 00:24:25.036 A:middle
on the line, or if it's not,
which side of the line is it on.

00:24:25.036 --> 00:24:27.856 A:middle
I can use the orient predicate
to answer that question.

00:24:27.856 --> 00:24:31.646 A:middle
Now this seems kind of
simple, and it is simple except

00:24:31.646 --> 00:24:33.996 A:middle
that it can be very hard to
actually answer this question

00:24:33.996 --> 00:24:36.086 A:middle
when the point is close to the
line, and I'm going to talk

00:24:36.086 --> 00:24:36.796 A:middle
about that more later.

00:24:38.056 --> 00:24:39.966 A:middle
So here's a simple
example of that.

00:24:39.966 --> 00:24:43.336 A:middle
I'm going to have a plane over
on the right-hand side of the,

00:24:43.336 --> 00:24:44.866 A:middle
the display here, and I have,

00:24:44.866 --> 00:24:46.296 A:middle
going to put some
points on that plane.

00:24:46.296 --> 00:24:50.876 A:middle
So I have three points,
a, b, and c, and I'm going

00:24:50.876 --> 00:24:53.706 A:middle
to query the orientation
of those with simd orient.

00:24:54.256 --> 00:24:57.426 A:middle
Now because we go
counterclockwise as we go from a

00:24:57.426 --> 00:25:01.826 A:middle
to b to c, there, we say that
these are positively oriented.

00:24:57.426 --> 00:25:01.826 A:middle
to b to c, there, we say that
these are positively oriented.

00:25:01.826 --> 00:25:02.466 A:middle
That's what it means

00:25:02.466 --> 00:25:03.996 A:middle
to be positively
oriented in the plane.

00:25:04.076 --> 00:25:09.146 A:middle
If we move one of the points so
that that order is clockwise,

00:25:09.416 --> 00:25:14.086 A:middle
now it's negatively oriented,
and if I move the point c

00:25:14.086 --> 00:25:16.546 A:middle
so it's exactly on the
line between a and b,

00:25:16.826 --> 00:25:19.706 A:middle
then they're co-linear
and the orientation is 0,

00:25:19.706 --> 00:25:20.696 A:middle
or you might say
it's degenerate.

00:25:21.606 --> 00:25:24.906 A:middle
Now it's very hard to tell in
general if a point is exactly

00:25:24.906 --> 00:25:27.056 A:middle
on the line especially with
floating point coordinates.

00:25:27.736 --> 00:25:30.936 A:middle
And that's because orientation
is numerically unstable.

00:25:31.526 --> 00:25:34.496 A:middle
So because of floating point
rounding, if the results

00:25:34.566 --> 00:25:36.066 A:middle
of this determinate
is very nearly 0,

00:25:36.416 --> 00:25:39.356 A:middle
you can easily get the wrong
sign, and for some algorithms,

00:25:39.356 --> 00:25:41.626 A:middle
that doesn't matter, but
for other algorithms,

00:25:41.626 --> 00:25:43.216 A:middle
you can actually
fail to converge,

00:25:43.216 --> 00:25:45.846 A:middle
or you may get the wrong result
when you're working with this.

00:25:45.846 --> 00:25:47.436 A:middle
For collision detection
and things like that,

00:25:47.436 --> 00:25:49.336 A:middle
it can actually be quite
important to be able

00:25:49.336 --> 00:25:50.096 A:middle
to answer this question.

00:25:50.096 --> 00:25:53.046 A:middle
Also for things like triangular
mesh generation for models

00:25:53.046 --> 00:25:54.386 A:middle
and things, this is
an important question

00:25:54.386 --> 00:25:55.896 A:middle
to be able to answer exactly.

00:25:56.646 --> 00:25:59.086 A:middle
So let's look at
an example where,

00:25:59.086 --> 00:26:01.026 A:middle
where this is actually
hard to answer.

00:25:59.086 --> 00:26:01.026 A:middle
where this is actually
hard to answer.

00:26:02.256 --> 00:26:04.946 A:middle
I'm going to put two vectors
in the plane, u and v,

00:26:04.946 --> 00:26:07.566 A:middle
and these are almost
identical vectors, right.

00:26:07.566 --> 00:26:08.456 A:middle
They, they differ

00:26:08.456 --> 00:26:10.566 A:middle
by the smallest amount
they could possibly differ.

00:26:11.826 --> 00:26:14.666 A:middle
And I've magnified
them enormously

00:26:14.666 --> 00:26:15.426 A:middle
on the right-hand side.

00:26:15.426 --> 00:26:16.866 A:middle
So you can see that
they're actually different.

00:26:16.866 --> 00:26:18.066 A:middle
If I drew these to
scale, they would,

00:26:18.066 --> 00:26:19.286 A:middle
they would overlap entirely.

00:26:20.606 --> 00:26:23.716 A:middle
And if we compute the
orientation in the way

00:26:23.716 --> 00:26:26.716 A:middle
that you would normally compute
this, we get a result of 0

00:26:26.716 --> 00:26:27.736 A:middle
because of floating
point rounding.

00:26:27.736 --> 00:26:29.806 A:middle
Now this is a simple
example where we get 0.

00:26:30.356 --> 00:26:32.236 A:middle
With dimensions bigger
than 2 x 2,

00:26:32.646 --> 00:26:34.866 A:middle
we can actually get the
wrong sign entirely for this.

00:26:34.866 --> 00:26:36.726 A:middle
So it's not just that it could
be 0 when it shouldn't be.

00:26:37.536 --> 00:26:39.036 A:middle
But if we use the
simd orient function,

00:26:39.676 --> 00:26:41.906 A:middle
we get a tiny positive
number, which is,

00:26:41.906 --> 00:26:42.626 A:middle
which is the right result.

00:26:42.696 --> 00:26:43.856 A:middle
These are positively oriented.

00:26:44.896 --> 00:26:46.636 A:middle
And I should point out here

00:26:46.636 --> 00:26:49.656 A:middle
that don't interpret
this tiny positive number

00:26:49.656 --> 00:26:50.616 A:middle
as meaning anything.

00:26:50.616 --> 00:26:51.606 A:middle
It is not the determinate.

00:26:52.496 --> 00:26:56.566 A:middle
It is sometimes the determinate,
but it's sometimes just going

00:26:56.566 --> 00:26:57.316 A:middle
to have the right sign.

00:26:57.316 --> 00:26:58.676 A:middle
So what we're really
interested here is

00:26:58.676 --> 00:27:00.036 A:middle
in the sign of this number.

00:26:58.676 --> 00:27:00.036 A:middle
in the sign of this number.

00:27:00.376 --> 00:27:01.446 A:middle
How are we able to do this?

00:27:01.726 --> 00:27:02.986 A:middle
So the geometric [inaudible]

00:27:02.986 --> 00:27:04.766 A:middle
that I'm showing you
today use something called

00:27:04.766 --> 00:27:05.576 A:middle
adaptive precision.

00:27:06.216 --> 00:27:09.246 A:middle
We go and compute as many
bits as we need to compute

00:27:09.356 --> 00:27:10.186 A:middle
to get the right result,

00:27:10.576 --> 00:27:12.626 A:middle
and this lets us return
the right result very,

00:27:12.626 --> 00:27:16.456 A:middle
very fast in most cases,
but if we need to go off

00:27:16.456 --> 00:27:18.726 A:middle
and do the exact computation
to give you the right answer,

00:27:18.976 --> 00:27:20.066 A:middle
we will, we will do that.

00:27:20.066 --> 00:27:21.806 A:middle
So you can just trust that that
this gives you the right answer

00:27:21.806 --> 00:27:24.006 A:middle
in your code, and you don't need
to worry about that yourself.

00:27:25.526 --> 00:27:27.116 A:middle
Incircle is very similar.

00:27:27.446 --> 00:27:28.916 A:middle
We take three points
in the plane.

00:27:29.116 --> 00:27:30.036 A:middle
That determines the circle.

00:27:30.276 --> 00:27:32.566 A:middle
You can notice that
they're positively oriented

00:27:32.566 --> 00:27:33.326 A:middle
around the circle here.

00:27:33.506 --> 00:27:34.096 A:middle
That's important.

00:27:35.006 --> 00:27:37.266 A:middle
And if I put a point
in the middle, x,

00:27:37.486 --> 00:27:39.816 A:middle
then simd incircle can
tell me if it's inside.

00:27:40.056 --> 00:27:41.886 A:middle
In that case, I get
a positive result.

00:27:42.476 --> 00:27:45.016 A:middle
If it's on the circle, I get 0.

00:27:45.696 --> 00:27:48.036 A:middle
And if it's outside the
circle, I get a negative result.

00:27:48.616 --> 00:27:51.926 A:middle
And insphere is exactly
the same thing.

00:27:52.426 --> 00:27:55.376 A:middle
Just in three dimensions now.

00:27:55.376 --> 00:27:56.806 A:middle
I need four dimensions
to determine the sphere.

00:27:57.506 --> 00:27:59.246 A:middle
I give it my point x,
and I get a result.

00:28:00.656 --> 00:28:02.856 A:middle
I'll show you an example I
talked about earlier figuring

00:28:02.856 --> 00:28:05.506 A:middle
out if a triangle faces
towards you or away from you.

00:28:06.166 --> 00:28:08.036 A:middle
Here I've got a really
simple struct

00:28:08.036 --> 00:28:09.466 A:middle
to represent a triangle
in Swift.

00:28:09.956 --> 00:28:11.766 A:middle
Triangle is determined
by three vertices

00:28:11.766 --> 00:28:13.646 A:middle
that I happen, a
collection here.

00:28:14.506 --> 00:28:18.296 A:middle
And I have this predicate
is facing to tell me if the,

00:28:18.296 --> 00:28:20.486 A:middle
the triangle is facing
towards the camera or not.

00:28:21.216 --> 00:28:23.526 A:middle
So the usual way you
would compute this is

00:28:23.526 --> 00:28:26.556 A:middle
to compute a normal to the
triangle with the cross product,

00:28:27.316 --> 00:28:30.216 A:middle
and then take the dot product
of that with the vector

00:28:30.276 --> 00:28:31.746 A:middle
to the camera, and
if that's positive,

00:28:31.796 --> 00:28:33.066 A:middle
then the triangle
faces the camera.

00:28:33.576 --> 00:28:36.886 A:middle
We can simplify this code
a lot and make it correct

00:28:38.326 --> 00:28:40.936 A:middle
by just using the
simd orient predicate.

00:28:41.306 --> 00:28:43.556 A:middle
So my code is simpler,
it's fast,

00:28:43.736 --> 00:28:44.766 A:middle
and it gives me the
right answer.

00:28:44.836 --> 00:28:47.286 A:middle
Is it, these are all things I
like, and that's what we try

00:28:47.286 --> 00:28:50.266 A:middle
to do in Accelerate across the
board is give you simple things

00:28:50.266 --> 00:28:52.336 A:middle
for complex mathematical
calculations.

00:28:53.976 --> 00:28:55.956 A:middle
So we showed you a bunch
of new stuff today.

00:28:57.136 --> 00:29:00.486 A:middle
We have some new
libraries entirely.

00:28:57.136 --> 00:29:00.486 A:middle
We have some new
libraries entirely.

00:29:00.666 --> 00:29:02.746 A:middle
We have BNNS for
neural networks.

00:29:02.956 --> 00:29:06.946 A:middle
We have Quadrature, and we
also have some new features,

00:29:07.706 --> 00:29:09.496 A:middle
orientation and incircle
in simd.

00:29:09.846 --> 00:29:13.926 A:middle
Every one of these features and
libraries is added in response

00:29:13.926 --> 00:29:16.766 A:middle
to a request that we
got from developers.

00:29:16.856 --> 00:29:19.676 A:middle
So we really want to hear from
you guys about what you need,

00:29:20.116 --> 00:29:22.736 A:middle
what we can add to make your
computational workloads easier.

00:29:22.736 --> 00:29:24.356 A:middle
We want to give you
simple interfaces

00:29:24.796 --> 00:29:26.716 A:middle
that let you do what you
need to do efficiently.

00:29:27.726 --> 00:29:29.386 A:middle
We've also done a ton of
other stuff this year.

00:29:30.206 --> 00:29:33.206 A:middle
In vImage, which Eric
mentioned briefly in passing,

00:29:34.006 --> 00:29:36.196 A:middle
we have a whole set
of geometry operations

00:29:36.196 --> 00:29:37.316 A:middle
for interleaved chroma planes.

00:29:37.346 --> 00:29:38.716 A:middle
This is absolutely one

00:29:38.716 --> 00:29:40.886 A:middle
of the most requested features
we've had in recent years.

00:29:41.306 --> 00:29:42.946 A:middle
So we're, we're really
excited to have that.

00:29:42.946 --> 00:29:44.706 A:middle
If you don't know what that
is, don't worry about it,

00:29:44.806 --> 00:29:46.906 A:middle
but if you know what it is,
you understand why it's useful.

00:29:46.906 --> 00:29:50.836 A:middle
And we also have expanded
supports for new formats

00:29:50.836 --> 00:29:51.976 A:middle
in the vImage conversion
routines.

00:29:52.036 --> 00:29:53.996 A:middle
This underlies a lot
of the deep color stuff

00:29:53.996 --> 00:29:54.846 A:middle
that you may have heard about.

00:29:54.846 --> 00:29:58.246 A:middle
So this is, this is
really important for that.

00:29:58.616 --> 00:30:00.516 A:middle
We have improved the performance

00:29:58.616 --> 00:30:00.516 A:middle
We have improved the performance

00:30:00.516 --> 00:30:02.476 A:middle
for interleaved complex
formats in vDSP.

00:30:02.646 --> 00:30:06.766 A:middle
With the FFT's, we support both
interleaved and planer layouts

00:30:06.766 --> 00:30:09.046 A:middle
where the complex and imaginary
parts are either separated

00:30:09.046 --> 00:30:09.976 A:middle
or put together.

00:30:11.746 --> 00:30:14.336 A:middle
Planer layouts are what we
really prefer to operate with,

00:30:14.336 --> 00:30:16.896 A:middle
and we recommend you do
that, but if you happen

00:30:16.896 --> 00:30:19.776 A:middle
to have interleaved data, you
can just use the FFT's now,

00:30:19.776 --> 00:30:20.766 A:middle
and they'll be really fast.

00:30:22.126 --> 00:30:23.386 A:middle
We've also improved
the performance

00:30:23.386 --> 00:30:24.926 A:middle
of all the Level
II BLAS operations.

00:30:24.926 --> 00:30:27.926 A:middle
Some of that's motivated by
the, the BNNS stuff you saw,

00:30:27.926 --> 00:30:30.186 A:middle
and some of that's just
opportunity that we saw to go

00:30:30.186 --> 00:30:32.356 A:middle
after that, and there's a
ton of other stuff that's new

00:30:32.356 --> 00:30:34.346 A:middle
in Accelerate, lots
of improved stuff.

00:30:35.056 --> 00:30:36.926 A:middle
Every time new processors
come out, we make sure

00:30:36.926 --> 00:30:37.946 A:middle
that we're tuning for that.

00:30:38.306 --> 00:30:39.646 A:middle
We want to take care

00:30:39.646 --> 00:30:41.556 A:middle
of all those low-level
computational details

00:30:41.586 --> 00:30:44.826 A:middle
so you can focus on writing
high-level algorithms that,

00:30:44.886 --> 00:30:46.226 A:middle
that just sit on top of that,

00:30:46.226 --> 00:30:47.806 A:middle
and you can do the
job you need to do.

00:30:49.256 --> 00:30:52.126 A:middle
In summary, we want to
be your single-stop shop

00:30:52.126 --> 00:30:53.806 A:middle
for computational algorithms

00:30:53.976 --> 00:30:55.506 A:middle
where we give you
implementations

00:30:55.506 --> 00:30:58.946 A:middle
that are correct, they're fast,
and they're energy efficient,

00:30:59.566 --> 00:31:01.716 A:middle
and we're going to keep
tuning them for new hardware

00:30:59.566 --> 00:31:01.716 A:middle
and we're going to keep
tuning them for new hardware

00:31:01.716 --> 00:31:03.666 A:middle
as it comes along so you don't
need to worry about that.

00:31:03.666 --> 00:31:06.626 A:middle
If you want to [inaudible]
yourself and we ship a,

00:31:06.666 --> 00:31:09.736 A:middle
a new processor, then you're
going to need to update

00:31:09.736 --> 00:31:12.646 A:middle
for that, and if you let
us handle that for you,

00:31:12.876 --> 00:31:14.066 A:middle
then you don't need to worry.

00:31:15.386 --> 00:31:16.756 A:middle
Keep the future requests coming.

00:31:16.886 --> 00:31:17.886 A:middle
We love to get them from you.

00:31:18.316 --> 00:31:20.316 A:middle
We talked to a bunch of you
in the labs earlier today.

00:31:20.316 --> 00:31:22.116 A:middle
We got a bunch of great
future requests already.

00:31:22.646 --> 00:31:23.596 A:middle
Can file radars.

00:31:23.636 --> 00:31:24.786 A:middle
We love for that to happen.

00:31:25.116 --> 00:31:30.236 A:middle
If you want more information,
the link for this talk is here.

00:31:30.816 --> 00:31:33.546 A:middle
I also encourage you to check
out previous years' talks

00:31:33.546 --> 00:31:35.946 A:middle
where you've gone into more
detail about other aspects

00:31:35.946 --> 00:31:37.116 A:middle
of the library that are useful.

00:31:38.316 --> 00:31:40.336 A:middle
There were two great
Metal sessions

00:31:40.336 --> 00:31:42.546 A:middle
that I really highly
recommend everyone check

00:31:42.546 --> 00:31:44.626 A:middle
out from yesterday,
especially if you're interested

00:31:44.626 --> 00:31:45.956 A:middle
in this kind of stuff.

00:31:46.046 --> 00:31:47.306 A:middle
Thanks very much for
coming out everyone.

00:31:48.516 --> 00:31:58.380 A:middle
[ Applause ]
