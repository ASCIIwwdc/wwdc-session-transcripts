WEBVTT
X-TIMESTAMP-MAP=MPEGTS:181083,LOCAL:00:00:00.000

00:00:07.516 --> 00:00:17.500 A:middle
[ Music ]

00:00:26.126 --> 00:00:26.536 A:middle
&gt;&gt; All right.

00:00:27.041 --> 00:00:29.041 A:middle
[ Applause ]

00:00:29.066 --> 00:00:29.936 A:middle
&gt;&gt; Good afternoon, everyone.

00:00:30.986 --> 00:00:32.456 A:middle
So, how many of you
want to build an app

00:00:32.526 --> 00:00:34.786 A:middle
with really cool audio
effects but thought

00:00:34.786 --> 00:00:35.576 A:middle
that that might be hard?

00:00:36.846 --> 00:00:38.376 A:middle
Or how many of you
want to focus more

00:00:38.376 --> 00:00:41.196 A:middle
on your application's overall
user experience but ended

00:00:41.196 --> 00:00:42.846 A:middle
up spending a little
more time on audio?

00:00:43.776 --> 00:00:45.936 A:middle
Well, we've been working
hard to make it easy for you.

00:00:46.646 --> 00:00:47.456 A:middle
My name is Saleem.

00:00:47.776 --> 00:00:49.336 A:middle
I'm a Craftsman on
the Core Audio Team.

00:00:49.596 --> 00:00:51.176 A:middle
I want to welcome you
to today's session

00:00:51.176 --> 00:00:53.356 A:middle
on delivering an
exceptional audio experience.

00:00:54.586 --> 00:00:57.046 A:middle
So let's look at an overview
of what's in our stack today.

00:00:57.816 --> 00:01:00.626 A:middle
We'll start with our
AVFoundation framework.


WEBVTT
X-TIMESTAMP-MAP=MPEGTS:181083,LOCAL:00:00:00.000

00:00:57.816 --> 00:01:00.626 A:middle
We'll start with our
AVFoundation framework.

00:01:01.326 --> 00:01:03.826 A:middle
We have a wide variety
of high-level APIs

00:01:04.686 --> 00:01:06.696 A:middle
that let you simply
play and record audio.

00:01:08.176 --> 00:01:12.726 A:middle
For more advanced use cases, we
have our AudioToolbox framework.

00:01:13.666 --> 00:01:15.626 A:middle
And you may have
heard of AudioUnits.

00:01:15.776 --> 00:01:17.546 A:middle
These are a fundamental
building block.

00:01:18.316 --> 00:01:22.416 A:middle
If you have to work with
MIDI devices or MIDI data,

00:01:22.526 --> 00:01:23.796 A:middle
we have our CoreMIDI framework.

00:01:24.946 --> 00:01:27.376 A:middle
For game development,
there's OpenAL.

00:01:27.976 --> 00:01:31.006 A:middle
And over the last two years,

00:01:31.006 --> 00:01:33.676 A:middle
we've been adding many new
APIs and features as well.

00:01:33.736 --> 00:01:35.886 A:middle
So you can see there
are many ways

00:01:35.886 --> 00:01:37.866 A:middle
that you can use audio
in your application.

00:01:38.296 --> 00:01:40.506 A:middle
So, our goal today
is to help guide you

00:01:40.506 --> 00:01:43.516 A:middle
to choosing the right API
for your application's needs.

00:01:44.126 --> 00:01:46.406 A:middle
But don't worry, we also
have a few new things

00:01:46.406 --> 00:01:50.116 A:middle
to share with you as well.

00:01:50.376 --> 00:01:52.346 A:middle
So, on the agenda
today, we'll first look

00:01:52.346 --> 00:01:54.996 A:middle
at some essential setup steps
for a few of our platforms.

00:01:55.876 --> 00:01:58.936 A:middle
Then, we'll dive straight into
simple and advanced playback

00:01:58.936 --> 00:01:59.956 A:middle
and recording scenarios.


WEBVTT
X-TIMESTAMP-MAP=MPEGTS:181083,LOCAL:00:00:00.000

00:02:00.966 --> 00:02:02.736 A:middle
We'll talk a bit about
multichannel audio.

00:02:03.806 --> 00:02:05.166 A:middle
And then later in
the presentation,

00:02:05.236 --> 00:02:06.466 A:middle
we'll look at real-time audio --

00:02:07.166 --> 00:02:09.276 A:middle
how you can build your
own effects, instruments,

00:02:09.276 --> 00:02:11.596 A:middle
and generators -- and then
we'll wrap up with MIDI.

00:02:12.316 --> 00:02:16.216 A:middle
So, let's get started.

00:02:16.216 --> 00:02:20.016 A:middle
iOS, watchOS, and tvOS all
have really rich audio features

00:02:20.306 --> 00:02:21.916 A:middle
and numerous writing
capabilities.

00:02:22.706 --> 00:02:26.896 A:middle
So users can make calls,
play music, play games,

00:02:27.356 --> 00:02:28.946 A:middle
work with various
productivity apps.

00:02:29.046 --> 00:02:32.326 A:middle
And they can do all of this
mixed in or independently.

00:02:33.306 --> 00:02:36.746 A:middle
So the operating system manages
a lot of default audio behaviors

00:02:36.746 --> 00:02:39.486 A:middle
in order to provide a
consistent user experience.

00:02:40.066 --> 00:02:43.576 A:middle
So let's look at a diagram
showing how audio is a

00:02:43.576 --> 00:02:44.596 A:middle
managed service.

00:02:45.166 --> 00:02:49.136 A:middle
So you have your device,
and it has a couple

00:02:49.136 --> 00:02:51.086 A:middle
of inputs and outputs.

00:02:51.646 --> 00:02:54.596 A:middle
And then there's the
operating system.

00:02:55.906 --> 00:02:58.876 A:middle
It may be hosting many apps,
some of which are using audio.


WEBVTT
X-TIMESTAMP-MAP=MPEGTS:181083,LOCAL:00:00:00.000

00:03:00.096 --> 00:03:01.876 A:middle
And lastly, there's
your application.

00:03:03.876 --> 00:03:07.216 A:middle
So AVAudioSession is your
interface, as a developer,

00:03:07.306 --> 00:03:09.886 A:middle
for expressing your
application needs to the system.

00:03:11.006 --> 00:03:15.076 A:middle
Let's go into a bit
more detail about that.

00:03:16.206 --> 00:03:17.976 A:middle
Categories express
the application's

00:03:18.096 --> 00:03:18.996 A:middle
highest-level needs.

00:03:19.816 --> 00:03:21.976 A:middle
We have modes and
category options

00:03:21.976 --> 00:03:25.036 A:middle
which help you further customize
and specialize your application.

00:03:26.996 --> 00:03:30.386 A:middle
If you're into some
more advanced use cases,

00:03:30.636 --> 00:03:32.486 A:middle
such as input selection,
you may want to be able

00:03:32.486 --> 00:03:33.896 A:middle
to choose the front microphone

00:03:33.896 --> 00:03:35.336 A:middle
on your iPhone instead
of the bottom.

00:03:36.456 --> 00:03:38.096 A:middle
If you're working with
multichannel audio

00:03:38.096 --> 00:03:40.726 A:middle
and multichannel content on
tvOS, you may be interested

00:03:40.726 --> 00:03:42.026 A:middle
in things like channel count.

00:03:43.126 --> 00:03:45.846 A:middle
If you had a USB audio device
connected to your iPhone,

00:03:45.846 --> 00:03:50.726 A:middle
you may be interested in
things like sample rate.

00:03:52.336 --> 00:03:55.416 A:middle
So when your application
is ready and configured

00:03:55.416 --> 00:03:58.686 A:middle
to use audio, it informs the
system to apply the session.

00:03:59.616 --> 00:04:01.496 A:middle
So this will configure
the device's hardware


WEBVTT
X-TIMESTAMP-MAP=MPEGTS:181083,LOCAL:00:00:00.000

00:03:59.616 --> 00:04:01.496 A:middle
So this will configure
the device's hardware

00:04:01.576 --> 00:04:04.436 A:middle
for your application's needs
and may actually result

00:04:04.436 --> 00:04:06.866 A:middle
in interrupting other audio
applications on the system,

00:04:07.306 --> 00:04:09.856 A:middle
mixing with them, and/or
ducking their volume level.

00:04:13.696 --> 00:04:15.836 A:middle
So let's look at some
of the essential steps

00:04:15.836 --> 00:04:17.296 A:middle
when working with
AVAudioSession.

00:04:18.055 --> 00:04:20.185 A:middle
The first step is to sign
up for notifications.

00:04:21.386 --> 00:04:23.286 A:middle
And the three most important
notifications are the

00:04:23.286 --> 00:04:25.216 A:middle
interruption, route change,

00:04:25.446 --> 00:04:27.596 A:middle
and mediaServicesWereReset
notification.

00:04:28.906 --> 00:04:31.236 A:middle
You can sign up for these
notifications before you

00:04:31.236 --> 00:04:32.106 A:middle
activate your session.

00:04:32.106 --> 00:04:34.576 A:middle
And in a few slides, I'll show
you how you can manage them.

00:04:35.066 --> 00:04:39.406 A:middle
Next, based on your
application's high-level needs,

00:04:39.626 --> 00:04:41.716 A:middle
you'll want to set the
appropriate category mode

00:04:41.716 --> 00:04:42.296 A:middle
and options.

00:04:42.296 --> 00:04:44.526 A:middle
So, let's look at
a few examples.

00:04:44.526 --> 00:04:48.756 A:middle
Let's just say I was
building a productivity app.

00:04:48.996 --> 00:04:51.426 A:middle
And in that application, I
want to play a simple sound

00:04:51.456 --> 00:04:52.906 A:middle
when the user saves
their document.

00:04:53.936 --> 00:04:56.376 A:middle
Here, we can see that audio
enhances the experience

00:04:56.536 --> 00:04:58.006 A:middle
but it's not necessarily
required.

00:04:58.736 --> 00:05:01.186 A:middle
So, in this case, I'd want
to use the AmbientCategory.


WEBVTT
X-TIMESTAMP-MAP=MPEGTS:181083,LOCAL:00:00:00.000

00:04:58.736 --> 00:05:01.186 A:middle
So, in this case, I'd want
to use the AmbientCategory.

00:05:02.646 --> 00:05:04.896 A:middle
This category obeys
the ringer switch.

00:05:05.526 --> 00:05:07.166 A:middle
It does not play audio
in the background,

00:05:07.556 --> 00:05:09.176 A:middle
and it'll always
mix in with others.

00:05:09.836 --> 00:05:14.156 A:middle
If I was building a podcast app,

00:05:14.366 --> 00:05:16.016 A:middle
I'd want to use the
PlaybackCategory,

00:05:17.326 --> 00:05:18.396 A:middle
the SpokenAudio mode.

00:05:19.486 --> 00:05:21.696 A:middle
And here, we can see that this
app location will interrupt

00:05:21.776 --> 00:05:25.356 A:middle
other applications
on the system.

00:05:25.576 --> 00:05:27.916 A:middle
Now if you want your
audio to continue playing

00:05:27.916 --> 00:05:29.046 A:middle
in the background,
you'll also have

00:05:29.046 --> 00:05:32.006 A:middle
to specify the background
audio key in your info.plist.

00:05:32.376 --> 00:05:34.516 A:middle
And this is essentially a
session property as well.

00:05:34.736 --> 00:05:37.306 A:middle
It's just expressed
through a different means.

00:05:39.616 --> 00:05:41.006 A:middle
For your navigation app,

00:05:41.006 --> 00:05:43.516 A:middle
let's look at how you can
configure the navigation prompt.

00:05:44.396 --> 00:05:46.656 A:middle
Here, you'd want to use
the PlaybackCategory,

00:05:47.506 --> 00:05:48.406 A:middle
the DefaultMode.

00:05:49.146 --> 00:05:50.896 A:middle
And there are a few
options of interest here.

00:05:51.556 --> 00:05:54.156 A:middle
You'd want to use both
the InterruptSpokenAudio

00:05:54.196 --> 00:05:56.686 A:middle
AndMixWithOthers as
well as the duckOthers.

00:05:57.776 --> 00:06:00.216 A:middle
So, if you're listening to
a podcast while navigating


WEBVTT
X-TIMESTAMP-MAP=MPEGTS:181083,LOCAL:00:00:00.000

00:05:57.776 --> 00:06:00.216 A:middle
So, if you're listening to
a podcast while navigating

00:06:00.216 --> 00:06:02.526 A:middle
and that navigation prompt
comes up saying, "Oh,

00:06:02.526 --> 00:06:03.876 A:middle
turn left in 500 feet,"

00:06:04.246 --> 00:06:07.386 A:middle
it'll actually interrupt
the podcast app.

00:06:07.386 --> 00:06:08.476 A:middle
If you're listening to music,

00:06:08.906 --> 00:06:12.326 A:middle
it'll duck the music's volume
level and mix in with it.

00:06:13.436 --> 00:06:15.256 A:middle
For this application,
you'll also want

00:06:15.256 --> 00:06:16.796 A:middle
to use a background
audio key as well.

00:06:17.426 --> 00:06:23.296 A:middle
So, next, let's look at how
we can manage activation

00:06:23.296 --> 00:06:24.026 A:middle
of our session.

00:06:24.646 --> 00:06:27.176 A:middle
So what does it mean
to go active?

00:06:27.786 --> 00:06:29.906 A:middle
Activating your session
informs the system

00:06:29.906 --> 00:06:32.426 A:middle
to configure the hardware
for your application's needs.

00:06:33.826 --> 00:06:34.826 A:middle
So let's say, for example,

00:06:34.826 --> 00:06:36.726 A:middle
I had an application
whose category was set

00:06:36.726 --> 00:06:37.556 A:middle
to PlayAndRecord.

00:06:38.446 --> 00:06:41.466 A:middle
When I active my session,
it'll configure the hardware

00:06:41.466 --> 00:06:44.696 A:middle
to use input and output.

00:06:46.626 --> 00:06:49.466 A:middle
Now, what happens if I activate
my session while listening

00:06:49.566 --> 00:06:50.976 A:middle
to music from the music app?

00:06:51.656 --> 00:06:53.096 A:middle
Here, we can see that
the current state

00:06:53.096 --> 00:06:55.006 A:middle
of the system is set
for playback only.

00:06:56.116 --> 00:06:59.156 A:middle
So, when I activate my
session, I inform the system

00:06:59.156 --> 00:07:02.486 A:middle
to configure the hardware
for both input and output.


WEBVTT
X-TIMESTAMP-MAP=MPEGTS:181083,LOCAL:00:00:00.000

00:06:59.156 --> 00:07:02.486 A:middle
to configure the hardware
for both input and output.

00:07:03.416 --> 00:07:05.586 A:middle
And since I'm in a
non-mixable app location,

00:07:05.856 --> 00:07:08.066 A:middle
I've interrupted the music app.

00:07:09.936 --> 00:07:12.826 A:middle
So let's just say my application
makes a quick recording.

00:07:12.826 --> 00:07:15.106 A:middle
Once I'm done, I
deactivate my session.

00:07:16.116 --> 00:07:17.976 A:middle
And if I choose to notify others

00:07:17.976 --> 00:07:19.246 A:middle
that I've deactivated
my session,

00:07:19.246 --> 00:07:21.576 A:middle
we'll see that the music
app would resume playback.

00:07:22.206 --> 00:07:28.116 A:middle
Next, let's look at how we can
handle the notifications we

00:07:28.116 --> 00:07:28.736 A:middle
signed up for.

00:07:29.336 --> 00:07:33.016 A:middle
We'll first look at the
interruption notification,

00:07:33.456 --> 00:07:34.656 A:middle
and we'll examine a case

00:07:34.656 --> 00:07:36.996 A:middle
where your application
does not have playback UI.

00:07:37.916 --> 00:07:39.956 A:middle
The first thing I do is I
get the interruptionType.

00:07:40.796 --> 00:07:42.626 A:middle
And if it's the beginning
of an interruption,

00:07:43.206 --> 00:07:44.656 A:middle
your session is already
inactive.

00:07:45.756 --> 00:07:48.896 A:middle
So your players have been
paused, and you'll use this time

00:07:48.896 --> 00:07:51.566 A:middle
to update any internal
state that you have.

00:07:52.636 --> 00:07:55.796 A:middle
When you receive the end
interruption, you go ahead

00:07:55.796 --> 00:07:58.516 A:middle
and activate your session,
start your players,

00:07:58.786 --> 00:07:59.916 A:middle
and update your internal state.


WEBVTT
X-TIMESTAMP-MAP=MPEGTS:181083,LOCAL:00:00:00.000

00:08:00.446 --> 00:08:03.116 A:middle
Now, let's see how that differs

00:08:03.116 --> 00:08:04.996 A:middle
for an application
that has playback UI.

00:08:05.966 --> 00:08:09.306 A:middle
So when you receive the
begin interruption --

00:08:09.836 --> 00:08:11.186 A:middle
again, your session
is inactive --

00:08:11.316 --> 00:08:14.596 A:middle
you update the internal state,
as well as your UI this time.

00:08:15.276 --> 00:08:18.096 A:middle
So if you have a Play/Pause
button, you'd want to go ahead

00:08:18.096 --> 00:08:22.196 A:middle
and set that to "play"
at this time.

00:08:22.376 --> 00:08:25.576 A:middle
And now when you receive the end
interruption, you should check

00:08:25.576 --> 00:08:27.636 A:middle
and see if the shouldResume
option was passed in.

00:08:28.536 --> 00:08:30.316 A:middle
If that was passed in,
then you can go ahead

00:08:30.316 --> 00:08:32.796 A:middle
and activate your
session, start playback,

00:08:32.796 --> 00:08:34.366 A:middle
and update your internal
state and UI.

00:08:35.496 --> 00:08:36.936 A:middle
If it wasn't passed
in, you should wait

00:08:36.936 --> 00:08:39.426 A:middle
until the user explicitly
resumes playback.

00:08:41.086 --> 00:08:44.776 A:middle
It's important to
note that you can have

00:08:44.776 --> 00:08:45.836 A:middle
unmatched interruptions.

00:08:45.836 --> 00:08:48.846 A:middle
So, not every begin interruption
is followed by a matching end.

00:08:48.846 --> 00:08:52.496 A:middle
And an example of this are
media-player applications

00:08:52.496 --> 00:08:53.436 A:middle
that interrupt each other.

00:08:53.436 --> 00:08:59.416 A:middle
Now, let's look at how we
can handle route changes.


WEBVTT
X-TIMESTAMP-MAP=MPEGTS:181083,LOCAL:00:00:00.000

00:09:00.686 --> 00:09:02.466 A:middle
Route changes happen for
a number of reasons --

00:09:03.266 --> 00:09:04.966 A:middle
the connected devices
may have changed,

00:09:05.316 --> 00:09:06.726 A:middle
a category may have changed,

00:09:07.196 --> 00:09:09.566 A:middle
you may have selected a
different data source or port.

00:09:10.616 --> 00:09:12.816 A:middle
So, the first thing you do is
you get the routeChangeReason.

00:09:15.116 --> 00:09:18.616 A:middle
If you receive a reason that
the old device is unavailable

00:09:18.976 --> 00:09:21.116 A:middle
in your media-playback
app, you should go ahead

00:09:21.116 --> 00:09:22.386 A:middle
and stop playback at this time.

00:09:22.606 --> 00:09:25.416 A:middle
An example of this is if
your user is streaming music

00:09:25.416 --> 00:09:27.826 A:middle
to the headsets and they
unplug the headsets.

00:09:28.086 --> 00:09:30.456 A:middle
They don't expect that
the music resumes playback

00:09:30.646 --> 00:09:31.736 A:middle
through the speakers right away.

00:09:34.336 --> 00:09:35.886 A:middle
For more advanced use cases,

00:09:35.886 --> 00:09:38.166 A:middle
if you receive the
oldDeviceUnavailable

00:09:38.166 --> 00:09:41.076 A:middle
or newDeviceAvailable
routeChangeReason, you may want

00:09:41.076 --> 00:09:43.486 A:middle
to re-evaluate certain
session properties

00:09:43.486 --> 00:09:44.906 A:middle
as it applies to
your application.

00:09:45.336 --> 00:09:50.986 A:middle
Lastly, let's look at how we
can handle the media services

00:09:50.986 --> 00:09:52.326 A:middle
where we set the notification.

00:09:53.446 --> 00:09:56.116 A:middle
This notification is
rare, but it does happen

00:09:56.116 --> 00:09:59.976 A:middle
because demons aren't
guaranteed to run forever.


WEBVTT
X-TIMESTAMP-MAP=MPEGTS:181083,LOCAL:00:00:00.000

00:10:00.056 --> 00:10:01.206 A:middle
The important thing
to note here is

00:10:01.206 --> 00:10:04.586 A:middle
that your AVAudioSession
sharedInstance is still valid.

00:10:05.996 --> 00:10:08.946 A:middle
You will need to reset your
category mode and other options.

00:10:11.016 --> 00:10:13.436 A:middle
You'll also need to destroy and
recreate your player objects,

00:10:13.436 --> 00:10:16.576 A:middle
such as your AVAudioEngine,
remote I/Os,

00:10:16.576 --> 00:10:18.046 A:middle
and other player
objects as well.

00:10:19.636 --> 00:10:22.686 A:middle
And we provide a means for
testing this on devices by going

00:10:22.946 --> 00:10:25.516 A:middle
to Settings, Developer,
Reset Media Services.

00:10:29.496 --> 00:10:32.676 A:middle
OK, so that just recaps
the four steps for working

00:10:32.676 --> 00:10:34.766 A:middle
with AVAudioSession --
the essential steps.

00:10:34.936 --> 00:10:36.216 A:middle
You sign up for notifications.

00:10:36.646 --> 00:10:38.956 A:middle
You set the appropriate
category mode and options.

00:10:39.356 --> 00:10:40.926 A:middle
You manage activation
of your session.

00:10:41.436 --> 00:10:42.846 A:middle
And you handle the
notifications.

00:10:43.436 --> 00:10:44.836 A:middle
So let's look at some
new stuff this year.

00:10:46.806 --> 00:10:49.126 A:middle
New this year, we're adding
two new category options --

00:10:49.506 --> 00:10:52.006 A:middle
allowAirPlay and
allowBluetoothA2DP --

00:10:52.106 --> 00:10:53.446 A:middle
to the PlayAndRecord category.

00:10:55.086 --> 00:10:58.466 A:middle
So, that means that you can now
use a microphone while playing

00:10:58.466 --> 00:11:00.086 A:middle
to a Bluetooth and
AirPlay destination.


WEBVTT
X-TIMESTAMP-MAP=MPEGTS:181083,LOCAL:00:00:00.000

00:10:58.466 --> 00:11:00.086 A:middle
to a Bluetooth and
AirPlay destination.

00:11:00.746 --> 00:11:04.336 A:middle
So if this is your
application's use case, go ahead

00:11:04.336 --> 00:11:06.486 A:middle
and set the category
and the options,

00:11:06.786 --> 00:11:08.176 A:middle
and then let the
user pick the route

00:11:08.176 --> 00:11:11.636 A:middle
from either an MPVolumeView
or Control Center.

00:11:13.536 --> 00:11:17.326 A:middle
We're also adding a new
property for VoIP apps

00:11:17.386 --> 00:11:19.386 A:middle
on our
AVAudioSessionPortDescription

00:11:19.576 --> 00:11:20.426 A:middle
that'll determine whether

00:11:20.426 --> 00:11:22.626 A:middle
or not the current
route has hardware voice

00:11:22.626 --> 00:11:23.536 A:middle
processing enabled.

00:11:24.726 --> 00:11:27.356 A:middle
So if your user is
connected to a CarPlay system

00:11:27.726 --> 00:11:30.946 A:middle
or a Bluetooth HFP headset that
has hardware voice processing,

00:11:31.416 --> 00:11:32.536 A:middle
you can use this property

00:11:32.886 --> 00:11:34.816 A:middle
to disable your software
voice processing

00:11:34.816 --> 00:11:38.146 A:middle
so you're not double-processing
the audio.

00:11:39.066 --> 00:11:41.976 A:middle
If you're already using Apple's
built-in voice processing IO

00:11:41.976 --> 00:11:43.906 A:middle
unit, you don't have
to worry about this.

00:11:45.236 --> 00:11:46.786 A:middle
And new this year, we
also introduced the

00:11:46.786 --> 00:11:47.646 A:middle
CallKit framework.

00:11:48.226 --> 00:11:50.966 A:middle
So, to see how you can enhance
your VoIP apps with CallKit,

00:11:51.286 --> 00:11:52.456 A:middle
we had a session
earlier this week.

00:11:52.706 --> 00:11:54.906 A:middle
And if you missed that, you can
go ahead and catch it online.

00:11:55.726 --> 00:12:00.286 A:middle
So that's just an
overview of AVAudioSession.


WEBVTT
X-TIMESTAMP-MAP=MPEGTS:181083,LOCAL:00:00:00.000

00:11:55.726 --> 00:12:00.286 A:middle
So that's just an
overview of AVAudioSession.

00:12:00.326 --> 00:12:01.946 A:middle
We've covered a lot
of this stuff in-depth

00:12:01.946 --> 00:12:02.866 A:middle
in previous sessions.

00:12:02.866 --> 00:12:04.476 A:middle
So we encourage you
to check those out,

00:12:04.476 --> 00:12:07.406 A:middle
as well as a programming
guide online.

00:12:09.536 --> 00:12:10.456 A:middle
So, moving on.

00:12:10.956 --> 00:12:12.446 A:middle
So you set up AVAudioSession

00:12:12.446 --> 00:12:13.896 A:middle
if it's applicable
to your platform.

00:12:14.286 --> 00:12:15.766 A:middle
Now, let's look at how
you can simply play

00:12:15.766 --> 00:12:17.726 A:middle
and record audio in
your application.

00:12:18.296 --> 00:12:21.666 A:middle
We'll start with the
AVFoundation framework.

00:12:22.506 --> 00:12:24.766 A:middle
There are a number of classes
here that can handle the job.

00:12:25.096 --> 00:12:26.306 A:middle
We have our AVAudioPlayer,

00:12:26.856 --> 00:12:31.526 A:middle
AVAudioRecorder,
and AVPlayer class.

00:12:32.316 --> 00:12:35.156 A:middle
AVAudioPlayer is the simplest
way to play audio from a file.

00:12:36.096 --> 00:12:38.046 A:middle
We support a wide
variety of formats.

00:12:39.326 --> 00:12:41.566 A:middle
We provide all the basic
playback operations.

00:12:42.436 --> 00:12:44.286 A:middle
We also support some
more advanced operations,

00:12:44.286 --> 00:12:45.546 A:middle
such as setting volume level.

00:12:46.156 --> 00:12:47.986 A:middle
You get metering on
a per-channel basis.

00:12:48.246 --> 00:12:50.786 A:middle
You can loop your playback,
adjust the playback rate,

00:12:51.446 --> 00:12:52.926 A:middle
work with stereo panning.

00:12:53.696 --> 00:12:56.406 A:middle
If you're on iOS or
tvOS, you can work

00:12:56.406 --> 00:12:57.556 A:middle
with channel assignments.

00:12:58.216 --> 00:13:03.096 A:middle
If you had multiple files
you wanted to playback,


WEBVTT
X-TIMESTAMP-MAP=MPEGTS:181083,LOCAL:00:00:00.000

00:12:58.216 --> 00:13:03.096 A:middle
If you had multiple files
you wanted to playback,

00:13:03.096 --> 00:13:05.576 A:middle
you can use multiple
AVAudioPlayer objects

00:13:05.766 --> 00:13:07.606 A:middle
and you can synchronize
your playback as well.

00:13:08.356 --> 00:13:12.746 A:middle
And new this year, we're adding
a method that lets you fade

00:13:12.746 --> 00:13:14.806 A:middle
to volume level over
a specified duration.

00:13:15.526 --> 00:13:19.496 A:middle
So let's look at a code example

00:13:19.496 --> 00:13:21.906 A:middle
of how you can use AVAudioPlayer
in your application.

00:13:23.006 --> 00:13:23.986 A:middle
Let's just say I was working

00:13:23.986 --> 00:13:26.246 A:middle
and building a simple
productivity app again

00:13:26.456 --> 00:13:27.946 A:middle
where I want to play an
acknowledgement sound

00:13:27.946 --> 00:13:29.306 A:middle
when the user saves
their document.

00:13:30.456 --> 00:13:33.636 A:middle
In this case, I have an
AVAudioPlayer and a URL

00:13:33.636 --> 00:13:36.166 A:middle
to my asset in my class.

00:13:36.826 --> 00:13:38.946 A:middle
Now in my setup function,
I go ahead

00:13:38.996 --> 00:13:42.266 A:middle
and I create the AVAudioPlayer
object with the contents

00:13:42.266 --> 00:13:45.156 A:middle
of my URL and I prepare
the player for playback.

00:13:45.986 --> 00:13:49.946 A:middle
And then, in my saveDocument
function, I may do some work

00:13:49.946 --> 00:13:52.506 A:middle
to see whether or not the
document was saved successfully.

00:13:52.506 --> 00:13:54.476 A:middle
And if it was, then I
simply play my file.

00:13:55.206 --> 00:13:55.906 A:middle
Really easy.

00:13:55.906 --> 00:14:00.086 A:middle
Now, let's look at
AVAudioRecorder.


WEBVTT
X-TIMESTAMP-MAP=MPEGTS:181083,LOCAL:00:00:00.000

00:13:55.906 --> 00:14:00.086 A:middle
Now, let's look at
AVAudioRecorder.

00:14:00.666 --> 00:14:03.826 A:middle
This is the simplest way
to record audio to a file.

00:14:04.796 --> 00:14:07.396 A:middle
You can record for a specified
duration, or you can record

00:14:07.396 --> 00:14:09.486 A:middle
until the user explicitly stops.

00:14:09.786 --> 00:14:12.166 A:middle
You get metering on
a per-channel basis,

00:14:12.426 --> 00:14:14.746 A:middle
and we support a wide
variety of encoded formats.

00:14:16.356 --> 00:14:17.566 A:middle
So, to set up a format,

00:14:17.596 --> 00:14:19.516 A:middle
we use the Recorder
Settings Dictionary.

00:14:20.006 --> 00:14:21.926 A:middle
And now this is a
dictionary of keys that has --

00:14:21.926 --> 00:14:25.176 A:middle
a list of keys that let you
set various format parameters

00:14:25.566 --> 00:14:27.846 A:middle
such as sample rate,
number of channels.

00:14:28.536 --> 00:14:30.816 A:middle
If you're working with Linear
PCM data, you can adjust things

00:14:30.816 --> 00:14:32.296 A:middle
like the bit depth
and endian-ness.

00:14:32.916 --> 00:14:35.036 A:middle
If you're working with encoded
formats, you can adjust things

00:14:35.036 --> 00:14:36.336 A:middle
such as quality and bit rate.

00:14:37.236 --> 00:14:38.426 A:middle
So, let's look at a code example

00:14:38.426 --> 00:14:40.006 A:middle
of how you can use
AVAudioRecorder.

00:14:40.786 --> 00:14:45.406 A:middle
So the first thing I do is
I create my format settings.

00:14:46.086 --> 00:14:49.116 A:middle
Here, I'm creating an AAC file
with a really high bit rate.

00:14:50.576 --> 00:14:52.186 A:middle
And then the next thing I do --

00:14:52.256 --> 00:14:54.346 A:middle
I go ahead and create my
AVAudioRecorder object

00:14:54.996 --> 00:14:56.966 A:middle
with a URL to the file location

00:14:57.546 --> 00:14:59.296 A:middle
and the format settings
I've just defined.

00:14:59.986 --> 00:15:04.586 A:middle
And in this example, I have a
simple button that I'm using


WEBVTT
X-TIMESTAMP-MAP=MPEGTS:181083,LOCAL:00:00:00.000

00:14:59.986 --> 00:15:04.586 A:middle
And in this example, I have a
simple button that I'm using

00:15:04.586 --> 00:15:06.136 A:middle
to toggle the state
of the recorder.

00:15:06.496 --> 00:15:09.026 A:middle
So when I press the button,
if the recorder is recording,

00:15:09.506 --> 00:15:10.926 A:middle
I go ahead and stop recording.

00:15:11.486 --> 00:15:12.886 A:middle
else -- I start my recording.

00:15:13.126 --> 00:15:14.866 A:middle
And I can use the
recorders built in meters

00:15:14.866 --> 00:15:18.396 A:middle
to provide feedback to the UI.

00:15:19.276 --> 00:15:20.796 A:middle
Lastly, let's look at AVPlayer.

00:15:22.336 --> 00:15:24.586 A:middle
AVPlayer works not
only with local files

00:15:24.586 --> 00:15:26.066 A:middle
but streaming content as well.

00:15:27.016 --> 00:15:29.056 A:middle
You have all the standard
control available.

00:15:30.566 --> 00:15:32.366 A:middle
We also provide built-in
user interfaces

00:15:32.366 --> 00:15:34.856 A:middle
that you can use directly,
such as the AVPlayerView

00:15:35.256 --> 00:15:36.636 A:middle
and the AVPlayerViewController.

00:15:38.386 --> 00:15:40.536 A:middle
And AVPlayer also works
with video content as well.

00:15:40.746 --> 00:15:43.416 A:middle
And this year, we added a number
of new features to AVPlayer.

00:15:43.536 --> 00:15:46.046 A:middle
So if you want to find out
what we did, you can check

00:15:46.046 --> 00:15:47.896 A:middle
out the Advances in
AVFoundation Playback.

00:15:47.896 --> 00:15:49.946 A:middle
And if you missed that, you can
go ahead and catch it online.

00:15:55.046 --> 00:15:58.346 A:middle
OK, so what we've seen so far is
just some very simple examples

00:15:58.346 --> 00:15:59.786 A:middle
of playback and recording.


WEBVTT
X-TIMESTAMP-MAP=MPEGTS:181083,LOCAL:00:00:00.000

00:16:00.406 --> 00:16:04.206 A:middle
So now let's look at some
more advanced use cases.

00:16:04.336 --> 00:16:07.956 A:middle
Advanced use cases include
playing back not only from files

00:16:08.006 --> 00:16:12.576 A:middle
but working with buffers
of audio data as well.

00:16:12.776 --> 00:16:15.126 A:middle
You may be interested in
doing some audio processing,

00:16:15.196 --> 00:16:16.466 A:middle
applying certain effects

00:16:16.466 --> 00:16:18.196 A:middle
and mixing together
multiple sources.

00:16:18.896 --> 00:16:22.346 A:middle
Or you may be interested
in implementing 3D audio.

00:16:22.986 --> 00:16:26.416 A:middle
So, some examples of this
are you're building a classic

00:16:26.416 --> 00:16:29.556 A:middle
karaoke app, you want
to build a deejay app

00:16:29.556 --> 00:16:32.496 A:middle
with really amazing effects,
or you want to build a game

00:16:32.496 --> 00:16:34.766 A:middle
and really immerse
your user in it.

00:16:35.876 --> 00:16:38.446 A:middle
So, for such advanced use
cases, we have a class

00:16:38.536 --> 00:16:40.826 A:middle
in AVFoundation called
AVAudioEngine.

00:16:42.796 --> 00:16:45.126 A:middle
AVAudioEngine is a powerful,

00:16:45.246 --> 00:16:47.246 A:middle
feature-rich Objective-C
and Swift API.

00:16:48.276 --> 00:16:53.076 A:middle
It's a real-time audio system,
and it simplifies working

00:16:53.076 --> 00:16:53.896 A:middle
with real-time audio

00:16:53.896 --> 00:16:56.276 A:middle
by providing a non-real-time
interface for you.

00:16:57.466 --> 00:16:59.866 A:middle
So this has a lot of
complexities dealing

00:16:59.866 --> 00:17:00.786 A:middle
with real-time audio,


WEBVTT
X-TIMESTAMP-MAP=MPEGTS:181083,LOCAL:00:00:00.000

00:16:59.866 --> 00:17:00.786 A:middle
with real-time audio,

00:17:00.866 --> 00:17:02.586 A:middle
and it makes your code
that much simpler.

00:17:04.455 --> 00:17:06.786 A:middle
The Engine manages
a graph of nodes,

00:17:07.276 --> 00:17:09.526 A:middle
and these nodes let you
play and record audio.

00:17:10.306 --> 00:17:12.056 A:middle
You can connect these
nodes in various ways

00:17:12.056 --> 00:17:14.566 A:middle
to form many different
processing chains

00:17:15.746 --> 00:17:16.796 A:middle
and perform mixing.

00:17:17.776 --> 00:17:19.526 A:middle
You can capture audio
at any point

00:17:19.526 --> 00:17:21.376 A:middle
in the processing chain as well.

00:17:22.076 --> 00:17:23.336 A:middle
And we provide a special node

00:17:23.336 --> 00:17:24.846 A:middle
that lets you spatialize
your audio.

00:17:25.486 --> 00:17:27.685 A:middle
So, let's look

00:17:27.685 --> 00:17:30.376 A:middle
at the fundamental building
block -- the AVAudioNode.

00:17:32.006 --> 00:17:33.106 A:middle
We have three types of nodes.

00:17:34.336 --> 00:17:36.636 A:middle
We have source nodes, which
provide data for rendering.

00:17:37.706 --> 00:17:39.156 A:middle
So these could be
your PlayerNode,

00:17:39.546 --> 00:17:41.766 A:middle
an InputNode, or a sampler unit.

00:17:42.496 --> 00:17:46.166 A:middle
We have processing nodes that
let you process audio data.

00:17:46.166 --> 00:17:48.686 A:middle
So these could be
effects such as delays,

00:17:48.866 --> 00:17:50.506 A:middle
distortions, and mixers.

00:17:51.226 --> 00:17:55.686 A:middle
And we have the destination
node,

00:17:55.686 --> 00:17:57.666 A:middle
which is the termination
node in your graph,

00:17:57.786 --> 00:17:59.826 A:middle
and it's connected directly
to the output hardware.


WEBVTT
X-TIMESTAMP-MAP=MPEGTS:181083,LOCAL:00:00:00.000

00:18:00.526 --> 00:18:03.996 A:middle
So let's look at a sample setup.

00:18:05.446 --> 00:18:08.386 A:middle
Let's just say I'm building
a classic karaoke app.

00:18:09.356 --> 00:18:11.346 A:middle
In this case, I have
three source nodes.

00:18:11.756 --> 00:18:14.376 A:middle
I'm using the InputNode to
capture the user's voice.

00:18:14.986 --> 00:18:17.106 A:middle
I'm using a PlayerNode
to play my Backing Track.

00:18:18.336 --> 00:18:20.776 A:middle
I'm using another PlayerNode
to play other sound effects

00:18:20.776 --> 00:18:22.576 A:middle
and feedback used to the user.

00:18:23.196 --> 00:18:25.686 A:middle
In terms of processing nodes,

00:18:25.786 --> 00:18:28.586 A:middle
I may want to apply a specific
EQ to the user's voice.

00:18:30.046 --> 00:18:31.856 A:middle
And then I'm going
to use the mixer

00:18:31.856 --> 00:18:34.176 A:middle
to mix all three sources
into a single output.

00:18:35.126 --> 00:18:36.876 A:middle
And then the single
output will then be played

00:18:36.976 --> 00:18:39.356 A:middle
through the OutputNode and then
out to the output hardware.

00:18:40.026 --> 00:18:44.456 A:middle
I can also capture the user's
voice and do some analysis

00:18:44.456 --> 00:18:45.986 A:middle
to see how well they're
performing

00:18:45.986 --> 00:18:47.196 A:middle
by installing a TapBlock.

00:18:48.326 --> 00:18:49.306 A:middle
And then based on that,

00:18:49.306 --> 00:18:52.596 A:middle
I can unconditionally
schedule these feedback queues

00:18:52.596 --> 00:18:54.426 A:middle
to be played out.

00:18:55.376 --> 00:18:58.426 A:middle
So let's now look at
a sample game setup.


WEBVTT
X-TIMESTAMP-MAP=MPEGTS:181083,LOCAL:00:00:00.000

00:19:00.016 --> 00:19:02.796 A:middle
The main node of interest
here is the EnvironmentNode,

00:19:03.106 --> 00:19:04.776 A:middle
which simulates a 3D space

00:19:05.176 --> 00:19:07.086 A:middle
and spatializes its
connected sources.

00:19:07.756 --> 00:19:11.696 A:middle
In this example, I'm using
the InputNode as well

00:19:11.696 --> 00:19:12.996 A:middle
as a PlayerNode as my source.

00:19:13.566 --> 00:19:17.106 A:middle
And you can also adjust
various 3D mixing properties

00:19:17.106 --> 00:19:20.416 A:middle
on your sources as well,
such as position, occlusion.

00:19:21.476 --> 00:19:22.756 A:middle
And in terms of the
EnvironmentNode,

00:19:22.756 --> 00:19:24.016 A:middle
you can also adjust
properties there,

00:19:24.016 --> 00:19:27.866 A:middle
such as the listenerPosition as
well as other reverb parameters.

00:19:28.646 --> 00:19:33.586 A:middle
So this 3D Space can then be
mixed in with a Backing Track

00:19:33.786 --> 00:19:35.726 A:middle
and then played through
the output.

00:19:39.536 --> 00:19:42.206 A:middle
So before we move any
further with AVAudioEngine,

00:19:42.206 --> 00:19:44.476 A:middle
I want to look at some
fundamental core classes

00:19:44.606 --> 00:19:46.086 A:middle
that the Engine uses
extensively.

00:19:47.356 --> 00:19:49.136 A:middle
I'll first start
with AVAudioFormat.

00:19:49.136 --> 00:19:53.336 A:middle
So, AVAudioFormat
describes the data format

00:19:53.586 --> 00:19:55.206 A:middle
in an audio file or stream.

00:19:56.066 --> 00:19:58.986 A:middle
So we have our standard
format, common formats,

00:19:59.226 --> 00:20:00.576 A:middle
as well as compressed formats.


WEBVTT
X-TIMESTAMP-MAP=MPEGTS:181083,LOCAL:00:00:00.000

00:19:59.226 --> 00:20:00.576 A:middle
as well as compressed formats.

00:20:01.716 --> 00:20:04.266 A:middle
This class also contains
an AVAudioChannelLayout

00:20:04.546 --> 00:20:07.276 A:middle
which you may use when dealing
with multichannel audio.

00:20:07.566 --> 00:20:08.816 A:middle
It's a modern interface

00:20:08.816 --> 00:20:10.646 A:middle
to our
AudioStreamBasicDescription

00:20:10.646 --> 00:20:12.766 A:middle
structure and our
AudioChannelLayout structure.

00:20:13.406 --> 00:20:16.916 A:middle
Now, let's look at
AVAudioBuffer.

00:20:17.496 --> 00:20:19.746 A:middle
This class has two subclasses.

00:20:19.746 --> 00:20:24.256 A:middle
It has the AVAudioPCMBuffer,
which is used to hold PCM data.

00:20:24.726 --> 00:20:26.736 A:middle
And it has the
AVAudioCompressBuffer,

00:20:26.736 --> 00:20:28.796 A:middle
which is used for holding
compressed audio data.

00:20:30.226 --> 00:20:32.516 A:middle
Both of these classes
provide a modern interface

00:20:32.516 --> 00:20:35.416 A:middle
to our AudioBufferList and our
AudioStreamPacketDescription.

00:20:35.986 --> 00:20:39.306 A:middle
Let's look at AVAudioFile.

00:20:40.746 --> 00:20:43.366 A:middle
This class lets you read and
write from any supported format.

00:20:44.656 --> 00:20:47.466 A:middle
It lets you read data into
PCM buffers and write data

00:20:47.466 --> 00:20:49.416 A:middle
into a file from PCM buffers.

00:20:49.926 --> 00:20:50.576 A:middle
And in doing so,

00:20:50.576 --> 00:20:53.156 A:middle
it transparently handles
any encoding and decoding.

00:20:53.706 --> 00:20:59.186 A:middle
And it supersedes now our
AudioFile and ExtAudioFile APIs.


WEBVTT
X-TIMESTAMP-MAP=MPEGTS:181083,LOCAL:00:00:00.000

00:21:00.326 --> 00:21:02.426 A:middle
Lastly, let's look
at AVAudioConverter.

00:21:03.166 --> 00:21:07.586 A:middle
This class handles
audio format conversion.

00:21:08.426 --> 00:21:12.256 A:middle
So, you can convert between one
form of PCM data to another.

00:21:13.076 --> 00:21:17.746 A:middle
You can also convert between
PCM and compressed audio formats

00:21:18.916 --> 00:21:21.036 A:middle
in which it handles the
encoding and decoding for you.

00:21:23.296 --> 00:21:25.816 A:middle
And this class supersedes
our AudioConverter API.

00:21:25.816 --> 00:21:31.126 A:middle
And new this year, we've also
added a minimum phase sample

00:21:31.126 --> 00:21:32.296 A:middle
rate converter algorithm.

00:21:34.116 --> 00:21:38.196 A:middle
So you can see that all these
core classes really work

00:21:38.196 --> 00:21:41.016 A:middle
together when interfacing
with audio data.

00:21:41.976 --> 00:21:45.316 A:middle
Now, let's look at how
these classes then interact

00:21:45.316 --> 00:21:46.346 A:middle
with AVAudioEngine.

00:21:46.946 --> 00:21:51.546 A:middle
So if you look at
AVAudioNode, it has both input

00:21:51.546 --> 00:21:53.956 A:middle
and output AVAudio formats.

00:21:55.076 --> 00:21:58.726 A:middle
If you look at the PlayerNode,
it can provide you to the Engine

00:21:58.976 --> 00:22:01.896 A:middle
from an AVAudioFile or
an AVAudioPCMBuffer.


WEBVTT
X-TIMESTAMP-MAP=MPEGTS:181083,LOCAL:00:00:00.000

00:21:58.976 --> 00:22:01.896 A:middle
from an AVAudioFile or
an AVAudioPCMBuffer.

00:22:03.196 --> 00:22:08.736 A:middle
When you install a NodeTap, the
block provides audio data to you

00:22:08.736 --> 00:22:10.546 A:middle
in the form of PCM buffers.

00:22:11.106 --> 00:22:13.566 A:middle
You can do analysis with
it, or then you can save it

00:22:13.566 --> 00:22:15.456 A:middle
to a file using an AVAudio file.

00:22:16.256 --> 00:22:19.296 A:middle
If you're working with
a compressed stream,

00:22:19.296 --> 00:22:21.136 A:middle
you can break it down
into compress buffers,

00:22:21.316 --> 00:22:24.536 A:middle
use an AVAudioConverter to
convert it to PCM buffers,

00:22:24.886 --> 00:22:27.106 A:middle
and then provide it to the
Engine through the PlayerNode.

00:22:32.156 --> 00:22:34.246 A:middle
So, new this year,
we're bringing a subset

00:22:34.246 --> 00:22:36.106 A:middle
of AVAudioEngine to the Watch.

00:22:37.066 --> 00:22:39.756 A:middle
Along with that, we're including
a subset of AVAudioSession,

00:22:39.756 --> 00:22:43.596 A:middle
as well as all the core
classes you've just seen.

00:22:43.826 --> 00:22:45.446 A:middle
So I'm sure you'd love
to see a demo of this.

00:22:46.606 --> 00:22:47.386 A:middle
So we have that for you.

00:22:47.786 --> 00:22:50.956 A:middle
We built a simple game
using both SceneKit

00:22:50.956 --> 00:22:52.326 A:middle
and AVAudioEngine directly.

00:22:53.396 --> 00:22:55.536 A:middle
And in this game, what I'm doing
is I'm launching an asteroid

00:22:55.536 --> 00:22:56.286 A:middle
into space.

00:22:56.766 --> 00:22:58.806 A:middle
And at the bottom of the
screen, I have a flame.

00:22:58.886 --> 00:23:01.836 A:middle
And I can control the flame
using the Watch's Digital Crown.


WEBVTT
X-TIMESTAMP-MAP=MPEGTS:181083,LOCAL:00:00:00.000

00:22:58.886 --> 00:23:01.836 A:middle
And I can control the flame
using the Watch's Digital Crown.

00:23:02.756 --> 00:23:04.656 A:middle
And now if the asteroid
makes contact with the flame,

00:23:04.656 --> 00:23:06.466 A:middle
it plays this really
loud explosion sound.

00:23:07.026 --> 00:23:08.000 A:middle
So, let's see this.

00:23:16.516 --> 00:23:27.606 A:middle
[ Explosions ]

00:23:28.106 --> 00:23:30.306 A:middle
I'm sure this game, like,
defies basic laws of physics

00:23:30.346 --> 00:23:32.146 A:middle
because it's playing
audio in space.

00:23:32.146 --> 00:23:32.976 A:middle
Right? And that's not possible.

00:23:33.516 --> 00:23:36.546 A:middle
[ Applause ]

00:23:37.046 --> 00:23:37.836 A:middle
All right, so let me just go

00:23:37.836 --> 00:23:41.456 A:middle
over quickly the
AVAudioEngine code in this game.

00:23:41.666 --> 00:23:44.276 A:middle
So, in my class, I
have my AVAudioEngine.

00:23:44.866 --> 00:23:46.106 A:middle
And I have two PlayerNodes --

00:23:46.546 --> 00:23:48.176 A:middle
one for playing the
explosion sound,

00:23:48.356 --> 00:23:50.486 A:middle
and one for playing
the launch sound.

00:23:51.376 --> 00:23:54.246 A:middle
I also have URLs
to my audio assets.

00:23:54.846 --> 00:23:58.406 A:middle
And in this example,
I'm using buffers

00:23:58.796 --> 00:24:00.186 A:middle
to provide data to the engine.


WEBVTT
X-TIMESTAMP-MAP=MPEGTS:181083,LOCAL:00:00:00.000

00:23:58.796 --> 00:24:00.186 A:middle
to provide data to the engine.

00:24:00.786 --> 00:24:05.106 A:middle
So, let's look at how
we set up the engine.

00:24:06.436 --> 00:24:07.876 A:middle
The first thing I
do is I go ahead

00:24:07.876 --> 00:24:09.346 A:middle
and I attach my PlayerNodes.

00:24:09.546 --> 00:24:11.976 A:middle
So I touch the explosionPlayer
and the launchPlayer.

00:24:12.726 --> 00:24:15.396 A:middle
Next, I'm going to
use the core classes.

00:24:15.816 --> 00:24:19.046 A:middle
I'm going to create an AVAudio
file from the URL of my assets.

00:24:19.766 --> 00:24:21.416 A:middle
And then, I'm going to
create a PCM buffer.

00:24:21.496 --> 00:24:22.826 A:middle
And I'm going to read the data

00:24:22.826 --> 00:24:24.776 A:middle
from the file into
the PCM buffer.

00:24:25.486 --> 00:24:28.346 A:middle
And I can do this because my
audio files are really short.

00:24:28.906 --> 00:24:33.056 A:middle
Next, I'll go ahead and
make the connections

00:24:33.276 --> 00:24:35.976 A:middle
between the source nodes
and the engine's main mixer.

00:24:36.606 --> 00:24:41.986 A:middle
So, when the game is
about to start, I go ahead

00:24:41.986 --> 00:24:44.416 A:middle
and I start my engine
and I start my players.

00:24:45.746 --> 00:24:49.606 A:middle
And when I launch an asteroid, I
simply schedule the launchBuffer

00:24:49.606 --> 00:24:51.016 A:middle
to be played on the
launchPlayer.

00:24:51.786 --> 00:24:54.626 A:middle
And when the asteroid makes
contact with the flame,

00:24:54.626 --> 00:24:57.136 A:middle
I simply schedule the
explosionBuffer to be played

00:24:57.136 --> 00:24:58.086 A:middle
on the explosionPlayer.

00:24:59.616 --> 00:25:00.996 A:middle
So, with a few lines of code,


WEBVTT
X-TIMESTAMP-MAP=MPEGTS:181083,LOCAL:00:00:00.000

00:24:59.616 --> 00:25:00.996 A:middle
So, with a few lines of code,

00:25:00.996 --> 00:25:03.306 A:middle
I'm able to build a really
rich audio experience

00:25:03.306 --> 00:25:04.976 A:middle
for my games on watchOS.

00:25:06.076 --> 00:25:08.036 A:middle
And that was a simple
example, so we can't wait

00:25:08.036 --> 00:25:11.656 A:middle
to see what you come up with.

00:25:13.286 --> 00:25:16.106 A:middle
So, before I wrap up with
AVAudioEngine, I want to talk

00:25:16.106 --> 00:25:17.286 A:middle
about multichannel audio

00:25:18.246 --> 00:25:21.966 A:middle
and specifically how
it relates to tvOS.

00:25:22.366 --> 00:25:25.026 A:middle
So, last October, we
introduced tvOS along

00:25:25.026 --> 00:25:26.586 A:middle
with the 4th generation
Apple TV.

00:25:27.266 --> 00:25:30.356 A:middle
And so this is the first time
we can talk about it at WWDC.

00:25:30.966 --> 00:25:32.806 A:middle
And one of the interesting
things about audio

00:25:32.806 --> 00:25:35.686 A:middle
on Apple TV is that many
users are already connected

00:25:35.776 --> 00:25:37.166 A:middle
to multichannel hardware

00:25:37.436 --> 00:25:40.706 A:middle
since many home theater
systems already support 5.1

00:25:40.706 --> 00:25:42.706 A:middle
or 7.1 surround sound systems.

00:25:43.276 --> 00:25:44.836 A:middle
So, today, I just want to go

00:25:44.836 --> 00:25:46.846 A:middle
over how you can render
multichannel audio

00:25:47.276 --> 00:25:48.546 A:middle
using AVAudioEngine.

00:25:49.146 --> 00:25:52.956 A:middle
So, first, let's review the
setup with AVAudioSession.

00:25:53.036 --> 00:25:57.596 A:middle
I first set my category
and other options,

00:25:57.596 --> 00:26:00.496 A:middle
and then I activate my session
to configure the hardware


WEBVTT
X-TIMESTAMP-MAP=MPEGTS:181083,LOCAL:00:00:00.000

00:25:57.596 --> 00:26:00.496 A:middle
and then I activate my session
to configure the hardware

00:26:00.496 --> 00:26:02.186 A:middle
for my application's needs.

00:26:03.936 --> 00:26:06.766 A:middle
Now, depending on the
rendering format I want to use,

00:26:06.856 --> 00:26:07.976 A:middle
I'll first need to check and see

00:26:07.976 --> 00:26:09.336 A:middle
if the current route
supports it.

00:26:09.466 --> 00:26:12.226 A:middle
And I can do that by
checking if my desired number

00:26:12.226 --> 00:26:13.796 A:middle
of channels are less
than or equal

00:26:13.796 --> 00:26:15.586 A:middle
to the maximum number
of output channels.

00:26:16.136 --> 00:26:17.726 A:middle
And if it is, then
I can go ahead

00:26:17.726 --> 00:26:19.626 A:middle
and set my preferred
number of output channels.

00:26:20.306 --> 00:26:24.186 A:middle
I can then query back the
actual number of channels

00:26:24.186 --> 00:26:26.806 A:middle
from the session and then
use that moving forward.

00:26:27.246 --> 00:26:31.036 A:middle
Optionally, I can
look at the array

00:26:31.036 --> 00:26:32.976 A:middle
of ChannelDescriptions
on the current port.

00:26:34.216 --> 00:26:37.046 A:middle
And each ChannelDescription
gives me a channelLabel

00:26:37.486 --> 00:26:38.396 A:middle
and a channelNumber.

00:26:39.416 --> 00:26:41.976 A:middle
So I can use this information
to figure out the exact format

00:26:42.436 --> 00:26:45.316 A:middle
and how I can map my content
to the connected hardware.

00:26:47.516 --> 00:26:49.826 A:middle
Now, let's switch gears and
look at the AVAudioEngine setup.

00:26:50.886 --> 00:26:52.276 A:middle
There are two use cases here.

00:26:52.436 --> 00:26:53.316 A:middle
The first use case is

00:26:53.316 --> 00:26:55.886 A:middle
if you already have
multichannel content.

00:26:57.766 --> 00:26:59.906 A:middle
And the second use case is
if you have mono content

00:26:59.956 --> 00:27:01.086 A:middle
and you want to spatialize it.


WEBVTT
X-TIMESTAMP-MAP=MPEGTS:181083,LOCAL:00:00:00.000

00:26:59.956 --> 00:27:01.086 A:middle
and you want to spatialize it.

00:27:01.166 --> 00:27:02.856 A:middle
And this is typically
geared towards games.

00:27:03.546 --> 00:27:09.506 A:middle
So, in the first use case,
I have multichannel content

00:27:09.746 --> 00:27:10.836 A:middle
and multichannel hardware.

00:27:11.576 --> 00:27:13.066 A:middle
I simply get the
hardware format.

00:27:13.396 --> 00:27:14.676 A:middle
I set that as my connection

00:27:14.676 --> 00:27:16.296 A:middle
between my Mixer
and my OutputNode.

00:27:17.036 --> 00:27:19.896 A:middle
And on the source side, I get
the content format and I set

00:27:19.926 --> 00:27:22.476 A:middle
that as my connection between
my SourceNode and the Mixer.

00:27:23.316 --> 00:27:26.046 A:middle
And here, the Mixer handles
the channel mapping for you.

00:27:26.046 --> 00:27:31.996 A:middle
Now, in the second use case, we
have a bunch of mono sources.

00:27:32.516 --> 00:27:33.836 A:middle
And we'll use the
EnvironmentNode

00:27:33.836 --> 00:27:34.746 A:middle
to spatialize them.

00:27:35.936 --> 00:27:38.266 A:middle
So, like before, we get
the hardware format.

00:27:38.806 --> 00:27:42.556 A:middle
But before we set the compatible
format, we have to map it to one

00:27:42.556 --> 00:27:44.066 A:middle
that the EnvironmentNode
supports.

00:27:45.296 --> 00:27:47.176 A:middle
And for a list of
supported formats,

00:27:47.176 --> 00:27:48.716 A:middle
you can check our
documentation online.

00:27:49.406 --> 00:27:51.326 A:middle
So, I set the compatible format.

00:27:51.876 --> 00:27:55.176 A:middle
And now on the source side, like
before, I get the content format

00:27:55.276 --> 00:27:57.736 A:middle
and I set that as my
connection between my player

00:27:58.076 --> 00:27:59.006 A:middle
and the EnvironmentNode.


WEBVTT
X-TIMESTAMP-MAP=MPEGTS:181083,LOCAL:00:00:00.000

00:28:00.106 --> 00:28:02.546 A:middle
Lastly, I'll also have

00:28:02.796 --> 00:28:05.036 A:middle
to set the multichannel
rendering algorithm

00:28:05.036 --> 00:28:07.306 A:middle
to SoundField, which is
what the EnvironmentNode

00:28:07.386 --> 00:28:08.256 A:middle
currently supports.

00:28:08.796 --> 00:28:12.716 A:middle
And at this point, I can start
my engine, start playback,

00:28:13.336 --> 00:28:15.656 A:middle
and then adjust all the
various 3D mixing properties

00:28:15.656 --> 00:28:16.366 A:middle
that we support.

00:28:17.006 --> 00:28:20.756 A:middle
So, just a recap.

00:28:21.266 --> 00:28:24.606 A:middle
AVAudioEngine is a
powerful, feature-rich API.

00:28:25.636 --> 00:28:28.946 A:middle
It simplifies working
with real-time audio.

00:28:30.546 --> 00:28:34.066 A:middle
It enables you to work with
multichannel audio and 3D audio.

00:28:34.976 --> 00:28:36.196 A:middle
And now, you can build games

00:28:36.196 --> 00:28:38.166 A:middle
with really rich audio
experiences on your Watch.

00:28:38.766 --> 00:28:43.976 A:middle
And it supersedes our
AUGraph and OpenAL APIs.

00:28:44.766 --> 00:28:47.086 A:middle
So we've talked a bit about the
Engine in previous sessions,

00:28:47.086 --> 00:28:50.556 A:middle
so we encourage you to
check those out if you can.

00:28:51.326 --> 00:28:53.246 A:middle
And at this point, I'd like to
hand it over to my colleague,

00:28:53.246 --> 00:28:54.576 A:middle
Doug, to keep it
rolling from here.

00:28:55.066 --> 00:28:55.286 A:middle
Doug?

00:28:56.516 --> 00:29:00.626 A:middle
[ Applause ]


WEBVTT
X-TIMESTAMP-MAP=MPEGTS:181083,LOCAL:00:00:00.000

00:28:56.516 --> 00:29:00.626 A:middle
[ Applause ]

00:29:01.126 --> 00:29:01.796 A:middle
&gt;&gt; Thank you, Saleem.

00:29:02.806 --> 00:29:05.476 A:middle
So, I'd like to continue
our tour

00:29:05.476 --> 00:29:07.136 A:middle
through the audio APIs here.

00:29:07.646 --> 00:29:12.436 A:middle
We talked about real-time audio
in passing with AVAudioEngine.

00:29:13.236 --> 00:29:14.896 A:middle
Saleem emphasized that,

00:29:15.676 --> 00:29:19.236 A:middle
while the audio processing is
happening in real-time context,

00:29:19.466 --> 00:29:22.286 A:middle
we're controlling it from
non-real-time context.

00:29:22.286 --> 00:29:24.026 A:middle
And that's the essence
of its simplicity.

00:29:24.706 --> 00:29:26.766 A:middle
But there are times when
you actually want to do work

00:29:26.766 --> 00:29:30.316 A:middle
in that real-time
process, or context.

00:29:30.566 --> 00:29:32.146 A:middle
So I'd like to go
into that a bit.

00:29:32.586 --> 00:29:34.576 A:middle
So, what is real-time audio?

00:29:34.696 --> 00:29:37.486 A:middle
The use cases where
we need to do things

00:29:37.546 --> 00:29:40.036 A:middle
in real-time are
characterized by low latency.

00:29:40.826 --> 00:29:44.536 A:middle
Possibly the oldest
example I'm familiar

00:29:44.536 --> 00:29:47.296 A:middle
with on our platforms is
with music applications.

00:29:47.696 --> 00:29:51.006 A:middle
For example, you may
be synthesizing a sound

00:29:51.086 --> 00:29:54.086 A:middle
when the user presses a
key on the MIDI keyboard.

00:29:54.766 --> 00:29:56.986 A:middle
And we want to minimize
the time from when

00:29:56.986 --> 00:30:00.036 A:middle
that MIDI note was struck
to when the note plays.


WEBVTT
X-TIMESTAMP-MAP=MPEGTS:181083,LOCAL:00:00:00.000

00:29:56.986 --> 00:30:00.036 A:middle
that MIDI note was struck
to when the note plays.

00:30:00.516 --> 00:30:04.316 A:middle
And so we have real-time audio
effects like guitar pedals.

00:30:05.296 --> 00:30:09.056 A:middle
We want to minimize the time it
takes from when the audio input

00:30:09.056 --> 00:30:11.096 A:middle
of the guitar comes
into the computer

00:30:12.076 --> 00:30:15.846 A:middle
through which we process it,
apply delays, distortion,

00:30:15.846 --> 00:30:17.646 A:middle
and then send it back
out to the amplifier.

00:30:18.246 --> 00:30:19.586 A:middle
So we need low latency there

00:30:19.586 --> 00:30:21.966 A:middle
so that the instrument,
again, is responsive.

00:30:22.596 --> 00:30:26.186 A:middle
Telephony is also characterized
by low latency requirements.

00:30:26.876 --> 00:30:29.986 A:middle
We've all been on phone calls
with people in other countries

00:30:29.986 --> 00:30:31.966 A:middle
and had very long delay times.

00:30:31.966 --> 00:30:33.756 A:middle
It's no good in telephony.

00:30:34.156 --> 00:30:35.706 A:middle
We do a lot of signal
processing.

00:30:36.076 --> 00:30:37.536 A:middle
We need to keep the
latency down.

00:30:38.416 --> 00:30:41.646 A:middle
Also, in game engines, we
like to keep the latency down.

00:30:42.166 --> 00:30:43.476 A:middle
The user is doing things --

00:30:43.476 --> 00:30:45.926 A:middle
interacting with
joysticks, whatever.

00:30:46.506 --> 00:30:49.006 A:middle
We want to produce those
sounds as quickly as possible.

00:30:49.006 --> 00:30:51.126 A:middle
Sometimes, we want to
manipulate those sounds

00:30:51.396 --> 00:30:52.366 A:middle
as they're being rendered.

00:30:53.026 --> 00:30:55.716 A:middle
Or maybe we just have
an existing game engine.

00:30:56.236 --> 00:31:00.066 A:middle
In all these cases, we have a
need to write code that runs


WEBVTT
X-TIMESTAMP-MAP=MPEGTS:181083,LOCAL:00:00:00.000

00:30:56.236 --> 00:31:00.066 A:middle
In all these cases, we have a
need to write code that runs

00:31:00.306 --> 00:31:01.536 A:middle
in a real-time context.

00:31:02.776 --> 00:31:10.476 A:middle
In this real-time context,
the main characteristic of --

00:31:10.476 --> 00:31:13.126 A:middle
our constraint is that we're
operating under deadlines.

00:31:14.176 --> 00:31:16.396 A:middle
Right? Every some-number
of milliseconds,

00:31:16.456 --> 00:31:20.106 A:middle
the system is waking us up,
asking us to produce some audio

00:31:20.106 --> 00:31:22.186 A:middle
for that equally-small
slice of time.

00:31:22.696 --> 00:31:26.306 A:middle
And we either accomplish it
and produce audio seamlessly.

00:31:26.836 --> 00:31:30.236 A:middle
Or if we fail, if we take too
long to produce that audio,

00:31:30.806 --> 00:31:32.916 A:middle
we create a gap in the output.

00:31:32.916 --> 00:31:35.236 A:middle
And the user hears
that as a glitch.

00:31:36.006 --> 00:31:38.306 A:middle
And this is a very small
interval that we have

00:31:38.786 --> 00:31:40.276 A:middle
to create our audio in.

00:31:40.276 --> 00:31:43.226 A:middle
Our deadlines are typically
as small as 3 milliseconds.

00:31:43.226 --> 00:31:46.726 A:middle
And 20 milliseconds,
which is default on iOS,

00:31:46.726 --> 00:31:50.496 A:middle
is still a pretty
constrained deadline.

00:31:51.536 --> 00:31:52.986 A:middle
So, in this environment, we have

00:31:52.986 --> 00:31:55.146 A:middle
to be really careful
about what we do.

00:31:56.176 --> 00:31:57.526 A:middle
We can't really block.

00:31:57.716 --> 00:31:59.616 A:middle
We can't allocate memory.

00:31:59.726 --> 00:32:01.496 A:middle
We can't use mutexes.


WEBVTT
X-TIMESTAMP-MAP=MPEGTS:181083,LOCAL:00:00:00.000

00:31:59.726 --> 00:32:01.496 A:middle
We can't use mutexes.

00:32:01.566 --> 00:32:04.226 A:middle
We can't access the
file system or sockets.

00:32:04.636 --> 00:32:05.416 A:middle
We can't log.

00:32:06.026 --> 00:32:07.866 A:middle
We can't even call
a dispatch "async"

00:32:07.946 --> 00:32:10.076 A:middle
because it allocates
continuations.

00:32:10.876 --> 00:32:14.086 A:middle
And we have to be careful not
to interact with the Objective-C

00:32:14.086 --> 00:32:18.506 A:middle
and Swift runtimes because they
are not entirely real-time safe.

00:32:18.576 --> 00:32:21.786 A:middle
There are cases when they,
too, will take mutexes.

00:32:22.756 --> 00:32:24.256 A:middle
So that's a partial list.

00:32:24.306 --> 00:32:25.716 A:middle
There other things we can't do.

00:32:25.716 --> 00:32:27.776 A:middle
The primary thing
to ask yourself is,

00:32:28.106 --> 00:32:32.136 A:middle
"Does this thing I'm doing
allocate memory or use mutexes?"

00:32:32.136 --> 00:32:35.266 A:middle
And if the answer is yes,
then it's not real-time safe.

00:32:36.076 --> 00:32:37.216 A:middle
Well, what can we do?

00:32:37.456 --> 00:32:39.896 A:middle
I'll show you an example
of that in a little bit.

00:32:41.396 --> 00:32:44.086 A:middle
But, first, I'd like
to just talk

00:32:44.086 --> 00:32:47.186 A:middle
about how we manage this problem

00:32:47.186 --> 00:32:50.296 A:middle
of packaging real-time
audio components.

00:32:50.596 --> 00:32:53.736 A:middle
And we do this with an API
set called Audio Units.

00:32:54.446 --> 00:32:58.626 A:middle
So this is a way
for us to package --

00:32:58.626 --> 00:33:00.616 A:middle
and for you, for that matter,
as another developer --


WEBVTT
X-TIMESTAMP-MAP=MPEGTS:181083,LOCAL:00:00:00.000

00:32:58.626 --> 00:33:00.616 A:middle
and for you, for that matter,
as another developer --

00:33:01.036 --> 00:33:03.416 A:middle
to package your signal
processing and modules

00:33:03.416 --> 00:33:06.316 A:middle
that can be reused in
other applications.

00:33:06.776 --> 00:33:11.386 A:middle
And it also provides an API
to manage the transitions

00:33:11.386 --> 00:33:15.516 A:middle
and interactions between
your non-real-time context

00:33:15.516 --> 00:33:17.596 A:middle
and your real-time
rendering context.

00:33:19.286 --> 00:33:22.246 A:middle
So, as an app developer,
you can host Audio Units.

00:33:23.296 --> 00:33:25.736 A:middle
That means you can let
the user choose one,

00:33:25.736 --> 00:33:28.526 A:middle
or you can simply
hardcode references

00:33:28.556 --> 00:33:30.026 A:middle
to system built-in units.

00:33:31.016 --> 00:33:33.466 A:middle
You can also build
your own Audio Units.

00:33:34.126 --> 00:33:36.976 A:middle
You can build them as app
extensions or plug-ins.

00:33:37.626 --> 00:33:41.886 A:middle
And you can also simply
register an Audio Unit privately

00:33:41.886 --> 00:33:42.876 A:middle
to your application.

00:33:42.876 --> 00:33:46.616 A:middle
And this is useful, for example,
if you've got some small piece

00:33:46.616 --> 00:33:48.516 A:middle
of signal processing
that you want to use

00:33:48.516 --> 00:33:51.046 A:middle
in the context of AVAudioEngine.

00:33:53.376 --> 00:33:56.186 A:middle
So, underneath Audio Units,

00:33:56.186 --> 00:33:58.636 A:middle
we have an even more
fundamental API

00:33:59.236 --> 00:34:00.836 A:middle
which we call Audio Components.


WEBVTT
X-TIMESTAMP-MAP=MPEGTS:181083,LOCAL:00:00:00.000

00:33:59.236 --> 00:34:00.836 A:middle
which we call Audio Components.

00:34:02.076 --> 00:34:06.346 A:middle
So this is a set of APIs in
the AudioToolbox framework.

00:34:07.006 --> 00:34:09.815 A:middle
The framework maintains
a registry of all

00:34:09.815 --> 00:34:11.565 A:middle
of the components on the system.

00:34:13.096 --> 00:34:16.866 A:middle
Every component has a type,
subtype, and manufacturer.

00:34:16.866 --> 00:34:18.255 A:middle
These are 4-character codes.

00:34:18.706 --> 00:34:19.985 A:middle
And those serve as the key

00:34:20.076 --> 00:34:22.246 A:middle
for discovering them
and registering them.

00:34:24.025 --> 00:34:25.856 A:middle
And there are a number
of different kinds

00:34:25.856 --> 00:34:27.525 A:middle
of Audio Components types.

00:34:28.126 --> 00:34:32.406 A:middle
The two main categories
of types are Audio Units

00:34:32.406 --> 00:34:33.716 A:middle
and Audio Codecs.

00:34:34.346 --> 00:34:37.065 A:middle
But amongst the Audio Units,
we have input/output units,

00:34:37.096 --> 00:34:38.806 A:middle
generators, effects,
instruments,

00:34:39.235 --> 00:34:41.916 A:middle
converters, mixers as well.

00:34:42.315 --> 00:34:45.275 A:middle
And amongst codecs, we
have encoders and decoders.

00:34:45.565 --> 00:34:48.206 A:middle
We also have audio file
components on macOS.

00:34:48.835 --> 00:34:55.136 A:middle
Getting into the
implementation of components,

00:34:55.996 --> 00:34:57.606 A:middle
there are a number
of different ways

00:34:57.706 --> 00:34:59.266 A:middle
that components are implemented.

00:34:59.266 --> 00:35:01.736 A:middle
Some of them you'll need to know
about if you're writing them.


WEBVTT
X-TIMESTAMP-MAP=MPEGTS:181083,LOCAL:00:00:00.000

00:34:59.266 --> 00:35:01.736 A:middle
Some of them you'll need to know
about if you're writing them.

00:35:02.146 --> 00:35:03.576 A:middle
And others, it's
just for background.

00:35:04.726 --> 00:35:08.966 A:middle
The most highly-recommended
way to create a component now

00:35:08.966 --> 00:35:10.156 A:middle
if it's an Audio Unit is

00:35:10.156 --> 00:35:12.616 A:middle
to create an Audio Unit
application extension.

00:35:13.246 --> 00:35:19.116 A:middle
We introduced this last year
with our 10.11 and 9.0 releases.

00:35:20.186 --> 00:35:21.466 A:middle
So those are app extensions.

00:35:21.516 --> 00:35:25.566 A:middle
Before that, Audio Units were
packaged in component bundles --

00:35:25.566 --> 00:35:27.176 A:middle
as were audio codecs, et cetera.

00:35:28.906 --> 00:35:31.026 A:middle
That goes back to
Mac OS 10.1 or so.

00:35:33.336 --> 00:35:34.336 A:middle
Interestingly enough,

00:35:34.336 --> 00:35:38.916 A:middle
audio components also include
inter-app audio nodes on iOS.

00:35:39.376 --> 00:35:41.746 A:middle
Node applications
register themselves

00:35:42.086 --> 00:35:45.756 A:middle
with a component subtype
and manufacturer key.

00:35:46.416 --> 00:35:49.646 A:middle
And host applications
discover node applications

00:35:50.236 --> 00:35:51.876 A:middle
through the Audio
Component Manager.

00:35:53.696 --> 00:35:56.326 A:middle
And finally, you can register
-- as I mentioned before --

00:35:56.326 --> 00:35:58.556 A:middle
you can register your own
components for the use

00:35:58.556 --> 00:35:59.656 A:middle
of your own application.


WEBVTT
X-TIMESTAMP-MAP=MPEGTS:181083,LOCAL:00:00:00.000

00:36:00.506 --> 00:36:01.546 A:middle
And just for completeness,

00:36:01.586 --> 00:36:03.876 A:middle
there are some Apple
built-in components.

00:36:04.296 --> 00:36:06.966 A:middle
On iOS, they're linked
into the AudioToolbox.

00:36:07.726 --> 00:36:12.376 A:middle
So those are the flavors of
component implementations.

00:36:12.806 --> 00:36:16.336 A:middle
Now I'd like to focus in on just
one kind of component here --

00:36:16.726 --> 00:36:18.516 A:middle
the audio input/output unit.

00:36:18.746 --> 00:36:19.826 A:middle
This is and Audio Unit.

00:36:21.886 --> 00:36:25.786 A:middle
And it's probably the one
component that you'll use

00:36:25.786 --> 00:36:27.206 A:middle
if you don't use any other.

00:36:28.086 --> 00:36:31.396 A:middle
And the reason is that this
is the preferred interface

00:36:31.446 --> 00:36:34.496 A:middle
to the system's basic
audio input/output path.

00:36:35.336 --> 00:36:39.826 A:middle
Now, on macOS, that basic path
is in the Core Audio framework.

00:36:40.346 --> 00:36:41.576 A:middle
We call it the Audio HAL,

00:36:42.366 --> 00:36:44.176 A:middle
and it's a pretty
low-level interface.

00:36:44.606 --> 00:36:48.986 A:middle
It makes its clients deal with
interesting stream typologies

00:36:48.986 --> 00:36:50.986 A:middle
on multichannel devices
for example.

00:36:52.026 --> 00:36:56.266 A:middle
So, it's much easier to deal
with the Audio HAL interface

00:36:56.626 --> 00:36:58.546 A:middle
through an audio
input/output unit.


WEBVTT
X-TIMESTAMP-MAP=MPEGTS:181083,LOCAL:00:00:00.000

00:37:00.066 --> 00:37:02.416 A:middle
On iOS, you don't
even have access

00:37:02.476 --> 00:37:04.316 A:middle
to the Core Audio framework.

00:37:04.526 --> 00:37:05.556 A:middle
It's not public there.

00:37:05.926 --> 00:37:07.986 A:middle
You have to use an
audio input/output unit

00:37:08.686 --> 00:37:13.276 A:middle
as your lowest-level way to get
audio in and out of the system.

00:37:15.336 --> 00:37:17.506 A:middle
And our preferred interface now

00:37:18.376 --> 00:37:21.626 A:middle
for audio input/output
units is AUAudioUnit

00:37:21.626 --> 00:37:23.266 A:middle
and the AudioToolbox framework.

00:37:24.146 --> 00:37:26.936 A:middle
If you've been working
with our APIs for a while,

00:37:27.306 --> 00:37:31.246 A:middle
you're familiar with version
2 Audio Units that are part

00:37:31.246 --> 00:37:36.956 A:middle
of the system AUHAL on the macOS
and AURemoteIO on iOS as well

00:37:36.956 --> 00:37:41.566 A:middle
as Watch -- actually, I'm not
sure we have it available there.

00:37:41.566 --> 00:37:46.296 A:middle
But in any case, AUAudioUnit
is your new modern interface

00:37:46.536 --> 00:37:48.916 A:middle
to this low-level I/O mechanism.

00:37:50.136 --> 00:37:53.026 A:middle
So I'd like to show
you what it looks

00:37:53.026 --> 00:37:58.806 A:middle
like to use AUAudioUnit
to do AudioIO.

00:37:59.346 --> 00:38:02.026 A:middle
So I've written a simple
program in Swift here


WEBVTT
X-TIMESTAMP-MAP=MPEGTS:181083,LOCAL:00:00:00.000

00:37:59.346 --> 00:38:02.026 A:middle
So I've written a simple
program in Swift here

00:38:02.816 --> 00:38:04.356 A:middle
that generates a square wave.

00:38:05.386 --> 00:38:08.336 A:middle
And here's my signal processing.

00:38:08.586 --> 00:38:10.716 A:middle
I mentioned earlier I
would show you what kinds

00:38:10.716 --> 00:38:11.996 A:middle
of things you can do here.

00:38:12.876 --> 00:38:16.226 A:middle
So this wave generator
shows you.

00:38:16.676 --> 00:38:21.146 A:middle
You can basically read memory,
write memory, and do math.

00:38:21.876 --> 00:38:24.286 A:middle
And that's all that's
going on here.

00:38:24.286 --> 00:38:27.616 A:middle
It's making the simplest of all
wave forms -- the square wave --

00:38:27.616 --> 00:38:30.376 A:middle
at least simplest from a
computational point of view.

00:38:31.836 --> 00:38:34.286 A:middle
So that class is called
SquareWaveGenerator.

00:38:35.526 --> 00:38:38.176 A:middle
And let's see how to play
a SqaureWaveGenerator

00:38:38.306 --> 00:38:39.806 A:middle
from an AUAudioUnit.

00:38:41.706 --> 00:38:43.806 A:middle
So the first thing we
do is create an audio

00:38:43.806 --> 00:38:45.076 A:middle
component description.

00:38:46.076 --> 00:38:49.316 A:middle
And this tells us which
component to go look for.

00:38:50.086 --> 00:38:51.286 A:middle
The type is output.

00:38:51.446 --> 00:38:55.116 A:middle
The subtype is something I chose
here depending on platform --

00:38:55.666 --> 00:38:57.436 A:middle
either RemoteIO or HalOutput.

00:38:58.126 --> 00:39:01.546 A:middle
We've got the Apple manufacturer
and some unused flags.


WEBVTT
X-TIMESTAMP-MAP=MPEGTS:181083,LOCAL:00:00:00.000

00:38:58.126 --> 00:39:01.546 A:middle
We've got the Apple manufacturer
and some unused flags.

00:39:02.886 --> 00:39:06.916 A:middle
Then I can create my AUAudioUnit
using my component description.

00:39:08.036 --> 00:39:09.786 A:middle
So I'll get that
unit that I wanted.

00:39:11.376 --> 00:39:15.406 A:middle
And now it's open and I
can start to configure it.

00:39:16.566 --> 00:39:19.376 A:middle
So the first thing I
want to do here is find

00:39:19.376 --> 00:39:23.236 A:middle
out how many channels of
audio are on the system.

00:39:23.236 --> 00:39:27.776 A:middle
There are ways to do this
with AVAudioSession on iOS.

00:39:28.406 --> 00:39:32.336 A:middle
But most simply and portably,

00:39:32.416 --> 00:39:36.476 A:middle
you can simply query
the outputBusses

00:39:36.796 --> 00:39:39.316 A:middle
of the input/output unit.

00:39:39.766 --> 00:39:45.006 A:middle
And outputBus[0] is the
output-directed stream.

00:39:45.616 --> 00:39:46.986 A:middle
So I'm going to fetch
its format,

00:39:46.986 --> 00:39:48.466 A:middle
and that's my hardware format.

00:39:48.996 --> 00:39:52.716 A:middle
Now this hardware format
may be something exotic.

00:39:52.716 --> 00:39:55.346 A:middle
It may be inertly for example.

00:39:55.346 --> 00:39:58.456 A:middle
And I don't know that I
want to deal with that.

00:39:58.456 --> 00:40:01.476 A:middle
So I'm just going to
create a renderFormat.


WEBVTT
X-TIMESTAMP-MAP=MPEGTS:181083,LOCAL:00:00:00.000

00:39:58.456 --> 00:40:01.476 A:middle
So I'm just going to
create a renderFormat.

00:40:01.796 --> 00:40:04.736 A:middle
That is a standard format
with the same sample rate.

00:40:05.446 --> 00:40:06.816 A:middle
And some number of channels.

00:40:07.296 --> 00:40:11.596 A:middle
Just to keep things short
and simple, I'm only going

00:40:11.596 --> 00:40:13.386 A:middle
to render two channels,

00:40:13.386 --> 00:40:15.436 A:middle
regardless of the
hardware channel count.

00:40:16.136 --> 00:40:17.816 A:middle
So that's my renderFormat.

00:40:18.386 --> 00:40:22.646 A:middle
Now, I can tell the I/O unit,
"This is the format I want

00:40:22.646 --> 00:40:24.536 A:middle
to give you on inputBus[0]."

00:40:24.536 --> 00:40:29.976 A:middle
So, having done this, the unit
will now convert my renderFormat

00:40:30.186 --> 00:40:31.346 A:middle
to the hardwareFormat.

00:40:31.946 --> 00:40:34.006 A:middle
And in this case, on my MacBook,

00:40:34.006 --> 00:40:37.806 A:middle
it's going to take this
deinterleaved floating point

00:40:38.426 --> 00:40:41.756 A:middle
and convert it to interleaved
floating point buffers.

00:40:43.636 --> 00:40:45.686 A:middle
OK. So, next, I'm going

00:40:45.686 --> 00:40:47.436 A:middle
to create my square
wave generators.

00:40:48.276 --> 00:40:50.906 A:middle
If you're a music and
math geek like me,

00:40:51.566 --> 00:40:54.836 A:middle
you know that A440 is
there, and multiplying it

00:40:54.836 --> 00:40:58.206 A:middle
by 1.5 will give you
a fifth above it.

00:40:59.676 --> 00:41:02.106 A:middle
So I'm going to render
A to my left channel


WEBVTT
X-TIMESTAMP-MAP=MPEGTS:181083,LOCAL:00:00:00.000

00:40:59.676 --> 00:41:02.106 A:middle
So I'm going to render
A to my left channel

00:41:02.106 --> 00:41:04.076 A:middle
and E to my right channel.

00:41:05.436 --> 00:41:08.796 A:middle
And here's the code that will
run in the real-time context.

00:41:10.406 --> 00:41:13.746 A:middle
There's a lot of
parameters here,

00:41:13.996 --> 00:41:16.176 A:middle
and I actually only
need a couple of them.

00:41:16.176 --> 00:41:20.096 A:middle
I only need the frameCount
and the rawBufferList.

00:41:20.966 --> 00:41:25.416 A:middle
The rawBufferList is a
difficult, low-level C structure

00:41:25.916 --> 00:41:32.446 A:middle
which I can rewrap in Swift
using an overlay on the SDK.

00:41:33.006 --> 00:41:35.306 A:middle
And this takes the
audio bufferList

00:41:35.966 --> 00:41:39.306 A:middle
and makes it look something
like a vector or array.

00:41:40.816 --> 00:41:42.646 A:middle
So having converted
the rawBufferList

00:41:42.646 --> 00:41:45.916 A:middle
to the nice Swift wrapper,
I can query its count.

00:41:46.596 --> 00:41:48.636 A:middle
And if I got at least
one buffer,

00:41:48.636 --> 00:41:50.316 A:middle
then I can render
the left channel.

00:41:50.866 --> 00:41:53.566 A:middle
If I got at least two buffers,
I can render the right channel.

00:41:54.326 --> 00:41:56.826 A:middle
And that's all the work
I'm doing right here.

00:41:56.826 --> 00:41:59.206 A:middle
Of course, there's more work
inside the wave generators,

00:41:59.486 --> 00:42:01.476 A:middle
but that's all of the
real-time context work.


WEBVTT
X-TIMESTAMP-MAP=MPEGTS:181083,LOCAL:00:00:00.000

00:41:59.486 --> 00:42:01.476 A:middle
but that's all of the
real-time context work.

00:42:02.696 --> 00:42:03.986 A:middle
So, now, I'm all setup.

00:42:04.056 --> 00:42:05.586 A:middle
I'm ready to render.

00:42:05.996 --> 00:42:08.826 A:middle
So I'm going to tell
the I/O unit,

00:42:08.826 --> 00:42:11.516 A:middle
"Do any allocations you need
to do to start rendering."

00:42:12.116 --> 00:42:14.006 A:middle
Then, I can have it
actually start the hardware,

00:42:14.776 --> 00:42:16.666 A:middle
run for 3 seconds, and stop.

00:42:16.666 --> 00:42:18.236 A:middle
And that's the end of
this simple program.

00:42:19.516 --> 00:42:22.666 A:middle
[ Monotone ]

00:42:23.166 --> 00:42:24.656 A:middle
So, that's AUAudioUnit.

00:42:26.066 --> 00:42:30.406 A:middle
I'd like to turn next briefly to
some other kinds of Audio Units.

00:42:31.016 --> 00:42:34.996 A:middle
We have effects which take audio
input, produce audio output.

00:42:35.436 --> 00:42:38.616 A:middle
Instruments which take something
resembling MIDI as input

00:42:39.026 --> 00:42:40.606 A:middle
and also produce audio output.

00:42:41.176 --> 00:42:44.486 A:middle
And generators which
produce audio output

00:42:44.616 --> 00:42:48.566 A:middle
without anything going in except
maybe some parametric control.

00:42:48.876 --> 00:42:52.866 A:middle
If I were to repackage my square
wave generator as an Audio Unit,

00:42:52.976 --> 00:42:54.176 A:middle
I would make it a generator.

00:42:54.906 --> 00:43:00.296 A:middle
So to host these
kinds of Audio Units,


WEBVTT
X-TIMESTAMP-MAP=MPEGTS:181083,LOCAL:00:00:00.000

00:42:54.906 --> 00:43:00.296 A:middle
So to host these
kinds of Audio Units,

00:43:00.356 --> 00:43:02.446 A:middle
you can also use AUAudioUnit.

00:43:03.566 --> 00:43:07.326 A:middle
You can use a separate block
to provide input to it.

00:43:07.566 --> 00:43:10.186 A:middle
It's very similar to the
output provider block

00:43:10.186 --> 00:43:12.696 A:middle
that you saw on the I/O unit.

00:43:13.646 --> 00:43:16.126 A:middle
You can chain together
these render blocks of units

00:43:16.196 --> 00:43:18.226 A:middle
to create your own
custom typologies.

00:43:18.936 --> 00:43:21.876 A:middle
You can control the units
using their parameters.

00:43:22.956 --> 00:43:26.266 A:middle
And also, many units,
especially third-party units,

00:43:26.856 --> 00:43:28.576 A:middle
have nice user interfaces.

00:43:28.576 --> 00:43:31.076 A:middle
As a hosting application,
you can obtain

00:43:31.076 --> 00:43:34.776 A:middle
that audio unit's view,
display it in your application,

00:43:34.836 --> 00:43:38.826 A:middle
and let the user
interact with it.

00:43:39.766 --> 00:43:41.846 A:middle
Now if you'd like to
write your own Audio Unit,

00:43:43.966 --> 00:43:46.656 A:middle
the way I would start
is just building it

00:43:46.656 --> 00:43:48.386 A:middle
within the context of an app.

00:43:48.676 --> 00:43:50.446 A:middle
This lets you debug
without worrying

00:43:50.446 --> 00:43:53.436 A:middle
about inter-process
communication issues.

00:43:53.436 --> 00:43:54.486 A:middle
It's all in one process.

00:43:54.576 --> 00:43:57.456 A:middle
So, you start by
subclassing AUAudioUnit.

00:43:58.036 --> 00:44:01.506 A:middle
You register it as a component
using this class method


WEBVTT
X-TIMESTAMP-MAP=MPEGTS:181083,LOCAL:00:00:00.000

00:43:58.036 --> 00:44:01.506 A:middle
You register it as a component
using this class method

00:44:01.506 --> 00:44:02.226 A:middle
of AUAudioUnit.

00:44:02.936 --> 00:44:03.886 A:middle
Then, you can debug it.

00:44:05.256 --> 00:44:06.756 A:middle
And once you've done that --

00:44:06.866 --> 00:44:09.086 A:middle
and if you decide you'd
like to distribute it

00:44:09.166 --> 00:44:11.236 A:middle
as an Audio Unit extension --

00:44:11.616 --> 00:44:14.706 A:middle
you can take that same
AUAudioUnit subclass.

00:44:15.396 --> 00:44:17.466 A:middle
You might fine-tune and
polish it some more.

00:44:18.286 --> 00:44:20.986 A:middle
But then you have to do a
small amount of additional work

00:44:21.336 --> 00:44:23.716 A:middle
to package this as an
Audio Unit extension.

00:44:24.206 --> 00:44:25.586 A:middle
So you've got an extension.

00:44:25.586 --> 00:44:27.236 A:middle
You can embed it
in an application.

00:44:27.486 --> 00:44:32.726 A:middle
You can sell that
application on the App Store.

00:44:33.696 --> 00:44:35.796 A:middle
So I'd like to have
my colleague, Torrey,

00:44:35.796 --> 00:44:39.316 A:middle
now show you some of the power
of Audio Unit extensions.

00:44:39.686 --> 00:44:41.736 A:middle
We've had some developers
doing some really cool things

00:44:41.736 --> 00:44:42.656 A:middle
with it in the last year.

00:44:43.676 --> 00:44:44.446 A:middle
&gt;&gt; How is everybody doing?

00:44:45.526 --> 00:44:46.726 A:middle
Happy to be at WWDC?

00:44:47.516 --> 00:44:50.596 A:middle
[ Applause ]

00:44:51.096 --> 00:44:52.416 A:middle
Let's make some noise.

00:44:52.716 --> 00:44:55.256 A:middle
I'm going to start here by
launching -- well, first of all,

00:44:55.256 --> 00:44:56.376 A:middle
I have my instrument here.

00:44:56.676 --> 00:44:58.116 A:middle
This is my iPad Pro.

00:44:58.116 --> 00:45:00.526 A:middle
And I'm going to start by
launching Arturia iSEM --


WEBVTT
X-TIMESTAMP-MAP=MPEGTS:181083,LOCAL:00:00:00.000

00:44:58.116 --> 00:45:00.526 A:middle
And I'm going to start by
launching Arturia iSEM --

00:45:01.306 --> 00:45:03.216 A:middle
a very powerful synthesizer
application.

00:45:03.216 --> 00:45:05.956 A:middle
And I have a synth trumpet
sound here that I like.

00:45:06.516 --> 00:45:10.546 A:middle
[ Music ]

00:45:11.046 --> 00:45:13.806 A:middle
So I like this sound
and I want to put it

00:45:13.806 --> 00:45:15.126 A:middle
in a track that I'm working on.

00:45:15.426 --> 00:45:18.996 A:middle
This is going to serve as our
Audio Unit plug-in application.

00:45:19.176 --> 00:45:21.976 A:middle
And now I'm going to launch
GarageBand, which is going

00:45:21.976 --> 00:45:24.496 A:middle
to serve as our Audio
Unit host application.

00:45:24.496 --> 00:45:28.466 A:middle
Now, in GarageBand, I have a
sick beat I've been working

00:45:28.466 --> 00:45:31.516 A:middle
on that I'm calling WWDC Demo.

00:45:31.786 --> 00:45:32.976 A:middle
Let's listen to it.

00:45:33.516 --> 00:45:43.046 A:middle
[ Music ]

00:45:43.546 --> 00:45:45.976 A:middle
Well move into what I call
"the verse portion" next.

00:45:46.516 --> 00:45:54.556 A:middle
[ Music ]

00:45:55.056 --> 00:45:57.996 A:middle
And next, we're going to
work on this chorus here.

00:45:57.996 --> 00:45:59.986 A:middle
This is supposed to be
the climax of the song.

00:45:59.986 --> 00:46:01.166 A:middle
I want some motion.


WEBVTT
X-TIMESTAMP-MAP=MPEGTS:181083,LOCAL:00:00:00.000

00:45:59.986 --> 00:46:01.166 A:middle
I want some motion.

00:46:01.166 --> 00:46:02.006 A:middle
I want some tension.

00:46:02.006 --> 00:46:05.526 A:middle
And let's create that by
bringing in an Audio Unit.

00:46:06.426 --> 00:46:07.966 A:middle
I'm going to add
a new track here.

00:46:09.036 --> 00:46:12.056 A:middle
Adding an instrument, I'll see
Audio Units is an option here.

00:46:12.946 --> 00:46:15.336 A:middle
If I select this, then I can
see all of the Audio Units

00:46:15.336 --> 00:46:16.556 A:middle
that are hosted here
on the system.

00:46:17.226 --> 00:46:21.456 A:middle
Right now, I see Arturia iSEM
because I practice this at home.

00:46:22.776 --> 00:46:25.226 A:middle
Selecting iSEM, GarageBand
is now going

00:46:25.226 --> 00:46:28.376 A:middle
to give me an onscreen MIDI
controller that I can use here.

00:46:28.886 --> 00:46:32.716 A:middle
It's complete with the scale
transforms and arpeggiator here

00:46:32.716 --> 00:46:35.306 A:middle
that I'm going to make use of
because I like a lot of motion.

00:46:35.366 --> 00:46:39.256 A:middle
Over here on the left, you
can see a Pitch/Mod Wheel.

00:46:39.306 --> 00:46:40.946 A:middle
You can even modify
the velocity.

00:46:41.326 --> 00:46:44.146 A:middle
And here is the view that the
Audio Unit has provided to me

00:46:44.146 --> 00:46:45.316 A:middle
that I can actually tweak.

00:46:45.866 --> 00:46:48.486 A:middle
For now, I'm going to record
in a little piece here

00:46:48.486 --> 00:46:50.546 A:middle
and see what it sounds
like in context.

00:46:50.676 --> 00:46:51.236 A:middle
So --

00:46:52.516 --> 00:47:03.686 A:middle
[ Music ]


WEBVTT
X-TIMESTAMP-MAP=MPEGTS:181083,LOCAL:00:00:00.000

00:46:52.516 --> 00:47:03.686 A:middle
[ Music ]

00:47:04.186 --> 00:47:05.076 A:middle
All right, pretty good.

00:47:05.076 --> 00:47:08.976 A:middle
Let's see what it
sounds like in context.

00:47:09.516 --> 00:47:13.636 A:middle
[ Music ]

00:47:14.136 --> 00:47:14.996 A:middle
There we go.

00:47:14.996 --> 00:47:17.006 A:middle
That's the tension that I want.

00:47:17.006 --> 00:47:19.376 A:middle
Now, let me dig in
here a little bit more

00:47:19.376 --> 00:47:22.366 A:middle
and show you what I've done.

00:47:22.566 --> 00:47:23.776 A:middle
I'm going to edit here.

00:47:23.856 --> 00:47:28.146 A:middle
And I'll look into this
loop a little bit more.

00:47:28.146 --> 00:47:31.066 A:middle
There are two observations
that I'd like you to make here.

00:47:31.166 --> 00:47:33.726 A:middle
The first one is that
these are MIDI events.

00:47:34.116 --> 00:47:36.606 A:middle
The difference between
using inter-app audio

00:47:36.606 --> 00:47:38.086 A:middle
and using Audio Units

00:47:38.086 --> 00:47:40.566 A:middle
as a plug-in is you'll
actually get MIDI notes here,

00:47:40.566 --> 00:47:42.636 A:middle
which is much easier
to edit after the fact.

00:47:43.046 --> 00:47:45.826 A:middle
The other observation I'd
like you to make here is

00:47:45.826 --> 00:47:47.956 A:middle
that you see these
individual MIDI notes here

00:47:48.016 --> 00:47:50.926 A:middle
but you saw me play one
big, fat-fingered chord.

00:47:51.406 --> 00:47:53.386 A:middle
So, it's because
I've taken advantage

00:47:53.386 --> 00:47:55.626 A:middle
of the arpeggiator that's
built into GarageBand

00:47:55.626 --> 00:47:56.996 A:middle
that I've got these
individual notes.

00:47:57.356 --> 00:47:59.146 A:middle
And I can play around
with these if I want to

00:47:59.146 --> 00:48:00.946 A:middle
and make them sound
a bit more human.


WEBVTT
X-TIMESTAMP-MAP=MPEGTS:181083,LOCAL:00:00:00.000

00:47:59.146 --> 00:48:00.946 A:middle
and make them sound
a bit more human.

00:48:01.356 --> 00:48:03.926 A:middle
But I'm happy with this
recording as it is.

00:48:04.496 --> 00:48:08.186 A:middle
The last thing that I'd actually
like to show you here is, first,

00:48:08.186 --> 00:48:12.046 A:middle
I'm going to copy this
into the adjacent cell.

00:48:13.806 --> 00:48:19.476 A:middle
And I told you earlier that the
Audio Unit view that's provided

00:48:19.476 --> 00:48:20.496 A:middle
here is actually interactive.

00:48:20.496 --> 00:48:21.556 A:middle
It's not just a pretty picture.

00:48:21.886 --> 00:48:24.546 A:middle
So if you were adventurous,
you could even try

00:48:24.546 --> 00:48:26.856 A:middle
to give a little
performance for your friends.

00:48:27.516 --> 00:48:31.566 A:middle
[ Music ]

00:48:32.066 --> 00:48:32.786 A:middle
Turn it up a little bit.

00:48:33.516 --> 00:49:32.546 A:middle
[ Music ]


WEBVTT
X-TIMESTAMP-MAP=MPEGTS:181083,LOCAL:00:00:00.000

00:48:33.516 --> 00:49:32.546 A:middle
[ Music ]

00:49:33.046 --> 00:49:33.686 A:middle
Let's wrap it up.

00:49:34.516 --> 00:50:00.586 A:middle
[ Music ]


WEBVTT
X-TIMESTAMP-MAP=MPEGTS:181083,LOCAL:00:00:00.000

00:49:34.516 --> 00:50:00.586 A:middle
[ Music ]

00:50:01.086 --> 00:50:01.976 A:middle
That concludes my demo.

00:50:02.516 --> 00:50:04.786 A:middle
[ Applause ]

00:50:05.286 --> 00:50:07.796 A:middle
I want to thank you for
your time, your attention,

00:50:07.796 --> 00:50:09.976 A:middle
and always for making dope apps.

00:50:10.516 --> 00:50:14.706 A:middle
[ Applause ]

00:50:15.206 --> 00:50:16.296 A:middle
&gt;&gt; Thank you, Torrey.

00:50:16.366 --> 00:50:22.696 A:middle
So, just to recap here, you can
see the session we did last year

00:50:22.696 --> 00:50:24.146 A:middle
about Audio Unit extensions.

00:50:24.146 --> 00:50:27.696 A:middle
It goes into a lot more detail
about the mechanics of the API.

00:50:28.146 --> 00:50:30.386 A:middle
We just wanted to show you here
what people have been doing

00:50:30.386 --> 00:50:33.016 A:middle
with it because it's so cool.

00:50:33.726 --> 00:50:38.306 A:middle
So, speaking of MIDI, we saw
how GarageBand recorded Torrey's

00:50:38.306 --> 00:50:39.416 A:middle
performance as MIDI.

00:50:40.666 --> 00:50:42.936 A:middle
We have a number of
APIs in the system

00:50:42.936 --> 00:50:46.266 A:middle
that communicate using MIDI,
and it's not always clear

00:50:46.266 --> 00:50:48.246 A:middle
which ones to use when.

00:50:48.936 --> 00:50:53.166 A:middle
So I'd like to try to help
clear that up just a little bit.

00:50:53.736 --> 00:50:56.706 A:middle
Now, you might just have a
standard MIDI file like --

00:50:57.566 --> 00:51:00.276 A:middle
well, an ugly cellphone
ringtone.


WEBVTT
X-TIMESTAMP-MAP=MPEGTS:181083,LOCAL:00:00:00.000

00:50:57.566 --> 00:51:00.276 A:middle
well, an ugly cellphone
ringtone.

00:51:00.396 --> 00:51:03.606 A:middle
But MIDI files are very
useful in music education.

00:51:03.796 --> 00:51:07.916 A:middle
I can get a MIDI file of
a piece I want to learn.

00:51:07.916 --> 00:51:09.226 A:middle
I can see what all
the notes are.

00:51:10.056 --> 00:51:12.126 A:middle
So if you have a MIDI
file, you can play it back

00:51:12.126 --> 00:51:13.556 A:middle
with AVAudioSequencer.

00:51:13.556 --> 00:51:17.206 A:middle
And that will play it back into
the context of an AVAudioEngine.

00:51:17.316 --> 00:51:21.786 A:middle
If you wish to control
a software synthesizer

00:51:22.246 --> 00:51:26.576 A:middle
as we saw GarageBand doing
with iSEM, the best API to do

00:51:26.576 --> 00:51:28.516 A:middle
that with is AUAudioUnit.

00:51:29.536 --> 00:51:32.046 A:middle
And if you'd like your
AUAudioUnit to play back

00:51:32.046 --> 00:51:35.546 A:middle
into your AVAudioEngine, you
can use AVAudioMIDIInstrument.

00:51:36.106 --> 00:51:40.686 A:middle
Now there's the core
MIDI framework

00:51:40.686 --> 00:51:42.226 A:middle
which people often
think does some

00:51:42.226 --> 00:51:44.546 A:middle
of these other higher-level
things.

00:51:44.636 --> 00:51:48.506 A:middle
But it's actually a very
low-level API that's basically

00:51:48.506 --> 00:51:50.546 A:middle
for communicating
with MIDI hardware --

00:51:50.546 --> 00:51:53.706 A:middle
for example, an external
USB MIDI interface

00:51:54.136 --> 00:51:55.706 A:middle
or a Bluetooth MIDI keyboard.

00:51:56.256 --> 00:51:58.196 A:middle
We also supply a
MIDI network driver.

00:51:58.996 --> 00:52:03.226 A:middle
You can use that to send raw
MIDI messages between an iPad


WEBVTT
X-TIMESTAMP-MAP=MPEGTS:181083,LOCAL:00:00:00.000

00:51:58.996 --> 00:52:03.226 A:middle
You can use that to send raw
MIDI messages between an iPad

00:52:03.226 --> 00:52:04.716 A:middle
and a MacBook for example.

00:52:06.316 --> 00:52:09.056 A:middle
You can also use the core
MIDI framework to send MIDI

00:52:09.056 --> 00:52:10.926 A:middle
between processes in real time.

00:52:12.106 --> 00:52:15.786 A:middle
Now this gets into a
gray area sometimes.

00:52:15.786 --> 00:52:19.086 A:middle
People wonder, "Well, should
I use core MIDI to communicate

00:52:19.086 --> 00:52:23.146 A:middle
between my sequencer and
my app that's listening

00:52:23.146 --> 00:52:25.226 A:middle
to MIDI and synthesizing?"

00:52:25.786 --> 00:52:29.126 A:middle
And I would say that's probably
not the right API for that case.

00:52:29.126 --> 00:52:31.276 A:middle
If you're using MIDI
and audio together,

00:52:31.776 --> 00:52:33.496 A:middle
I would use AUAudioUnit.

00:52:33.936 --> 00:52:36.136 A:middle
It's in the case where
you're doing pure MIDI

00:52:36.136 --> 00:52:39.606 A:middle
in two applications
or two entities

00:52:39.606 --> 00:52:42.646 A:middle
within an application --
maybe one is a static library

00:52:42.646 --> 00:52:43.766 A:middle
from another developer.

00:52:44.706 --> 00:52:48.676 A:middle
In those situations, you can
use core MIDI for inter-process

00:52:48.846 --> 00:52:50.826 A:middle
or inter-entity real-time MIDI.

00:52:52.596 --> 00:52:53.886 A:middle
So that takes us to the end

00:52:53.886 --> 00:52:56.386 A:middle
of our grand tour
of the audio APIs.

00:52:57.326 --> 00:53:00.096 A:middle
We started with applications
-- and at the bottom,


WEBVTT
X-TIMESTAMP-MAP=MPEGTS:181083,LOCAL:00:00:00.000

00:52:57.326 --> 00:53:00.096 A:middle
We started with applications
-- and at the bottom,

00:53:00.096 --> 00:53:03.546 A:middle
the CoreAudio framework
and drivers.

00:53:04.456 --> 00:53:07.746 A:middle
We looked at AVAudioEngine,
how you use AVAudioSession

00:53:07.746 --> 00:53:11.326 A:middle
to get things setup on all of
our platforms except macOS.

00:53:12.036 --> 00:53:14.166 A:middle
We saw how you can
use AVAudioPlayer

00:53:14.166 --> 00:53:16.676 A:middle
and the AVAudioRecorder
for simple playback

00:53:16.676 --> 00:53:18.066 A:middle
and recording from files.

00:53:18.776 --> 00:53:21.806 A:middle
Or if your files or network
streams involve video,

00:53:21.806 --> 00:53:23.066 A:middle
you can use AVPlayer.

00:53:24.056 --> 00:53:27.056 A:middle
AVAudioEngine is a very
good, high-level interface

00:53:27.426 --> 00:53:30.176 A:middle
for building complex
processing graphs

00:53:30.516 --> 00:53:32.596 A:middle
and will solve a
lot of problems.

00:53:32.596 --> 00:53:36.216 A:middle
You usually won't have to use
any of the lower-level APIs.

00:53:36.216 --> 00:53:41.586 A:middle
But if you do, we saw how in
AudioToolbox there's AUAudioUnit

00:53:41.586 --> 00:53:45.156 A:middle
that lets you communicate
directly with the I/O cycle

00:53:46.256 --> 00:53:49.476 A:middle
and third-party, or
your own instruments,

00:53:49.476 --> 00:53:50.916 A:middle
effects, and generators.

00:53:51.336 --> 00:53:54.286 A:middle
And finally, we took a quick
look at the core MIDI framework.

00:53:54.286 --> 00:53:58.446 A:middle
So that's the end
of my talk here.

00:53:58.576 --> 00:54:01.936 A:middle
You can visit this link
for some more information.


WEBVTT
X-TIMESTAMP-MAP=MPEGTS:181083,LOCAL:00:00:00.000

00:53:58.576 --> 00:54:01.936 A:middle
You can visit this link
for some more information.

00:54:02.696 --> 00:54:04.776 A:middle
We have a number of
related sessions here.

00:54:05.306 --> 00:54:05.976 A:middle
Thank you very much.

00:54:06.516 --> 00:54:09.500 A:middle
[ Applause ]

