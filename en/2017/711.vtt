WEBVTT

00:00:26.156 --> 00:00:26.516 A:middle
&gt;&gt; Thank you.

00:00:26.516 --> 00:00:27.806 A:middle
And welcome to the Accelerate

00:00:27.806 --> 00:00:28.166 A:middle
Session.

00:00:28.166 --> 00:00:29.296 A:middle
My name is Eric Bainville.

00:00:29.296 --> 00:00:30.826 A:middle
I'm with the CoreOS Vector and

00:00:30.826 --> 00:00:31.566 A:middle
Numerics Group.

00:00:32.326 --> 00:00:34.916 A:middle
Our group is maintaining the

00:00:34.916 --> 00:00:35.956 A:middle
Accelerate framework.

00:00:36.336 --> 00:00:37.566 A:middle
And this is our agenda for

00:00:37.566 --> 00:00:37.786 A:middle
today.

00:00:37.786 --> 00:00:38.926 A:middle
So first I will introduce

00:00:38.926 --> 00:00:41.286 A:middle
Accelerate, what's inside, how

00:00:41.286 --> 00:00:42.676 A:middle
to use it, and through a few

00:00:42.676 --> 00:00:43.956 A:middle
examples I'll show you why you

00:00:43.956 --> 00:00:44.736 A:middle
want to use that.

00:00:44.736 --> 00:00:47.346 A:middle
And then we focus on new

00:00:47.346 --> 00:00:48.576 A:middle
improvements and additions we

00:00:48.576 --> 00:00:50.196 A:middle
have this year, first with

00:00:50.196 --> 00:00:51.786 A:middle
Compression, lossless data

00:00:51.786 --> 00:00:54.176 A:middle
compression library, then BNNS,

00:00:54.236 --> 00:00:55.566 A:middle
Basic Neural Network

00:00:55.566 --> 00:00:56.126 A:middle
Subroutines.

00:00:57.006 --> 00:00:58.676 A:middle
And after that my colleague

00:00:58.676 --> 00:00:59.836 A:middle
Steve will come on stage and

00:00:59.836 --> 00:01:01.246 A:middle
tell us what's new in simd.

00:01:01.556 --> 00:01:02.796 A:middle
And finally, our very own

00:01:02.996 --> 00:01:04.436 A:middle
Jonathan Hogg will come on stage

00:01:04.436 --> 00:01:05.876 A:middle
and tell us -- introduce the

00:01:05.876 --> 00:01:07.176 A:middle
sparse matrices package.

00:01:07.686 --> 00:01:08.666 A:middle
It will be the first time

00:01:08.716 --> 00:01:10.206 A:middle
commercialized shipping with

00:01:10.206 --> 00:01:12.386 A:middle
sparse matrices solver package.

00:01:12.806 --> 00:01:14.196 A:middle
But let's start with Accelerate.

00:01:15.186 --> 00:01:17.536 A:middle
Accelerate is a low-level system

00:01:17.536 --> 00:01:19.356 A:middle
framework dedicated to high

00:01:19.356 --> 00:01:20.846 A:middle
performance primitive on the

00:01:20.846 --> 00:01:21.356 A:middle
CPU.

00:01:22.576 --> 00:01:25.186 A:middle
Actually, it's everywhere when

00:01:25.186 --> 00:01:26.686 A:middle
you have a significant workload

00:01:26.686 --> 00:01:27.706 A:middle
to run on the CPU.

00:01:28.466 --> 00:01:30.136 A:middle
Inside Accelerate there's a lot

00:01:30.136 --> 00:01:32.196 A:middle
of libraries, like vImage for

00:01:32.196 --> 00:01:33.516 A:middle
image processing, the Swiss

00:01:34.116 --> 00:01:35.466 A:middle
knife of image processing.

00:01:35.856 --> 00:01:37.636 A:middle
We also have vDSP inside for

00:01:37.636 --> 00:01:39.326 A:middle
DFT's and FFT's signal

00:01:39.326 --> 00:01:41.786 A:middle
processing, vForce for vector

00:01:41.786 --> 00:01:42.426 A:middle
functions.

00:01:43.046 --> 00:01:44.686 A:middle
And then we have a whole lot of

00:01:44.766 --> 00:01:46.586 A:middle
linear algebra libraries for

00:01:46.586 --> 00:01:48.626 A:middle
dense and sparse matrices.

00:01:48.626 --> 00:01:50.126 A:middle
So that's BLAS and LAPACK for

00:01:50.126 --> 00:01:51.436 A:middle
dense vectors and matrices.

00:01:51.906 --> 00:01:53.056 A:middle
And Sparse BLAS and Sparse

00:01:53.056 --> 00:01:54.996 A:middle
Solvers for sparse vectors and

00:01:54.996 --> 00:01:55.576 A:middle
matrices.

00:01:56.196 --> 00:01:57.656 A:middle
Last year we also introduced

00:01:57.656 --> 00:01:59.076 A:middle
BNNS, Basic Neural Network

00:01:59.076 --> 00:02:01.146 A:middle
Subroutines, which is also

00:02:01.146 --> 00:02:02.146 A:middle
[inaudible] for neural networks.

00:02:02.356 --> 00:02:04.766 A:middle
This is used by, for example,

00:02:04.766 --> 00:02:06.726 A:middle
Core ML and also Vision and NLP

00:02:06.726 --> 00:02:07.406 A:middle
frameworks.

00:02:08.216 --> 00:02:09.626 A:middle
Slightly outside Accelerate but

00:02:09.626 --> 00:02:11.476 A:middle
still maintained by us, we have

00:02:11.476 --> 00:02:13.066 A:middle
simd, which is a set of headers

00:02:13.066 --> 00:02:14.806 A:middle
and function allowing you to

00:02:14.806 --> 00:02:16.536 A:middle
directly talk to the CPU vector

00:02:16.536 --> 00:02:17.026 A:middle
units.

00:02:17.996 --> 00:02:19.186 A:middle
And finally, Compression, a

00:02:19.516 --> 00:02:20.616 A:middle
lossless data compression

00:02:20.616 --> 00:02:21.046 A:middle
library.

00:02:22.306 --> 00:02:23.696 A:middle
How do you use Accelerate?

00:02:24.676 --> 00:02:26.216 A:middle
Well, so you import Accelerate

00:02:26.216 --> 00:02:27.336 A:middle
or you include the Accelerate

00:02:27.366 --> 00:02:28.946 A:middle
header and then link with

00:02:28.946 --> 00:02:30.286 A:middle
Accelerate frameworks on your

00:02:30.286 --> 00:02:30.726 A:middle
own set.

00:02:30.726 --> 00:02:31.976 A:middle
Now, I will show you a few

00:02:31.976 --> 00:02:34.086 A:middle
examples about why you want to

00:02:34.086 --> 00:02:34.996 A:middle
use Accelerate.

00:02:35.466 --> 00:02:36.596 A:middle
Let's start with that one.

00:02:37.106 --> 00:02:37.886 A:middle
So let's say you have this

00:02:37.886 --> 00:02:39.276 A:middle
vector X of floating point

00:02:39.276 --> 00:02:40.936 A:middle
values and you want to scale

00:02:40.936 --> 00:02:41.236 A:middle
them.

00:02:42.126 --> 00:02:43.166 A:middle
So you have this scale.

00:02:43.166 --> 00:02:44.996 A:middle
And so you multiply each value

00:02:44.996 --> 00:02:46.216 A:middle
and store it in the Y vector.

00:02:46.216 --> 00:02:48.206 A:middle
So it's a very simple loop.

00:02:48.696 --> 00:02:49.676 A:middle
That's perfectly fine.

00:02:50.176 --> 00:02:51.066 A:middle
But actually, there is a

00:02:51.066 --> 00:02:52.946 A:middle
function for you in Accelerate

00:02:52.946 --> 00:02:53.926 A:middle
doing the same thing, it's

00:02:53.926 --> 00:02:54.696 A:middle
called vsmul.

00:02:55.786 --> 00:02:57.666 A:middle
So that's one single line

00:02:57.666 --> 00:02:58.646 A:middle
replacing your code.

00:02:58.986 --> 00:03:00.996 A:middle
And the good thing is that we

00:03:00.996 --> 00:03:02.766 A:middle
optimize it for you on all the

00:03:02.766 --> 00:03:03.716 A:middle
supported hardware.

00:03:04.226 --> 00:03:05.256 A:middle
And of course, we maintain it

00:03:05.256 --> 00:03:06.316 A:middle
for you so you don't have your

00:03:06.316 --> 00:03:07.566 A:middle
loop to maintain again.

00:03:08.016 --> 00:03:09.856 A:middle
And, of course, it's faster, as

00:03:09.856 --> 00:03:10.846 A:middle
Accelerate says.

00:03:11.356 --> 00:03:12.766 A:middle
So that's a reference speed and

00:03:12.766 --> 00:03:14.176 A:middle
energy consumption for the loop.

00:03:14.406 --> 00:03:15.536 A:middle
And this is what you get with

00:03:15.536 --> 00:03:16.136 A:middle
Accelerate.

00:03:16.856 --> 00:03:18.166 A:middle
Six times faster.

00:03:19.516 --> 00:03:21.616 A:middle
[ Applause ]

00:03:22.116 --> 00:03:23.946 A:middle
And six times less energy

00:03:23.946 --> 00:03:25.086 A:middle
consumed, which is very

00:03:25.086 --> 00:03:25.566 A:middle
important.

00:03:25.796 --> 00:03:26.876 A:middle
Let me give you another one.

00:03:27.256 --> 00:03:28.736 A:middle
So this time we still have this

00:03:28.876 --> 00:03:30.596 A:middle
array X, and we want to clip the

00:03:30.596 --> 00:03:31.976 A:middle
values between the low and high

00:03:31.976 --> 00:03:33.746 A:middle
bound and store that in Y.

00:03:34.176 --> 00:03:35.156 A:middle
Again, you could go with this

00:03:35.156 --> 00:03:35.466 A:middle
code.

00:03:35.576 --> 00:03:36.376 A:middle
That's perfectly fine.

00:03:36.376 --> 00:03:36.786 A:middle
That's good.

00:03:37.336 --> 00:03:38.646 A:middle
But we have a function for you

00:03:38.646 --> 00:03:40.306 A:middle
in vDSP; it's called vclip.

00:03:40.936 --> 00:03:42.096 A:middle
Does the same thing.

00:03:42.096 --> 00:03:43.586 A:middle
And, again, we maintain it for

00:03:43.586 --> 00:03:43.756 A:middle
you.

00:03:43.756 --> 00:03:44.846 A:middle
We optimize it for you.

00:03:45.616 --> 00:03:46.796 A:middle
And, of course, it's faster.

00:03:46.846 --> 00:03:48.276 A:middle
That's a reference for the loop.

00:03:48.696 --> 00:03:49.736 A:middle
And this is what you get with

00:03:49.736 --> 00:03:52.126 A:middle
Accelerate, eight times faster

00:03:52.126 --> 00:03:53.756 A:middle
and also eight times less energy

00:03:53.756 --> 00:03:54.246 A:middle
consumed.

00:03:55.146 --> 00:03:58.166 A:middle
Another one, matrices.

00:03:58.166 --> 00:03:59.016 A:middle
So let's say you have two

00:03:59.016 --> 00:04:00.766 A:middle
matrices A and B, and you want

00:04:00.766 --> 00:04:02.126 A:middle
to compute the product of A and

00:04:02.126 --> 00:04:04.016 A:middle
B and add the result into the C

00:04:04.016 --> 00:04:04.506 A:middle
matrix.

00:04:05.026 --> 00:04:06.736 A:middle
It doesn't sound simple.

00:04:06.776 --> 00:04:08.266 A:middle
But actually, it's very simple.

00:04:08.266 --> 00:04:09.516 A:middle
That's just three lines of code.

00:04:09.516 --> 00:04:10.166 A:middle
You can see that.

00:04:11.356 --> 00:04:12.436 A:middle
And we have, of course, a

00:04:12.436 --> 00:04:13.716 A:middle
function for you in Accelerate

00:04:13.716 --> 00:04:15.396 A:middle
doing that; it's called sgemm

00:04:15.396 --> 00:04:16.746 A:middle
for the Cblas package.

00:04:17.906 --> 00:04:20.336 A:middle
And really, really, you never

00:04:20.336 --> 00:04:22.046 A:middle
want to write code computing

00:04:22.046 --> 00:04:23.206 A:middle
metrics vector products or

00:04:23.206 --> 00:04:24.786 A:middle
matrix metrics or anything

00:04:24.786 --> 00:04:25.646 A:middle
related to metrics.

00:04:25.646 --> 00:04:26.576 A:middle
You don't want to write that

00:04:26.576 --> 00:04:27.076 A:middle
code ever.

00:04:28.196 --> 00:04:29.966 A:middle
You want to call BLAS, LAPACK,

00:04:29.966 --> 00:04:31.196 A:middle
or anything in Accelerate.

00:04:31.636 --> 00:04:32.946 A:middle
Why? Well, first because we

00:04:32.946 --> 00:04:34.246 A:middle
maintain that for you, and it

00:04:34.246 --> 00:04:35.296 A:middle
will be optimized and

00:04:35.326 --> 00:04:37.336 A:middle
multithreaded on all the

00:04:37.336 --> 00:04:38.546 A:middle
architectures we support.

00:04:39.806 --> 00:04:41.676 A:middle
And this time -- this is a

00:04:41.676 --> 00:04:42.706 A:middle
reference for the loop -- this

00:04:42.706 --> 00:04:43.986 A:middle
is what you get with Accelerate.

00:04:43.986 --> 00:04:46.996 A:middle
I'm not sure you can see it.

00:04:47.686 --> 00:04:52.126 A:middle
Yeah. That's 100 times faster

00:04:52.126 --> 00:04:54.236 A:middle
and 26 times more energy

00:04:54.236 --> 00:04:54.716 A:middle
efficient.

00:04:54.926 --> 00:04:56.296 A:middle
That's your battery here.

00:04:56.986 --> 00:04:58.296 A:middle
Okay. One more example, this

00:04:58.296 --> 00:04:59.346 A:middle
time in vImage.

00:04:59.486 --> 00:05:01.156 A:middle
So let's say you have a 32-bit

00:05:01.156 --> 00:05:02.656 A:middle
per pixel image with four

00:05:02.656 --> 00:05:04.006 A:middle
components per pixel.

00:05:04.006 --> 00:05:05.206 A:middle
That's alpha, red, green, and

00:05:05.206 --> 00:05:05.536 A:middle
blue.

00:05:05.916 --> 00:05:07.746 A:middle
And you want to apply a 4 by 4

00:05:07.746 --> 00:05:09.106 A:middle
transformation matrix to every

00:05:09.106 --> 00:05:10.116 A:middle
pixel in the image.

00:05:10.876 --> 00:05:11.746 A:middle
But you could write the code.

00:05:11.746 --> 00:05:12.736 A:middle
You could write it here, that

00:05:12.736 --> 00:05:13.276 A:middle
would be too long.

00:05:13.536 --> 00:05:14.526 A:middle
But really, you don't want to

00:05:14.526 --> 00:05:15.186 A:middle
write this code.

00:05:15.346 --> 00:05:16.316 A:middle
We have a function for you in

00:05:16.316 --> 00:05:18.036 A:middle
vImage, MatrixMultiply -- that's

00:05:18.036 --> 00:05:19.176 A:middle
one of the most used functions

00:05:19.176 --> 00:05:21.026 A:middle
in vImage -- doing exactly that

00:05:21.026 --> 00:05:23.906 A:middle
and optimized to as fast as it

00:05:23.906 --> 00:05:25.446 A:middle
can on all the architectures we

00:05:25.446 --> 00:05:25.866 A:middle
support.

00:05:26.826 --> 00:05:30.066 A:middle
Last one, this is convolution

00:05:30.066 --> 00:05:30.306 A:middle
layer.

00:05:30.376 --> 00:05:33.036 A:middle
That's the working horse of the

00:05:33.036 --> 00:05:34.526 A:middle
neural networks, the convolution

00:05:34.526 --> 00:05:35.406 A:middle
neural networks.

00:05:35.956 --> 00:05:38.416 A:middle
So this layer takes input stacks

00:05:38.486 --> 00:05:40.016 A:middle
-- that's the right thing on the

00:05:40.016 --> 00:05:42.076 A:middle
left -- that's a stack of

00:05:42.076 --> 00:05:42.716 A:middle
images.

00:05:43.066 --> 00:05:45.196 A:middle
And it will put output image

00:05:45.196 --> 00:05:46.056 A:middle
stack, the blue thing.

00:05:46.286 --> 00:05:48.046 A:middle
And each pixel in the output is

00:05:48.046 --> 00:05:49.146 A:middle
the result of a treaty

00:05:49.146 --> 00:05:51.376 A:middle
convolution on all the entire

00:05:51.376 --> 00:05:52.226 A:middle
input stack.

00:05:52.226 --> 00:05:53.886 A:middle
And we do that for every of the

00:05:53.886 --> 00:05:54.806 A:middle
three that I mentioned for

00:05:54.806 --> 00:05:55.506 A:middle
output image.

00:05:56.066 --> 00:05:56.916 A:middle
So at the end, that's a

00:05:56.916 --> 00:05:59.176 A:middle
six-dimensional loop.

00:05:59.176 --> 00:06:00.146 A:middle
And you really don't want to

00:06:00.146 --> 00:06:00.746 A:middle
write this loop.

00:06:01.336 --> 00:06:02.896 A:middle
And even when the dimensions are

00:06:02.896 --> 00:06:04.336 A:middle
small, you're multiplying all of

00:06:04.336 --> 00:06:04.886 A:middle
them together.

00:06:04.886 --> 00:06:07.066 A:middle
So that's millions or even

00:06:07.066 --> 00:06:08.376 A:middle
billions of floating points

00:06:08.376 --> 00:06:09.146 A:middle
operation here.

00:06:09.786 --> 00:06:11.076 A:middle
Of course, we have a function

00:06:11.076 --> 00:06:12.626 A:middle
for you this time in BNNS doing

00:06:12.626 --> 00:06:12.906 A:middle
that.

00:06:13.396 --> 00:06:14.516 A:middle
And when you run a Core ML

00:06:14.516 --> 00:06:16.616 A:middle
model, you will spend, like, 80%

00:06:16.616 --> 00:06:17.866 A:middle
of the time in this function.

00:06:18.636 --> 00:06:19.386 A:middle
All right.

00:06:19.386 --> 00:06:21.106 A:middle
So that was a few examples.

00:06:21.186 --> 00:06:22.956 A:middle
I could continue almost forever

00:06:22.956 --> 00:06:24.936 A:middle
because we have more than 2,800

00:06:24.936 --> 00:06:26.516 A:middle
API's in Accelerate.

00:06:27.146 --> 00:06:28.246 A:middle
So usually that would be a

00:06:28.246 --> 00:06:29.386 A:middle
function for you inside.

00:06:29.656 --> 00:06:30.756 A:middle
And every time you use an

00:06:30.756 --> 00:06:32.456 A:middle
Accelerate function, the

00:06:32.456 --> 00:06:34.146 A:middle
benefits are that's less code

00:06:34.146 --> 00:06:36.516 A:middle
for you to write; we maintain it

00:06:36.516 --> 00:06:38.086 A:middle
for you; of course, it will be

00:06:38.086 --> 00:06:39.216 A:middle
faster and more energy

00:06:39.216 --> 00:06:40.756 A:middle
efficient; and it will be

00:06:40.756 --> 00:06:43.326 A:middle
optimal -- as close to optimal

00:06:43.326 --> 00:06:44.346 A:middle
as possible on all the

00:06:44.346 --> 00:06:45.476 A:middle
architectures we support,

00:06:45.476 --> 00:06:46.606 A:middle
including the new ones.

00:06:46.606 --> 00:06:47.986 A:middle
So when we release new hardware,

00:06:48.696 --> 00:06:49.816 A:middle
you will get your code running

00:06:49.816 --> 00:06:51.506 A:middle
as fast as it can from day one.

00:06:52.596 --> 00:06:53.966 A:middle
Okay. So that was it for

00:06:53.966 --> 00:06:54.536 A:middle
Accelerate.

00:06:54.536 --> 00:06:56.466 A:middle
Now let's focus on Compression.

00:06:58.826 --> 00:07:00.906 A:middle
Compression is a lossless data

00:07:00.906 --> 00:07:01.746 A:middle
compression library.

00:07:01.746 --> 00:07:04.096 A:middle
It's a very simple API with a

00:07:04.096 --> 00:07:06.076 A:middle
few select compressors inside.

00:07:06.406 --> 00:07:08.136 A:middle
So they are represented on this

00:07:08.136 --> 00:07:09.046 A:middle
little graph here.

00:07:09.466 --> 00:07:11.096 A:middle
On the x-axis you see the

00:07:11.096 --> 00:07:12.506 A:middle
relative compression ratio

00:07:12.736 --> 00:07:13.526 A:middle
compared to ZLIB.

00:07:13.526 --> 00:07:14.616 A:middle
ZLIB is in the center.

00:07:14.616 --> 00:07:16.596 A:middle
And on the y-axis is the

00:07:16.596 --> 00:07:18.416 A:middle
compression speed.

00:07:18.516 --> 00:07:20.556 A:middle
It's not that exponential.

00:07:21.076 --> 00:07:22.346 A:middle
So inside the compression

00:07:22.346 --> 00:07:24.486 A:middle
library we have a selection of

00:07:24.486 --> 00:07:25.476 A:middle
compressors, as I said.

00:07:26.026 --> 00:07:27.866 A:middle
We offer LZMA for better

00:07:27.866 --> 00:07:30.166 A:middle
compression, optimized versions

00:07:30.166 --> 00:07:32.356 A:middle
of LZ4 for fast compression.

00:07:32.836 --> 00:07:35.146 A:middle
Of course we have ZLIB with the

00:07:35.146 --> 00:07:36.776 A:middle
optimized ZLIB decoder, and our

00:07:36.776 --> 00:07:39.196 A:middle
very own LZFSE, which compresses

00:07:39.196 --> 00:07:41.416 A:middle
slightly more than ZLIB but much

00:07:41.416 --> 00:07:41.776 A:middle
faster.

00:07:42.796 --> 00:07:44.366 A:middle
And last year we open sourced

00:07:44.536 --> 00:07:46.226 A:middle
LZFSE; it's on GitHub.

00:07:47.136 --> 00:07:49.316 A:middle
Okay. The API now.

00:07:49.866 --> 00:07:51.626 A:middle
That, too, API's in the

00:07:51.626 --> 00:07:52.286 A:middle
compression.

00:07:52.686 --> 00:07:54.116 A:middle
The first one is a buffer API.

00:07:54.116 --> 00:07:55.166 A:middle
So that's when you have the

00:07:55.166 --> 00:07:57.986 A:middle
entire data to compress.

00:07:57.986 --> 00:07:59.106 A:middle
And so you have a buffer with

00:07:59.106 --> 00:08:00.246 A:middle
the data to compress, you just

00:08:00.246 --> 00:08:02.406 A:middle
call one function, provide the

00:08:02.406 --> 00:08:03.456 A:middle
output buffer, and you will get

00:08:03.456 --> 00:08:04.986 A:middle
the output in one single code.

00:08:05.146 --> 00:08:06.476 A:middle
That's good for encode and

00:08:06.476 --> 00:08:06.856 A:middle
decode.

00:08:07.226 --> 00:08:09.306 A:middle
And if the data is huge or you

00:08:09.306 --> 00:08:12.086 A:middle
get it in small pieces, you want

00:08:12.086 --> 00:08:13.086 A:middle
to use a stream API.

00:08:13.926 --> 00:08:15.606 A:middle
In that case you will create a

00:08:15.606 --> 00:08:17.806 A:middle
stream object and send data

00:08:17.806 --> 00:08:19.386 A:middle
inside and get smaller data

00:08:19.386 --> 00:08:19.916 A:middle
outside.

00:08:19.916 --> 00:08:20.776 A:middle
And you will call that

00:08:20.776 --> 00:08:22.076 A:middle
repeatedly until the entire

00:08:22.076 --> 00:08:23.606 A:middle
stream is processed.

00:08:25.436 --> 00:08:27.076 A:middle
And this, what we have, what's

00:08:27.076 --> 00:08:29.316 A:middle
new, we added a compression tool

00:08:29.316 --> 00:08:29.986 A:middle
command line.

00:08:30.866 --> 00:08:33.146 A:middle
So you can compress with

00:08:33.146 --> 00:08:34.296 A:middle
Compression from the command

00:08:34.296 --> 00:08:34.516 A:middle
line.

00:08:35.656 --> 00:08:37.026 A:middle
Okay. That's for Compression.

00:08:37.026 --> 00:08:39.046 A:middle
Now let's switch to BNNS, Basic

00:08:39.046 --> 00:08:40.366 A:middle
Neural Network Subroutines.

00:08:40.776 --> 00:08:42.456 A:middle
As I said, this is the energy

00:08:42.456 --> 00:08:44.386 A:middle
running of the CPU and

00:08:44.386 --> 00:08:45.916 A:middle
supporting all the neural

00:08:45.916 --> 00:08:48.746 A:middle
network and the machine learning

00:08:48.836 --> 00:08:49.446 A:middle
libraries.

00:08:49.886 --> 00:08:52.686 A:middle
So you use BNNS almost anytime.

00:08:52.686 --> 00:08:55.156 A:middle
When you type on the keyboard or

00:08:55.156 --> 00:08:56.766 A:middle
when you run face recognition,

00:08:57.486 --> 00:09:00.216 A:middle
all these applications use BNNS.

00:09:00.216 --> 00:09:03.566 A:middle
And BNNS provides lower-level

00:09:03.806 --> 00:09:04.846 A:middle
functions for this.

00:09:05.006 --> 00:09:07.276 A:middle
So that's convolutional layers,

00:09:07.456 --> 00:09:08.286 A:middle
pooling layers.

00:09:08.856 --> 00:09:10.296 A:middle
We also have fully connected

00:09:10.296 --> 00:09:10.686 A:middle
layers.

00:09:10.686 --> 00:09:13.646 A:middle
And these, we added separate

00:09:13.696 --> 00:09:15.536 A:middle
activation layers doing just the

00:09:15.536 --> 00:09:16.516 A:middle
activation function.

00:09:16.816 --> 00:09:18.236 A:middle
And these layers also can do

00:09:18.236 --> 00:09:20.256 A:middle
efficient conversion, data type

00:09:20.546 --> 00:09:21.026 A:middle
conversions.

00:09:21.656 --> 00:09:24.466 A:middle
Speaking of which, data types.

00:09:24.786 --> 00:09:27.276 A:middle
So when you train a machine

00:09:27.276 --> 00:09:29.526 A:middle
learning model, what you get is

00:09:29.596 --> 00:09:31.246 A:middle
megabytes or hundreds of

00:09:31.246 --> 00:09:33.026 A:middle
megabytes of data, usually

00:09:33.026 --> 00:09:34.656 A:middle
32-bit floating point data for

00:09:34.656 --> 00:09:35.076 A:middle
your model.

00:09:35.076 --> 00:09:37.076 A:middle
That's the convolution weights,

00:09:37.076 --> 00:09:40.216 A:middle
etc. It turns out you can

00:09:40.216 --> 00:09:41.936 A:middle
convert these guys into smaller

00:09:41.936 --> 00:09:43.626 A:middle
types, like 16-bit floating

00:09:43.626 --> 00:09:45.656 A:middle
point or even 8-bit integer

00:09:45.956 --> 00:09:47.926 A:middle
signed or unsigned and still get

00:09:47.926 --> 00:09:49.076 A:middle
the same precision for your

00:09:49.076 --> 00:09:49.476 A:middle
model.

00:09:50.056 --> 00:09:51.606 A:middle
But, of course, when you convert

00:09:51.606 --> 00:09:53.356 A:middle
32-bit floating point to 8-bit,

00:09:53.356 --> 00:09:54.566 A:middle
your model is four times

00:09:54.566 --> 00:09:55.116 A:middle
smaller.

00:09:55.656 --> 00:09:56.656 A:middle
And that's something you ship

00:09:56.656 --> 00:09:57.316 A:middle
with your app.

00:09:57.316 --> 00:09:59.906 A:middle
So you want to consider that.

00:09:59.906 --> 00:10:02.316 A:middle
This year we optimized BNNS to

00:10:02.316 --> 00:10:03.456 A:middle
support these types.

00:10:03.846 --> 00:10:06.526 A:middle
So this is what we support now,

00:10:06.806 --> 00:10:08.456 A:middle
optimized in the convolutional

00:10:08.456 --> 00:10:08.746 A:middle
layers.

00:10:08.746 --> 00:10:10.216 A:middle
The green stuff is new.

00:10:11.186 --> 00:10:13.416 A:middle
See, we added fp16 storage for

00:10:13.416 --> 00:10:15.126 A:middle
input and weights and also int8.

00:10:16.206 --> 00:10:18.176 A:middle
And this is what we support for

00:10:18.176 --> 00:10:19.846 A:middle
the fully connected layers.

00:10:19.846 --> 00:10:21.656 A:middle
So we're still accumulating to

00:10:21.656 --> 00:10:24.426 A:middle
32 bits, but we can take 16-bit

00:10:24.426 --> 00:10:25.746 A:middle
or even 8-bit inputs and

00:10:25.746 --> 00:10:26.126 A:middle
weights.

00:10:27.096 --> 00:10:28.006 A:middle
Now, for the activation

00:10:28.006 --> 00:10:30.546 A:middle
functions, this is what we had

00:10:30.546 --> 00:10:30.946 A:middle
last year.

00:10:30.946 --> 00:10:32.166 A:middle
And this year we added a few

00:10:32.166 --> 00:10:33.696 A:middle
more, including the most

00:10:33.696 --> 00:10:34.646 A:middle
requested, Softmax.

00:10:34.646 --> 00:10:36.036 A:middle
So now we have an optimized

00:10:36.036 --> 00:10:37.066 A:middle
Softmax in BNNS.

00:10:37.986 --> 00:10:39.616 A:middle
And if you set the activation

00:10:39.616 --> 00:10:41.716 A:middle
function to identity, then you

00:10:41.716 --> 00:10:43.526 A:middle
can change the input and output

00:10:44.116 --> 00:10:45.466 A:middle
types to different combinations.

00:10:45.716 --> 00:10:47.236 A:middle
This is what we support now.

00:10:47.236 --> 00:10:48.986 A:middle
And you will get optimized type

00:10:48.986 --> 00:10:51.146 A:middle
conversion from BNNS.

00:10:51.666 --> 00:10:53.386 A:middle
Last but not least, performance.

00:10:54.296 --> 00:10:56.306 A:middle
So we worked a lot with the Core

00:10:56.306 --> 00:10:58.166 A:middle
ML team and the Vision and NLP

00:10:58.616 --> 00:11:00.786 A:middle
teams to optimize what they

00:11:00.786 --> 00:11:01.736 A:middle
really use a lot.

00:11:02.686 --> 00:11:05.496 A:middle
So that includes convolutions

00:11:05.496 --> 00:11:08.676 A:middle
with padding and Stride 1 and 2

00:11:08.676 --> 00:11:10.616 A:middle
convolutions also and smaller

00:11:10.616 --> 00:11:10.976 A:middle
kernels.

00:11:10.976 --> 00:11:13.296 A:middle
Because the new neural networks

00:11:13.376 --> 00:11:14.556 A:middle
have a lot of layers with

00:11:14.556 --> 00:11:16.716 A:middle
smaller convolutions -- that's 3

00:11:16.716 --> 00:11:17.966 A:middle
by 3 and 1 by 1.

00:11:18.066 --> 00:11:19.436 A:middle
So we optimized these cases.

00:11:19.846 --> 00:11:21.276 A:middle
And especially for the 3 by 3

00:11:21.276 --> 00:11:22.386 A:middle
case, we have Winograd

00:11:22.386 --> 00:11:24.166 A:middle
convolutions, which can be up to

00:11:24.166 --> 00:11:25.776 A:middle
four times faster than the

00:11:25.776 --> 00:11:27.086 A:middle
reference implementation.

00:11:27.326 --> 00:11:29.056 A:middle
And that's it for BNNS.

00:11:29.056 --> 00:11:31.046 A:middle
So let me invite Steve on stage,

00:11:31.046 --> 00:11:32.186 A:middle
and he will tell us everything

00:11:32.186 --> 00:11:32.796 A:middle
about simd.

00:11:32.936 --> 00:11:33.216 A:middle
Thank you.

00:11:34.516 --> 00:11:37.216 A:middle
[ Applause ]

00:11:37.716 --> 00:11:38.576 A:middle
&gt;&gt; Thanks very much, Eric.

00:11:38.766 --> 00:11:39.506 A:middle
Thank you, everyone.

00:11:40.126 --> 00:11:41.416 A:middle
As Eric said, my name's Steve.

00:11:41.416 --> 00:11:42.446 A:middle
And today I'm going to talk to

00:11:42.446 --> 00:11:43.506 A:middle
you a little bit about the simd

00:11:43.506 --> 00:11:43.836 A:middle
module.

00:11:43.836 --> 00:11:44.636 A:middle
I don't think I'll get to cover

00:11:44.636 --> 00:11:45.966 A:middle
everything, but we'll cover

00:11:45.966 --> 00:11:46.206 A:middle
some.

00:11:47.316 --> 00:11:50.366 A:middle
So the simd module lives outside

00:11:50.366 --> 00:11:51.036 A:middle
of Accelerate.

00:11:51.036 --> 00:11:52.536 A:middle
It's a small collection of

00:11:52.536 --> 00:11:53.576 A:middle
headers and user include.

00:11:53.836 --> 00:11:55.016 A:middle
And it's a module you import in

00:11:55.016 --> 00:11:55.406 A:middle
Swift.

00:11:55.956 --> 00:11:57.226 A:middle
And there's sort of three main

00:11:57.226 --> 00:11:58.546 A:middle
use cases that are going to

00:11:58.546 --> 00:11:59.406 A:middle
drive you to use simd.

00:12:00.206 --> 00:12:01.906 A:middle
The first is if you're doing 2

00:12:01.906 --> 00:12:04.986 A:middle
by 2, 3 by 3, 4 by 4 vector and

00:12:04.986 --> 00:12:06.046 A:middle
matrix arithmetic, the sort of

00:12:06.046 --> 00:12:07.156 A:middle
thing that comes up all the time

00:12:07.156 --> 00:12:08.506 A:middle
when doing graphics or geometry

00:12:08.506 --> 00:12:09.176 A:middle
operations.

00:12:10.576 --> 00:12:12.436 A:middle
It also provides a bigger set of

00:12:12.436 --> 00:12:13.596 A:middle
vector times, both integer

00:12:13.596 --> 00:12:14.616 A:middle
vectors and floating point

00:12:14.616 --> 00:12:16.726 A:middle
vectors on lengths up to 64

00:12:16.726 --> 00:12:18.626 A:middle
bytes for doing sort of general

00:12:18.626 --> 00:12:19.306 A:middle
vector programming.

00:12:19.306 --> 00:12:21.226 A:middle
It lets you target all the

00:12:21.226 --> 00:12:22.446 A:middle
architectures we support on all

00:12:22.446 --> 00:12:24.666 A:middle
the platforms we support pretty

00:12:24.666 --> 00:12:25.046 A:middle
easily.

00:12:25.046 --> 00:12:26.216 A:middle
And you can write one piece of

00:12:26.216 --> 00:12:27.636 A:middle
code that gets you good vector

00:12:27.836 --> 00:12:28.796 A:middle
code gen for all of those

00:12:28.796 --> 00:12:29.436 A:middle
architectures.

00:12:30.616 --> 00:12:31.966 A:middle
The last reason you'll use simd

00:12:32.656 --> 00:12:34.416 A:middle
is that it's a great set of

00:12:34.416 --> 00:12:35.686 A:middle
types and operations for

00:12:35.686 --> 00:12:37.676 A:middle
interoperating between all of

00:12:37.676 --> 00:12:39.636 A:middle
the various things that do 3 by

00:12:39.636 --> 00:12:41.096 A:middle
3, 4 by 4 operations on the

00:12:41.096 --> 00:12:41.416 A:middle
platform.

00:12:41.416 --> 00:12:42.486 A:middle
So things like SceneKit,

00:12:42.686 --> 00:12:45.556 A:middle
SpriteKit, ARKit, Vision -- all

00:12:45.556 --> 00:12:46.556 A:middle
those different things you have

00:12:46.806 --> 00:12:48.406 A:middle
lots of matrices and vectors

00:12:48.406 --> 00:12:49.076 A:middle
flying around.

00:12:49.516 --> 00:12:51.246 A:middle
And the simd types are a great

00:12:51.246 --> 00:12:52.126 A:middle
set of things to use with that.

00:12:52.526 --> 00:12:54.136 A:middle
I should say that SpriteKit in

00:12:54.136 --> 00:12:55.166 A:middle
particular -- or SceneKit in

00:12:55.166 --> 00:12:56.336 A:middle
particular added a bunch of new

00:12:56.336 --> 00:12:56.946 A:middle
stuff this year.

00:12:56.946 --> 00:12:57.906 A:middle
So check out their session.

00:12:57.906 --> 00:12:59.006 A:middle
There's some nice stuff for

00:12:59.006 --> 00:12:59.796 A:middle
working with simd there.

00:12:59.796 --> 00:13:01.346 A:middle
I'm going to show you a few

00:13:01.346 --> 00:13:02.706 A:middle
examples of what you can do.

00:13:04.296 --> 00:13:04.966 A:middle
So let's say you want to

00:13:04.966 --> 00:13:06.756 A:middle
multiply a three-dimensional

00:13:06.756 --> 00:13:07.726 A:middle
matrix by a vector.

00:13:07.986 --> 00:13:09.616 A:middle
You can do that using BLAS,

00:13:09.696 --> 00:13:10.666 A:middle
which Eric talked about earlier,

00:13:11.176 --> 00:13:11.826 A:middle
looks like this.

00:13:12.376 --> 00:13:15.776 A:middle
And this is fine, but BLAS takes

00:13:15.826 --> 00:13:17.106 A:middle
all the parameters just as raw

00:13:17.106 --> 00:13:17.556 A:middle
pointers.

00:13:17.586 --> 00:13:19.066 A:middle
So we have to tell it these are

00:13:19.066 --> 00:13:20.106 A:middle
the dimensions of the matrix,

00:13:20.106 --> 00:13:20.906 A:middle
these are the dimensions of the

00:13:20.906 --> 00:13:22.296 A:middle
vector, this is how the memory's

00:13:22.296 --> 00:13:22.706 A:middle
laid out.

00:13:23.016 --> 00:13:23.706 A:middle
There's a lot of other

00:13:23.706 --> 00:13:24.796 A:middle
information we have to pass,

00:13:25.826 --> 00:13:26.706 A:middle
which both makes the code a

00:13:26.706 --> 00:13:28.656 A:middle
little harder to write and it

00:13:28.656 --> 00:13:29.676 A:middle
makes it harder to read.

00:13:29.976 --> 00:13:31.476 A:middle
In you're not fluent with BLAS

00:13:31.476 --> 00:13:33.146 A:middle
already, this last line here,

00:13:33.146 --> 00:13:34.266 A:middle
it's not really obvious that

00:13:34.266 --> 00:13:35.206 A:middle
that's doing a matrix vector

00:13:35.206 --> 00:13:35.596 A:middle
product.

00:13:36.056 --> 00:13:37.066 A:middle
So we'd like to have something

00:13:37.066 --> 00:13:38.306 A:middle
simpler than that.

00:13:39.056 --> 00:13:40.206 A:middle
We could also write this with

00:13:40.206 --> 00:13:40.626 A:middle
GLKit.

00:13:41.136 --> 00:13:42.796 A:middle
GLKit makes it considerably

00:13:42.796 --> 00:13:43.156 A:middle
nicer.

00:13:43.516 --> 00:13:44.966 A:middle
We have dedicated types for a

00:13:44.966 --> 00:13:46.496 A:middle
three-dimensional matrix and for

00:13:46.496 --> 00:13:47.956 A:middle
three-dimensional vector; we

00:13:47.956 --> 00:13:48.366 A:middle
call this

00:13:48.366 --> 00:13:50.696 A:middle
GLKMatrix3MultiplyVector3

00:13:50.696 --> 00:13:51.146 A:middle
function.

00:13:51.676 --> 00:13:52.486 A:middle
It's pretty clear that's a

00:13:52.486 --> 00:13:53.196 A:middle
multiplication.

00:13:53.536 --> 00:13:55.416 A:middle
But we can make this even nicer

00:13:55.786 --> 00:13:56.246 A:middle
using simd.

00:13:56.246 --> 00:13:58.176 A:middle
This is what it looks like in

00:13:58.176 --> 00:13:58.496 A:middle
simd.

00:13:59.016 --> 00:14:00.786 A:middle
Okay? Totally explicit that it's

00:14:00.786 --> 00:14:01.776 A:middle
diagonal matrix.

00:14:02.176 --> 00:14:03.686 A:middle
And multiply a matrix by a

00:14:03.686 --> 00:14:05.006 A:middle
vector is just using the

00:14:05.006 --> 00:14:05.936 A:middle
multiplication operator.

00:14:06.356 --> 00:14:07.936 A:middle
It's really nice, it's really

00:14:07.936 --> 00:14:09.576 A:middle
simple, and it's also really

00:14:09.576 --> 00:14:09.796 A:middle
fast.

00:14:10.536 --> 00:14:12.046 A:middle
All of simd for the most part is

00:14:12.046 --> 00:14:13.296 A:middle
implemented as header inlines.

00:14:13.616 --> 00:14:14.786 A:middle
So this is just going to give me

00:14:14.786 --> 00:14:16.626 A:middle
three vector multiplications on

00:14:16.626 --> 00:14:17.236 A:middle
my CPU.

00:14:17.646 --> 00:14:18.706 A:middle
It's really fast, there's no

00:14:18.706 --> 00:14:20.056 A:middle
call overhead, there's no

00:14:20.056 --> 00:14:21.066 A:middle
parameter checking, there's no

00:14:21.066 --> 00:14:21.396 A:middle
nothing.

00:14:21.726 --> 00:14:23.516 A:middle
I get just nice simple code gen

00:14:23.516 --> 00:14:23.806 A:middle
from this.

00:14:24.526 --> 00:14:25.496 A:middle
So that's a Swift example.

00:14:26.076 --> 00:14:27.706 A:middle
The next example that I'm going

00:14:27.706 --> 00:14:28.896 A:middle
to show you will be in C.

00:14:30.286 --> 00:14:31.906 A:middle
Here I'm going to show you an

00:14:31.906 --> 00:14:33.426 A:middle
example of how you can use simd

00:14:33.426 --> 00:14:34.206 A:middle
to write vector code.

00:14:34.806 --> 00:14:35.806 A:middle
So let's say that we want to

00:14:35.806 --> 00:14:37.506 A:middle
compute a logistic curve with a

00:14:37.506 --> 00:14:39.266 A:middle
given midpoint and maximum

00:14:39.266 --> 00:14:39.666 A:middle
slope.

00:14:40.176 --> 00:14:41.236 A:middle
This is a function that comes up

00:14:41.316 --> 00:14:43.006 A:middle
all the time in sort of

00:14:43.006 --> 00:14:43.866 A:middle
computational mathematics,

00:14:43.866 --> 00:14:44.856 A:middle
especially machine learning.

00:14:44.856 --> 00:14:46.216 A:middle
So this is a useful function to

00:14:46.216 --> 00:14:46.996 A:middle
be able to optimize.

00:14:46.996 --> 00:14:47.936 A:middle
We care a lot about this.

00:14:48.876 --> 00:14:50.126 A:middle
And what I've put in the comment

00:14:50.286 --> 00:14:51.406 A:middle
in the body of the function here

00:14:52.176 --> 00:14:54.286 A:middle
is sort of a typical scalar

00:14:54.286 --> 00:14:55.216 A:middle
implementation, just

00:14:55.216 --> 00:14:56.496 A:middle
mathematically what this looks

00:14:56.496 --> 00:14:56.696 A:middle
like.

00:14:56.906 --> 00:14:59.366 A:middle
But we want to compute this on

00:14:59.576 --> 00:15:01.236 A:middle
16 floating point values

00:15:01.236 --> 00:15:02.456 A:middle
simultaneously because we can

00:15:02.456 --> 00:15:03.656 A:middle
get better efficiency that way.

00:15:03.896 --> 00:15:05.946 A:middle
So that's what this simd float16

00:15:05.946 --> 00:15:06.896 A:middle
type in the function is --

00:15:06.926 --> 00:15:08.226 A:middle
that's just a vector of 16

00:15:08.226 --> 00:15:08.716 A:middle
floats.

00:15:09.376 --> 00:15:10.766 A:middle
And we're going to see if we can

00:15:10.766 --> 00:15:11.676 A:middle
implement the body of this

00:15:11.676 --> 00:15:11.966 A:middle
function.

00:15:12.676 --> 00:15:14.746 A:middle
So vector code is complicated.

00:15:14.746 --> 00:15:16.806 A:middle
I just break this apart into

00:15:16.806 --> 00:15:18.886 A:middle
three pieces so we can write

00:15:18.886 --> 00:15:20.126 A:middle
each one of them individually.

00:15:20.486 --> 00:15:21.496 A:middle
Let's start with this first

00:15:21.496 --> 00:15:22.876 A:middle
section, the linear section.

00:15:23.366 --> 00:15:25.976 A:middle
So we're just subtracting a

00:15:25.976 --> 00:15:26.756 A:middle
scalar from a vector.

00:15:26.756 --> 00:15:27.556 A:middle
We're going to subtract it from

00:15:27.556 --> 00:15:28.676 A:middle
every lane of the vector, and

00:15:28.676 --> 00:15:29.676 A:middle
then we're going to multiply by

00:15:29.676 --> 00:15:30.076 A:middle
a scalar.

00:15:30.686 --> 00:15:32.576 A:middle
What does that look like in

00:15:33.496 --> 00:15:34.136 A:middle
simd?

00:15:34.206 --> 00:15:35.406 A:middle
We just subtract and we

00:15:35.406 --> 00:15:35.896 A:middle
multiply.

00:15:35.896 --> 00:15:36.846 A:middle
It's really, really simple.

00:15:37.366 --> 00:15:38.756 A:middle
This works in C, it works in

00:15:38.756 --> 00:15:40.426 A:middle
Swift, it works in C++, it works

00:15:40.426 --> 00:15:41.146 A:middle
in Objective-C.

00:15:41.536 --> 00:15:42.286 A:middle
It's really nice.

00:15:42.286 --> 00:15:43.506 A:middle
It looks a lot like shader

00:15:43.506 --> 00:15:44.476 A:middle
programming if you've done any

00:15:44.476 --> 00:15:44.776 A:middle
of that.

00:15:45.766 --> 00:15:47.176 A:middle
And this last piece down here

00:15:48.176 --> 00:15:49.446 A:middle
where we take a reciprocal,

00:15:49.556 --> 00:15:51.326 A:middle
again, the same thing, we just

00:15:51.406 --> 00:15:52.866 A:middle
write code that looks like the

00:15:52.866 --> 00:15:53.016 A:middle
math.

00:15:53.016 --> 00:15:54.196 A:middle
It looks just like the scaler

00:15:54.196 --> 00:15:55.136 A:middle
code, it looks just like the

00:15:55.136 --> 00:15:55.736 A:middle
math we're doing.

00:15:56.876 --> 00:15:57.906 A:middle
What about this middle section?

00:15:58.336 --> 00:15:58.996 A:middle
That's a little bit more

00:15:58.996 --> 00:15:59.536 A:middle
complicated.

00:15:59.536 --> 00:16:01.206 A:middle
What we want to do here is for

00:16:01.296 --> 00:16:02.456 A:middle
every element in the vector, we

00:16:02.456 --> 00:16:03.776 A:middle
want to compute the exponential

00:16:03.776 --> 00:16:05.536 A:middle
function of that and put that in

00:16:05.536 --> 00:16:06.936 A:middle
the corresponding element of the

00:16:06.936 --> 00:16:07.586 A:middle
result factor.

00:16:07.836 --> 00:16:10.056 A:middle
We can do that with a for loop.

00:16:10.716 --> 00:16:12.806 A:middle
And this is fine, this works.

00:16:13.686 --> 00:16:14.986 A:middle
But we have a nice new feature

00:16:14.986 --> 00:16:16.836 A:middle
for you this year, which is that

00:16:17.406 --> 00:16:18.606 A:middle
basically all of the math

00:16:18.606 --> 00:16:20.206 A:middle
functions just work on vectors

00:16:20.206 --> 00:16:21.116 A:middle
of floats and doubles.

00:16:21.116 --> 00:16:22.876 A:middle
So any length of vector, float,

00:16:22.876 --> 00:16:23.226 A:middle
and double.

00:16:23.706 --> 00:16:25.236 A:middle
You can call XPath, you can call

00:16:25.236 --> 00:16:26.406 A:middle
sine, you can call cosine,

00:16:26.576 --> 00:16:26.866 A:middle
whatever.

00:16:27.146 --> 00:16:28.096 A:middle
The math functions, they just

00:16:28.096 --> 00:16:28.536 A:middle
work on them.

00:16:28.916 --> 00:16:30.176 A:middle
It's a really nice convenience

00:16:30.176 --> 00:16:31.506 A:middle
feature when you're writing this

00:16:31.506 --> 00:16:31.966 A:middle
kind of code.

00:16:32.736 --> 00:16:35.086 A:middle
And this is going to target the

00:16:35.086 --> 00:16:38.086 A:middle
NEON extensions when I write for

00:16:38.086 --> 00:16:38.356 A:middle
ARM.

00:16:38.456 --> 00:16:39.846 A:middle
And when I compile for Intel,

00:16:39.846 --> 00:16:41.206 A:middle
it's going to target AVX and

00:16:41.206 --> 00:16:41.576 A:middle
SSE.

00:16:41.806 --> 00:16:43.206 A:middle
So I'm going to get fast code on

00:16:43.206 --> 00:16:43.906 A:middle
all the platforms.

00:16:43.906 --> 00:16:44.726 A:middle
This is really nice.

00:16:45.306 --> 00:16:46.856 A:middle
We have one other big new

00:16:46.856 --> 00:16:47.986 A:middle
feature this year that a lot of

00:16:47.986 --> 00:16:49.846 A:middle
people have asked for, which is

00:16:49.846 --> 00:16:50.546 A:middle
quaternions.

00:16:50.986 --> 00:16:52.076 A:middle
I'm going to give you a real

00:16:52.076 --> 00:16:53.036 A:middle
quick introduction to them.

00:16:53.816 --> 00:16:55.816 A:middle
Just that quaternions extend the

00:16:55.816 --> 00:16:57.666 A:middle
complex numbers in the same way

00:16:57.666 --> 00:16:58.716 A:middle
the complex numbers extend the

00:16:58.716 --> 00:16:59.176 A:middle
reals.

00:17:00.676 --> 00:17:02.196 A:middle
So complex number, you might

00:17:02.196 --> 00:17:03.796 A:middle
remember from school, has a real

00:17:03.796 --> 00:17:06.366 A:middle
part and an imaginary part.

00:17:06.916 --> 00:17:10.066 A:middle
Quaternions have a real part and

00:17:10.066 --> 00:17:12.016 A:middle
they have three imaginary parts;

00:17:12.126 --> 00:17:12.836 A:middle
sometimes you call that the

00:17:12.836 --> 00:17:13.376 A:middle
vector part.

00:17:13.776 --> 00:17:15.506 A:middle
My mom always said that if

00:17:15.506 --> 00:17:16.756 A:middle
having one square root of minus

00:17:16.756 --> 00:17:17.896 A:middle
one is good, having three square

00:17:17.896 --> 00:17:18.856 A:middle
roots of minus one must be

00:17:18.856 --> 00:17:19.146 A:middle
great.

00:17:20.706 --> 00:17:21.486 A:middle
She was a smart lady.

00:17:21.486 --> 00:17:24.996 A:middle
You should listen to her.

00:17:25.246 --> 00:17:26.426 A:middle
There's a lot of fascinating

00:17:26.426 --> 00:17:27.526 A:middle
mathematical structure about the

00:17:27.526 --> 00:17:28.876 A:middle
quaternions -- we don't really

00:17:28.876 --> 00:17:29.436 A:middle
care about that.

00:17:29.956 --> 00:17:30.926 A:middle
We're interested in one thing.

00:17:32.026 --> 00:17:33.066 A:middle
Quaternions have a notion of

00:17:33.066 --> 00:17:34.196 A:middle
length that's just like the

00:17:34.196 --> 00:17:35.016 A:middle
complex numbers.

00:17:35.016 --> 00:17:36.166 A:middle
You sum the squares of the

00:17:36.166 --> 00:17:37.006 A:middle
components and you take the

00:17:37.006 --> 00:17:38.396 A:middle
square root, that's the length

00:17:38.396 --> 00:17:38.886 A:middle
of a quaternion.

00:17:39.796 --> 00:17:41.996 A:middle
Quaternions of length one, we

00:17:41.996 --> 00:17:43.886 A:middle
call those unit quaternions, and

00:17:43.886 --> 00:17:45.556 A:middle
they have this really nice

00:17:45.556 --> 00:17:48.286 A:middle
property, which is that you can

00:17:48.286 --> 00:17:50.126 A:middle
use them to represent rotations

00:17:50.456 --> 00:17:51.726 A:middle
in three-dimensional space.

00:17:52.716 --> 00:17:53.796 A:middle
That's all we care about.

00:17:53.796 --> 00:17:55.156 A:middle
Forget about all the other math

00:17:55.156 --> 00:17:57.816 A:middle
that I just mentioned.

00:17:57.896 --> 00:17:59.386 A:middle
So I'll show you a quick code

00:17:59.386 --> 00:18:00.136 A:middle
example of that.

00:18:00.566 --> 00:18:01.606 A:middle
Say we have a vector that's a

00:18:01.606 --> 00:18:02.646 A:middle
vector in the XY plane.

00:18:03.846 --> 00:18:05.126 A:middle
And let's build a quaternion.

00:18:05.476 --> 00:18:06.336 A:middle
This is a quaternion that

00:18:06.336 --> 00:18:08.146 A:middle
represents a rotation around the

00:18:08.146 --> 00:18:08.956 A:middle
y-axis.

00:18:10.526 --> 00:18:12.666 A:middle
Quaternions act on vectors --

00:18:12.666 --> 00:18:14.006 A:middle
that's the simd act function.

00:18:14.806 --> 00:18:16.356 A:middle
It's not multiplication.

00:18:16.356 --> 00:18:18.236 A:middle
When you multiply a vector by a

00:18:18.236 --> 00:18:20.666 A:middle
matrix -- when you rotate a

00:18:20.666 --> 00:18:21.986 A:middle
vector by a matrix, you just

00:18:21.986 --> 00:18:22.686 A:middle
multiply it.

00:18:22.926 --> 00:18:23.886 A:middle
With quaternions it's called an

00:18:23.886 --> 00:18:24.286 A:middle
action.

00:18:24.706 --> 00:18:26.356 A:middle
And we don't care too much about

00:18:26.356 --> 00:18:28.126 A:middle
the details of that, but that's

00:18:28.126 --> 00:18:29.146 A:middle
why this is the act function.

00:18:29.506 --> 00:18:31.106 A:middle
So this is a nice way to

00:18:31.106 --> 00:18:32.246 A:middle
represent rotations.

00:18:32.686 --> 00:18:33.836 A:middle
There's a lot of other ways to

00:18:33.836 --> 00:18:34.856 A:middle
represent rotations.

00:18:35.056 --> 00:18:36.376 A:middle
Why do you want to use this one

00:18:36.376 --> 00:18:37.346 A:middle
and when do you want to use this

00:18:37.346 --> 00:18:37.506 A:middle
one?

00:18:37.596 --> 00:18:40.116 A:middle
You know, you might use matrices

00:18:40.116 --> 00:18:41.346 A:middle
instead, you might use Euler

00:18:41.346 --> 00:18:42.836 A:middle
angles or yaw/pitch/roll, you

00:18:42.836 --> 00:18:43.916 A:middle
can use axis and angle

00:18:43.916 --> 00:18:44.496 A:middle
representation.

00:18:44.496 --> 00:18:45.486 A:middle
There's lots of choices.

00:18:45.946 --> 00:18:47.626 A:middle
Quaternions are an especially

00:18:47.626 --> 00:18:48.856 A:middle
good choice for a certain class

00:18:48.856 --> 00:18:50.106 A:middle
of operations that I'm going to

00:18:50.106 --> 00:18:51.586 A:middle
tell you about.

00:18:52.336 --> 00:18:53.786 A:middle
The first nice thing about

00:18:53.786 --> 00:18:54.716 A:middle
quaternions is they take less

00:18:54.716 --> 00:18:55.746 A:middle
memory than matrices do.

00:18:56.016 --> 00:18:59.316 A:middle
This is nice and it's great, but

00:18:59.316 --> 00:19:00.346 A:middle
that's not really why we want to

00:19:00.346 --> 00:19:01.486 A:middle
use them.

00:19:02.066 --> 00:19:02.876 A:middle
The better thing about

00:19:02.876 --> 00:19:04.616 A:middle
quaternions is that while they

00:19:04.616 --> 00:19:06.936 A:middle
are not especially fast to do a

00:19:06.936 --> 00:19:08.166 A:middle
rotation with, you can see here

00:19:08.166 --> 00:19:09.476 A:middle
they're about a third the speed

00:19:09.476 --> 00:19:10.696 A:middle
of using matrices to actually

00:19:11.056 --> 00:19:11.956 A:middle
compute a rotation.

00:19:13.416 --> 00:19:15.086 A:middle
When you want to do a sequence

00:19:15.086 --> 00:19:16.206 A:middle
of operations, when you want to

00:19:16.206 --> 00:19:17.596 A:middle
combine rotations or you want to

00:19:17.596 --> 00:19:18.766 A:middle
interpolate rotations, you want

00:19:18.766 --> 00:19:19.486 A:middle
to do anything like that,

00:19:19.846 --> 00:19:21.836 A:middle
they're the most natural setting

00:19:21.836 --> 00:19:23.016 A:middle
to do those operations in.

00:19:23.476 --> 00:19:24.806 A:middle
So when we want to multiply two

00:19:24.806 --> 00:19:25.916 A:middle
rotations together, you can see

00:19:25.916 --> 00:19:27.846 A:middle
quaternions are about 30% faster

00:19:28.046 --> 00:19:29.196 A:middle
than vectors and matrices.

00:19:29.576 --> 00:19:31.316 A:middle
But they also let us do things

00:19:31.316 --> 00:19:32.366 A:middle
that are hard to do with

00:19:32.366 --> 00:19:32.856 A:middle
matrices.

00:19:32.856 --> 00:19:35.336 A:middle
So let's say we want to

00:19:35.336 --> 00:19:36.946 A:middle
interpolate between two

00:19:36.946 --> 00:19:38.106 A:middle
different rotated coordinate

00:19:38.106 --> 00:19:38.556 A:middle
frames.

00:19:39.256 --> 00:19:40.536 A:middle
That's a little subtle with

00:19:40.536 --> 00:19:42.036 A:middle
matrices, but it's really

00:19:42.036 --> 00:19:43.106 A:middle
natural with quaternions.

00:19:43.536 --> 00:19:45.026 A:middle
And the reason it's natural has

00:19:45.026 --> 00:19:46.886 A:middle
to do with this sphere that I've

00:19:46.886 --> 00:19:47.806 A:middle
drawn over on the side of the

00:19:47.806 --> 00:19:48.306 A:middle
screen here.

00:19:48.426 --> 00:19:49.336 A:middle
You might say, "Well, why are

00:19:49.336 --> 00:19:50.836 A:middle
you drawing rotations on a

00:19:50.836 --> 00:19:51.166 A:middle
sphere?"

00:19:51.876 --> 00:19:52.766 A:middle
There's a good reason for that.

00:19:53.606 --> 00:19:55.746 A:middle
Quaternions you can think of as

00:19:55.846 --> 00:19:57.066 A:middle
points on the four-dimensional

00:19:57.066 --> 00:19:57.796 A:middle
projective sphere.

00:19:58.866 --> 00:20:00.586 A:middle
And that sounds complicated and

00:20:00.586 --> 00:20:02.926 A:middle
mathematical, and it is, but

00:20:03.166 --> 00:20:04.816 A:middle
it's the natural space of

00:20:04.816 --> 00:20:05.956 A:middle
rotations for three-dimensional

00:20:05.956 --> 00:20:06.376 A:middle
space.

00:20:06.826 --> 00:20:08.406 A:middle
So when I do operations on the

00:20:08.406 --> 00:20:09.966 A:middle
surface of this sphere, that

00:20:09.966 --> 00:20:11.446 A:middle
corresponds exactly to your

00:20:11.446 --> 00:20:12.816 A:middle
natural intuition for what

00:20:12.816 --> 00:20:13.846 A:middle
should happen with rotations.

00:20:13.946 --> 00:20:16.576 A:middle
So for instance, if we want to

00:20:16.576 --> 00:20:17.726 A:middle
interpolate between them, we

00:20:17.726 --> 00:20:19.666 A:middle
just interpolate along a great

00:20:19.666 --> 00:20:20.766 A:middle
circle on the sphere -- that's

00:20:20.766 --> 00:20:22.196 A:middle
this simd slerp function that

00:20:22.196 --> 00:20:23.376 A:middle
stands for spherical linear

00:20:23.376 --> 00:20:24.056 A:middle
interpolation.

00:20:24.676 --> 00:20:26.656 A:middle
And that's really easy to use

00:20:26.656 --> 00:20:27.926 A:middle
and it does exactly what you

00:20:27.926 --> 00:20:28.266 A:middle
want.

00:20:28.636 --> 00:20:30.346 A:middle
If we have a whole sequence of

00:20:30.386 --> 00:20:31.416 A:middle
points that we want to

00:20:31.416 --> 00:20:33.596 A:middle
interpolate between, we could

00:20:33.596 --> 00:20:35.496 A:middle
call slerp repeatedly to

00:20:35.496 --> 00:20:36.786 A:middle
interpolate between them, but

00:20:37.056 --> 00:20:38.946 A:middle
that will have a noticeable jump

00:20:38.946 --> 00:20:40.726 A:middle
a little bit in the rotation

00:20:40.826 --> 00:20:41.836 A:middle
when you get to corners.

00:20:42.886 --> 00:20:45.136 A:middle
Instead we can use the simd

00:20:45.136 --> 00:20:46.616 A:middle
spline function to get a

00:20:46.616 --> 00:20:48.566 A:middle
completely smooth interpolation

00:20:48.566 --> 00:20:49.786 A:middle
using a whole sequence of

00:20:49.786 --> 00:20:50.936 A:middle
rotated coordinate frames.

00:20:51.626 --> 00:20:52.806 A:middle
There's a ton of other

00:20:53.016 --> 00:20:54.796 A:middle
operations like this in the simd

00:20:54.796 --> 00:20:56.746 A:middle
headers that you can use to do

00:20:56.746 --> 00:20:58.616 A:middle
these types of operations.

00:20:58.936 --> 00:20:59.846 A:middle
If you want to work with

00:20:59.846 --> 00:21:01.786 A:middle
rotations, the API is really

00:21:01.786 --> 00:21:02.356 A:middle
pretty full.

00:21:02.356 --> 00:21:03.666 A:middle
I encourage you to check it out.

00:21:04.176 --> 00:21:05.326 A:middle
And as I said, this was one of

00:21:05.326 --> 00:21:06.306 A:middle
the most requested features from

00:21:06.306 --> 00:21:06.846 A:middle
developers.

00:21:06.846 --> 00:21:08.726 A:middle
So I encourage you to file bugs

00:21:08.726 --> 00:21:10.016 A:middle
to say, "Hey, can you guys add

00:21:10.016 --> 00:21:11.016 A:middle
this other thing as well?"

00:21:11.016 --> 00:21:12.016 A:middle
We're really responsive to that.

00:21:12.016 --> 00:21:13.206 A:middle
We love to get feature requests

00:21:13.206 --> 00:21:13.846 A:middle
from our users.

00:21:14.656 --> 00:21:16.206 A:middle
With that, I'm going to turn you

00:21:16.206 --> 00:21:18.346 A:middle
over to Jonathan Hogg, who's

00:21:18.346 --> 00:21:19.426 A:middle
going to tell you all about

00:21:19.586 --> 00:21:21.366 A:middle
really, really big matrices.

00:21:22.516 --> 00:21:26.946 A:middle
[ Applause ]

00:21:27.446 --> 00:21:29.846 A:middle
&gt;&gt; Hi. So you've heard from

00:21:29.846 --> 00:21:32.206 A:middle
Steve about simd, which is

00:21:32.206 --> 00:21:33.196 A:middle
really great for very, very

00:21:33.196 --> 00:21:34.146 A:middle
small matrices.

00:21:34.336 --> 00:21:35.896 A:middle
And I'm going to tell you in a

00:21:35.896 --> 00:21:37.386 A:middle
little while about very big

00:21:37.386 --> 00:21:38.226 A:middle
matrices.

00:21:38.516 --> 00:21:39.856 A:middle
But first I'm going to tell you

00:21:39.856 --> 00:21:40.886 A:middle
about BLAS and LAPACK.

00:21:41.396 --> 00:21:43.346 A:middle
These are libraries for handling

00:21:43.726 --> 00:21:44.406 A:middle
dense matrices.

00:21:44.406 --> 00:21:46.436 A:middle
So you can get up to 30,000 or

00:21:46.436 --> 00:21:48.766 A:middle
40,000 rows of columns here on a

00:21:48.766 --> 00:21:49.266 A:middle
MacBook.

00:21:50.846 --> 00:21:51.516 A:middle
What do we have?

00:21:52.816 --> 00:21:55.046 A:middle
So BLAS stands for Basic Linear

00:21:55.046 --> 00:21:56.886 A:middle
Algebra Subroutines, and these

00:21:56.886 --> 00:21:59.056 A:middle
do basic operations on matrices

00:21:59.056 --> 00:21:59.716 A:middle
and vectors.

00:22:00.646 --> 00:22:02.436 A:middle
We have BLAS 1, which does

00:22:02.556 --> 00:22:03.996 A:middle
vector vector operations.

00:22:04.176 --> 00:22:05.846 A:middle
And then we move through BLAS 2

00:22:05.846 --> 00:22:08.056 A:middle
for matrix vector, to BLAS 3 for

00:22:08.056 --> 00:22:09.356 A:middle
matrix matrix operations.

00:22:09.746 --> 00:22:10.546 A:middle
And you've already seen from

00:22:10.546 --> 00:22:13.046 A:middle
Eric that we can be 100 times

00:22:13.046 --> 00:22:14.546 A:middle
faster on the matrix matrix

00:22:14.546 --> 00:22:16.236 A:middle
multiply than your simple loop.

00:22:17.256 --> 00:22:18.236 A:middle
If you want to do things more

00:22:18.236 --> 00:22:20.036 A:middle
complicated than this, then we

00:22:20.036 --> 00:22:21.186 A:middle
have LAPACK.

00:22:21.906 --> 00:22:22.876 A:middle
These do your matrix

00:22:22.876 --> 00:22:24.386 A:middle
factorizations, your linear

00:22:24.386 --> 00:22:26.646 A:middle
solves, your find Eigenvalues,

00:22:26.736 --> 00:22:28.486 A:middle
Eigenvectors, SVD's -- pretty

00:22:28.946 --> 00:22:31.066 A:middle
much everything you want to do.

00:22:32.896 --> 00:22:33.866 A:middle
That's all I'm going to tell you

00:22:33.866 --> 00:22:35.646 A:middle
about dense matrices because we

00:22:35.646 --> 00:22:38.286 A:middle
want to actually talk about

00:22:38.956 --> 00:22:39.376 A:middle
sparse matrices.

00:22:39.376 --> 00:22:41.406 A:middle
What is a sparse matrix?

00:22:44.136 --> 00:22:46.516 A:middle
So James Wilkinson was one of

00:22:46.516 --> 00:22:47.606 A:middle
the founding fathers of

00:22:47.606 --> 00:22:48.996 A:middle
computational linear algebra.

00:22:49.226 --> 00:22:51.646 A:middle
This was his definition, "Sparse

00:22:51.646 --> 00:22:52.896 A:middle
matrix is any one where

00:22:52.896 --> 00:22:56.076 A:middle
exploiting zeros is useful to

00:22:57.196 --> 00:22:57.296 A:middle
us."

00:22:57.516 --> 00:22:59.046 A:middle
Let's see what sparse matrix

00:22:59.046 --> 00:23:00.446 A:middle
actually looks like.

00:23:01.416 --> 00:23:03.776 A:middle
So here are two sparse matrices.

00:23:03.876 --> 00:23:05.096 A:middle
One's actually the Cholesky

00:23:05.096 --> 00:23:06.786 A:middle
factorization of the other.

00:23:07.416 --> 00:23:09.266 A:middle
And each pixel here represents

00:23:09.266 --> 00:23:10.396 A:middle
multiple non-zeros.

00:23:11.366 --> 00:23:14.826 A:middle
Where they are white, all of the

00:23:14.826 --> 00:23:16.936 A:middle
entries behind that pixel are

00:23:16.936 --> 00:23:17.306 A:middle
zero.

00:23:17.836 --> 00:23:19.746 A:middle
Where there's blue, that means

00:23:19.746 --> 00:23:21.066 A:middle
at least one non-zero's present.

00:23:21.066 --> 00:23:22.756 A:middle
So you can see these matrices

00:23:23.026 --> 00:23:24.156 A:middle
are mostly empty.

00:23:25.246 --> 00:23:26.936 A:middle
In fact, if you were to store

00:23:26.936 --> 00:23:28.896 A:middle
this as a dense matrix, it's

00:23:28.896 --> 00:23:30.226 A:middle
about 30,000 by 30,000.

00:23:30.226 --> 00:23:32.696 A:middle
So it takes us 6.5 gigabytes.

00:23:33.396 --> 00:23:34.446 A:middle
If we store it as a sparse

00:23:34.446 --> 00:23:37.706 A:middle
matrix, we require 260 times

00:23:38.016 --> 00:23:40.366 A:middle
less storage -- only 26

00:23:40.366 --> 00:23:41.026 A:middle
megabytes.

00:23:41.386 --> 00:23:43.106 A:middle
If we wanted to multiply this

00:23:43.106 --> 00:23:46.246 A:middle
matrix by a vector, we'd require

00:23:46.436 --> 00:23:48.586 A:middle
almost 200 times fewer floating

00:23:48.586 --> 00:23:49.746 A:middle
point operations.

00:23:50.656 --> 00:23:51.716 A:middle
But if we want to do something

00:23:51.716 --> 00:23:53.906 A:middle
more complicated like factorize

00:23:53.906 --> 00:23:58.246 A:middle
this matrix, things get better,

00:23:58.776 --> 00:24:00.596 A:middle
at least in the floating point.

00:24:00.966 --> 00:24:03.616 A:middle
We require 2,000 times fewer

00:24:03.616 --> 00:24:05.026 A:middle
floating point operations to

00:24:05.246 --> 00:24:06.506 A:middle
factorize this matrix.

00:24:07.496 --> 00:24:09.086 A:middle
And the dense matrix is still

00:24:09.086 --> 00:24:10.466 A:middle
the same size, the factor's

00:24:10.466 --> 00:24:11.266 A:middle
filled in a bit.

00:24:11.806 --> 00:24:13.136 A:middle
It's slightly less sparse than

00:24:13.136 --> 00:24:15.516 A:middle
it was, so we're only 30 times

00:24:15.516 --> 00:24:16.596 A:middle
better on the storage.

00:24:17.446 --> 00:24:19.086 A:middle
Now, to drive that point home,

00:24:19.256 --> 00:24:20.666 A:middle
we've set up a little race.

00:24:21.196 --> 00:24:22.746 A:middle
We decided we'd run the sparse

00:24:22.746 --> 00:24:25.556 A:middle
solver on a Watch and we put it

00:24:25.686 --> 00:24:27.666 A:middle
up against our best dense matrix

00:24:27.666 --> 00:24:30.286 A:middle
solver from LAPACK on a Macbook

00:24:30.356 --> 00:24:30.686 A:middle
Air.

00:24:31.506 --> 00:24:32.806 A:middle
And this is what happened.

00:24:33.466 --> 00:24:36.896 A:middle
Now, this is running at five

00:24:36.896 --> 00:24:38.816 A:middle
times real time.

00:24:39.136 --> 00:24:40.656 A:middle
And you can see that the Watch

00:24:40.916 --> 00:24:43.756 A:middle
is finished in only 16 seconds.

00:24:43.756 --> 00:24:44.946 A:middle
You got to remember while

00:24:44.946 --> 00:24:46.966 A:middle
watching this that the floating

00:24:46.966 --> 00:24:48.176 A:middle
point throughput on the MacBook

00:24:48.176 --> 00:24:50.466 A:middle
Air is about 50 times that

00:24:50.466 --> 00:24:51.736 A:middle
available on the Watch.

00:24:52.516 --> 00:24:56.546 A:middle
[ Laughter ]

00:24:57.046 --> 00:24:59.076 A:middle
Now, there's actually two phases

00:24:59.076 --> 00:25:00.046 A:middle
to this factorization in the

00:25:00.046 --> 00:25:00.626 A:middle
sparse world.

00:25:00.986 --> 00:25:02.596 A:middle
First we find where the

00:25:02.596 --> 00:25:03.476 A:middle
positions of the non-zero

00:25:03.476 --> 00:25:05.036 A:middle
entry's going to be -- that's

00:25:05.036 --> 00:25:05.886 A:middle
the symbolic phase.

00:25:06.116 --> 00:25:07.966 A:middle
Then we have a numeric phase

00:25:08.166 --> 00:25:09.056 A:middle
which calculates the actual

00:25:09.056 --> 00:25:09.566 A:middle
values.

00:25:10.646 --> 00:25:11.846 A:middle
These times are just for the

00:25:11.846 --> 00:25:14.436 A:middle
numeric phase, but the symbolic

00:25:14.436 --> 00:25:15.816 A:middle
phase only takes about two

00:25:15.816 --> 00:25:16.926 A:middle
seconds on the Watch.

00:25:17.286 --> 00:25:19.266 A:middle
So rather than 16 seconds, you

00:25:19.266 --> 00:25:21.046 A:middle
could say it takes about 18

00:25:21.046 --> 00:25:21.526 A:middle
seconds.

00:25:21.526 --> 00:25:22.846 A:middle
But if you're doing more than

00:25:22.846 --> 00:25:25.596 A:middle
one factorization on the same

00:25:26.236 --> 00:25:27.676 A:middle
pattern, you can skip that

00:25:27.676 --> 00:25:29.636 A:middle
symbolic phase on the second and

00:25:29.636 --> 00:25:30.526 A:middle
third factorization.

00:25:31.336 --> 00:25:32.506 A:middle
Even if you include that, we're

00:25:32.506 --> 00:25:33.916 A:middle
still more than 10 times faster

00:25:33.916 --> 00:25:35.266 A:middle
than the Macbook due to dense

00:25:35.266 --> 00:25:36.006 A:middle
computation.

00:25:37.336 --> 00:25:39.316 A:middle
Hopefully I've now convinced you

00:25:39.586 --> 00:25:40.716 A:middle
that it's worth using sparse

00:25:40.716 --> 00:25:41.346 A:middle
matrices.

00:25:41.576 --> 00:25:43.296 A:middle
So let's tell you how to

00:25:43.296 --> 00:25:44.616 A:middle
actually define one.

00:25:45.436 --> 00:25:47.476 A:middle
So here is a very, very small

00:25:47.476 --> 00:25:48.426 A:middle
sparse matrix.

00:25:48.796 --> 00:25:49.596 A:middle
You'll notice it's missing

00:25:49.596 --> 00:25:51.886 A:middle
entries -- those are zero.

00:25:51.886 --> 00:25:54.896 A:middle
We are going to store this using

00:25:54.896 --> 00:25:55.936 A:middle
a standard format called

00:25:55.936 --> 00:25:57.496 A:middle
compressed sparse column, which

00:25:57.496 --> 00:25:59.806 A:middle
uses these three arrays.

00:26:01.146 --> 00:26:02.946 A:middle
And we'll start off with the

00:26:02.946 --> 00:26:03.926 A:middle
rowIndices rate.

00:26:04.186 --> 00:26:06.676 A:middle
So let's put the row numbers

00:26:06.996 --> 00:26:08.156 A:middle
onto our matrix.

00:26:09.166 --> 00:26:10.526 A:middle
And we'll just copy those up

00:26:10.656 --> 00:26:11.816 A:middle
into the rowIndices array.

00:26:12.226 --> 00:26:14.416 A:middle
And let's do something similar

00:26:14.416 --> 00:26:15.206 A:middle
for the values.

00:26:15.776 --> 00:26:17.846 A:middle
You can see we got a one-to-one

00:26:17.846 --> 00:26:18.736 A:middle
correspondence here.

00:26:19.316 --> 00:26:21.386 A:middle
That first entry is in row zero

00:26:21.386 --> 00:26:22.386 A:middle
and has value two.

00:26:23.796 --> 00:26:26.076 A:middle
So let's just put on the

00:26:26.076 --> 00:26:28.256 A:middle
positions of those entries in

00:26:28.256 --> 00:26:29.456 A:middle
that rowIndices and values

00:26:29.456 --> 00:26:29.756 A:middle
array.

00:26:30.656 --> 00:26:31.746 A:middle
And the trick to compressed

00:26:31.746 --> 00:26:34.046 A:middle
sparse column is that all these

00:26:34.046 --> 00:26:36.106 A:middle
values have to occur in order of

00:26:36.106 --> 00:26:37.106 A:middle
increasing column.

00:26:37.536 --> 00:26:38.966 A:middle
All entries in column zero

00:26:38.966 --> 00:26:40.666 A:middle
actually occur before those from

00:26:40.666 --> 00:26:41.686 A:middle
column one.

00:26:41.896 --> 00:26:44.116 A:middle
And then we get to the trick to

00:26:44.116 --> 00:26:45.556 A:middle
this format, which is we're only

00:26:45.556 --> 00:26:47.876 A:middle
going to store the position of

00:26:47.876 --> 00:26:50.846 A:middle
the first entry in each column

00:26:51.826 --> 00:26:52.906 A:middle
and one additional piece of

00:26:52.906 --> 00:26:55.346 A:middle
information, the total number of

00:26:55.346 --> 00:26:57.446 A:middle
entries in the matrix.

00:26:57.446 --> 00:26:58.886 A:middle
That means that we know how long

00:26:58.886 --> 00:27:00.026 A:middle
that last column is.

00:27:01.386 --> 00:27:02.986 A:middle
If you've already using a sparse

00:27:02.986 --> 00:27:04.986 A:middle
solver, you should probably have

00:27:04.986 --> 00:27:06.546 A:middle
your data in either this format

00:27:06.856 --> 00:27:08.606 A:middle
or a coordinate format and we

00:27:08.676 --> 00:27:11.076 A:middle
provide a converter.

00:27:11.176 --> 00:27:12.846 A:middle
To use it in Accelerate, we need

00:27:12.846 --> 00:27:14.556 A:middle
to wrap it in some metadata,

00:27:15.036 --> 00:27:16.526 A:middle
just telling it how many rows,

00:27:16.526 --> 00:27:17.396 A:middle
how many columns.

00:27:17.756 --> 00:27:19.326 A:middle
And we're going to say this one

00:27:19.326 --> 00:27:20.636 A:middle
is an ordinary sparse matrix.

00:27:20.636 --> 00:27:21.996 A:middle
I've got an example of an

00:27:21.996 --> 00:27:24.426 A:middle
unordinary sparse matrix in a

00:27:24.426 --> 00:27:25.386 A:middle
couple of slides.

00:27:26.136 --> 00:27:27.736 A:middle
Now I've got my sparse matrix,

00:27:27.866 --> 00:27:30.226 A:middle
what can I do with it?

00:27:31.426 --> 00:27:33.386 A:middle
So you can do pretty much

00:27:33.386 --> 00:27:34.296 A:middle
anything you'd expect.

00:27:34.296 --> 00:27:36.356 A:middle
You can multiply a dense vector

00:27:36.356 --> 00:27:38.416 A:middle
or a dense matrix by it; you can

00:27:38.506 --> 00:27:39.886 A:middle
add two sparse matrices or

00:27:39.886 --> 00:27:41.376 A:middle
sparse vectors together; you can

00:27:41.376 --> 00:27:43.516 A:middle
permute the rows or columns; or

00:27:43.516 --> 00:27:45.076 A:middle
you can find various useful

00:27:45.076 --> 00:27:46.046 A:middle
matrix norms.

00:27:46.766 --> 00:27:47.996 A:middle
All that functionality is

00:27:47.996 --> 00:27:49.476 A:middle
provided by the Sparse BLAS,

00:27:49.776 --> 00:27:51.516 A:middle
which we introduced a few years

00:27:51.516 --> 00:27:51.616 A:middle
ago.

00:27:52.476 --> 00:27:53.536 A:middle
So what's new this time?

00:27:54.756 --> 00:27:56.636 A:middle
The ability to solve sparse

00:27:56.636 --> 00:28:00.846 A:middle
systems, that is, given a matrix

00:28:00.846 --> 00:28:03.596 A:middle
equation A times X equals B

00:28:03.676 --> 00:28:04.866 A:middle
where we know the matrix A and

00:28:04.866 --> 00:28:07.376 A:middle
to the right-hand side B, find

00:28:07.526 --> 00:28:09.366 A:middle
that vector of unknowns X.

00:28:10.666 --> 00:28:12.656 A:middle
So we got two approaches to this

00:28:12.656 --> 00:28:12.976 A:middle
for you.

00:28:13.586 --> 00:28:15.636 A:middle
The first is matrix

00:28:15.636 --> 00:28:16.166 A:middle
factorization.

00:28:16.246 --> 00:28:17.686 A:middle
This is exactly what happens in

00:28:17.686 --> 00:28:18.226 A:middle
LAPACK.

00:28:19.086 --> 00:28:20.396 A:middle
It's simple, it's accurate, it's

00:28:20.536 --> 00:28:21.616 A:middle
easy to use.

00:28:22.446 --> 00:28:23.916 A:middle
But mathematicians being

00:28:23.916 --> 00:28:25.396 A:middle
mathematicians, they came up

00:28:25.516 --> 00:28:26.646 A:middle
with a more complicated way of

00:28:26.646 --> 00:28:27.376 A:middle
doing things.

00:28:28.696 --> 00:28:29.946 A:middle
That's iterative methods.

00:28:29.946 --> 00:28:30.746 A:middle
And I'll tell you a bit more

00:28:30.746 --> 00:28:31.546 A:middle
about those later.

00:28:32.106 --> 00:28:35.536 A:middle
So now a matrix factorization.

00:28:35.686 --> 00:28:37.196 A:middle
For those of you who haven't

00:28:37.196 --> 00:28:39.096 A:middle
come across this before, we want

00:28:39.096 --> 00:28:41.256 A:middle
to take our green matrix on the

00:28:41.256 --> 00:28:42.926 A:middle
left here and factorize it into

00:28:42.926 --> 00:28:44.166 A:middle
the products of two triangular

00:28:44.166 --> 00:28:45.116 A:middle
matrices on the right.

00:28:46.176 --> 00:28:47.106 A:middle
That's because we know how to

00:28:47.106 --> 00:28:48.226 A:middle
solve a system with a triangular

00:28:48.226 --> 00:28:49.616 A:middle
matrix very well.

00:28:50.626 --> 00:28:52.186 A:middle
If we're not square, we have to

00:28:52.186 --> 00:28:53.376 A:middle
do things slightly differently;

00:28:54.136 --> 00:28:55.586 A:middle
we have to pick a rectangular

00:28:55.586 --> 00:28:57.656 A:middle
and orthogonal factor here.

00:28:57.656 --> 00:28:59.136 A:middle
And this is your QR

00:28:59.136 --> 00:29:00.746 A:middle
factorization if you've heard of

00:29:00.746 --> 00:29:01.466 A:middle
that before.

00:29:02.526 --> 00:29:05.276 A:middle
So let's see how to actually do

00:29:05.976 --> 00:29:06.076 A:middle
this.

00:29:06.256 --> 00:29:08.076 A:middle
Here is a sparse matrix

00:29:08.206 --> 00:29:08.686 A:middle
equation.

00:29:09.866 --> 00:29:11.346 A:middle
And let's define that matrix

00:29:11.346 --> 00:29:11.896 A:middle
just to begin with.

00:29:12.566 --> 00:29:14.586 A:middle
So this is going to be very

00:29:14.586 --> 00:29:15.896 A:middle
similar to what I just showed

00:29:15.896 --> 00:29:17.526 A:middle
you, except this matrix is

00:29:17.526 --> 00:29:17.936 A:middle
special.

00:29:18.626 --> 00:29:19.466 A:middle
It's symmetric.

00:29:19.956 --> 00:29:21.316 A:middle
That means that the lower

00:29:21.316 --> 00:29:23.426 A:middle
triangle is just the mirror

00:29:23.426 --> 00:29:24.366 A:middle
reflection of the other

00:29:24.366 --> 00:29:24.746 A:middle
triangle.

00:29:24.796 --> 00:29:27.036 A:middle
So we can take advantage of that

00:29:27.036 --> 00:29:28.376 A:middle
and let's only store those lower

00:29:28.376 --> 00:29:29.366 A:middle
triangle entries.

00:29:30.486 --> 00:29:31.966 A:middle
Wrap it in that metadata and

00:29:31.966 --> 00:29:34.606 A:middle
this time we're going to specify

00:29:34.606 --> 00:29:35.746 A:middle
that this is not ordinary, this

00:29:35.746 --> 00:29:37.086 A:middle
is a symmetric matrix.

00:29:37.476 --> 00:29:38.666 A:middle
And we're going to tell it that

00:29:38.666 --> 00:29:40.076 A:middle
we're passing the lower

00:29:40.076 --> 00:29:40.606 A:middle
triangle.

00:29:40.946 --> 00:29:41.936 A:middle
We could pass the upper triangle

00:29:41.936 --> 00:29:43.306 A:middle
if we wanted, we've chosen the

00:29:43.306 --> 00:29:44.246 A:middle
lower triangle here.

00:29:45.156 --> 00:29:46.806 A:middle
So we got our matrix.

00:29:47.206 --> 00:29:48.616 A:middle
Next let's look at that

00:29:48.616 --> 00:29:49.446 A:middle
right-hand side.

00:29:50.036 --> 00:29:51.526 A:middle
So this is a dense vector.

00:29:52.606 --> 00:29:54.456 A:middle
Let's just have a simple array.

00:29:55.146 --> 00:29:56.136 A:middle
Wrap it in a little bit of

00:29:56.136 --> 00:29:57.296 A:middle
metadata, telling us how long it

00:29:57.296 --> 00:30:00.906 A:middle
is, and that's how easy this is.

00:30:01.516 --> 00:30:03.196 A:middle
Which gets us to the interesting

00:30:03.196 --> 00:30:04.936 A:middle
part, how do we actually find

00:30:05.036 --> 00:30:08.836 A:middle
that vector X?

00:30:09.056 --> 00:30:11.016 A:middle
So let's define some storage to

00:30:11.126 --> 00:30:11.866 A:middle
put the answer in.

00:30:11.866 --> 00:30:13.736 A:middle
This is exactly the same as flat

00:30:13.736 --> 00:30:15.206 A:middle
dense vector B we just saw,

00:30:15.436 --> 00:30:16.646 A:middle
except we don't have to supply

00:30:16.646 --> 00:30:17.606 A:middle
any values.

00:30:18.126 --> 00:30:20.476 A:middle
Then we're going to call

00:30:20.476 --> 00:30:21.376 A:middle
SparseFactor.

00:30:21.776 --> 00:30:23.136 A:middle
And I know this matrix is

00:30:23.136 --> 00:30:24.876 A:middle
positive definite; therefore, I

00:30:24.876 --> 00:30:25.876 A:middle
can tell it use a Cholesky

00:30:25.876 --> 00:30:26.496 A:middle
factorization.

00:30:26.646 --> 00:30:28.516 A:middle
I've got a flowchart for you in

00:30:28.516 --> 00:30:29.606 A:middle
a couple of slides which tells

00:30:29.606 --> 00:30:30.716 A:middle
you how to pick a factorization

00:30:30.716 --> 00:30:33.046 A:middle
to use, but here we're using

00:30:33.046 --> 00:30:33.476 A:middle
Cholesky.

00:30:33.886 --> 00:30:35.646 A:middle
That gets us L times L transpose

00:30:35.646 --> 00:30:37.346 A:middle
factorization, which we then

00:30:37.346 --> 00:30:39.436 A:middle
feed into SparseSolve, pass that

00:30:39.436 --> 00:30:40.766 A:middle
right-hand side and the storage

00:30:40.766 --> 00:30:41.416 A:middle
we specify it.

00:30:41.746 --> 00:30:42.876 A:middle
And we get our answer.

00:30:43.576 --> 00:30:44.626 A:middle
We can put that back into the

00:30:44.626 --> 00:30:45.226 A:middle
equation.

00:30:45.466 --> 00:30:48.876 A:middle
And we can see this is correct.

00:30:49.006 --> 00:30:51.226 A:middle
So what if A is not square?

00:30:53.356 --> 00:30:55.136 A:middle
Well, this is where we have to

00:30:55.136 --> 00:30:56.116 A:middle
use that QR factorization

00:30:56.116 --> 00:30:57.026 A:middle
[inaudible] I mentioned before.

00:30:57.026 --> 00:30:59.346 A:middle
But we got two different cases

00:30:59.346 --> 00:30:59.546 A:middle
here.

00:30:59.596 --> 00:31:00.906 A:middle
It's not entirely simple.

00:31:01.266 --> 00:31:03.286 A:middle
We can be overdetermined since

00:31:03.286 --> 00:31:05.676 A:middle
we have more rows than columns.

00:31:06.676 --> 00:31:07.656 A:middle
Unless you've picked a very

00:31:07.656 --> 00:31:08.626 A:middle
special system here, that

00:31:08.626 --> 00:31:09.786 A:middle
probably means there's no

00:31:09.786 --> 00:31:11.546 A:middle
exactly correct answer.

00:31:12.106 --> 00:31:13.026 A:middle
In fact, you're in this sort of

00:31:13.026 --> 00:31:13.676 A:middle
situation.

00:31:14.976 --> 00:31:17.186 A:middle
Put a straight line through

00:31:17.186 --> 00:31:18.206 A:middle
these four points.

00:31:20.056 --> 00:31:22.246 A:middle
Clearly that's impossible, but

00:31:22.246 --> 00:31:22.926 A:middle
if you remember back to your

00:31:22.926 --> 00:31:24.586 A:middle
school days, you probably did

00:31:24.886 --> 00:31:26.136 A:middle
some least squares fitting.

00:31:27.396 --> 00:31:29.716 A:middle
We pick a line which minimizes

00:31:29.866 --> 00:31:31.426 A:middle
sum of the square of the arrows.

00:31:31.616 --> 00:31:32.746 A:middle
It's exactly what we do in this

00:31:32.746 --> 00:31:33.266 A:middle
case.

00:31:34.076 --> 00:31:35.296 A:middle
Remember we want to solve X

00:31:35.296 --> 00:31:35.896 A:middle
equals B.

00:31:35.996 --> 00:31:37.966 A:middle
So the arrow is X minus B.

00:31:38.506 --> 00:31:39.686 A:middle
Let's take the two normals out,

00:31:39.776 --> 00:31:41.046 A:middle
which is effectively the sum of

00:31:41.046 --> 00:31:41.936 A:middle
the square of the arrows in this

00:31:42.436 --> 00:31:45.126 A:middle
example, and minimize that.

00:31:46.676 --> 00:31:48.336 A:middle
If we're underdetermined, that

00:31:48.336 --> 00:31:49.876 A:middle
is, we have more columns than

00:31:49.876 --> 00:31:51.486 A:middle
rows, we're in a slightly

00:31:51.486 --> 00:31:52.456 A:middle
different situation.

00:31:53.816 --> 00:31:55.836 A:middle
It's equivalent to saying put a

00:31:55.836 --> 00:31:57.196 A:middle
line through this point.

00:31:58.026 --> 00:32:00.086 A:middle
Obviously, there's an infinite

00:32:00.086 --> 00:32:01.386 A:middle
family of lines which goes

00:32:01.386 --> 00:32:02.036 A:middle
through that point.

00:32:02.526 --> 00:32:04.906 A:middle
So how do we pick one to return

00:32:04.906 --> 00:32:05.496 A:middle
to you?

00:32:06.476 --> 00:32:08.536 A:middle
We give you the solution with

00:32:08.536 --> 00:32:09.056 A:middle
minimum norm.

00:32:09.056 --> 00:32:13.446 A:middle
Let's look at that on a code

00:32:13.446 --> 00:32:13.856 A:middle
slide.

00:32:15.276 --> 00:32:17.416 A:middle
Here it's very, very similar to

00:32:17.416 --> 00:32:18.616 A:middle
that Cholesky factorization we

00:32:18.616 --> 00:32:19.116 A:middle
saw before.

00:32:19.656 --> 00:32:22.356 A:middle
In fact, the only difference is

00:32:22.356 --> 00:32:23.426 A:middle
that we say to use a QR

00:32:23.426 --> 00:32:25.136 A:middle
factorization rather than

00:32:25.136 --> 00:32:25.656 A:middle
Cholesky.

00:32:25.876 --> 00:32:27.756 A:middle
And this will automatically pick

00:32:27.756 --> 00:32:28.836 A:middle
whether to do the least squares

00:32:28.836 --> 00:32:30.336 A:middle
or the minimum norm depending on

00:32:30.336 --> 00:32:32.336 A:middle
the dimensions of your matrix.

00:32:32.816 --> 00:32:34.906 A:middle
And I told you that we had a

00:32:34.906 --> 00:32:36.246 A:middle
flowchart for you on how to

00:32:36.246 --> 00:32:37.636 A:middle
decide which factorization to

00:32:37.636 --> 00:32:38.016 A:middle
use.

00:32:38.596 --> 00:32:40.306 A:middle
So the first question you have

00:32:41.046 --> 00:32:45.346 A:middle
to ask is: Is your matrix

00:32:45.416 --> 00:32:46.006 A:middle
symmetric?

00:32:47.536 --> 00:32:49.706 A:middle
If it isn't, you have to use the

00:32:49.706 --> 00:32:50.956 A:middle
QR factorization.

00:32:51.556 --> 00:32:53.076 A:middle
But if it is, we have another

00:32:53.076 --> 00:32:55.496 A:middle
question for you: Is your matrix

00:32:55.496 --> 00:32:56.266 A:middle
positive definite?

00:32:56.706 --> 00:32:57.756 A:middle
Now, if you don't know the

00:32:57.756 --> 00:33:00.416 A:middle
answer or if you're sure it

00:33:00.416 --> 00:33:02.646 A:middle
isn't, you can do a symmetric

00:33:02.646 --> 00:33:05.356 A:middle
indefinite factorization, LDL

00:33:05.446 --> 00:33:06.626 A:middle
transpose.

00:33:07.246 --> 00:33:08.636 A:middle
But if you know that extra bit

00:33:08.636 --> 00:33:09.696 A:middle
of information that you've got

00:33:09.696 --> 00:33:11.496 A:middle
positive definite matrix, you

00:33:11.496 --> 00:33:12.286 A:middle
can use the Cholesky

00:33:12.286 --> 00:33:14.206 A:middle
factorization L times L

00:33:14.206 --> 00:33:15.076 A:middle
transposes.

00:33:15.926 --> 00:33:17.406 A:middle
And that's all we have to tell

00:33:17.406 --> 00:33:18.906 A:middle
you on matrix factorizations.

00:33:18.906 --> 00:33:21.006 A:middle
Now, I said there was this other

00:33:21.096 --> 00:33:22.936 A:middle
technique, iterative methods.

00:33:22.936 --> 00:33:26.906 A:middle
So what is an iterative method?

00:33:27.886 --> 00:33:29.806 A:middle
Well, we pick a starting point,

00:33:29.806 --> 00:33:31.476 A:middle
our best guess at the solution

00:33:31.476 --> 00:33:32.566 A:middle
before we start.

00:33:32.616 --> 00:33:34.326 A:middle
And this can be zero if you

00:33:34.326 --> 00:33:35.676 A:middle
don't have any idea what the

00:33:35.676 --> 00:33:37.276 A:middle
actual answer is going to look

00:33:37.276 --> 00:33:37.536 A:middle
like.

00:33:38.136 --> 00:33:40.346 A:middle
And we want to get within some

00:33:40.346 --> 00:33:42.336 A:middle
small radius of our actual

00:33:42.336 --> 00:33:42.936 A:middle
solution.

00:33:42.936 --> 00:33:45.286 A:middle
And the way we do that is we

00:33:45.286 --> 00:33:46.646 A:middle
iterate through a series of

00:33:46.646 --> 00:33:48.676 A:middle
points which converge to that

00:33:48.676 --> 00:33:49.186 A:middle
solution.

00:33:50.156 --> 00:33:51.616 A:middle
Now, there's a couple of caveats

00:33:51.616 --> 00:33:52.376 A:middle
with using these.

00:33:53.816 --> 00:33:55.156 A:middle
Typically they're only going to

00:33:55.156 --> 00:33:56.906 A:middle
be faster than that matrix

00:33:56.906 --> 00:33:59.176 A:middle
factorization approach if you've

00:33:59.176 --> 00:34:01.286 A:middle
got a really, really, really big

00:34:01.286 --> 00:34:02.316 A:middle
sparse matrix.

00:34:03.026 --> 00:34:04.616 A:middle
And further, to actually get to

00:34:04.616 --> 00:34:06.256 A:middle
that performance, you need to

00:34:06.256 --> 00:34:07.576 A:middle
know a bit mathematically about

00:34:07.576 --> 00:34:08.146 A:middle
your problem.

00:34:08.306 --> 00:34:09.146 A:middle
You need something called a

00:34:09.146 --> 00:34:11.126 A:middle
preconditioner, which is a very

00:34:11.126 --> 00:34:12.126 A:middle
approximate solution.

00:34:13.246 --> 00:34:14.846 A:middle
And if you check the literature

00:34:14.846 --> 00:34:16.176 A:middle
for your field, you'll probably

00:34:16.176 --> 00:34:17.776 A:middle
find quite a number have been

00:34:18.126 --> 00:34:20.036 A:middle
derived by mathematicians.

00:34:21.206 --> 00:34:22.266 A:middle
What does this actually look

00:34:22.266 --> 00:34:23.046 A:middle
like to use?

00:34:23.676 --> 00:34:25.516 A:middle
So here's that matrix equation

00:34:25.516 --> 00:34:26.066 A:middle
we had earlier.

00:34:26.456 --> 00:34:27.456 A:middle
This time I'm going to use

00:34:27.456 --> 00:34:28.686 A:middle
iterative method to solve it.

00:34:29.546 --> 00:34:30.946 A:middle
In fact, I'm going to use

00:34:30.946 --> 00:34:32.106 A:middle
conjugate gradients.

00:34:32.846 --> 00:34:34.416 A:middle
So this is positive definite.

00:34:35.316 --> 00:34:38.676 A:middle
So we just specify to use the

00:34:38.676 --> 00:34:39.656 A:middle
conjugate gradient methods.

00:34:39.656 --> 00:34:40.526 A:middle
And you'll notice there's some

00:34:40.526 --> 00:34:41.646 A:middle
brackets ever this.

00:34:42.736 --> 00:34:43.666 A:middle
That's actually because this is

00:34:43.666 --> 00:34:45.126 A:middle
a factory function which

00:34:45.126 --> 00:34:46.846 A:middle
produces the methods and you can

00:34:46.846 --> 00:34:48.956 A:middle
specify method-specific

00:34:48.956 --> 00:34:50.486 A:middle
parameters in those brackets.

00:34:51.216 --> 00:34:52.146 A:middle
The other thing I'm going to do

00:34:52.146 --> 00:34:53.506 A:middle
is I'm going to use a diagonal

00:34:53.506 --> 00:34:54.066 A:middle
precondition.

00:34:54.556 --> 00:34:55.796 A:middle
This matrix is diagonally

00:34:55.796 --> 00:34:56.166 A:middle
dominant.

00:34:56.166 --> 00:34:57.486 A:middle
That means that the entries down

00:34:57.486 --> 00:34:58.716 A:middle
the diagonal are very large

00:34:58.716 --> 00:34:59.646 A:middle
compared to those off the

00:34:59.646 --> 00:35:02.006 A:middle
diagonal; therefore, I know this

00:35:02.006 --> 00:35:03.206 A:middle
diagonal preconditioner will

00:35:03.206 --> 00:35:04.146 A:middle
work very well.

00:35:04.736 --> 00:35:06.886 A:middle
And indeed, if we look at the

00:35:06.886 --> 00:35:08.536 A:middle
output of the algorithm, you can

00:35:08.536 --> 00:35:10.316 A:middle
see that this arrow AX minus B

00:35:10.316 --> 00:35:12.556 A:middle
is decreasing its iteration and

00:35:12.556 --> 00:35:13.696 A:middle
we get to machine precision in

00:35:13.696 --> 00:35:14.766 A:middle
four iterations.

00:35:14.876 --> 00:35:16.316 A:middle
That's because it's 4 by 4

00:35:16.316 --> 00:35:16.906 A:middle
matrix.

00:35:17.176 --> 00:35:18.046 A:middle
Mathematically, we should

00:35:18.046 --> 00:35:21.006 A:middle
converge in at most N iterations

00:35:21.216 --> 00:35:22.646 A:middle
where N is the size of matrix.

00:35:23.526 --> 00:35:24.826 A:middle
So this is behaving as expected.

00:35:24.856 --> 00:35:25.916 A:middle
But if you've got a much larger

00:35:25.916 --> 00:35:27.126 A:middle
matrix, you probably don't want

00:35:27.126 --> 00:35:28.676 A:middle
to go that many iterations,

00:35:29.266 --> 00:35:30.196 A:middle
which is why you get an

00:35:30.196 --> 00:35:31.186 A:middle
approximate solution.

00:35:32.366 --> 00:35:33.556 A:middle
And you can see you the get same

00:35:33.556 --> 00:35:34.286 A:middle
answer as before.

00:35:35.096 --> 00:35:37.196 A:middle
Now, let's say we want to solve

00:35:37.196 --> 00:35:38.026 A:middle
the least squares problem

00:35:38.026 --> 00:35:38.516 A:middle
instead.

00:35:39.466 --> 00:35:41.426 A:middle
We offer a least square solver,

00:35:41.506 --> 00:35:42.196 A:middle
which is iterative.

00:35:42.496 --> 00:35:43.126 A:middle
We don't offer an

00:35:43.126 --> 00:35:45.606 A:middle
underdetermined system solver,

00:35:45.606 --> 00:35:45.946 A:middle
however.

00:35:46.196 --> 00:35:47.986 A:middle
In that case you can just pick a

00:35:47.986 --> 00:35:48.956 A:middle
[inaudible] of zeros and call

00:35:48.956 --> 00:35:50.086 A:middle
them in solver square system

00:35:50.086 --> 00:35:50.556 A:middle
instead.

00:35:51.476 --> 00:35:53.916 A:middle
And to use this, we use the

00:35:53.956 --> 00:35:56.176 A:middle
method LSMR and a slightly

00:35:56.176 --> 00:35:57.806 A:middle
different preconditioner which

00:35:57.806 --> 00:35:59.486 A:middle
is, again, problem-specific.

00:35:59.996 --> 00:36:01.946 A:middle
And you can see that we get

00:36:01.946 --> 00:36:03.326 A:middle
there three iterations this time

00:36:03.326 --> 00:36:05.176 A:middle
because this is a 4 by 3 matrix.

00:36:05.796 --> 00:36:07.696 A:middle
But there are some very cool

00:36:07.696 --> 00:36:11.226 A:middle
things about this particular way

00:36:11.226 --> 00:36:11.976 A:middle
of doing things.

00:36:13.446 --> 00:36:15.646 A:middle
The first is that I don't

00:36:15.646 --> 00:36:16.996 A:middle
actually need my matrix

00:36:16.996 --> 00:36:17.606 A:middle
explicitly.

00:36:18.946 --> 00:36:20.856 A:middle
As long as I have a function

00:36:21.306 --> 00:36:22.866 A:middle
which performs the mathematical

00:36:22.866 --> 00:36:25.006 A:middle
operation A times X or A

00:36:25.006 --> 00:36:26.606 A:middle
transpose times X, that is, you

00:36:26.606 --> 00:36:28.886 A:middle
have an operator, I can

00:36:28.886 --> 00:36:30.676 A:middle
substitute a block of code in

00:36:30.676 --> 00:36:31.896 A:middle
place of this actual matrix

00:36:31.896 --> 00:36:32.376 A:middle
argument.

00:36:33.476 --> 00:36:35.496 A:middle
The second is you're not

00:36:35.496 --> 00:36:36.996 A:middle
restricted to using our

00:36:36.996 --> 00:36:37.956 A:middle
preconditioners.

00:36:38.786 --> 00:36:40.206 A:middle
You can write your own and just

00:36:40.206 --> 00:36:41.376 A:middle
provide a function pointer in

00:36:41.376 --> 00:36:41.966 A:middle
this argument.

00:36:44.646 --> 00:36:46.276 A:middle
Now, you're probably saying,

00:36:46.366 --> 00:36:47.776 A:middle
"How do I know which iterative

00:36:47.776 --> 00:36:48.366 A:middle
method to use?"

00:36:48.446 --> 00:36:49.156 A:middle
I've got another one of those

00:36:49.156 --> 00:36:49.896 A:middle
flowcharts for you.

00:36:50.996 --> 00:36:52.386 A:middle
This time our first question is

00:36:52.386 --> 00:36:54.046 A:middle
not whether you are symmetric

00:36:54.046 --> 00:36:55.366 A:middle
but whether you are square.

00:36:55.846 --> 00:36:57.216 A:middle
If you're not square, you're

00:36:57.216 --> 00:36:58.126 A:middle
going to have to do a least

00:36:58.126 --> 00:36:58.706 A:middle
square solve.

00:36:59.926 --> 00:37:01.666 A:middle
However, if you are, we go

00:37:01.666 --> 00:37:03.066 A:middle
straight to that question are

00:37:03.066 --> 00:37:04.056 A:middle
you positive definite?

00:37:05.286 --> 00:37:07.406 A:middle
If you're not, we have GMRES --

00:37:07.406 --> 00:37:08.926 A:middle
that will handle pretty much any

00:37:08.926 --> 00:37:09.866 A:middle
square matrix.

00:37:10.916 --> 00:37:12.516 A:middle
But if you know that extra bit

00:37:12.516 --> 00:37:13.856 A:middle
of information, you can, of

00:37:13.856 --> 00:37:15.316 A:middle
course, use the famous conjugate

00:37:15.316 --> 00:37:15.996 A:middle
gradient method.

00:37:16.526 --> 00:37:18.946 A:middle
Now, that's everything I have to

00:37:18.946 --> 00:37:20.476 A:middle
tell you today about sparse

00:37:20.476 --> 00:37:21.106 A:middle
matrices.

00:37:22.056 --> 00:37:23.956 A:middle
So we've got one thing we want

00:37:23.956 --> 00:37:27.396 A:middle
to point out, and that is that

00:37:27.776 --> 00:37:29.706 A:middle
you can now use Accelerate on

00:37:29.966 --> 00:37:30.426 A:middle
the Watch.

00:37:30.426 --> 00:37:31.956 A:middle
We have provided you that SDK.

00:37:31.956 --> 00:37:34.996 A:middle
Now, the framework has always

00:37:34.996 --> 00:37:35.626 A:middle
been there.

00:37:35.866 --> 00:37:36.906 A:middle
So it's even better.

00:37:36.976 --> 00:37:39.576 A:middle
Using today's SDK you can back

00:37:39.686 --> 00:37:42.166 A:middle
deploy to previous Watch OS's.

00:37:42.536 --> 00:37:44.256 A:middle
So that means that you get

00:37:44.856 --> 00:37:46.316 A:middle
everything that we've told you

00:37:46.316 --> 00:37:47.986 A:middle
about today on the Watch.

00:37:48.796 --> 00:37:51.056 A:middle
So let's just summarize what

00:37:51.056 --> 00:37:51.496 A:middle
that is.

00:37:51.916 --> 00:37:53.966 A:middle
By using Accelerate your code

00:37:53.966 --> 00:37:54.936 A:middle
will run faster.

00:37:55.306 --> 00:37:56.616 A:middle
It will be more energy

00:37:56.616 --> 00:37:57.076 A:middle
efficient.

00:37:57.396 --> 00:37:58.596 A:middle
It will run across all our

00:37:58.596 --> 00:37:59.296 A:middle
devices.

00:37:59.356 --> 00:38:00.746 A:middle
And at the end of the day you

00:38:00.746 --> 00:38:02.026 A:middle
have less code to maintain.

00:38:02.486 --> 00:38:03.676 A:middle
You get everything here we've

00:38:03.676 --> 00:38:04.626 A:middle
told you about today -- that

00:38:04.626 --> 00:38:06.246 A:middle
sparse solver library, the new

00:38:06.246 --> 00:38:08.996 A:middle
compression tool, changes to the

00:38:09.656 --> 00:38:11.356 A:middle
BNNS, improvements to simd, and

00:38:11.356 --> 00:38:12.756 A:middle
many more, and increased

00:38:12.756 --> 00:38:13.926 A:middle
performance across the

00:38:13.926 --> 00:38:14.386 A:middle
framework.

00:38:16.166 --> 00:38:17.446 A:middle
So if you want some more

00:38:17.446 --> 00:38:19.206 A:middle
information, including some

00:38:19.206 --> 00:38:20.306 A:middle
extensive sample code we've

00:38:20.306 --> 00:38:21.666 A:middle
developed for the sparse solver,

00:38:21.986 --> 00:38:22.896 A:middle
it's all available here.

00:38:23.756 --> 00:38:27.336 A:middle
And you may be interested in

00:38:27.336 --> 00:38:28.026 A:middle
reviewing some of these

00:38:28.026 --> 00:38:29.186 A:middle
sessions, which have already

00:38:29.186 --> 00:38:30.656 A:middle
been or going to the Metal

00:38:30.656 --> 00:38:32.096 A:middle
session this afternoon.

00:38:33.416 --> 00:38:34.786 A:middle
Thank you for your time.

00:38:35.016 --> 00:38:37.000 A:middle
[ Applause ]