WEBVTT

00:00:28.596 --> 00:00:29.576 A:middle
&gt;&gt; Good afternoon everyone,

00:00:30.076 --> 00:00:32.006 A:middle
welcome to our talk on using

00:00:32.006 --> 00:00:32.946 A:middle
Metal 2 for Compute.

00:00:33.886 --> 00:00:35.296 A:middle
My name is Anna Tikhonova.

00:00:35.296 --> 00:00:36.686 A:middle
I'm an engineer on the GPU

00:00:36.686 --> 00:00:37.976 A:middle
Software Team, so let's begin.

00:00:42.156 --> 00:00:44.046 A:middle
The Metal 2 echo system is so

00:00:44.046 --> 00:00:45.836 A:middle
much more than the Metal API and

00:00:45.836 --> 00:00:46.356 A:middle
the language.

00:00:46.796 --> 00:00:48.946 A:middle
We also have the GPU Tools and

00:00:48.946 --> 00:00:50.336 A:middle
we have the MetalKit and Metal

00:00:50.336 --> 00:00:51.586 A:middle
Performance Shaders frameworks.

00:00:53.006 --> 00:00:54.146 A:middle
You might know Metal as this

00:00:54.496 --> 00:00:56.376 A:middle
great technology for developing

00:00:56.376 --> 00:00:57.566 A:middle
high-end games and graphics.

00:00:58.396 --> 00:00:59.616 A:middle
But it can also be used for

00:00:59.616 --> 00:01:00.506 A:middle
Compute processing.

00:01:01.626 --> 00:01:02.806 A:middle
In fact, the Compute side of

00:01:02.806 --> 00:01:04.616 A:middle
Metal is so powerful and

00:01:04.616 --> 00:01:06.526 A:middle
flexible that the Metal

00:01:06.526 --> 00:01:08.196 A:middle
Performance Shaders framework is

00:01:08.196 --> 00:01:09.326 A:middle
built completely on top of

00:01:09.406 --> 00:01:09.756 A:middle
Compute.

00:01:11.026 --> 00:01:12.486 A:middle
And in this session, we'll talk

00:01:12.486 --> 00:01:14.076 A:middle
about what's new in the Metal

00:01:14.076 --> 00:01:15.176 A:middle
Performance Shaders framework.

00:01:17.956 --> 00:01:19.596 A:middle
We introduced the Metal

00:01:19.596 --> 00:01:21.026 A:middle
Performers Shaders framework, or

00:01:21.026 --> 00:01:22.756 A:middle
MPS in 2015.

00:01:23.486 --> 00:01:24.546 A:middle
And the videos of our past

00:01:24.546 --> 00:01:25.916 A:middle
sessions are available on our

00:01:25.916 --> 00:01:28.906 A:middle
developer website.

00:01:29.266 --> 00:01:30.686 A:middle
MPS uses the compute power of

00:01:30.686 --> 00:01:33.446 A:middle
the GPU to bring GPU accelerated

00:01:33.446 --> 00:01:33.816 A:middle
primitives.

00:01:34.286 --> 00:01:35.886 A:middle
For image processing, linear

00:01:35.926 --> 00:01:37.416 A:middle
algebra and machine learning.

00:01:39.146 --> 00:01:40.416 A:middle
The framework is optimized for

00:01:40.416 --> 00:01:42.336 A:middle
iOS and we're happy to announce

00:01:42.336 --> 00:01:44.096 A:middle
that this year we're also

00:01:44.096 --> 00:01:44.836 A:middle
bringing MPS to the Mac.

00:01:45.516 --> 00:01:49.786 A:middle
[ Applause ]

00:01:50.286 --> 00:01:50.716 A:middle
Thank you.

00:01:51.926 --> 00:01:53.366 A:middle
The entire feature set is

00:01:53.366 --> 00:01:55.616 A:middle
available in both iOS and macOS.

00:01:55.616 --> 00:01:58.476 A:middle
So let's begin with a quick

00:01:58.476 --> 00:02:00.336 A:middle
update on our image processing

00:02:00.336 --> 00:02:00.686 A:middle
support.

00:02:02.046 --> 00:02:03.866 A:middle
So here's a list of all of the

00:02:03.866 --> 00:02:05.576 A:middle
primitives for image processing

00:02:05.696 --> 00:02:07.486 A:middle
that we had available in iOS 10.

00:02:08.106 --> 00:02:09.676 A:middle
So there's Convolution, Gaussian

00:02:09.676 --> 00:02:11.586 A:middle
Blur, Lanczos Resampling, just

00:02:11.586 --> 00:02:12.196 A:middle
to name a few.

00:02:13.126 --> 00:02:14.726 A:middle
They're all now available in

00:02:14.726 --> 00:02:14.996 A:middle
macOS.

00:02:16.146 --> 00:02:17.656 A:middle
And this year we're bringing you

00:02:17.656 --> 00:02:18.926 A:middle
four new image processing

00:02:18.926 --> 00:02:19.336 A:middle
primitives.

00:02:20.466 --> 00:02:21.816 A:middle
The Image Keypoints primitive

00:02:22.206 --> 00:02:24.316 A:middle
can be used -- is often used in

00:02:24.316 --> 00:02:26.256 A:middle
computer vision algorithms such

00:02:26.256 --> 00:02:28.596 A:middle
as image stabilization and

00:02:28.596 --> 00:02:29.926 A:middle
Bilinear Rescale, Image

00:02:29.926 --> 00:02:31.726 A:middle
Statistics, and Element-wise

00:02:31.726 --> 00:02:33.246 A:middle
Arithmetic Operators, are

00:02:33.326 --> 00:02:34.826 A:middle
commonly used to pre-process

00:02:34.826 --> 00:02:35.156 A:middle
images.

00:02:35.466 --> 00:02:36.386 A:middle
For example, in machine

00:02:36.386 --> 00:02:36.656 A:middle
learning.

00:02:37.556 --> 00:02:38.926 A:middle
And the arithmetic filters also

00:02:38.926 --> 00:02:40.446 A:middle
support broadcasting operations.

00:02:41.256 --> 00:02:42.716 A:middle
Which, for example, allow you to

00:02:42.716 --> 00:02:44.766 A:middle
add a 2D image or the 1D image.

00:02:46.176 --> 00:02:48.406 A:middle
So that's it for our very quick

00:02:48.406 --> 00:02:49.606 A:middle
update on image processing.

00:02:49.986 --> 00:02:51.286 A:middle
And now let's talk about the new

00:02:51.286 --> 00:02:52.386 A:middle
Linear Algebra operations.

00:02:54.286 --> 00:02:55.686 A:middle
Without support, Matrix

00:02:55.686 --> 00:02:57.436 A:middle
Multiplication, Matrix Vector

00:02:57.436 --> 00:02:59.736 A:middle
Multiplication, and Triangular

00:03:00.076 --> 00:03:01.866 A:middle
Matrix Factorization and Linear

00:03:01.866 --> 00:03:02.306 A:middle
Solvers.

00:03:05.356 --> 00:03:06.376 A:middle
To support Linear Algebra

00:03:06.376 --> 00:03:09.256 A:middle
operations, we now have multiple

00:03:09.256 --> 00:03:10.356 A:middle
new data representations.

00:03:11.066 --> 00:03:13.076 A:middle
First, we have the MPSVector

00:03:13.076 --> 00:03:15.186 A:middle
object which interprets the data

00:03:15.186 --> 00:03:16.496 A:middle
in a metal buffer as a

00:03:16.496 --> 00:03:17.416 A:middle
one-dimensional array.

00:03:19.106 --> 00:03:21.506 A:middle
And we have an MPSMatrix object

00:03:22.076 --> 00:03:23.276 A:middle
which interprets the data in a

00:03:23.276 --> 00:03:24.886 A:middle
metal buffer as a rectangular

00:03:24.886 --> 00:03:25.156 A:middle
array.

00:03:25.886 --> 00:03:27.656 A:middle
And MPS matrices are in role

00:03:27.656 --> 00:03:28.176 A:middle
major order.

00:03:28.956 --> 00:03:30.166 A:middle
And you can think of both

00:03:30.166 --> 00:03:32.956 A:middle
MPSVectors and MPSMatrices as

00:03:33.456 --> 00:03:34.736 A:middle
wrappers around user data

00:03:34.736 --> 00:03:35.096 A:middle
buffers.

00:03:37.436 --> 00:03:39.296 A:middle
And we also support a temporary

00:03:39.296 --> 00:03:40.926 A:middle
variance of MPSMatrix.

00:03:42.296 --> 00:03:45.196 A:middle
MPS images -- temporary images

00:03:45.196 --> 00:03:47.056 A:middle
and MPSTemporaryMatrices are

00:03:47.056 --> 00:03:48.666 A:middle
allocated from a Metal heap

00:03:48.906 --> 00:03:49.956 A:middle
associated with a command

00:03:49.956 --> 00:03:50.216 A:middle
buffer.

00:03:50.766 --> 00:03:51.856 A:middle
And they are called temporary

00:03:52.226 --> 00:03:54.066 A:middle
because their lifespan is

00:03:54.216 --> 00:03:55.996 A:middle
limited to the lifetime of the

00:03:55.996 --> 00:03:56.516 A:middle
command buffer.

00:03:57.546 --> 00:03:58.866 A:middle
And we recommend you use

00:03:58.896 --> 00:04:00.176 A:middle
temporary images and matrices

00:04:00.636 --> 00:04:02.526 A:middle
for most of your intermediate

00:04:03.006 --> 00:04:03.206 A:middle
storage.

00:04:04.316 --> 00:04:07.416 A:middle
Both MPSVector and MPSMatrix

00:04:07.576 --> 00:04:09.116 A:middle
support a number of input types.

00:04:09.646 --> 00:04:11.596 A:middle
We support single-precision and

00:04:11.596 --> 00:04:14.236 A:middle
half-precision input types and a

00:04:14.236 --> 00:04:15.266 A:middle
floating-point input types.

00:04:15.756 --> 00:04:17.696 A:middle
And 16-bits and 8-bit signed

00:04:17.986 --> 00:04:19.126 A:middle
integer input types.

00:04:21.016 --> 00:04:22.136 A:middle
And now let's take a look at how

00:04:22.136 --> 00:04:24.446 A:middle
we can create an MPSVector of

00:04:24.536 --> 00:04:24.886 A:middle
size N.

00:04:24.886 --> 00:04:26.856 A:middle
So if you don't already have a

00:04:26.856 --> 00:04:28.346 A:middle
Metal buffer, you need to create

00:04:28.346 --> 00:04:28.606 A:middle
one.

00:04:29.666 --> 00:04:30.666 A:middle
And then you need to create a

00:04:30.666 --> 00:04:31.686 A:middle
descriptor for your vector.

00:04:32.526 --> 00:04:34.546 A:middle
And note here that you specify

00:04:34.726 --> 00:04:36.086 A:middle
the length of the vector.

00:04:36.666 --> 00:04:38.146 A:middle
That's because the vector can be

00:04:38.146 --> 00:04:39.946 A:middle
made from a portion of the

00:04:39.946 --> 00:04:40.926 A:middle
original Metal buffer.

00:04:41.716 --> 00:04:43.236 A:middle
And other related offsets can be

00:04:43.236 --> 00:04:44.556 A:middle
set in a kernel that will use

00:04:44.556 --> 00:04:45.016 A:middle
this vector.

00:04:45.996 --> 00:04:47.406 A:middle
And then the last step is

00:04:47.466 --> 00:04:49.586 A:middle
creating a vector from the

00:04:49.586 --> 00:04:50.906 A:middle
buffer with a descriptor.

00:04:52.966 --> 00:04:53.976 A:middle
And now let's take a look at how

00:04:53.976 --> 00:04:56.256 A:middle
you can create an MPSMatrix with

00:04:56.326 --> 00:04:57.906 A:middle
M rows and N columns.

00:04:59.516 --> 00:05:00.906 A:middle
So it's very similar to the way

00:05:00.906 --> 00:05:02.916 A:middle
you would create MPSVector, but

00:05:02.916 --> 00:05:04.006 A:middle
there's just a few things we

00:05:04.006 --> 00:05:04.726 A:middle
want to mention.

00:05:06.176 --> 00:05:08.256 A:middle
We provide a convenient API that

00:05:08.256 --> 00:05:09.606 A:middle
you can use to find the

00:05:09.606 --> 00:05:11.966 A:middle
recommended bytes per row value

00:05:12.556 --> 00:05:13.756 A:middle
for sizing your Metal buffers.

00:05:14.666 --> 00:05:15.716 A:middle
And if you choose to use the

00:05:15.796 --> 00:05:17.246 A:middle
API, this is how you would

00:05:17.246 --> 00:05:18.816 A:middle
create a metal buffer with this

00:05:18.816 --> 00:05:19.596 A:middle
recommended value.

00:05:20.596 --> 00:05:22.106 A:middle
And using this API is completely

00:05:22.106 --> 00:05:24.416 A:middle
optional, but recommended for

00:05:24.416 --> 00:05:25.136 A:middle
better performance.

00:05:25.986 --> 00:05:27.156 A:middle
And then the rest is simple.

00:05:28.256 --> 00:05:29.356 A:middle
You create a descriptor for your

00:05:29.356 --> 00:05:30.886 A:middle
matrix, and then you create a

00:05:30.886 --> 00:05:32.346 A:middle
matrix with a descriptor.

00:05:34.936 --> 00:05:36.506 A:middle
And now that we talked about the

00:05:36.506 --> 00:05:37.806 A:middle
data presentations, now let's

00:05:37.806 --> 00:05:39.046 A:middle
talk about the primitives.

00:05:39.896 --> 00:05:41.176 A:middle
So for Matrix-Matrix and

00:05:41.176 --> 00:05:43.136 A:middle
Matrix-Vector multiplication our

00:05:43.136 --> 00:05:44.586 A:middle
API is modeled after the

00:05:44.586 --> 00:05:46.036 A:middle
standard BLAS GEMM and GEMV

00:05:46.036 --> 00:05:46.766 A:middle
interfaces.

00:05:47.646 --> 00:05:48.846 A:middle
And for triangular matrix

00:05:48.846 --> 00:05:50.196 A:middle
vectorization and linear

00:05:50.196 --> 00:05:52.216 A:middle
solvers, our API is modeled

00:05:52.216 --> 00:05:53.246 A:middle
after standard LAPACK

00:05:53.246 --> 00:05:54.916 A:middle
decomposition and solve

00:05:54.916 --> 00:05:55.486 A:middle
interfaces.

00:05:55.846 --> 00:05:57.036 A:middle
So if you're familiar with those

00:05:57.036 --> 00:05:59.106 A:middle
interfaces our API will look

00:05:59.106 --> 00:06:00.526 A:middle
very familiar to you as well.

00:06:02.576 --> 00:06:04.216 A:middle
And now let's take a look at a

00:06:04.216 --> 00:06:05.546 A:middle
very simple code example.

00:06:05.836 --> 00:06:07.536 A:middle
So we'll be doing matrix

00:06:07.536 --> 00:06:08.996 A:middle
multiplication and computing

00:06:08.996 --> 00:06:10.426 A:middle
just C = A times B.

00:06:10.426 --> 00:06:12.716 A:middle
So first we need to create our

00:06:12.716 --> 00:06:14.476 A:middle
matrices A, B and C.

00:06:14.706 --> 00:06:15.646 A:middle
But I know you know how to do

00:06:15.646 --> 00:06:16.786 A:middle
this, I showed you in a previous

00:06:16.836 --> 00:06:18.306 A:middle
slide, so let's move on.

00:06:19.336 --> 00:06:21.116 A:middle
Now we want to run matrix

00:06:21.116 --> 00:06:22.596 A:middle
multiplication on the GPU.

00:06:23.886 --> 00:06:25.386 A:middle
So first we do our usual Metal

00:06:25.466 --> 00:06:27.446 A:middle
setup to getting a device, a

00:06:27.446 --> 00:06:29.076 A:middle
command queue, and a command

00:06:29.076 --> 00:06:29.356 A:middle
buffer.

00:06:29.356 --> 00:06:31.766 A:middle
And then we need to create our

00:06:31.846 --> 00:06:33.186 A:middle
matrix multiplication kernel.

00:06:33.766 --> 00:06:35.196 A:middle
And note here that you specify

00:06:35.196 --> 00:06:36.296 A:middle
the size of the result.

00:06:36.826 --> 00:06:38.216 A:middle
That's because this kernel can

00:06:38.216 --> 00:06:39.896 A:middle
operate on subregions of

00:06:39.896 --> 00:06:40.436 A:middle
matrices.

00:06:41.066 --> 00:06:45.576 A:middle
And then we encode this kernel

00:06:45.656 --> 00:06:47.056 A:middle
to the GPU and tell it to start

00:06:47.056 --> 00:06:47.476 A:middle
doing the work.

00:06:47.476 --> 00:06:51.036 A:middle
And we already have sample code

00:06:51.036 --> 00:06:52.346 A:middle
from Matrix multiplication

00:06:52.586 --> 00:06:53.906 A:middle
available on our developer

00:06:53.906 --> 00:06:56.096 A:middle
website, and the sample code for

00:06:56.096 --> 00:06:57.896 A:middle
triangular matrix vectorization

00:06:58.206 --> 00:06:59.556 A:middle
and solving a system of linear

00:06:59.556 --> 00:07:00.986 A:middle
equations is coming very soon.

00:07:03.026 --> 00:07:05.526 A:middle
So that's it for our -- for the

00:07:05.756 --> 00:07:07.026 A:middle
linear algebra operations.

00:07:07.386 --> 00:07:08.996 A:middle
Let's now move on to the next

00:07:08.996 --> 00:07:10.756 A:middle
topic, which is Accelerating

00:07:10.756 --> 00:07:12.156 A:middle
Machine Learning Primitives on

00:07:12.156 --> 00:07:12.636 A:middle
the GPU.

00:07:14.116 --> 00:07:16.246 A:middle
There are a number of

00:07:16.246 --> 00:07:17.906 A:middle
machine-learning related talks

00:07:17.906 --> 00:07:19.156 A:middle
at WWDC this year.

00:07:19.456 --> 00:07:20.346 A:middle
And we are a part of the

00:07:20.346 --> 00:07:21.416 A:middle
machine-learning community.

00:07:22.306 --> 00:07:23.586 A:middle
And this slide shows the overall

00:07:23.586 --> 00:07:24.206 A:middle
architecture.

00:07:25.086 --> 00:07:26.606 A:middle
So as an application developer,

00:07:26.816 --> 00:07:27.876 A:middle
you can add machine learning

00:07:27.876 --> 00:07:28.816 A:middle
functionality to your

00:07:28.816 --> 00:07:30.956 A:middle
applications by using high-level

00:07:30.996 --> 00:07:32.856 A:middle
domain-specific frameworks such

00:07:32.856 --> 00:07:34.456 A:middle
as division framework and the

00:07:34.456 --> 00:07:35.566 A:middle
natural language processing

00:07:35.616 --> 00:07:37.496 A:middle
framework, which rely on the

00:07:37.496 --> 00:07:38.366 A:middle
Core ML framework.

00:07:39.216 --> 00:07:40.376 A:middle
And the Core ML framework is

00:07:40.446 --> 00:07:42.316 A:middle
powered by the accelerates

00:07:42.316 --> 00:07:44.076 A:middle
framework BNNS primitives on the

00:07:44.076 --> 00:07:44.606 A:middle
CPU.

00:07:45.066 --> 00:07:46.176 A:middle
And by the machine learning --

00:07:47.066 --> 00:07:49.046 A:middle
and by the Metal Performance

00:07:49.046 --> 00:07:51.946 A:middle
Shaders framework on the GPU.

00:07:51.946 --> 00:07:52.706 A:middle
But if you're writing an

00:07:52.706 --> 00:07:54.556 A:middle
application that uses Metal,

00:07:54.906 --> 00:07:56.046 A:middle
then you can use the MPS

00:07:56.046 --> 00:07:58.176 A:middle
framework directly and I will

00:07:58.176 --> 00:07:59.386 A:middle
show you how in this session.

00:08:01.666 --> 00:08:02.676 A:middle
So let's start with what are we

00:08:02.736 --> 00:08:03.486 A:middle
talking about here?

00:08:04.246 --> 00:08:05.116 A:middle
What is deep learning?

00:08:05.366 --> 00:08:06.236 A:middle
What is Machine learning?

00:08:07.686 --> 00:08:08.766 A:middle
So imagine that this is you.

00:08:08.766 --> 00:08:11.436 A:middle
And when you see an image, you

00:08:11.436 --> 00:08:13.076 A:middle
know immediately what's depicted

00:08:13.076 --> 00:08:13.316 A:middle
on it.

00:08:13.416 --> 00:08:13.956 A:middle
It's a panda.

00:08:14.936 --> 00:08:16.686 A:middle
But now think about all of the

00:08:16.686 --> 00:08:18.006 A:middle
images on your iPhone.

00:08:18.596 --> 00:08:20.206 A:middle
Or all of those pictures in your

00:08:20.206 --> 00:08:20.996 A:middle
family albums.

00:08:21.606 --> 00:08:22.646 A:middle
Or all of the images on the

00:08:22.646 --> 00:08:22.966 A:middle
internet.

00:08:23.816 --> 00:08:27.096 A:middle
No human can possibly -- can

00:08:27.096 --> 00:08:28.816 A:middle
classify these many images.

00:08:29.086 --> 00:08:30.506 A:middle
But deep-learning algorithms is

00:08:30.506 --> 00:08:32.056 A:middle
designed specifically to do

00:08:32.056 --> 00:08:32.196 A:middle
that.

00:08:33.186 --> 00:08:34.416 A:middle
They can be used for sifting

00:08:34.416 --> 00:08:35.576 A:middle
through large amounts of data

00:08:36.016 --> 00:08:37.866 A:middle
and answering questions such as

00:08:37.996 --> 00:08:41.676 A:middle
what is in this image?

00:08:42.236 --> 00:08:43.306 A:middle
Deep learning algorithms have

00:08:43.406 --> 00:08:43.906 A:middle
two phases.

00:08:44.206 --> 00:08:45.156 A:middle
Training and inference.

00:08:45.426 --> 00:08:46.426 A:middle
So let's talk about training

00:08:46.426 --> 00:08:46.736 A:middle
first.

00:08:47.946 --> 00:08:49.246 A:middle
And let's actually use an

00:08:49.246 --> 00:08:49.676 A:middle
example.

00:08:49.676 --> 00:08:51.326 A:middle
Let's train a system to classify

00:08:51.326 --> 00:08:51.716 A:middle
images.

00:08:52.586 --> 00:08:53.536 A:middle
So the training system to

00:08:53.536 --> 00:08:56.696 A:middle
classify images, for example, if

00:08:56.696 --> 00:08:58.066 A:middle
you want to have it recognize

00:08:58.126 --> 00:08:58.536 A:middle
animals.

00:08:58.966 --> 00:09:00.516 A:middle
Like, to have it recognize cats,

00:09:00.516 --> 00:09:02.126 A:middle
you need to feed this system a

00:09:02.556 --> 00:09:04.196 A:middle
large number of labeled images

00:09:04.196 --> 00:09:06.126 A:middle
of cats and then rabbits, and

00:09:06.126 --> 00:09:07.336 A:middle
all the other animals that you

00:09:07.336 --> 00:09:08.406 A:middle
want your system to be able to

00:09:08.406 --> 00:09:08.866 A:middle
recognize.

00:09:10.546 --> 00:09:12.316 A:middle
And this training step is a

00:09:12.316 --> 00:09:13.916 A:middle
one-time computationally

00:09:13.916 --> 00:09:16.476 A:middle
expensive and labor-intensive

00:09:16.566 --> 00:09:16.786 A:middle
step.

00:09:17.896 --> 00:09:19.076 A:middle
And it's usually done offline.

00:09:19.696 --> 00:09:20.896 A:middle
But the results of this training

00:09:20.896 --> 00:09:22.336 A:middle
phase is trained parameters

00:09:23.306 --> 00:09:24.656 A:middle
which are required for the next

00:09:24.656 --> 00:09:25.856 A:middle
phase, the inference phase.

00:09:26.906 --> 00:09:28.096 A:middle
This is when your system is

00:09:28.186 --> 00:09:30.156 A:middle
presented with a new image that

00:09:30.156 --> 00:09:31.736 A:middle
it has never seen before and it

00:09:31.736 --> 00:09:33.186 A:middle
needs to classify, this is a

00:09:33.186 --> 00:09:33.396 A:middle
cap.

00:09:35.126 --> 00:09:37.016 A:middle
We provide view acceleration for

00:09:37.016 --> 00:09:38.306 A:middle
the second phase; the inference

00:09:38.306 --> 00:09:38.526 A:middle
phase.

00:09:39.096 --> 00:09:40.966 A:middle
Specifically, last year we

00:09:40.966 --> 00:09:42.936 A:middle
talked about the building blocks

00:09:43.056 --> 00:09:44.296 A:middle
for building convolutional

00:09:44.296 --> 00:09:45.716 A:middle
neural networks on the GPU for

00:09:45.716 --> 00:09:46.146 A:middle
inference.

00:09:48.466 --> 00:09:50.176 A:middle
So before we move onto any of

00:09:50.176 --> 00:09:51.966 A:middle
the new features for machine

00:09:51.966 --> 00:09:52.826 A:middle
learning that we brought you

00:09:52.856 --> 00:09:53.936 A:middle
this year, we are going to

00:09:53.936 --> 00:09:55.136 A:middle
review some of the core

00:09:55.136 --> 00:09:56.886 A:middle
information that was covered in

00:09:56.886 --> 00:09:58.146 A:middle
our last year's presentation.

00:09:58.736 --> 00:10:00.416 A:middle
Such as, what are convolutional

00:10:00.416 --> 00:10:00.966 A:middle
neural networks?

00:10:02.396 --> 00:10:04.326 A:middle
And once we do that then we can

00:10:04.326 --> 00:10:06.056 A:middle
talk about the new primitives

00:10:06.106 --> 00:10:06.836 A:middle
that we've added for

00:10:06.836 --> 00:10:07.906 A:middle
convolutional neural networks

00:10:07.966 --> 00:10:09.426 A:middle
this year, and then we'll

00:10:09.426 --> 00:10:11.146 A:middle
introduce the new, easy-to-use

00:10:11.516 --> 00:10:12.636 A:middle
neural network graph API.

00:10:13.166 --> 00:10:14.746 A:middle
And our last topic will be

00:10:14.746 --> 00:10:15.726 A:middle
recurrent neural networks.

00:10:18.606 --> 00:10:20.976 A:middle
So let's go into our recap.

00:10:21.106 --> 00:10:22.066 A:middle
So what are convolutional neural

00:10:22.066 --> 00:10:22.366 A:middle
networks?

00:10:24.446 --> 00:10:25.576 A:middle
Convolutional neural networks

00:10:25.576 --> 00:10:27.426 A:middle
are biologically inspired and

00:10:27.426 --> 00:10:28.836 A:middle
designed to resemble the visual

00:10:28.836 --> 00:10:29.246 A:middle
cortex.

00:10:29.796 --> 00:10:31.406 A:middle
So let's think about how our

00:10:31.406 --> 00:10:33.076 A:middle
brain processes visual inputs.

00:10:34.256 --> 00:10:35.636 A:middle
The first hierarchy of neurons

00:10:35.736 --> 00:10:37.056 A:middle
that receive information in the

00:10:37.056 --> 00:10:39.396 A:middle
visual cortex is sensitive to

00:10:39.396 --> 00:10:40.786 A:middle
specific edges and blobs of

00:10:40.886 --> 00:10:41.196 A:middle
color.

00:10:42.366 --> 00:10:43.466 A:middle
While the brain region's further

00:10:43.466 --> 00:10:45.966 A:middle
down the visual pipeline respond

00:10:45.966 --> 00:10:47.596 A:middle
to more complex structures such

00:10:47.596 --> 00:10:49.366 A:middle
as faces of our friends or kinds

00:10:49.366 --> 00:10:50.166 A:middle
of animals like cats.

00:10:50.996 --> 00:10:53.646 A:middle
So in a similar way, CNNs are

00:10:53.646 --> 00:10:55.836 A:middle
organized into a hierarchy of

00:10:55.836 --> 00:10:58.176 A:middle
layers where high-level features

00:10:58.296 --> 00:10:59.836 A:middle
are derived from low-level

00:10:59.836 --> 00:11:00.246 A:middle
features.

00:11:01.156 --> 00:11:02.516 A:middle
So the first few layers in your

00:11:02.516 --> 00:11:04.566 A:middle
network respond to low-level

00:11:04.566 --> 00:11:06.766 A:middle
features like edges and blobs of

00:11:06.826 --> 00:11:07.196 A:middle
color.

00:11:07.886 --> 00:11:10.646 A:middle
While subsequent layers respond

00:11:10.766 --> 00:11:12.516 A:middle
to progressively more complex

00:11:12.616 --> 00:11:14.886 A:middle
features such as faces.

00:11:16.106 --> 00:11:17.406 A:middle
And I keep saying features.

00:11:17.846 --> 00:11:19.556 A:middle
So think of a feature as a

00:11:19.556 --> 00:11:21.176 A:middle
filter that filters your input

00:11:21.176 --> 00:11:22.706 A:middle
data; that particular feature.

00:11:25.316 --> 00:11:26.906 A:middle
And here's a list of all of the

00:11:26.976 --> 00:11:28.076 A:middle
CNN primitives that we had

00:11:28.076 --> 00:11:29.356 A:middle
available in iOS 10.

00:11:29.756 --> 00:11:31.806 A:middle
And in this recap I will be just

00:11:31.806 --> 00:11:33.686 A:middle
talking about the core

00:11:34.176 --> 00:11:35.146 A:middle
convolution layer.

00:11:35.216 --> 00:11:36.426 A:middle
The core building block of a

00:11:36.546 --> 00:11:36.756 A:middle
CNN.

00:11:36.756 --> 00:11:39.046 A:middle
And the rest of these primitives

00:11:39.106 --> 00:11:40.906 A:middle
are covered in great detail in

00:11:40.906 --> 00:11:42.266 A:middle
our presentation -- in our

00:11:42.266 --> 00:11:43.076 A:middle
documentation.

00:11:43.196 --> 00:11:44.566 A:middle
So Pooling, Fully-Connected and

00:11:44.616 --> 00:11:45.156 A:middle
SoftMax.

00:11:45.746 --> 00:11:46.856 A:middle
You can find information on

00:11:46.856 --> 00:11:47.056 A:middle
those.

00:11:48.626 --> 00:11:50.386 A:middle
So let's talk about the core

00:11:50.386 --> 00:11:51.186 A:middle
building block.

00:11:52.596 --> 00:11:54.216 A:middle
So the function of this core

00:11:54.216 --> 00:11:56.196 A:middle
convolution layer is to

00:11:56.196 --> 00:11:57.766 A:middle
recognize features in the input

00:11:57.766 --> 00:11:58.906 A:middle
data and it's called a

00:11:58.906 --> 00:12:01.076 A:middle
convolution layer because it

00:12:01.126 --> 00:12:02.576 A:middle
performs a convolution on its

00:12:02.576 --> 00:12:02.846 A:middle
input.

00:12:03.916 --> 00:12:05.246 A:middle
So let's recall how regular

00:12:05.246 --> 00:12:06.076 A:middle
convolution works.

00:12:06.906 --> 00:12:08.146 A:middle
You have your inputs, your

00:12:08.186 --> 00:12:09.366 A:middle
outputs and the filter.

00:12:10.366 --> 00:12:12.386 A:middle
And to convole a filter with the

00:12:12.386 --> 00:12:14.506 A:middle
input data you need to multiply

00:12:14.756 --> 00:12:16.626 A:middle
each value in your filter with

00:12:16.626 --> 00:12:18.446 A:middle
the value in the input data and

00:12:18.446 --> 00:12:19.686 A:middle
combine that information to

00:12:19.686 --> 00:12:21.106 A:middle
compute a single output value.

00:12:22.046 --> 00:12:23.536 A:middle
And you do the same for the rest

00:12:23.636 --> 00:12:25.166 A:middle
of the output pixels.

00:12:27.596 --> 00:12:29.856 A:middle
And now the convolution layer is

00:12:29.856 --> 00:12:31.606 A:middle
a generalization of regular

00:12:31.606 --> 00:12:32.226 A:middle
convolution.

00:12:32.396 --> 00:12:34.666 A:middle
It allows you to have multiple

00:12:34.666 --> 00:12:35.136 A:middle
filters.

00:12:35.526 --> 00:12:37.536 A:middle
So you have as many filters as

00:12:37.536 --> 00:12:38.916 A:middle
you have output channels -- or

00:12:38.916 --> 00:12:39.816 A:middle
16 in this case.

00:12:41.666 --> 00:12:43.046 A:middle
And these are the filters which

00:12:43.046 --> 00:12:44.656 A:middle
are going to be filtering input

00:12:44.656 --> 00:12:46.006 A:middle
data for particular features.

00:12:47.726 --> 00:12:49.016 A:middle
Now imagine that you're working

00:12:49.016 --> 00:12:50.266 A:middle
with RGB data.

00:12:50.356 --> 00:12:52.186 A:middle
So you actually have three

00:12:52.186 --> 00:12:53.266 A:middle
channels in your input.

00:12:54.156 --> 00:12:56.276 A:middle
And just because how CNNs work,

00:12:56.476 --> 00:12:58.816 A:middle
this means you need three sets

00:12:58.886 --> 00:13:00.156 A:middle
of 16 filters.

00:13:00.866 --> 00:13:02.576 A:middle
One set for each input channel.

00:13:03.856 --> 00:13:05.916 A:middle
And then these filters are

00:13:05.916 --> 00:13:07.296 A:middle
applied to the input data

00:13:08.486 --> 00:13:09.036 A:middle
separately.

00:13:09.036 --> 00:13:10.816 A:middle
And then the final step combines

00:13:10.816 --> 00:13:12.386 A:middle
all of this information to

00:13:12.386 --> 00:13:13.796 A:middle
compute a single output pixel.

00:13:15.516 --> 00:13:17.156 A:middle
So that's it for our recap of

00:13:17.156 --> 00:13:18.006 A:middle
the convolution layer.

00:13:18.536 --> 00:13:19.526 A:middle
Now let's talk about the new

00:13:19.576 --> 00:13:20.846 A:middle
primitives we've added for

00:13:20.846 --> 00:13:22.026 A:middle
convolutional neural networks.

00:13:22.116 --> 00:13:25.176 A:middle
So as you can see, we've added

00:13:25.176 --> 00:13:25.686 A:middle
quite a few.

00:13:27.736 --> 00:13:29.096 A:middle
And I'll be talking about the

00:13:29.096 --> 00:13:31.476 A:middle
ones highlighted in yellow, but

00:13:31.476 --> 00:13:33.206 A:middle
the rest of them like L2Norm

00:13:33.256 --> 00:13:34.686 A:middle
Pooling, Resampling,

00:13:34.686 --> 00:13:36.086 A:middle
Up-sampling, they will all be

00:13:36.086 --> 00:13:37.396 A:middle
covered in our documentation.

00:13:39.286 --> 00:13:40.986 A:middle
So let's talk about updates to

00:13:40.986 --> 00:13:42.786 A:middle
our core convolution layer.

00:13:43.916 --> 00:13:45.146 A:middle
We used to support only single

00:13:45.186 --> 00:13:46.716 A:middle
precision floating-point weight

00:13:46.716 --> 00:13:47.026 A:middle
types.

00:13:47.466 --> 00:13:49.076 A:middle
And now to help you reduce the

00:13:49.076 --> 00:13:51.126 A:middle
memory footprint and to prove

00:13:51.126 --> 00:13:51.966 A:middle
the performance of your

00:13:51.966 --> 00:13:52.326 A:middle
networks.

00:13:52.876 --> 00:13:54.546 A:middle
We also support half-precision

00:13:54.546 --> 00:13:56.466 A:middle
floating points, 8-bit integer,

00:13:56.806 --> 00:13:58.076 A:middle
and binary weight types.

00:13:59.496 --> 00:14:01.006 A:middle
We used to support only standard

00:14:01.006 --> 00:14:02.676 A:middle
convolution and now we also

00:14:02.736 --> 00:14:03.926 A:middle
support binary and XNOR

00:14:03.926 --> 00:14:04.646 A:middle
convolution.

00:14:04.986 --> 00:14:06.096 A:middle
Dilated convolution.

00:14:06.096 --> 00:14:08.406 A:middle
Sub-pixel convolution and

00:14:08.406 --> 00:14:09.366 A:middle
convolution transpose

00:14:09.366 --> 00:14:09.996 A:middle
operations.

00:14:11.056 --> 00:14:12.156 A:middle
And many of these are

00:14:12.156 --> 00:14:13.716 A:middle
orthogonal, so you can even have

00:14:14.006 --> 00:14:15.946 A:middle
dilated sub-pixel convolution if

00:14:15.946 --> 00:14:16.276 A:middle
you want.

00:14:17.706 --> 00:14:18.486 A:middle
So let's go through them

00:14:18.486 --> 00:14:19.046 A:middle
one-by-one.

00:14:20.806 --> 00:14:22.216 A:middle
Binary and XNOR convolution

00:14:22.256 --> 00:14:24.276 A:middle
perform the same exact operation

00:14:24.306 --> 00:14:26.336 A:middle
as regular convolution but they

00:14:26.336 --> 00:14:28.016 A:middle
do so with improved performance

00:14:28.476 --> 00:14:29.566 A:middle
and great space savings.

00:14:30.016 --> 00:14:31.726 A:middle
So in regular convolution, you

00:14:31.726 --> 00:14:33.546 A:middle
may have floating point inputs

00:14:33.846 --> 00:14:35.166 A:middle
and floating point weights.

00:14:36.036 --> 00:14:37.446 A:middle
What binary convolution allow

00:14:37.516 --> 00:14:39.236 A:middle
you to do is to use your

00:14:39.236 --> 00:14:41.266 A:middle
full-sized input with binary

00:14:41.266 --> 00:14:41.486 A:middle
weights.

00:14:42.416 --> 00:14:44.776 A:middle
And for XNOR convolution the

00:14:44.776 --> 00:14:46.706 A:middle
first thing that happens is that

00:14:47.226 --> 00:14:48.626 A:middle
your input is first converted to

00:14:48.626 --> 00:14:50.826 A:middle
binary so that both your inputs

00:14:51.176 --> 00:14:52.396 A:middle
and the weights are binary.

00:14:53.476 --> 00:14:55.286 A:middle
In regular convolution, the

00:14:55.286 --> 00:14:57.066 A:middle
input has to be multiplied with

00:14:57.106 --> 00:14:57.446 A:middle
the weights.

00:14:57.886 --> 00:14:59.876 A:middle
And for XNOR convolution the

00:14:59.876 --> 00:15:01.806 A:middle
separation becomes a simple XNOR

00:15:01.806 --> 00:15:02.336 A:middle
operation.

00:15:04.746 --> 00:15:06.066 A:middle
And now let's talk about dilated

00:15:06.066 --> 00:15:06.656 A:middle
convolution.

00:15:07.626 --> 00:15:08.996 A:middle
So we already know how regular

00:15:08.996 --> 00:15:09.786 A:middle
convolution works.

00:15:10.486 --> 00:15:12.396 A:middle
You need to apply a filter to

00:15:12.396 --> 00:15:13.656 A:middle
the input data to compute a

00:15:13.656 --> 00:15:14.706 A:middle
single output value.

00:15:17.526 --> 00:15:18.786 A:middle
But say you're working on an

00:15:18.786 --> 00:15:21.736 A:middle
algorithm that requires global

00:15:21.736 --> 00:15:24.846 A:middle
integration of a wider context

00:15:25.216 --> 00:15:26.166 A:middle
of your input data.

00:15:26.816 --> 00:15:28.466 A:middle
So instead of a 3 by 3 kernel,

00:15:28.536 --> 00:15:30.496 A:middle
you may be using a 5 by 5 kernel

00:15:31.736 --> 00:15:32.476 A:middle
to look out further.

00:15:33.026 --> 00:15:33.666 A:middle
But that's a lot more

00:15:33.666 --> 00:15:34.756 A:middle
computationally expensive.

00:15:35.256 --> 00:15:37.266 A:middle
What you can do instead is use

00:15:37.266 --> 00:15:39.046 A:middle
dilated convolutions which

00:15:39.046 --> 00:15:43.206 A:middle
allows you to -- which allows

00:15:43.206 --> 00:15:45.256 A:middle
you to use dilation factors to

00:15:45.256 --> 00:15:46.836 A:middle
introduce gaps into your

00:15:46.836 --> 00:15:48.836 A:middle
convolution kernel so that

00:15:48.836 --> 00:15:50.576 A:middle
you're still using just a 3 by 3

00:15:50.576 --> 00:15:52.676 A:middle
kernel, but you can look out

00:15:52.676 --> 00:15:53.016 A:middle
further.

00:15:54.996 --> 00:15:55.876 A:middle
And now let's talk about

00:15:55.946 --> 00:15:57.266 A:middle
subluxal convolution and

00:15:57.266 --> 00:15:58.846 A:middle
convolution transpose primitive;

00:15:59.766 --> 00:16:01.266 A:middle
very commonly used for image

00:16:01.266 --> 00:16:01.836 A:middle
upscaling.

00:16:03.066 --> 00:16:04.126 A:middle
And let's think about how

00:16:04.176 --> 00:16:05.366 A:middle
upscaling usually works.

00:16:05.456 --> 00:16:07.456 A:middle
So you have your input data and

00:16:07.516 --> 00:16:09.196 A:middle
you want to upscale it by a

00:16:09.196 --> 00:16:09.956 A:middle
factor of 2.

00:16:12.336 --> 00:16:13.376 A:middle
So you won't -- you have some

00:16:13.376 --> 00:16:14.566 A:middle
missing pixels to compute.

00:16:15.216 --> 00:16:16.536 A:middle
And usually upscaling is the

00:16:16.536 --> 00:16:18.156 A:middle
fixed operation with a constant

00:16:18.156 --> 00:16:18.606 A:middle
filter.

00:16:18.766 --> 00:16:20.336 A:middle
So for example how would a box

00:16:20.336 --> 00:16:22.336 A:middle
filter help you to upscale this

00:16:22.386 --> 00:16:22.756 A:middle
image?

00:16:23.416 --> 00:16:25.146 A:middle
So the box filter, which is take

00:16:25.316 --> 00:16:28.006 A:middle
the known pixels and copy the

00:16:28.006 --> 00:16:29.326 A:middle
known data into the missing

00:16:29.326 --> 00:16:30.786 A:middle
location to get you upscaled

00:16:30.786 --> 00:16:31.216 A:middle
results.

00:16:33.326 --> 00:16:35.196 A:middle
For sub-pixel convolution, your

00:16:35.196 --> 00:16:36.646 A:middle
filters are not constant.

00:16:36.976 --> 00:16:38.226 A:middle
Your filters are learned from

00:16:38.226 --> 00:16:38.626 A:middle
the data.

00:16:38.766 --> 00:16:40.286 A:middle
They are your trained parameters

00:16:40.716 --> 00:16:41.786 A:middle
that you get from the training

00:16:41.786 --> 00:16:42.906 A:middle
step where the system was

00:16:42.986 --> 00:16:45.106 A:middle
trained to do this task; to do

00:16:45.106 --> 00:16:45.946 A:middle
image upscaling.

00:16:47.086 --> 00:16:49.176 A:middle
So for 2x upscaling you get 4

00:16:49.236 --> 00:16:49.686 A:middle
filters.

00:16:49.806 --> 00:16:51.656 A:middle
For 4x upscaling you get 16

00:16:51.656 --> 00:16:52.656 A:middle
filters and so on.

00:16:53.566 --> 00:16:55.446 A:middle
So for our 2x upscaling we get

00:16:55.446 --> 00:16:57.456 A:middle
our 4 filters and we apply them

00:16:57.456 --> 00:16:58.166 A:middle
to the input data.

00:16:58.166 --> 00:17:00.216 A:middle
And then the output of that

00:17:00.216 --> 00:17:02.076 A:middle
operation is reshuffled to get

00:17:02.076 --> 00:17:03.476 A:middle
your final full-resolution

00:17:03.476 --> 00:17:03.856 A:middle
image.

00:17:04.926 --> 00:17:06.446 A:middle
And now let's talk about how the

00:17:06.446 --> 00:17:08.076 A:middle
convolution transpose primitive

00:17:08.156 --> 00:17:09.536 A:middle
can be used to upscale images.

00:17:10.436 --> 00:17:12.026 A:middle
So we have our inputs and we

00:17:12.026 --> 00:17:13.466 A:middle
still have to compute our

00:17:13.466 --> 00:17:14.146 A:middle
missing data.

00:17:14.946 --> 00:17:16.666 A:middle
So the way that this primitive

00:17:17.156 --> 00:17:18.616 A:middle
computes the missing data is

00:17:18.616 --> 00:17:19.586 A:middle
that it applies a kind of

00:17:19.586 --> 00:17:21.296 A:middle
convolution pass to this

00:17:21.296 --> 00:17:23.166 A:middle
intermediate result with gaps to

00:17:23.166 --> 00:17:24.596 A:middle
compute each output pixel.

00:17:25.186 --> 00:17:27.316 A:middle
So that's how you get your

00:17:27.356 --> 00:17:28.256 A:middle
upscaled output.

00:17:31.136 --> 00:17:32.216 A:middle
And now we're going to show you

00:17:32.216 --> 00:17:33.556 A:middle
how you can use these new

00:17:33.556 --> 00:17:35.046 A:middle
convolution primitives to

00:17:35.046 --> 00:17:36.626 A:middle
implement a real-world network.

00:17:37.096 --> 00:17:38.606 A:middle
So we took this colorization

00:17:38.606 --> 00:17:41.486 A:middle
network that takes black and

00:17:41.486 --> 00:17:42.886 A:middle
white images as input and

00:17:42.886 --> 00:17:44.356 A:middle
produces colorized images.

00:17:44.356 --> 00:17:47.156 A:middle
And this particular network uses

00:17:47.236 --> 00:17:48.426 A:middle
the dilated convolution

00:17:48.426 --> 00:17:50.396 A:middle
primitive to integrate wider

00:17:50.396 --> 00:17:52.296 A:middle
global context quicker.

00:17:52.926 --> 00:17:54.756 A:middle
And it uses the convolution

00:17:54.756 --> 00:17:56.726 A:middle
transpose primitive to upscale

00:17:56.726 --> 00:17:57.776 A:middle
the results of the network.

00:18:00.166 --> 00:18:01.276 A:middle
And now let's look at this

00:18:01.666 --> 00:18:03.966 A:middle
colorization network in action.

00:18:10.226 --> 00:18:11.466 A:middle
So in this demo we have a

00:18:11.466 --> 00:18:12.886 A:middle
collection of black and white

00:18:12.886 --> 00:18:14.046 A:middle
images like this image of a

00:18:14.046 --> 00:18:14.406 A:middle
lion.

00:18:15.046 --> 00:18:16.416 A:middle
And as soon as I tap on this

00:18:16.416 --> 00:18:17.796 A:middle
image, the colorization network

00:18:17.796 --> 00:18:20.046 A:middle
will run right here live on the

00:18:20.046 --> 00:18:21.126 A:middle
device, and we'll see a

00:18:21.126 --> 00:18:21.946 A:middle
colorized image.

00:18:23.906 --> 00:18:25.456 A:middle
And let's try another example

00:18:25.456 --> 00:18:27.046 A:middle
for this beautiful snowy

00:18:27.046 --> 00:18:27.616 A:middle
mountain.

00:18:28.836 --> 00:18:30.036 A:middle
And now we see it in color.

00:18:31.586 --> 00:18:33.756 A:middle
And this beautiful lovely image

00:18:33.756 --> 00:18:35.016 A:middle
of a dad and a daughter playing

00:18:35.016 --> 00:18:35.366 A:middle
guitar.

00:18:35.366 --> 00:18:37.386 A:middle
And now you can see them playing

00:18:37.386 --> 00:18:38.026 A:middle
guitar in color.

00:18:39.516 --> 00:18:40.896 A:middle
And I really like this one, the

00:18:40.896 --> 00:18:42.306 A:middle
brown bear walking in the

00:18:42.356 --> 00:18:42.696 A:middle
forest.

00:18:42.696 --> 00:18:43.756 A:middle
So I think this network does

00:18:43.786 --> 00:18:45.146 A:middle
just a really wonderful job.

00:18:46.886 --> 00:18:48.726 A:middle
Okay. So that's it for the live

00:18:48.726 --> 00:18:48.916 A:middle
demo.

00:18:49.516 --> 00:18:54.686 A:middle
[ Applause ]

00:18:55.186 --> 00:18:55.796 A:middle
Thank you so much.

00:18:57.766 --> 00:18:59.216 A:middle
So we've added all of these new

00:18:59.216 --> 00:19:01.456 A:middle
convolution CNN primitives, but

00:19:01.456 --> 00:19:02.056 A:middle
that's not all.

00:19:02.976 --> 00:19:04.666 A:middle
We also went back and improved

00:19:04.666 --> 00:19:06.046 A:middle
the performance of some of the

00:19:06.206 --> 00:19:07.936 A:middle
core CNN kernels that were

00:19:07.936 --> 00:19:09.646 A:middle
available to you in iOS 10.

00:19:10.406 --> 00:19:11.776 A:middle
So this chart will show the

00:19:11.806 --> 00:19:13.846 A:middle
performance of the Inception-v3

00:19:13.846 --> 00:19:15.386 A:middle
network, which is a commonly

00:19:15.386 --> 00:19:16.936 A:middle
used network for image

00:19:17.056 --> 00:19:17.546 A:middle
recognition.

00:19:18.756 --> 00:19:20.396 A:middle
So it shows the performance of

00:19:20.396 --> 00:19:22.196 A:middle
this network in iOS 11.

00:19:22.196 --> 00:19:23.786 A:middle
And as you can see, we're

00:19:23.786 --> 00:19:25.866 A:middle
bringing you at least 20 percent

00:19:25.866 --> 00:19:27.546 A:middle
performance improvement across

00:19:27.756 --> 00:19:28.696 A:middle
different iOS hardware.

00:19:30.406 --> 00:19:33.786 A:middle
And now let's talk about the new

00:19:33.786 --> 00:19:37.096 A:middle
neural network graph API.

00:19:37.716 --> 00:19:40.036 A:middle
the neural networks are commonly

00:19:40.036 --> 00:19:41.416 A:middle
described using a graph

00:19:41.416 --> 00:19:42.476 A:middle
abstraction like this

00:19:42.476 --> 00:19:43.506 A:middle
visualization of the

00:19:43.506 --> 00:19:44.616 A:middle
Inception-v3 network.

00:19:44.616 --> 00:19:46.696 A:middle
And we're now allowing to do

00:19:46.696 --> 00:19:48.706 A:middle
just this using the new graph

00:19:48.706 --> 00:19:48.966 A:middle
API.

00:19:50.446 --> 00:19:51.556 A:middle
So let's zoom in on one of these

00:19:51.556 --> 00:19:53.186 A:middle
inception modules.

00:19:54.676 --> 00:19:56.496 A:middle
You have filter nodes which

00:19:56.496 --> 00:19:58.176 A:middle
describe the operations that you

00:19:58.176 --> 00:19:59.216 A:middle
can perform on your data.

00:19:59.526 --> 00:20:01.146 A:middle
Such as convolution, pooling,

00:20:01.146 --> 00:20:01.566 A:middle
etcetera.

00:20:02.556 --> 00:20:04.316 A:middle
And you have image nodes which

00:20:04.316 --> 00:20:05.736 A:middle
describe how the data flows

00:20:05.786 --> 00:20:06.546 A:middle
between these different

00:20:06.546 --> 00:20:07.096 A:middle
operations.

00:20:07.826 --> 00:20:11.306 A:middle
So why did we add this new graph

00:20:11.306 --> 00:20:11.556 A:middle
API?

00:20:11.926 --> 00:20:13.156 A:middle
Well because it's easy to use.

00:20:13.686 --> 00:20:14.656 A:middle
You get this compact

00:20:14.656 --> 00:20:16.116 A:middle
representation of your entire

00:20:16.116 --> 00:20:18.326 A:middle
network and you can save it to

00:20:18.326 --> 00:20:20.356 A:middle
disk and restore it, and that

00:20:20.466 --> 00:20:21.546 A:middle
works across platforms.

00:20:23.166 --> 00:20:24.426 A:middle
You only need to initialize the

00:20:24.426 --> 00:20:26.366 A:middle
graph once and then you can

00:20:26.366 --> 00:20:27.716 A:middle
reuse it for multiple input

00:20:27.716 --> 00:20:28.106 A:middle
images.

00:20:29.516 --> 00:20:31.476 A:middle
And you can execute the entire

00:20:31.476 --> 00:20:34.056 A:middle
graph on the GPU with a single

00:20:34.056 --> 00:20:34.346 A:middle
call.

00:20:36.276 --> 00:20:37.536 A:middle
There are no intermediate images

00:20:37.536 --> 00:20:39.196 A:middle
for you to manage, you just need

00:20:39.196 --> 00:20:40.756 A:middle
to take care of your input and

00:20:40.756 --> 00:20:41.036 A:middle
output.

00:20:42.146 --> 00:20:45.016 A:middle
Internally we use Metal heaps to

00:20:45.016 --> 00:20:46.796 A:middle
make sure that the memory

00:20:46.796 --> 00:20:47.916 A:middle
footprint of all your

00:20:47.916 --> 00:20:49.506 A:middle
intermediate images is as small

00:20:49.506 --> 00:20:50.126 A:middle
as possible.

00:20:50.606 --> 00:20:51.296 A:middle
For example, for the

00:20:51.296 --> 00:20:53.376 A:middle
Inception-v3 network this means

00:20:53.826 --> 00:20:56.856 A:middle
5x memory savings and 10x viewer

00:20:56.856 --> 00:20:58.496 A:middle
allocations, which I think is

00:20:58.556 --> 00:20:59.256 A:middle
pretty impressive.

00:21:00.836 --> 00:21:02.926 A:middle
So as I said, the graph does all

00:21:02.926 --> 00:21:03.956 A:middle
the groundwork for you.

00:21:04.316 --> 00:21:05.646 A:middle
It takes care of creating

00:21:05.846 --> 00:21:06.846 A:middle
intermediate images.

00:21:06.996 --> 00:21:08.706 A:middle
It takes care of sizing them.

00:21:09.296 --> 00:21:11.086 A:middle
It also -- it even sizes your

00:21:11.086 --> 00:21:11.366 A:middle
outputs.

00:21:11.786 --> 00:21:12.766 A:middle
It takes care of the padding

00:21:12.766 --> 00:21:13.406 A:middle
policies.

00:21:13.796 --> 00:21:15.016 A:middle
It takes care of censoring.

00:21:15.426 --> 00:21:17.516 A:middle
So in short, it's a lot less

00:21:17.566 --> 00:21:19.406 A:middle
code for you to write and a lot

00:21:19.486 --> 00:21:20.746 A:middle
fewer bugs for you to write as

00:21:20.746 --> 00:21:20.956 A:middle
well.

00:21:21.956 --> 00:21:24.066 A:middle
And when I say less code, I mean

00:21:24.596 --> 00:21:25.376 A:middle
a lot less code.

00:21:26.206 --> 00:21:27.906 A:middle
So last year we released this

00:21:27.996 --> 00:21:30.726 A:middle
Metal recognition sample that

00:21:30.726 --> 00:21:32.036 A:middle
uses the Inception-v3 network

00:21:32.036 --> 00:21:33.286 A:middle
for image recognition.

00:21:34.326 --> 00:21:36.026 A:middle
And we took that sample and

00:21:36.026 --> 00:21:37.576 A:middle
converted it to use the new

00:21:37.806 --> 00:21:40.036 A:middle
graph API and found that we had

00:21:40.036 --> 00:21:41.956 A:middle
to write four times less code.

00:21:42.356 --> 00:21:43.956 A:middle
And that's pretty much the same

00:21:43.956 --> 00:21:45.846 A:middle
number of lines as Python code

00:21:46.226 --> 00:21:47.916 A:middle
you would have to write in the

00:21:47.916 --> 00:21:48.886 A:middle
open-source sensor flow

00:21:48.886 --> 00:21:50.436 A:middle
framework to implement the same

00:21:50.436 --> 00:21:50.846 A:middle
network.

00:21:51.476 --> 00:21:52.956 A:middle
And we just want to mention that

00:21:52.956 --> 00:21:54.046 A:middle
we will be releasing this

00:21:54.086 --> 00:21:57.196 A:middle
updated sample code -- updated

00:21:57.196 --> 00:21:58.346 A:middle
example as sample code.

00:21:59.026 --> 00:22:01.796 A:middle
And now having all this

00:22:01.796 --> 00:22:03.276 A:middle
information about your entire

00:22:03.276 --> 00:22:06.116 A:middle
network allows us to deliver the

00:22:06.116 --> 00:22:07.956 A:middle
best performance across

00:22:08.106 --> 00:22:08.866 A:middle
different views.

00:22:09.336 --> 00:22:10.946 A:middle
We make it easy for your to

00:22:10.946 --> 00:22:12.766 A:middle
parallelize between the CPU and

00:22:12.766 --> 00:22:13.206 A:middle
the GPU.

00:22:13.976 --> 00:22:15.876 A:middle
so as the graph is executing --

00:22:16.326 --> 00:22:18.076 A:middle
as the GPU is executing the

00:22:18.076 --> 00:22:19.986 A:middle
graph of one input image, the

00:22:19.986 --> 00:22:21.686 A:middle
CPU can already prepare to

00:22:21.686 --> 00:22:22.776 A:middle
execute the graph for a

00:22:22.776 --> 00:22:23.716 A:middle
different input image.

00:22:25.176 --> 00:22:26.606 A:middle
We can also fuse graph nodes

00:22:26.676 --> 00:22:28.446 A:middle
together like the convolution

00:22:28.856 --> 00:22:29.966 A:middle
and neuron nodes.

00:22:31.856 --> 00:22:33.746 A:middle
And we can execute graph nodes

00:22:33.746 --> 00:22:34.386 A:middle
concurrently.

00:22:34.386 --> 00:22:36.256 A:middle
So if we look at this inception

00:22:36.256 --> 00:22:38.056 A:middle
module again, you can see that

00:22:38.056 --> 00:22:39.886 A:middle
there are multiple rows of these

00:22:39.886 --> 00:22:41.386 A:middle
nodes that can be executed

00:22:41.456 --> 00:22:43.036 A:middle
completely independently of each

00:22:43.036 --> 00:22:43.236 A:middle
other.

00:22:44.176 --> 00:22:45.486 A:middle
And of course the output of

00:22:45.486 --> 00:22:47.326 A:middle
these independent executions

00:22:47.926 --> 00:22:49.216 A:middle
need to be concatenated via

00:22:49.216 --> 00:22:50.216 A:middle
concatenation nodes.

00:22:51.206 --> 00:22:52.656 A:middle
And the graph is smart enough to

00:22:52.656 --> 00:22:54.276 A:middle
optimize those away as well.

00:22:54.886 --> 00:22:57.706 A:middle
And now let's take a look at how

00:22:57.706 --> 00:22:59.586 A:middle
you can use the new graph API.

00:23:00.296 --> 00:23:02.176 A:middle
So this is the code for creating

00:23:02.176 --> 00:23:04.126 A:middle
a convolution node using the

00:23:04.126 --> 00:23:04.646 A:middle
graph API.

00:23:05.826 --> 00:23:07.256 A:middle
So it takes an image as source

00:23:08.226 --> 00:23:09.486 A:middle
and it also has weights.

00:23:09.486 --> 00:23:10.926 A:middle
So let's talk about weights for

00:23:10.926 --> 00:23:11.176 A:middle
a minute.

00:23:13.056 --> 00:23:14.256 A:middle
Neural networks keep growing

00:23:14.256 --> 00:23:15.396 A:middle
larger and larger in size.

00:23:16.136 --> 00:23:17.736 A:middle
And if you have many convolution

00:23:17.736 --> 00:23:19.346 A:middle
nodes in your networks, that

00:23:19.346 --> 00:23:21.436 A:middle
means that the overall size of

00:23:21.496 --> 00:23:22.466 A:middle
the weights for your entire

00:23:22.466 --> 00:23:23.616 A:middle
network could be quite

00:23:23.616 --> 00:23:24.246 A:middle
considerable.

00:23:25.216 --> 00:23:27.016 A:middle
And to help with that we've

00:23:27.016 --> 00:23:30.126 A:middle
added a convolution data source

00:23:30.186 --> 00:23:31.296 A:middle
protocol that you can implement

00:23:31.656 --> 00:23:33.186 A:middle
and it provides just in time

00:23:33.606 --> 00:23:35.036 A:middle
loading and purging of weights

00:23:35.076 --> 00:23:35.276 A:middle
data.

00:23:36.296 --> 00:23:40.046 A:middle
So the idea is that the weights

00:23:40.046 --> 00:23:41.546 A:middle
for your entire network do not

00:23:41.546 --> 00:23:43.196 A:middle
have to be loaded in memory all

00:23:43.196 --> 00:23:44.086 A:middle
at the same time.

00:23:44.626 --> 00:23:45.956 A:middle
They also do not have to be

00:23:45.956 --> 00:23:46.886 A:middle
loaded in advance.

00:23:48.396 --> 00:23:49.656 A:middle
To help minimize the memory

00:23:49.656 --> 00:23:51.556 A:middle
footprint, when we initialize

00:23:51.556 --> 00:23:53.136 A:middle
the graph and we process a

00:23:53.136 --> 00:23:54.516 A:middle
particular convolution layer,

00:23:55.096 --> 00:23:56.126 A:middle
we'll load the weights for that

00:23:56.126 --> 00:23:57.726 A:middle
convolution layer and then we

00:23:57.856 --> 00:23:59.486 A:middle
purge them before we move on to

00:23:59.486 --> 00:24:00.726 A:middle
the next convolution layer.

00:24:02.226 --> 00:24:03.586 A:middle
What you have to do is to

00:24:03.586 --> 00:24:05.146 A:middle
implement this initialization

00:24:05.146 --> 00:24:06.916 A:middle
method which just knows where

00:24:06.916 --> 00:24:08.376 A:middle
the data is but it doesn't

00:24:08.376 --> 00:24:09.086 A:middle
actually load it.

00:24:10.096 --> 00:24:11.256 A:middle
And then when the graph calls

00:24:11.256 --> 00:24:13.266 A:middle
the load function that alerts

00:24:13.266 --> 00:24:14.846 A:middle
you that the weights need to be

00:24:14.846 --> 00:24:15.236 A:middle
loaded.

00:24:15.366 --> 00:24:16.546 A:middle
And then when the purge function

00:24:16.546 --> 00:24:18.166 A:middle
is called by the graph then you

00:24:18.166 --> 00:24:19.316 A:middle
can release the weights.

00:24:21.586 --> 00:24:22.526 A:middle
And now let's build a graph.

00:24:23.446 --> 00:24:24.926 A:middle
So here we're implementing this

00:24:24.926 --> 00:24:26.116 A:middle
makeGraph function.

00:24:26.596 --> 00:24:28.366 A:middle
And on the left you can see all

00:24:28.366 --> 00:24:29.636 A:middle
the nodes that make up our

00:24:29.636 --> 00:24:30.826 A:middle
network that we need to build.

00:24:31.256 --> 00:24:32.926 A:middle
So then we create the nodes.

00:24:33.226 --> 00:24:34.446 A:middle
So we create the convolution

00:24:34.446 --> 00:24:34.836 A:middle
node.

00:24:35.016 --> 00:24:35.616 A:middle
The pooling node.

00:24:35.616 --> 00:24:37.456 A:middle
And then the rest of the nodes.

00:24:37.756 --> 00:24:38.606 A:middle
So we have the nodes.

00:24:38.606 --> 00:24:40.226 A:middle
How do we connect them into a

00:24:40.226 --> 00:24:40.446 A:middle
graph?

00:24:41.646 --> 00:24:43.186 A:middle
So we just take the result image

00:24:43.186 --> 00:24:44.986 A:middle
of one node and pass it as a

00:24:45.066 --> 00:24:46.516 A:middle
source image to the next node.

00:24:46.516 --> 00:24:48.106 A:middle
And then we have our graph.

00:24:49.736 --> 00:24:51.236 A:middle
And now let's run it on the GPU.

00:24:51.906 --> 00:24:54.066 A:middle
So first we do our usual Metal

00:24:54.066 --> 00:24:54.456 A:middle
setup.

00:24:54.886 --> 00:24:56.016 A:middle
We initialize the graph.

00:24:56.676 --> 00:24:58.166 A:middle
We take care of our input data

00:24:58.866 --> 00:25:01.066 A:middle
and then we encode the graph to

00:25:01.066 --> 00:25:01.506 A:middle
the GPU.

00:25:02.276 --> 00:25:04.026 A:middle
And the data in the output image

00:25:04.556 --> 00:25:06.546 A:middle
will be -- the output image will

00:25:06.546 --> 00:25:08.466 A:middle
be populated with data when the

00:25:08.466 --> 00:25:09.576 A:middle
command buffer completes.

00:25:10.086 --> 00:25:11.526 A:middle
And then we have an option to

00:25:11.526 --> 00:25:12.996 A:middle
wait for the GPU to finish.

00:25:13.406 --> 00:25:14.686 A:middle
But we don't want you to do

00:25:14.686 --> 00:25:14.956 A:middle
that.

00:25:15.886 --> 00:25:17.506 A:middle
When this happens the CPU is

00:25:17.506 --> 00:25:19.246 A:middle
waiting for the GPU to finish

00:25:19.836 --> 00:25:21.486 A:middle
before it can start encoding the

00:25:21.486 --> 00:25:22.846 A:middle
next run of the graph.

00:25:23.486 --> 00:25:25.256 A:middle
And this introduces bubbles into

00:25:25.256 --> 00:25:26.266 A:middle
your pipeline, which can

00:25:26.526 --> 00:25:28.036 A:middle
adversely affect performance.

00:25:29.816 --> 00:25:30.686 A:middle
So what we want you to do

00:25:30.686 --> 00:25:32.606 A:middle
instead is to use the new

00:25:32.606 --> 00:25:34.596 A:middle
asynchronous executeAsync API.

00:25:35.426 --> 00:25:37.896 A:middle
So with this API your Metal

00:25:37.966 --> 00:25:39.436 A:middle
setup is even smaller.

00:25:39.626 --> 00:25:41.076 A:middle
So you just need to get the

00:25:41.076 --> 00:25:41.736 A:middle
Metal device.

00:25:42.136 --> 00:25:43.096 A:middle
Then you still need to

00:25:43.096 --> 00:25:44.016 A:middle
initialize your graph.

00:25:44.056 --> 00:25:46.426 A:middle
Prepare the input data and then

00:25:46.426 --> 00:25:47.856 A:middle
you executeAsync call.

00:25:49.586 --> 00:25:52.376 A:middle
It returns immediately and then

00:25:52.376 --> 00:25:54.216 A:middle
the output image will be ready

00:25:55.196 --> 00:25:56.236 A:middle
when this code inside the

00:25:56.236 --> 00:25:57.086 A:middle
closure executes.

00:25:57.796 --> 00:25:59.056 A:middle
But in the meantime, you don't

00:25:59.056 --> 00:26:00.136 A:middle
have to wait for the GPU to

00:26:00.306 --> 00:26:02.056 A:middle
finish, you can already proceed

00:26:02.056 --> 00:26:03.676 A:middle
with a coding and new GPU task.

00:26:04.396 --> 00:26:07.236 A:middle
And this way the CPU and the GPU

00:26:07.236 --> 00:26:08.846 A:middle
are executing concurrently.

00:26:09.406 --> 00:26:10.316 A:middle
There are no bubbles in your

00:26:10.316 --> 00:26:12.756 A:middle
pipeline and they're both

00:26:12.756 --> 00:26:14.376 A:middle
utilized to full capacity.

00:26:16.756 --> 00:26:18.996 A:middle
Okay. And now I will do a live

00:26:18.996 --> 00:26:21.106 A:middle
demo that demonstrates the

00:26:21.146 --> 00:26:22.846 A:middle
performance difference between

00:26:22.966 --> 00:26:24.526 A:middle
the synchronous and asynchronous

00:26:24.526 --> 00:26:24.786 A:middle
APIs.

00:26:24.786 --> 00:26:27.576 A:middle
And this demo will be using the

00:26:27.576 --> 00:26:29.576 A:middle
Inception-v3 network for image

00:26:29.576 --> 00:26:30.116 A:middle
recognition.

00:26:30.516 --> 00:26:30.806 A:middle
All right.

00:26:31.136 --> 00:26:32.726 A:middle
So I will be starting with

00:26:32.726 --> 00:26:34.416 A:middle
synchronous API and here we're

00:26:34.416 --> 00:26:35.706 A:middle
detecting a water bottle.

00:26:35.706 --> 00:26:38.276 A:middle
And we're getting about 50

00:26:38.276 --> 00:26:41.756 A:middle
milliseconds per second per

00:26:41.756 --> 00:26:42.596 A:middle
image on average.

00:26:42.826 --> 00:26:44.066 A:middle
And now I will switch to the

00:26:44.066 --> 00:26:44.956 A:middle
asynchronous API.

00:26:44.956 --> 00:26:47.586 A:middle
And now we're getting about 36

00:26:47.586 --> 00:26:49.546 A:middle
milliseconds per image on

00:26:49.546 --> 00:26:49.936 A:middle
average.

00:26:49.936 --> 00:26:51.656 A:middle
So that's pretty good

00:26:51.686 --> 00:26:52.666 A:middle
performance improvement.

00:26:54.776 --> 00:26:55.066 A:middle
All right.

00:26:55.196 --> 00:26:56.436 A:middle
So that's it for the live demo.

00:26:58.516 --> 00:27:04.016 A:middle
[ Applause ]

00:27:04.516 --> 00:27:04.876 A:middle
Thank you.

00:27:06.566 --> 00:27:07.656 A:middle
Okay. Now that we've talked

00:27:07.656 --> 00:27:08.626 A:middle
about the new neural network

00:27:08.626 --> 00:27:10.926 A:middle
graph API and I showed you how

00:27:10.956 --> 00:27:12.716 A:middle
easy it is to use and what great

00:27:12.716 --> 00:27:14.016 A:middle
performance you can achieve with

00:27:14.016 --> 00:27:16.086 A:middle
it, let's now switch gears and

00:27:16.086 --> 00:27:17.166 A:middle
talk about recurrent neural

00:27:17.166 --> 00:27:17.566 A:middle
networks.

00:27:19.416 --> 00:27:20.386 A:middle
So what are recurrent neural

00:27:20.386 --> 00:27:20.786 A:middle
networks?

00:27:23.406 --> 00:27:25.456 A:middle
So one disadvantage of CNNs is

00:27:25.456 --> 00:27:27.276 A:middle
their inability to remember

00:27:27.306 --> 00:27:28.326 A:middle
anything that happened in the

00:27:28.326 --> 00:27:28.466 A:middle
past.

00:27:29.456 --> 00:27:31.226 A:middle
They can take one image as input

00:27:31.896 --> 00:27:33.926 A:middle
and generate a single output

00:27:34.446 --> 00:27:36.316 A:middle
such as the set of probabilities

00:27:36.356 --> 00:27:37.396 A:middle
of what is depicted in the

00:27:37.396 --> 00:27:37.836 A:middle
image.

00:27:39.056 --> 00:27:41.016 A:middle
RNNs on the other hand have

00:27:41.016 --> 00:27:41.446 A:middle
memory.

00:27:42.346 --> 00:27:43.466 A:middle
And they're good at operating on

00:27:43.546 --> 00:27:44.066 A:middle
sequences.

00:27:44.536 --> 00:27:48.256 A:middle
So they can take one input such

00:27:48.256 --> 00:27:49.576 A:middle
as a set of probabilities of

00:27:49.636 --> 00:27:51.016 A:middle
what is depicted in the image

00:27:51.616 --> 00:27:52.966 A:middle
and generate a sequence of

00:27:53.036 --> 00:27:53.366 A:middle
outputs.

00:27:53.366 --> 00:27:56.196 A:middle
So a sequence of words that make

00:27:56.196 --> 00:27:57.586 A:middle
up a caption for this image.

00:27:59.416 --> 00:28:01.716 A:middle
They can also take a sequence of

00:28:01.806 --> 00:28:03.506 A:middle
inputs such as a sentence in

00:28:03.506 --> 00:28:06.626 A:middle
English and generate a sequence

00:28:06.626 --> 00:28:08.506 A:middle
of outputs such as the same

00:28:08.666 --> 00:28:09.946 A:middle
sentence translated to a

00:28:09.946 --> 00:28:12.376 A:middle
different language like Russian

00:28:12.466 --> 00:28:13.056 A:middle
or Finnish.

00:28:13.366 --> 00:28:16.356 A:middle
And we support a number of

00:28:16.356 --> 00:28:17.756 A:middle
different of variants of RNNs.

00:28:18.586 --> 00:28:20.146 A:middle
The single gate RNN, the long

00:28:20.146 --> 00:28:22.156 A:middle
short-term memory RNN or LSTM,

00:28:22.596 --> 00:28:24.586 A:middle
and multiple variants of LSTMs.

00:28:24.866 --> 00:28:26.336 A:middle
The GRU and the MGU.

00:28:27.766 --> 00:28:29.336 A:middle
So let's talk about the simplest

00:28:29.366 --> 00:28:31.326 A:middle
kind of RNN, the single gate

00:28:31.326 --> 00:28:31.526 A:middle
RNN.

00:28:33.666 --> 00:28:34.966 A:middle
the single gate RNN has a

00:28:34.966 --> 00:28:37.006 A:middle
recurrent unit which enables the

00:28:37.066 --> 00:28:38.676 A:middle
previous output over RNN to

00:28:38.996 --> 00:28:40.346 A:middle
affect the output of the

00:28:40.406 --> 00:28:41.846 A:middle
subsequent iterations of the

00:28:41.846 --> 00:28:42.406 A:middle
same RNN.

00:28:43.606 --> 00:28:45.496 A:middle
But the single gate RNNs are not

00:28:45.546 --> 00:28:47.466 A:middle
powerful enough to carry on

00:28:47.466 --> 00:28:48.746 A:middle
important information for many

00:28:48.746 --> 00:28:49.286 A:middle
iterations.

00:28:50.136 --> 00:28:51.676 A:middle
Because the current output of an

00:28:51.786 --> 00:28:53.976 A:middle
RNN -- of the single gate RNN is

00:28:53.976 --> 00:28:54.996 A:middle
also its current state.

00:28:54.996 --> 00:28:55.976 A:middle
There's nothing else there.

00:28:57.146 --> 00:28:59.426 A:middle
The solution to this is the long

00:28:59.476 --> 00:29:01.596 A:middle
short-term memory RNN or LSTM.

00:29:02.376 --> 00:29:03.906 A:middle
It's built from single gate RNNs

00:29:03.906 --> 00:29:06.246 A:middle
and it has an internal memory

00:29:06.246 --> 00:29:06.506 A:middle
cell.

00:29:07.436 --> 00:29:08.856 A:middle
And a certain combination of

00:29:08.956 --> 00:29:10.596 A:middle
gates control how the

00:29:10.596 --> 00:29:13.106 A:middle
information flows inside LSTM.

00:29:13.436 --> 00:29:15.016 A:middle
And what is stored and not

00:29:15.136 --> 00:29:16.266 A:middle
stored in the memory cell.

00:29:16.846 --> 00:29:19.656 A:middle
So let's take a look at the

00:29:19.656 --> 00:29:21.316 A:middle
architecture of LSTM in more

00:29:21.316 --> 00:29:21.776 A:middle
detail.

00:29:22.206 --> 00:29:25.246 A:middle
As I said, the most important

00:29:25.356 --> 00:29:27.846 A:middle
entity inside LSTM is the memory

00:29:27.886 --> 00:29:30.406 A:middle
cell which is updated in every

00:29:30.476 --> 00:29:31.536 A:middle
duration of LSTM.

00:29:31.536 --> 00:29:33.426 A:middle
So you can think of each

00:29:33.846 --> 00:29:35.246 A:middle
iteration of LSTM is this

00:29:35.306 --> 00:29:37.456 A:middle
transition between the old and

00:29:37.456 --> 00:29:38.016 A:middle
new memory.

00:29:38.676 --> 00:29:40.786 A:middle
And now let's talk about the

00:29:40.816 --> 00:29:41.036 A:middle
gates.

00:29:41.566 --> 00:29:43.376 A:middle
So first there is a forget gate

00:29:44.216 --> 00:29:45.876 A:middle
which decides what to keep and

00:29:45.876 --> 00:29:47.206 A:middle
what not to keep from old

00:29:47.206 --> 00:29:47.566 A:middle
memory.

00:29:48.966 --> 00:29:50.456 A:middle
And then there are the inputs

00:29:50.456 --> 00:29:51.836 A:middle
and the cell gates and their

00:29:51.836 --> 00:29:53.996 A:middle
combined contribution determines

00:29:54.066 --> 00:29:55.786 A:middle
what from the current input will

00:29:55.786 --> 00:29:56.936 A:middle
affect the new memory.

00:29:56.936 --> 00:29:59.136 A:middle
And then the combination of all

00:29:59.136 --> 00:30:00.866 A:middle
of these three gates is combined

00:30:01.226 --> 00:30:04.596 A:middle
to update the memory cell.

00:30:05.696 --> 00:30:07.576 A:middle
And finally, there is the output

00:30:07.626 --> 00:30:09.656 A:middle
gate which determines what from

00:30:09.656 --> 00:30:11.976 A:middle
the previous inputs the -- the

00:30:12.476 --> 00:30:14.076 A:middle
previous output, the current

00:30:14.076 --> 00:30:16.126 A:middle
inputs and the new memory will

00:30:16.126 --> 00:30:17.936 A:middle
affect the output of LSTM.

00:30:19.196 --> 00:30:20.626 A:middle
So now that you know what LSTM

00:30:20.626 --> 00:30:22.206 A:middle
is made up of, let's take a look

00:30:22.206 --> 00:30:24.006 A:middle
at how you can create one using

00:30:24.006 --> 00:30:24.576 A:middle
our framework.

00:30:25.616 --> 00:30:27.536 A:middle
So first you create a descriptor

00:30:27.756 --> 00:30:28.576 A:middle
for the LSTM.

00:30:29.146 --> 00:30:31.306 A:middle
And then you need to initialize

00:30:31.526 --> 00:30:31.876 A:middle
the gates.

00:30:32.436 --> 00:30:33.676 A:middle
So what controls the gates?

00:30:33.676 --> 00:30:35.146 A:middle
So what controls the gates with

00:30:35.306 --> 00:30:36.156 A:middle
-- what controls how they

00:30:36.156 --> 00:30:37.756 A:middle
operate is the trained

00:30:37.756 --> 00:30:38.386 A:middle
parameters.

00:30:39.146 --> 00:30:40.156 A:middle
The ones that come from the

00:30:40.156 --> 00:30:41.726 A:middle
training step where you train a

00:30:41.726 --> 00:30:43.526 A:middle
system to do a particular task.

00:30:45.566 --> 00:30:47.496 A:middle
And there are multiple gates for

00:30:47.496 --> 00:30:48.586 A:middle
you to initialize as you can

00:30:48.646 --> 00:30:49.856 A:middle
see, but we're only showing two

00:30:49.856 --> 00:30:52.356 A:middle
initializations just to be

00:30:52.356 --> 00:30:52.726 A:middle
brief.

00:30:53.206 --> 00:30:54.336 A:middle
And as you can see, we're also

00:30:54.336 --> 00:30:56.046 A:middle
using a data source provider.

00:30:56.046 --> 00:30:57.446 A:middle
The same one I showed you before

00:30:57.556 --> 00:30:58.606 A:middle
to initialize the weights.

00:30:59.656 --> 00:31:01.536 A:middle
And the next step is to create

00:31:01.536 --> 00:31:03.606 A:middle
our LSTM layer and now we want

00:31:03.606 --> 00:31:04.876 A:middle
to run it on the GPU.

00:31:06.586 --> 00:31:08.646 A:middle
So we need to create our arrays

00:31:08.646 --> 00:31:10.976 A:middle
that will hold the input and

00:31:10.976 --> 00:31:13.236 A:middle
output for the sequence of the

00:31:13.236 --> 00:31:14.266 A:middle
LSTM executions.

00:31:14.886 --> 00:31:16.076 A:middle
And then we encode the sequence

00:31:16.076 --> 00:31:16.716 A:middle
to the GPU.

00:31:17.416 --> 00:31:19.446 A:middle
And here we're showing you the

00:31:19.666 --> 00:31:21.546 A:middle
-- a matrix-based RNN, but we

00:31:21.546 --> 00:31:22.646 A:middle
just want to mention that we

00:31:22.686 --> 00:31:25.726 A:middle
also support RNNs that operate

00:31:25.726 --> 00:31:27.636 A:middle
on MPS images via convolutions.

00:31:30.176 --> 00:31:31.216 A:middle
And now let's take a look at an

00:31:31.216 --> 00:31:32.016 A:middle
actual example.

00:31:32.596 --> 00:31:34.366 A:middle
So we'll use image captioning as

00:31:34.366 --> 00:31:35.706 A:middle
an example of using LSTM.

00:31:36.726 --> 00:31:38.566 A:middle
So as you recall, I told you

00:31:38.896 --> 00:31:40.646 A:middle
that deep learning algorithms

00:31:40.646 --> 00:31:42.076 A:middle
have two phases.

00:31:42.346 --> 00:31:43.316 A:middle
The training phase and the

00:31:43.316 --> 00:31:44.026 A:middle
inference phase.

00:31:44.816 --> 00:31:46.816 A:middle
So to train a system to caption

00:31:46.816 --> 00:31:49.196 A:middle
images you need to feed it a

00:31:49.196 --> 00:31:51.056 A:middle
large number of images with

00:31:51.056 --> 00:31:52.356 A:middle
human-generated captions.

00:31:53.836 --> 00:31:56.606 A:middle
So what does this system have?

00:31:56.656 --> 00:31:57.686 A:middle
Like what is it made out of?

00:31:58.536 --> 00:32:02.016 A:middle
So this system has a CNN and a

00:32:02.016 --> 00:32:03.906 A:middle
RNN working together to generate

00:32:03.906 --> 00:32:04.346 A:middle
captions.

00:32:04.746 --> 00:32:07.316 A:middle
The CNN is used to figure out

00:32:07.316 --> 00:32:09.316 A:middle
what's depicted in the image and

00:32:09.316 --> 00:32:10.886 A:middle
then the RNN is used to generate

00:32:10.956 --> 00:32:11.806 A:middle
the actual caption.

00:32:13.256 --> 00:32:15.076 A:middle
And the output of that process

00:32:15.156 --> 00:32:17.006 A:middle
is the trained parameters which

00:32:17.006 --> 00:32:19.036 A:middle
are required for the next step,

00:32:20.186 --> 00:32:20.886 A:middle
the inference step.

00:32:21.676 --> 00:32:25.606 A:middle
So in the inference phase, the

00:32:25.656 --> 00:32:27.766 A:middle
trained parameters control both

00:32:27.766 --> 00:32:29.826 A:middle
the operation of the CNN layers

00:32:29.826 --> 00:32:31.866 A:middle
and the operation of the RNN

00:32:31.866 --> 00:32:32.146 A:middle
gates.

00:32:33.206 --> 00:32:37.276 A:middle
And then for each image it's

00:32:37.336 --> 00:32:39.166 A:middle
processed by both the CNN and

00:32:39.446 --> 00:32:41.326 A:middle
the RNN to generate a caption.

00:32:42.216 --> 00:32:43.246 A:middle
So we already know a good

00:32:43.246 --> 00:32:44.636 A:middle
network for figuring out what's

00:32:44.706 --> 00:32:45.816 A:middle
depicted in the image.

00:32:46.086 --> 00:32:47.506 A:middle
It's the Inception-v3 network,

00:32:47.876 --> 00:32:48.626 A:middle
so we'll use that.

00:32:49.186 --> 00:32:50.456 A:middle
And we just talked about LSTMs,

00:32:50.456 --> 00:32:52.236 A:middle
so let's use that to generate

00:32:52.446 --> 00:32:53.036 A:middle
our caption.

00:32:53.996 --> 00:32:56.456 A:middle
And the caption generation phase

00:32:57.266 --> 00:32:58.176 A:middle
-- the caption generation

00:32:58.176 --> 00:32:59.726 A:middle
process also has two phases.

00:33:00.006 --> 00:33:02.206 A:middle
So first we have the LSTM

00:33:02.486 --> 00:33:03.646 A:middle
initialization phase.

00:33:04.846 --> 00:33:06.126 A:middle
So we run our Inception-v3

00:33:06.126 --> 00:33:08.616 A:middle
network and we actually run all

00:33:08.616 --> 00:33:10.496 A:middle
of the layers except the very

00:33:10.496 --> 00:33:11.986 A:middle
last SoftMax layer.

00:33:12.236 --> 00:33:13.386 A:middle
And the output of that is a

00:33:13.386 --> 00:33:15.016 A:middle
feature vector which has

00:33:15.016 --> 00:33:16.196 A:middle
information about what is

00:33:16.196 --> 00:33:17.226 A:middle
depicted in the image.

00:33:17.906 --> 00:33:19.016 A:middle
And then we take that feature

00:33:19.016 --> 00:33:20.566 A:middle
vector and convert it to a

00:33:20.566 --> 00:33:23.246 A:middle
compact representation that's

00:33:23.246 --> 00:33:24.286 A:middle
required by LSTM.

00:33:24.286 --> 00:33:26.596 A:middle
And then run that through LSTM

00:33:26.946 --> 00:33:27.746 A:middle
to initialize it.

00:33:28.736 --> 00:33:30.466 A:middle
And then once we have our

00:33:30.466 --> 00:33:32.436 A:middle
initialized LSTM, then we're

00:33:32.436 --> 00:33:33.586 A:middle
ready for the next phase.

00:33:34.606 --> 00:33:36.066 A:middle
Our actual caption generation

00:33:36.066 --> 00:33:36.376 A:middle
phase.

00:33:38.116 --> 00:33:39.676 A:middle
And we start this process by

00:33:39.676 --> 00:33:41.366 A:middle
passing in a special sentence

00:33:41.436 --> 00:33:44.026 A:middle
start ID token to our LSTM.

00:33:44.236 --> 00:33:45.046 A:middle
And the output of that

00:33:45.046 --> 00:33:46.756 A:middle
operations is a sequence of

00:33:46.756 --> 00:33:50.006 A:middle
words which are, you know, the

00:33:50.006 --> 00:33:51.276 A:middle
words that are connected to what

00:33:51.276 --> 00:33:52.666 A:middle
is depicted in the image.

00:33:53.526 --> 00:33:55.806 A:middle
And then we pass those words to

00:33:55.806 --> 00:33:57.226 A:middle
a SoftMax layer which computes

00:33:57.226 --> 00:33:58.796 A:middle
probabilities for these words.

00:33:59.326 --> 00:34:01.176 A:middle
And we pick the three best ones.

00:34:01.326 --> 00:34:03.276 A:middle
And these three best words are

00:34:03.276 --> 00:34:05.816 A:middle
also our one-word partial

00:34:05.816 --> 00:34:07.546 A:middle
captions for a particular image.

00:34:08.126 --> 00:34:09.726 A:middle
So we take those words and pass

00:34:09.806 --> 00:34:11.856 A:middle
them to the next situation of

00:34:11.946 --> 00:34:15.046 A:middle
LSTM which function is to now

00:34:15.096 --> 00:34:16.886 A:middle
come up with three best two-word

00:34:16.916 --> 00:34:19.216 A:middle
captions for our image and so

00:34:19.216 --> 00:34:19.386 A:middle
on.

00:34:19.466 --> 00:34:21.416 A:middle
We execute for N iterations

00:34:21.886 --> 00:34:23.076 A:middle
until we reach a stopping

00:34:23.076 --> 00:34:25.596 A:middle
condition, which is when we

00:34:25.626 --> 00:34:27.116 A:middle
either reach the maximum number

00:34:27.116 --> 00:34:28.966 A:middle
of words that we want to be in

00:34:28.966 --> 00:34:30.856 A:middle
our caption, or when the

00:34:30.856 --> 00:34:32.136 A:middle
probabilities evolving the newly

00:34:32.136 --> 00:34:33.906 A:middle
generating captions drop to 0.

00:34:34.986 --> 00:34:35.996 A:middle
So I know this is still pretty

00:34:35.996 --> 00:34:36.506 A:middle
abstract.

00:34:36.846 --> 00:34:39.006 A:middle
So let's look at the output of

00:34:39.386 --> 00:34:42.266 A:middle
LSTM -- of an actual output of

00:34:42.326 --> 00:34:44.556 A:middle
LSTM for several iterations for

00:34:44.556 --> 00:34:45.546 A:middle
a particular image.

00:34:46.216 --> 00:34:49.786 A:middle
So in this image we have, you

00:34:49.786 --> 00:34:51.636 A:middle
know, our surfer riding a wave.

00:34:51.636 --> 00:34:53.146 A:middle
And we want to compute the top

00:34:53.146 --> 00:34:54.666 A:middle
three captions for this image.

00:34:55.696 --> 00:34:57.286 A:middle
And in the first iteration of

00:34:57.366 --> 00:34:59.936 A:middle
LSTM we generate three best

00:34:59.936 --> 00:35:00.306 A:middle
words.

00:35:02.336 --> 00:35:04.446 A:middle
So -- which are our best

00:35:04.446 --> 00:35:05.686 A:middle
one-word captions for this

00:35:05.686 --> 00:35:05.996 A:middle
image.

00:35:06.506 --> 00:35:07.596 A:middle
So "man", "a", and "the".

00:35:08.316 --> 00:35:10.596 A:middle
And the word "a" has the highest

00:35:10.596 --> 00:35:11.186 A:middle
probability.

00:35:11.936 --> 00:35:13.546 A:middle
So we take these three words and

00:35:13.546 --> 00:35:15.196 A:middle
we pass them to the next

00:35:15.196 --> 00:35:16.436 A:middle
iteration of LSTM.

00:35:17.166 --> 00:35:19.356 A:middle
And in this iteration, for every

00:35:19.356 --> 00:35:21.416 A:middle
one of these three starter

00:35:22.436 --> 00:35:24.416 A:middle
words, LSTM generates three new

00:35:24.416 --> 00:35:26.026 A:middle
words that have the highest

00:35:26.026 --> 00:35:28.496 A:middle
probability of following each

00:35:28.496 --> 00:35:29.606 A:middle
one of these starter words.

00:35:30.666 --> 00:35:31.966 A:middle
Right? So we have three new

00:35:31.966 --> 00:35:33.556 A:middle
words to follow the word "man".

00:35:33.946 --> 00:35:35.256 A:middle
Three new words to follow the

00:35:35.256 --> 00:35:37.766 A:middle
word "a", and three new words to

00:35:37.826 --> 00:35:38.976 A:middle
follow the word "the".

00:35:40.686 --> 00:35:42.296 A:middle
And now as you can see, each one

00:35:42.296 --> 00:35:44.486 A:middle
of these two-word captions also

00:35:44.486 --> 00:35:45.486 A:middle
have a probability.

00:35:46.026 --> 00:35:47.866 A:middle
And because the word "a" had

00:35:47.936 --> 00:35:49.286 A:middle
such a high probability in the

00:35:49.286 --> 00:35:53.366 A:middle
first iteration, then the

00:35:53.366 --> 00:35:54.646 A:middle
captions that start with the

00:35:54.646 --> 00:35:56.426 A:middle
word "a" in the second iteration

00:35:56.476 --> 00:35:58.156 A:middle
also end up having the highest

00:35:58.156 --> 00:35:58.796 A:middle
probability.

00:35:58.896 --> 00:36:00.916 A:middle
Why? Because the probability of

00:36:00.916 --> 00:36:02.506 A:middle
a two-word caption is just a

00:36:02.536 --> 00:36:04.516 A:middle
product of the probabilities of

00:36:04.516 --> 00:36:05.706 A:middle
the words that make up that

00:36:05.706 --> 00:36:06.006 A:middle
caption.

00:36:07.116 --> 00:36:08.886 A:middle
So that's how we get these three

00:36:08.886 --> 00:36:09.396 A:middle
best ones.

00:36:09.696 --> 00:36:11.226 A:middle
And then we take them and we

00:36:11.226 --> 00:36:12.836 A:middle
move on to the next iteration.

00:36:13.186 --> 00:36:14.406 A:middle
And in the next iteration we

00:36:14.406 --> 00:36:15.936 A:middle
just add one more word to our

00:36:15.936 --> 00:36:17.536 A:middle
captions so that we have

00:36:17.846 --> 00:36:18.806 A:middle
three-word captions.

00:36:19.206 --> 00:36:19.996 A:middle
And then we compute the

00:36:19.996 --> 00:36:21.836 A:middle
probabilities of those captions

00:36:21.836 --> 00:36:22.886 A:middle
and pick the best three.

00:36:23.936 --> 00:36:25.106 A:middle
And we move on to the next

00:36:25.106 --> 00:36:27.026 A:middle
iteration where we just end up

00:36:27.026 --> 00:36:28.746 A:middle
adding one more word to our

00:36:28.746 --> 00:36:29.186 A:middle
caption.

00:36:29.186 --> 00:36:30.566 A:middle
So we have four-word captions.

00:36:30.976 --> 00:36:31.876 A:middle
And then we compute the

00:36:31.926 --> 00:36:33.186 A:middle
probabilities of all these

00:36:33.236 --> 00:36:34.546 A:middle
captions and pick the best

00:36:34.606 --> 00:36:34.816 A:middle
three.

00:36:36.176 --> 00:36:37.306 A:middle
And so on -- I think you get the

00:36:37.306 --> 00:36:37.646 A:middle
idea.

00:36:37.866 --> 00:36:38.936 A:middle
Let's just skip to the end.

00:36:39.296 --> 00:36:41.416 A:middle
So in the end, we get our three

00:36:41.416 --> 00:36:43.506 A:middle
top captions for this particular

00:36:43.506 --> 00:36:43.936 A:middle
image.

00:36:43.936 --> 00:36:45.906 A:middle
And the best one is a man riding

00:36:45.906 --> 00:36:47.426 A:middle
a wave on top of a surfboard,

00:36:47.656 --> 00:36:48.616 A:middle
which I think it's pretty close.

00:36:50.966 --> 00:36:52.346 A:middle
So -- [applause] and let's now

00:36:52.346 --> 00:36:52.726 A:middle
do a demo.

00:36:53.508 --> 00:36:55.508 A:middle
[ Applause ]

00:36:58.476 --> 00:37:00.116 A:middle
So now we'll do a demo of this

00:37:00.606 --> 00:37:02.206 A:middle
-- of the captioning network.

00:37:03.346 --> 00:37:04.856 A:middle
So we have a collection of

00:37:04.856 --> 00:37:07.156 A:middle
images here, and as soon as I

00:37:07.156 --> 00:37:09.036 A:middle
tap on an image then the CNN

00:37:09.136 --> 00:37:10.826 A:middle
will run to determine what is

00:37:10.826 --> 00:37:12.526 A:middle
depicted in the image.

00:37:12.526 --> 00:37:14.046 A:middle
And then the RNN will run to

00:37:14.046 --> 00:37:15.226 A:middle
generate the actual caption.

00:37:15.356 --> 00:37:16.036 A:middle
So let's try it out.

00:37:17.826 --> 00:37:19.446 A:middle
&gt;&gt; A man riding a wave on top of

00:37:19.446 --> 00:37:19.766 A:middle
a surfboard.

00:37:19.766 --> 00:37:22.356 A:middle
&gt;&gt; So we already know this.

00:37:23.526 --> 00:37:24.566 A:middle
Now let's try another image.

00:37:24.996 --> 00:37:26.536 A:middle
&gt;&gt; An old truck is parked in the

00:37:26.536 --> 00:37:27.106 A:middle
field.

00:37:27.626 --> 00:37:28.936 A:middle
&gt;&gt; So the network actually knows

00:37:28.976 --> 00:37:30.356 A:middle
that it's an old truck and that

00:37:30.356 --> 00:37:31.786 A:middle
it's parked and not moving,

00:37:31.966 --> 00:37:33.336 A:middle
which I think is pretty

00:37:33.556 --> 00:37:34.156 A:middle
impressive.

00:37:34.786 --> 00:37:35.656 A:middle
Now let's try one more.

00:37:37.026 --> 00:37:38.496 A:middle
&gt;&gt; A black and white dog laying

00:37:38.496 --> 00:37:39.086 A:middle
in the grass.

00:37:39.796 --> 00:37:40.896 A:middle
&gt;&gt; So the network knows that

00:37:40.896 --> 00:37:42.176 A:middle
it's a black and white dog and

00:37:42.176 --> 00:37:43.636 A:middle
that it's laying in the grass,

00:37:44.326 --> 00:37:45.106 A:middle
not running.

00:37:45.236 --> 00:37:46.356 A:middle
Not walking.

00:37:46.726 --> 00:37:47.876 A:middle
Not sitting.

00:37:48.336 --> 00:37:50.266 A:middle
Laying in the grass.

00:37:50.766 --> 00:37:52.796 A:middle
So pretty cool.

00:37:53.516 --> 00:37:57.786 A:middle
[ Applause ]

00:37:58.286 --> 00:37:58.726 A:middle
Thank you.

00:37:59.306 --> 00:38:01.166 A:middle
And on this note, let's go to

00:38:01.236 --> 00:38:01.586 A:middle
the summary.

00:38:02.066 --> 00:38:03.536 A:middle
So in this session we talked

00:38:03.536 --> 00:38:05.246 A:middle
about all of the new primitives

00:38:05.276 --> 00:38:07.006 A:middle
that we added to the MPS

00:38:07.336 --> 00:38:08.206 A:middle
framework this year.

00:38:08.586 --> 00:38:10.236 A:middle
We've expanded our support for

00:38:10.236 --> 00:38:12.386 A:middle
image processing primitives and

00:38:12.786 --> 00:38:13.736 A:middle
for convolutional neural

00:38:13.736 --> 00:38:14.136 A:middle
networks.

00:38:15.006 --> 00:38:16.596 A:middle
And we've added support for

00:38:16.596 --> 00:38:18.886 A:middle
linear algebra and recurrent

00:38:18.886 --> 00:38:22.226 A:middle
neural networks.

00:38:23.096 --> 00:38:24.666 A:middle
the framework is optimized for

00:38:24.666 --> 00:38:26.146 A:middle
iOS, as I told you, and now

00:38:26.596 --> 00:38:29.226 A:middle
these primitives are also all

00:38:29.226 --> 00:38:30.016 A:middle
available on the Mac.

00:38:31.116 --> 00:38:32.416 A:middle
We also talked about the new

00:38:32.416 --> 00:38:34.676 A:middle
neural network graph API and we

00:38:34.676 --> 00:38:36.406 A:middle
showed you how easy it is to

00:38:36.406 --> 00:38:38.336 A:middle
use, to build and execute your

00:38:38.336 --> 00:38:39.666 A:middle
networks on the GPU.

00:38:40.426 --> 00:38:42.186 A:middle
And that it makes it possible

00:38:42.186 --> 00:38:43.586 A:middle
for us to deliver the best

00:38:43.666 --> 00:38:45.346 A:middle
performance for your networks

00:38:45.346 --> 00:38:46.336 A:middle
across the different GPUs.

00:38:46.336 --> 00:38:49.776 A:middle
And we would love to see you use

00:38:49.776 --> 00:38:51.976 A:middle
all of this new functionality to

00:38:51.976 --> 00:38:53.526 A:middle
create a really great apps and

00:38:53.526 --> 00:38:55.836 A:middle
tell us about it.

00:38:56.116 --> 00:38:57.296 A:middle
So please check out the related

00:38:57.296 --> 00:38:58.856 A:middle
Metal 2 sessions and the

00:38:58.856 --> 00:39:01.186 A:middle
sessions on the core ML and

00:39:01.676 --> 00:39:03.006 A:middle
Accelerate and Vision

00:39:03.006 --> 00:39:03.486 A:middle
Frameworks.

00:39:04.716 --> 00:39:06.036 A:middle
And for more information about

00:39:06.096 --> 00:39:07.636 A:middle
this session and for links of

00:39:07.706 --> 00:39:09.936 A:middle
sample code, please check out

00:39:09.986 --> 00:39:11.426 A:middle
this link on our developer

00:39:11.426 --> 00:39:14.276 A:middle
website and thank you so much

00:39:14.276 --> 00:39:15.636 A:middle
for coming, and have a great

00:39:15.636 --> 00:39:15.846 A:middle
WWDC.

00:39:16.516 --> 00:39:20.500 A:middle
[ Applause ]