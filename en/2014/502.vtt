WEBVTT
X-TIMESTAMP-MAP=MPEGTS:181083,LOCAL:00:00:00.000

00:00:12.056 --> 00:00:12.606 A:middle
&gt;&gt; Good morning, everyone.

00:00:12.976 --> 00:00:15.786 A:middle
My name is Kapil Krishnamurthy
and I work in Core Audio.

00:00:16.746 --> 00:00:17.956 A:middle
I'm here today to talk to you

00:00:17.956 --> 00:00:20.506 A:middle
about a new API called
AVAudioEngine

00:00:20.746 --> 00:00:25.106 A:middle
that we are introducing for
Mac OS X Yosemite and iOS 8.

00:00:26.986 --> 00:00:29.706 A:middle
As part of today's talk we'll
first look at an overview

00:00:29.706 --> 00:00:32.866 A:middle
of Core Audio and then we'll
dive into AVAudioEngine,

00:00:33.536 --> 00:00:35.376 A:middle
look at some of the
goals behind the project,

00:00:35.796 --> 00:00:36.986 A:middle
features of the new API,

00:00:38.006 --> 00:00:39.716 A:middle
the different building
blocks you'll be using

00:00:40.616 --> 00:00:43.816 A:middle
and finally we'll do a
section on gaming and 3D audio.

00:00:45.276 --> 00:00:46.036 A:middle
So let's get started.

00:00:46.036 --> 00:00:49.846 A:middle
For those of you who aren't
familiar with Core Audio,

00:00:50.726 --> 00:00:53.896 A:middle
Core Audio provides a
number of C APIs as part

00:00:53.896 --> 00:00:57.176 A:middle
of a different frameworks
on both iOS and Mac OS X.

00:00:57.176 --> 00:00:59.766 A:middle
And you can use these
different APIs

00:00:59.916 --> 00:01:02.286 A:middle
to implement audio features
in your applications.


WEBVTT
X-TIMESTAMP-MAP=MPEGTS:181083,LOCAL:00:00:00.000

00:00:59.916 --> 00:01:02.286 A:middle
to implement audio features
in your applications.

00:01:03.746 --> 00:01:07.026 A:middle
So using these APIs you will be
able to play in the card sounds

00:01:07.126 --> 00:01:10.966 A:middle
with low latency, convert
between different file

00:01:10.966 --> 00:01:15.716 A:middle
and data formats, read and write
audio files, work with many data

00:01:15.856 --> 00:01:18.486 A:middle
and also play sounds
that get spatialized.

00:01:20.996 --> 00:01:25.076 A:middle
Several years ago we added
some simple objective C classes

00:01:25.146 --> 00:01:28.226 A:middle
to AVFoundation and
they're called AVAudioPlayer

00:01:28.226 --> 00:01:29.366 A:middle
and AVAudioRecorder.

00:01:29.996 --> 00:01:33.176 A:middle
And using these classes you
can play sounds from files

00:01:33.306 --> 00:01:35.526 A:middle
or record directly to a file.

00:01:35.986 --> 00:01:37.806 A:middle
Now while these classes
worked really well

00:01:37.806 --> 00:01:39.046 A:middle
for simple use cases,

00:01:39.386 --> 00:01:41.936 A:middle
a more advanced user might
find themselves a bit limited.

00:01:42.586 --> 00:01:45.276 A:middle
So this year we're adding
a whole new set of API

00:01:45.676 --> 00:01:48.746 A:middle
to AVFoundation called
AVAudioEngine

00:01:49.376 --> 00:01:51.716 A:middle
and my colleague Doug
also spoke about a number

00:01:51.716 --> 00:01:54.826 A:middle
of AV audio utility
classes in session 501.

00:01:55.806 --> 00:01:58.706 A:middle
So using this new
API you will be able

00:01:58.706 --> 00:02:02.206 A:middle
to write powerful features with
just a fraction of the amount


WEBVTT
X-TIMESTAMP-MAP=MPEGTS:181083,LOCAL:00:00:00.000

00:01:58.706 --> 00:02:02.206 A:middle
to write powerful features with
just a fraction of the amount

00:02:02.206 --> 00:02:04.186 A:middle
of code that you may have
had to previously write.

00:02:05.296 --> 00:02:06.106 A:middle
So let's get started.

00:02:06.926 --> 00:02:08.556 A:middle
What were the goals
behind this project?

00:02:09.436 --> 00:02:12.156 A:middle
One of the biggest goals
was to provide a powerful

00:02:12.156 --> 00:02:13.476 A:middle
and feature-rich API set.

00:02:13.476 --> 00:02:16.646 A:middle
And we're able to do that
because we're building on top

00:02:16.646 --> 00:02:18.936 A:middle
of our existing Core Audio APIs.

00:02:19.866 --> 00:02:22.246 A:middle
Using this API we want
to developers to be able

00:02:22.246 --> 00:02:24.726 A:middle
to achieve simple as
well as complex tasks.

00:02:25.316 --> 00:02:27.926 A:middle
And a simple task could be
something like playing a sound

00:02:27.926 --> 00:02:29.356 A:middle
and running it through
an effect.

00:02:29.776 --> 00:02:31.886 A:middle
A complex task could
be something as big

00:02:31.886 --> 00:02:34.026 A:middle
as writing an entire
audio engine for a game.

00:02:35.376 --> 00:02:37.666 A:middle
We also wanted to
simplify real-time audio.

00:02:38.546 --> 00:02:40.036 A:middle
For those of you
who are not familiar

00:02:40.036 --> 00:02:42.906 A:middle
with real-time audio it
can be quite challenging.

00:02:43.856 --> 00:02:46.446 A:middle
You have a number of audio
callbacks every second

00:02:47.056 --> 00:02:48.366 A:middle
and for each callback you have

00:02:48.366 --> 00:02:50.136 A:middle
to provide data in
a timely fashion.

00:02:50.916 --> 00:02:53.386 A:middle
You can't do things like
take locks on the I/O thread

00:02:53.676 --> 00:02:55.806 A:middle
or call functions that
could block indefinitely.

00:02:56.636 --> 00:02:58.896 A:middle
So we make all of this
easier for you to work

00:02:58.896 --> 00:03:02.486 A:middle
with by giving you a
real-time audio system but one


WEBVTT
X-TIMESTAMP-MAP=MPEGTS:181083,LOCAL:00:00:00.000

00:02:58.896 --> 00:03:02.486 A:middle
with by giving you a
real-time audio system but one

00:03:02.486 --> 00:03:05.446 A:middle
that you interact with in
a non real-time context.

00:03:06.616 --> 00:03:10.686 A:middle
Features of the new API: this
is a full-featured Objective-C

00:03:10.736 --> 00:03:11.976 A:middle
API set.

00:03:12.446 --> 00:03:15.416 A:middle
You get a real-time audio
system to work with meaning

00:03:15.416 --> 00:03:17.376 A:middle
that any changes
that you make on any

00:03:17.376 --> 00:03:19.516 A:middle
of the blocks take
effect immediately.

00:03:20.336 --> 00:03:24.006 A:middle
Using this API you will be able
to read and write audio files,

00:03:24.646 --> 00:03:28.846 A:middle
play and record audio, connect
different audio processing

00:03:28.846 --> 00:03:31.856 A:middle
blocks together and then
while the engine is running

00:03:31.936 --> 00:03:35.096 A:middle
and audio is flowing through
this system you can tap the

00:03:35.096 --> 00:03:37.276 A:middle
output of each of these
processing blocks.

00:03:37.966 --> 00:03:40.426 A:middle
You'll also be able to
implement 3D audio for games.

00:03:41.166 --> 00:03:44.436 A:middle
Now before we actually jump

00:03:44.556 --> 00:03:46.676 A:middle
into the engine's building
blocks I thought I would give

00:03:46.676 --> 00:03:49.656 A:middle
you two sample use cases
to give you a little flavor

00:03:49.656 --> 00:03:51.636 A:middle
of what you'll be able
to do using this API.

00:03:52.606 --> 00:03:55.986 A:middle
So the first sample use case
is a karaoke application.

00:03:56.696 --> 00:03:58.316 A:middle
You have a backing
track that's playing

00:03:58.316 --> 00:04:00.966 A:middle
and the user is singing
along with it in real-time.


WEBVTT
X-TIMESTAMP-MAP=MPEGTS:181083,LOCAL:00:00:00.000

00:03:58.316 --> 00:04:00.966 A:middle
and the user is singing
along with it in real-time.

00:04:01.806 --> 00:04:04.916 A:middle
The output of the microphone
is passed through a delay,

00:04:04.976 --> 00:04:07.636 A:middle
which is just a musical
effect and both

00:04:07.636 --> 00:04:10.856 A:middle
of these audio chains are mixed
and sent to the output hardware.

00:04:11.116 --> 00:04:12.916 A:middle
This could be a speaker
or headphones.

00:04:14.206 --> 00:04:18.026 A:middle
Let's also say that you tap
the output of the microphone

00:04:18.646 --> 00:04:23.066 A:middle
and analyze that raw data
to see the user's on pitch,

00:04:23.066 --> 00:04:24.016 A:middle
he's doing a great job.

00:04:24.666 --> 00:04:26.916 A:middle
And if he is, play
some sound effects,

00:04:27.156 --> 00:04:29.626 A:middle
so this stream also
gets mixed in and played

00:04:29.626 --> 00:04:30.746 A:middle
out to the output hardware.

00:04:32.276 --> 00:04:33.556 A:middle
Here's another use case.

00:04:33.976 --> 00:04:36.766 A:middle
You have a streaming
application and you receive data

00:04:36.766 --> 00:04:37.866 A:middle
from the remote location.

00:04:38.556 --> 00:04:41.606 A:middle
You can now stuff this
data into different buffers

00:04:41.606 --> 00:04:43.066 A:middle
and schedule them on a player.

00:04:44.286 --> 00:04:45.836 A:middle
You can run the output
of the player

00:04:46.196 --> 00:04:49.406 A:middle
through an EQ whose UI
you present to the user

00:04:49.546 --> 00:04:52.326 A:middle
so that they can tweak the
EQ based on that preference.

00:04:53.116 --> 00:04:55.966 A:middle
The output of the EQ then
goes to the output hardware.

00:04:56.876 --> 00:04:59.276 A:middle
So these are just
two sample use cases.

00:04:59.276 --> 00:05:01.736 A:middle
You'll be able to do a
whole lot more once we talk


WEBVTT
X-TIMESTAMP-MAP=MPEGTS:181083,LOCAL:00:00:00.000

00:04:59.276 --> 00:05:01.736 A:middle
You'll be able to do a
whole lot more once we talk

00:05:01.736 --> 00:05:02.866 A:middle
about AVAudioEngine.

00:05:03.276 --> 00:05:04.086 A:middle
So let's get started.

00:05:04.716 --> 00:05:08.946 A:middle
The two main objects
that we're going to start

00:05:08.946 --> 00:05:11.616 A:middle
with are the engine
object and the node object.

00:05:11.616 --> 00:05:15.176 A:middle
And there are three specific
types of nodes: the output node,

00:05:15.346 --> 00:05:16.736 A:middle
mixer node and the player node.

00:05:17.826 --> 00:05:19.986 A:middle
We have other nodes as
well that we will get to

00:05:20.136 --> 00:05:22.126 A:middle
but these are the initial
building block nodes.

00:05:22.426 --> 00:05:24.726 A:middle
So the engine is an object

00:05:25.006 --> 00:05:27.256 A:middle
that maintains a
graph of audio nodes.

00:05:28.396 --> 00:05:31.026 A:middle
You create nodes and you
attach them to the engine

00:05:31.026 --> 00:05:33.306 A:middle
and then you use the
engine to make connections

00:05:33.346 --> 00:05:34.776 A:middle
between these different
audio nodes.

00:05:35.616 --> 00:05:38.986 A:middle
The engine will analyze these
connections and determine

00:05:38.986 --> 00:05:41.036 A:middle
which ones add up
to an active chain.

00:05:41.926 --> 00:05:43.496 A:middle
When you then start the engine,

00:05:44.226 --> 00:05:46.636 A:middle
audio flows through all
of the active chains.

00:05:47.746 --> 00:05:51.816 A:middle
A powerful feature that the
engine has is that it allows you

00:05:51.816 --> 00:05:54.056 A:middle
to dynamically reconfigure
these nodes.

00:05:54.476 --> 00:05:58.476 A:middle
This means that while the engine
is rendering you can add new

00:05:58.476 --> 00:06:00.796 A:middle
nodes and then wire them up.


WEBVTT
X-TIMESTAMP-MAP=MPEGTS:181083,LOCAL:00:00:00.000

00:05:58.476 --> 00:06:00.796 A:middle
nodes and then wire them up.

00:06:00.956 --> 00:06:04.066 A:middle
And so essentially you're adding
or removing chains dynamically.

00:06:04.726 --> 00:06:07.936 A:middle
So the typical workflow
of the engine is

00:06:07.936 --> 00:06:10.876 A:middle
that you create an instance of
the engine, create instances

00:06:10.876 --> 00:06:12.636 A:middle
of all the nodes you
want to work with,

00:06:13.076 --> 00:06:15.946 A:middle
attach them to the engine so
the engine is now aware of them

00:06:15.946 --> 00:06:19.756 A:middle
and then connect them
together, start the engine.

00:06:20.436 --> 00:06:22.166 A:middle
This will create an
active render thread

00:06:22.166 --> 00:06:25.666 A:middle
and audio will flow through
all of the active chains.

00:06:26.856 --> 00:06:29.236 A:middle
So let's now talk about a node.

00:06:30.106 --> 00:06:33.486 A:middle
A node is a basic audio
block and we have three types

00:06:33.486 --> 00:06:36.376 A:middle
of nodes: there are source
nodes, which are nodes

00:06:36.376 --> 00:06:37.406 A:middle
that generate an audio.

00:06:37.806 --> 00:06:40.726 A:middle
And examples of this are the
player or the input node.

00:06:41.646 --> 00:06:43.596 A:middle
You have nodes that
process audio.

00:06:44.066 --> 00:06:47.326 A:middle
So they take some audio and do
something to it and push it up.

00:06:47.736 --> 00:06:50.626 A:middle
And examples are a
mixer or an effect.

00:06:51.046 --> 00:06:53.886 A:middle
You also have destination
nodes that receive audio

00:06:53.976 --> 00:06:55.966 A:middle
and do something with it.

00:06:56.396 --> 00:06:58.796 A:middle
Every one of these nodes
has a certain number

00:06:58.796 --> 00:07:01.766 A:middle
of input and output buses.


WEBVTT
X-TIMESTAMP-MAP=MPEGTS:181083,LOCAL:00:00:00.000

00:06:58.796 --> 00:07:01.766 A:middle
of input and output buses.

00:07:01.766 --> 00:07:05.886 A:middle
And typically you see that
most nodes have a single input

00:07:05.886 --> 00:07:06.976 A:middle
and output bus.

00:07:07.366 --> 00:07:09.516 A:middle
But an exception to
this is a mixer node

00:07:09.726 --> 00:07:12.796 A:middle
that has multiple input busses
and a single output bus.

00:07:13.816 --> 00:07:17.946 A:middle
Every bus now has an audio
data format associated with it.

00:07:18.756 --> 00:07:20.486 A:middle
So let's talk about connections.

00:07:21.806 --> 00:07:24.276 A:middle
If you have a connection
between a source node

00:07:24.516 --> 00:07:27.416 A:middle
and a destination node,
that forms an active chain.

00:07:28.326 --> 00:07:30.636 A:middle
You can insert any
number of processing nodes

00:07:30.636 --> 00:07:32.956 A:middle
between the source node
and the destination node.

00:07:33.456 --> 00:07:35.576 A:middle
But as long as you
wire every bit

00:07:35.576 --> 00:07:37.806 A:middle
of this chain up,
it's an active chain.

00:07:38.546 --> 00:07:41.986 A:middle
As soon as you break one of the
connections, all of the nodes

00:07:42.016 --> 00:07:44.766 A:middle
that are upstream of the
point of disconnection go

00:07:44.766 --> 00:07:45.876 A:middle
into an inactive state.

00:07:46.716 --> 00:07:48.636 A:middle
In this case, I've
broken the connection

00:07:48.796 --> 00:07:51.236 A:middle
between the processing node
and the destination node,

00:07:51.736 --> 00:07:52.806 A:middle
so my processing node

00:07:52.806 --> 00:07:55.226 A:middle
and my source node are
now in an inactive state.

00:07:56.316 --> 00:07:57.976 A:middle
The same holds true
in this example.

00:07:57.976 --> 00:08:03.016 A:middle
So let's now look at
the specific node types.


WEBVTT
X-TIMESTAMP-MAP=MPEGTS:181083,LOCAL:00:00:00.000

00:07:57.976 --> 00:08:03.016 A:middle
So let's now look at
the specific node types.

00:08:03.796 --> 00:08:05.256 A:middle
The first node that
we're going to talk

00:08:05.256 --> 00:08:06.236 A:middle
about is the output node.

00:08:06.916 --> 00:08:09.486 A:middle
The engine has an
implicit destination node

00:08:09.666 --> 00:08:10.876 A:middle
and it's called the output node.

00:08:10.876 --> 00:08:14.156 A:middle
And the role of the output
node is to take the data

00:08:14.156 --> 00:08:16.756 A:middle
that it receives and hand
it to the output hardware,

00:08:17.076 --> 00:08:18.136 A:middle
so this could be the speaker.

00:08:19.406 --> 00:08:22.236 A:middle
You cannot create a standalone
instance of the output node.

00:08:22.526 --> 00:08:24.196 A:middle
You have to get it
from the instance

00:08:24.196 --> 00:08:27.466 A:middle
of the engine that
you've created.

00:08:27.566 --> 00:08:29.386 A:middle
Let's move on to the mixer node.

00:08:30.186 --> 00:08:33.726 A:middle
Mixer nodes are processing
nodes and they receive data

00:08:34.116 --> 00:08:36.666 A:middle
on different input
busses which they then mix

00:08:37.176 --> 00:08:40.096 A:middle
to a single output, which
goes out on the output bus.

00:08:41.145 --> 00:08:43.346 A:middle
When you use a mixer,
you get control

00:08:43.346 --> 00:08:45.236 A:middle
of the volume of each input bus.

00:08:45.696 --> 00:08:47.946 A:middle
And if you add an application
that was playing a number

00:08:47.946 --> 00:08:50.196 A:middle
of sounds and you put
each of these sounds

00:08:50.256 --> 00:08:51.746 A:middle
in on a separate input bus,

00:08:52.406 --> 00:08:55.376 A:middle
using this volume control
you can essentially blend

00:08:55.376 --> 00:08:57.926 A:middle
in the amount of each sound
that you want to hear.

00:08:58.236 --> 00:08:59.556 A:middle
So you create a mix.


WEBVTT
X-TIMESTAMP-MAP=MPEGTS:181083,LOCAL:00:00:00.000

00:09:00.656 --> 00:09:03.806 A:middle
You now have control
over the output volume

00:09:03.946 --> 00:09:05.086 A:middle
as well using a mixer.

00:09:05.486 --> 00:09:06.956 A:middle
So you are controlling
the volume

00:09:06.956 --> 00:09:08.576 A:middle
of the mix that you've created.

00:09:09.456 --> 00:09:13.206 A:middle
If your application has
several categories of sound,

00:09:13.496 --> 00:09:16.026 A:middle
you can make use of a
concept called submixing

00:09:16.346 --> 00:09:17.546 A:middle
to create submixers.

00:09:18.276 --> 00:09:20.516 A:middle
So let's say that you
have some UI sounds

00:09:20.576 --> 00:09:21.586 A:middle
and you have some music.

00:09:22.076 --> 00:09:24.166 A:middle
And you put all of the UI
sounds through one mixer,

00:09:24.346 --> 00:09:25.996 A:middle
all of the music
through another mixer.

00:09:26.726 --> 00:09:29.246 A:middle
Using the output volumes
of each of these mixers,

00:09:29.246 --> 00:09:30.886 A:middle
you can essentially
control the volume

00:09:30.886 --> 00:09:32.366 A:middle
of each of these submixers.

00:09:33.196 --> 00:09:35.976 A:middle
Let's take that concept a
step further and put all

00:09:35.976 --> 00:09:38.456 A:middle
of the submixers
through a master mixer.

00:09:39.646 --> 00:09:43.136 A:middle
The output volume of the master
mixer will essentially control

00:09:43.136 --> 00:09:45.676 A:middle
the volume of the entire
mix in your application.

00:09:46.926 --> 00:09:49.776 A:middle
Now the engine has an
implicit mixer node.

00:09:50.306 --> 00:09:52.666 A:middle
And when you ask the
engine for its mixer node,

00:09:52.986 --> 00:09:54.576 A:middle
it creates an instance
of a mixer.

00:09:54.576 --> 00:09:56.696 A:middle
It creates an instance
of the output node

00:09:56.996 --> 00:09:59.196 A:middle
and connects it together
for you by default.


WEBVTT
X-TIMESTAMP-MAP=MPEGTS:181083,LOCAL:00:00:00.000

00:10:00.326 --> 00:10:02.346 A:middle
The difference here
between the mixer node

00:10:02.346 --> 00:10:05.396 A:middle
and the output node is that you
can create additional instances

00:10:05.856 --> 00:10:07.496 A:middle
and then attach them
to the engine

00:10:07.496 --> 00:10:08.846 A:middle
and use them how you please.

00:10:10.396 --> 00:10:14.346 A:middle
Mixers can also have
different audio data formats

00:10:14.456 --> 00:10:15.826 A:middle
for each input bus.

00:10:16.166 --> 00:10:17.696 A:middle
And the mixer will do the work

00:10:17.866 --> 00:10:21.456 A:middle
of efficiently converting
the input data formats

00:10:21.896 --> 00:10:23.846 A:middle
to the output data format.

00:10:24.836 --> 00:10:28.426 A:middle
So now that we have looked
at these initial nodes,

00:10:28.806 --> 00:10:32.156 A:middle
let's talk about how this works
in the context of the engine.

00:10:32.696 --> 00:10:33.676 A:middle
So let's say that I have an app

00:10:33.996 --> 00:10:35.666 A:middle
that creates an instance
of the engine.

00:10:35.666 --> 00:10:39.376 A:middle
We can now ask the engine
for its main mixer node

00:10:39.376 --> 00:10:41.676 A:middle
so it's going to create
the instance of a mixer,

00:10:41.886 --> 00:10:43.566 A:middle
create a mixer of
the output node

00:10:43.886 --> 00:10:45.556 A:middle
and connect the two together.

00:10:46.196 --> 00:10:49.666 A:middle
I can now create a clear note
and attach it to the engine

00:10:50.166 --> 00:10:51.386 A:middle
and connect it to the mixer.

00:10:52.166 --> 00:10:54.746 A:middle
So at this point I have a
connection chain going all the

00:10:54.746 --> 00:10:56.636 A:middle
way from a source
to a destination,

00:10:57.126 --> 00:10:58.686 A:middle
so I have an active chain.

00:10:59.746 --> 00:11:01.486 A:middle
When I then start up the engine,


WEBVTT
X-TIMESTAMP-MAP=MPEGTS:181083,LOCAL:00:00:00.000

00:10:59.746 --> 00:11:01.486 A:middle
When I then start up the engine,

00:11:01.846 --> 00:11:03.776 A:middle
an active render
thread is created

00:11:03.776 --> 00:11:05.916 A:middle
and data is pulled
by the destination.

00:11:06.676 --> 00:11:08.606 A:middle
So I have an active
flow of data here.

00:11:09.626 --> 00:11:13.316 A:middle
The app can now interact
with each one of these blocks

00:11:13.806 --> 00:11:16.106 A:middle
and any change that
it makes on any

00:11:16.106 --> 00:11:18.246 A:middle
of the nodes will take
effect immediately.

00:11:19.886 --> 00:11:21.006 A:middle
So now that we've talked

00:11:21.156 --> 00:11:26.056 A:middle
about an active render thread
how do you push your audio data

00:11:26.266 --> 00:11:27.186 A:middle
on the render thread.

00:11:27.736 --> 00:11:29.486 A:middle
You use a player to do that.

00:11:30.636 --> 00:11:31.716 A:middle
Let's look at player nodes.

00:11:32.656 --> 00:11:34.556 A:middle
Player nodes are nodes
that can play data

00:11:34.556 --> 00:11:36.876 A:middle
from files and from buffers.

00:11:37.116 --> 00:11:40.346 A:middle
And the way that it happens
or the way that it's done is

00:11:40.346 --> 00:11:41.506 A:middle
by scheduling events,

00:11:42.086 --> 00:11:45.046 A:middle
which simply means play
data at a specified time.

00:11:45.866 --> 00:11:49.466 A:middle
That data, that time could be
now or sometime in the future.

00:11:51.266 --> 00:11:54.666 A:middle
When you're scheduling buffers
you can schedule either multiple

00:11:54.666 --> 00:11:57.956 A:middle
buffers and as each
buffer is consumed

00:11:57.956 --> 00:12:00.076 A:middle
by the player you get
an individual callback,


WEBVTT
X-TIMESTAMP-MAP=MPEGTS:181083,LOCAL:00:00:00.000

00:11:57.956 --> 00:12:00.076 A:middle
by the player you get
an individual callback,

00:12:00.626 --> 00:12:04.216 A:middle
which you can then use a cue to
go ahead and schedule more data.

00:12:05.516 --> 00:12:07.806 A:middle
You can also schedule
a single buffer

00:12:07.806 --> 00:12:09.256 A:middle
that plays in a loop fashion.

00:12:10.056 --> 00:12:12.976 A:middle
And this is useful in the case
when you may have a musical loop

00:12:13.016 --> 00:12:15.836 A:middle
or a sound effect that you want
to play over and over again.

00:12:16.476 --> 00:12:19.436 A:middle
So, you load the data and
then you play the buffer

00:12:19.436 --> 00:12:22.886 A:middle
and it'll continue to play
until you stop the player

00:12:23.276 --> 00:12:25.776 A:middle
or you interrupt it
with another buffer.

00:12:25.776 --> 00:12:27.076 A:middle
We'll get into that.

00:12:27.916 --> 00:12:30.146 A:middle
You can also schedule
a file or a portion

00:12:30.146 --> 00:12:31.486 A:middle
of a file called a segment.

00:12:32.986 --> 00:12:35.966 A:middle
So going back to our previous
diagram, we had an engine

00:12:35.966 --> 00:12:37.026 A:middle
that was in a running state.

00:12:37.676 --> 00:12:41.356 A:middle
So now I can create an instance
of a buffer and load my data

00:12:41.426 --> 00:12:43.856 A:middle
into it, shown by the red arrow.

00:12:44.156 --> 00:12:48.216 A:middle
Once I do that, I can schedule
this buffer on the player.

00:12:48.666 --> 00:12:52.326 A:middle
And when the player is playing,
the player will consume the data

00:12:52.566 --> 00:12:55.516 A:middle
in the buffer and push
it on the render thread.

00:12:55.516 --> 00:12:59.526 A:middle
In a similar manner, I can
work with multiple buffers.

00:12:59.926 --> 00:13:02.086 A:middle
So over here I have
multiple buffers.


WEBVTT
X-TIMESTAMP-MAP=MPEGTS:181083,LOCAL:00:00:00.000

00:12:59.926 --> 00:13:02.086 A:middle
So over here I have
multiple buffers.

00:13:02.176 --> 00:13:05.796 A:middle
I load data into each one of
them and I schedule each one

00:13:05.796 --> 00:13:08.126 A:middle
of them to play in
sequence on the player.

00:13:09.456 --> 00:13:12.026 A:middle
As each buffer is
consumed by the player,

00:13:12.496 --> 00:13:14.916 A:middle
I get individual
callbacks letting me know

00:13:14.916 --> 00:13:16.006 A:middle
that that buffer is done.

00:13:16.006 --> 00:13:18.996 A:middle
I can use that as a cue
and schedule more data.

00:13:19.656 --> 00:13:24.316 A:middle
In a similar manner you
can work with a file.

00:13:24.316 --> 00:13:27.606 A:middle
And the difference here is you
don't have to actually deal

00:13:27.606 --> 00:13:29.046 A:middle
with the audio data yourself.

00:13:29.786 --> 00:13:32.996 A:middle
All you need is a URL
to a physical audio file

00:13:33.316 --> 00:13:35.996 A:middle
with which you can create
an AVAudioFile object

00:13:36.566 --> 00:13:38.516 A:middle
and then schedule that
directly on the player.

00:13:39.146 --> 00:13:41.966 A:middle
The player will do the work of
reading the data from the file

00:13:42.086 --> 00:13:45.716 A:middle
and pushing it on
the render thread.

00:13:45.896 --> 00:13:48.426 A:middle
So, let's now look
at a good example

00:13:48.456 --> 00:13:49.806 A:middle
of how we can achieve this.

00:13:50.366 --> 00:13:54.966 A:middle
I first create an instance of
the engine, create an instance

00:13:54.966 --> 00:13:57.816 A:middle
of a player and attach
the player to the engine,

00:13:57.986 --> 00:13:59.516 A:middle
so the engine is now
aware of the player.

00:13:59.816 --> 00:14:02.986 A:middle
I'm now going to
split my example up


WEBVTT
X-TIMESTAMP-MAP=MPEGTS:181083,LOCAL:00:00:00.000

00:13:59.816 --> 00:14:02.986 A:middle
I'm now going to
split my example up

00:14:02.986 --> 00:14:05.066 A:middle
and show how you can
first work with a file.

00:14:05.986 --> 00:14:08.636 A:middle
So, given a URL to
an audio file,

00:14:08.726 --> 00:14:10.936 A:middle
I can create the
AudioFile object.

00:14:12.366 --> 00:14:15.866 A:middle
The next thing that I do is ask
the engine for its mainMixer.

00:14:16.676 --> 00:14:19.066 A:middle
So the engine will create
an instance of a mixer node,

00:14:19.716 --> 00:14:22.226 A:middle
create an output node and
connect the two together.

00:14:22.226 --> 00:14:26.726 A:middle
I can now go ahead and
connect the player to the mixer

00:14:27.386 --> 00:14:28.936 A:middle
with the files processing
format.

00:14:28.936 --> 00:14:31.526 A:middle
So I have so I have a connection
chain going all the way

00:14:31.526 --> 00:14:33.026 A:middle
from a player that's a source

00:14:33.586 --> 00:14:35.516 A:middle
to the output node
that's a destination.

00:14:37.176 --> 00:14:40.776 A:middle
Now I can schedule my
file to play atTime:nil,

00:14:41.056 --> 00:14:42.666 A:middle
which is as soon as possible.

00:14:43.206 --> 00:14:45.776 A:middle
And in this case I pass a nil
for the completion handler.

00:14:46.486 --> 00:14:48.326 A:middle
If I had some work that
I needed to be done

00:14:48.426 --> 00:14:50.276 A:middle
after the file is
consumed by the player,

00:14:50.486 --> 00:14:51.826 A:middle
I can pass in a block over here.

00:14:53.146 --> 00:14:56.986 A:middle
So in a similar manner I can
work with a buffer as well.

00:14:57.636 --> 00:15:01.066 A:middle
Let's say that I create
an AVAudioPCMBuffer object


WEBVTT
X-TIMESTAMP-MAP=MPEGTS:181083,LOCAL:00:00:00.000

00:14:57.636 --> 00:15:01.066 A:middle
Let's say that I create
an AVAudioPCMBuffer object

00:15:01.156 --> 00:15:02.996 A:middle
and load some data into it.

00:15:03.546 --> 00:15:06.876 A:middle
The specifics of that part
are covered in session 501.

00:15:07.016 --> 00:15:09.496 A:middle
So if you missed that,
please refer to that session.

00:15:10.346 --> 00:15:13.966 A:middle
Once I have my buffer object I
can go ahead and ask the engine

00:15:13.966 --> 00:15:17.116 A:middle
for its mixer and make the
connection between the player

00:15:17.526 --> 00:15:20.216 A:middle
to the mixer with
the buffer's format.

00:15:21.196 --> 00:15:25.226 A:middle
Now I can go ahead and
schedule this buffer atTime:nil,

00:15:25.226 --> 00:15:27.036 A:middle
which is as soon as possible.

00:15:27.726 --> 00:15:29.776 A:middle
But note that we have
an additional argument

00:15:29.966 --> 00:15:32.856 A:middle
when we are working with
buffers, the options argument.

00:15:33.536 --> 00:15:35.656 A:middle
We're going to talk about
that right after this.

00:15:35.766 --> 00:15:37.246 A:middle
But for now I'm going
to pass nil,

00:15:38.386 --> 00:15:40.296 A:middle
and nil for the completion
handler as well.

00:15:41.256 --> 00:15:42.816 A:middle
So now that I've
scheduled my data

00:15:43.206 --> 00:15:46.046 A:middle
on the player I can go
ahead and start the engine.

00:15:46.506 --> 00:15:48.196 A:middle
This creates an active
render thread

00:15:48.196 --> 00:15:50.376 A:middle
and then call play
on the player.

00:15:50.376 --> 00:15:53.336 A:middle
And the player will do the
work of creating the data

00:15:53.706 --> 00:15:56.666 A:middle
from the file in the buffer and
pushing it on the render thread.

00:15:57.566 --> 00:15:59.946 A:middle
So let's now talk about
the different buffer

00:15:59.946 --> 00:16:01.186 A:middle
scheduling options.


WEBVTT
X-TIMESTAMP-MAP=MPEGTS:181083,LOCAL:00:00:00.000

00:15:59.946 --> 00:16:01.186 A:middle
scheduling options.

00:16:03.436 --> 00:16:06.596 A:middle
In all of the examples that
I'm going to talk about now,

00:16:06.596 --> 00:16:10.126 A:middle
I'm going to specify nil
for the atTime argument

00:16:10.126 --> 00:16:12.976 A:middle
and that just means that
in all of these examples,

00:16:13.156 --> 00:16:15.976 A:middle
I'm going to schedule something
to play as soon as possible.

00:16:16.606 --> 00:16:20.076 A:middle
So let's talk about the first
option and that's when you want

00:16:20.076 --> 00:16:22.746 A:middle
to schedule a buffer to
play as soon as possible.

00:16:23.506 --> 00:16:27.826 A:middle
In that case all you need
to do is schedule a buffer

00:16:27.826 --> 00:16:28.946 A:middle
with the option set to nil.

00:16:29.566 --> 00:16:32.066 A:middle
You call play on the player
and that buffer gets played.

00:16:33.676 --> 00:16:36.146 A:middle
If you have a buffer that's
playing now and you want

00:16:36.146 --> 00:16:39.816 A:middle
to append a new buffer,
it's the exact same call.

00:16:40.416 --> 00:16:43.506 A:middle
You schedule the new buffer
with the option set to nil

00:16:44.036 --> 00:16:47.216 A:middle
and so the new buffer
gets appended to the cue

00:16:47.216 --> 00:16:48.676 A:middle
of currently playing buffers.

00:16:48.676 --> 00:16:51.536 A:middle
On the other hand if I want

00:16:51.536 --> 00:16:54.686 A:middle
to interrupt my currently
playing buffer

00:16:54.856 --> 00:16:58.156 A:middle
with a new buffer I can
schedule the new buffer

00:16:58.346 --> 00:17:01.486 A:middle
with the AVAudioPlayerNode
BufferInterrupts option.


WEBVTT
X-TIMESTAMP-MAP=MPEGTS:181083,LOCAL:00:00:00.000

00:16:58.346 --> 00:17:01.486 A:middle
with the AVAudioPlayerNode
BufferInterrupts option.

00:17:02.056 --> 00:17:04.476 A:middle
So that will interrupt the
currently playing buffer

00:17:04.596 --> 00:17:06.836 A:middle
and start playing my
new buffer right away.

00:17:08.445 --> 00:17:11.616 A:middle
Let's now look at the different
variants with a looping buffer.

00:17:12.346 --> 00:17:14.695 A:middle
So like I said earlier,
if I have a buffer that's

00:17:14.695 --> 00:17:17.016 A:middle
to be played in a looped
fashion, like a sound effect,

00:17:17.016 --> 00:17:21.066 A:middle
for instance, I can load the
data in that buffer and schedule

00:17:21.066 --> 00:17:24.046 A:middle
that buffer with the
AVAudioPlayerNodeBufferLoops

00:17:24.046 --> 00:17:24.435 A:middle
option.

00:17:25.346 --> 00:17:27.976 A:middle
When I call play on the
player, that buffer starts

00:17:27.976 --> 00:17:29.306 A:middle
to play in a looped fashion.

00:17:30.946 --> 00:17:35.826 A:middle
If I want to interrupt a looping
buffer it's the same option

00:17:35.876 --> 00:17:37.206 A:middle
as what we've seen before.

00:17:37.206 --> 00:17:39.346 A:middle
I have to schedule a new buffer

00:17:39.566 --> 00:17:42.646 A:middle
with the AVAudioPlayerNode
BufferInterrupts option.

00:17:43.296 --> 00:17:46.106 A:middle
So essentially it's the same
option for when you want

00:17:46.106 --> 00:17:49.706 A:middle
to interrupt a regular
buffer or a looping buffer.

00:17:51.256 --> 00:17:53.786 A:middle
The last case is
an interesting one.

00:17:54.626 --> 00:17:56.936 A:middle
So if you have a looping
buffer but you want

00:17:56.936 --> 00:18:00.636 A:middle
to let the current loop finish
before you start playing your


WEBVTT
X-TIMESTAMP-MAP=MPEGTS:181083,LOCAL:00:00:00.000

00:17:56.936 --> 00:18:00.636 A:middle
to let the current loop finish
before you start playing your

00:18:00.636 --> 00:18:04.516 A:middle
new data you can
schedule your new buffer

00:18:04.646 --> 00:18:08.576 A:middle
with the AVAudioPlayerNodeBuffer
InterruptsAtLoop option.

00:18:09.576 --> 00:18:13.276 A:middle
So this will let the current
loop finish and as soon

00:18:13.276 --> 00:18:16.266 A:middle
as that loop is done the
new buffer starts playing.

00:18:17.006 --> 00:18:20.666 A:middle
Now that was a whole bunch
of options, so let's look

00:18:20.666 --> 00:18:23.736 A:middle
at one practical example of
how we can use these options.

00:18:24.476 --> 00:18:26.616 A:middle
So let's say that I have
a sound that's broken

00:18:26.616 --> 00:18:27.696 A:middle
up into three parts.

00:18:27.786 --> 00:18:30.106 A:middle
And the example that I'm
going to use here is a siren.

00:18:31.066 --> 00:18:33.996 A:middle
So you have the initial buildup
of the sound which is the

00:18:33.996 --> 00:18:35.346 A:middle
"attack" portion of the siren.

00:18:36.316 --> 00:18:38.706 A:middle
You have the droning
portion of the siren,

00:18:38.776 --> 00:18:41.196 A:middle
which can be modeled using
just a looping buffer.

00:18:41.896 --> 00:18:43.976 A:middle
And this is the "sustain"
portion of the sound.

00:18:44.746 --> 00:18:46.976 A:middle
And then you have the
dying down of the siren,

00:18:47.246 --> 00:18:48.966 A:middle
which is the "release"
portion of the sound.

00:18:49.376 --> 00:18:52.776 A:middle
So let's say that I load
up each of these sounds

00:18:52.826 --> 00:18:53.886 A:middle
into different buffers.

00:18:54.186 --> 00:18:57.346 A:middle
The way that I can
implement this in code is

00:18:57.346 --> 00:19:00.786 A:middle
to first schedule the attack
buffer with my options set


WEBVTT
X-TIMESTAMP-MAP=MPEGTS:181083,LOCAL:00:00:00.000

00:18:57.346 --> 00:19:00.786 A:middle
to first schedule the attack
buffer with my options set

00:19:00.786 --> 00:19:04.576 A:middle
to nil and then schedule
the sustain buffer

00:19:04.996 --> 00:19:07.746 A:middle
with the AVAudioPlayerNodeBuffer
loops option.

00:19:08.876 --> 00:19:10.586 A:middle
So when I call play
on the player,

00:19:10.966 --> 00:19:13.996 A:middle
what this will do is play the
attack portion of the sound

00:19:14.476 --> 00:19:17.036 A:middle
and then immediately start
playing the sustain portion

00:19:17.036 --> 00:19:20.866 A:middle
of the sound and continue
to loop that sustain buffer

00:19:21.356 --> 00:19:23.856 A:middle
and that goes on until
I'm ready to interrupt it.

00:19:24.716 --> 00:19:27.746 A:middle
So after some time has gone
by when I'm ready to interrupt

00:19:27.786 --> 00:19:30.866 A:middle
that I can schedule
my release buffer

00:19:31.056 --> 00:19:34.486 A:middle
with the AVAudioPlayerNodeBuffer
InterruptsAtLoop option.

00:19:34.966 --> 00:19:38.596 A:middle
So this will let the last loop
of the sustained buffer finish

00:19:38.596 --> 00:19:42.486 A:middle
up and then play the
release portion of my sound.

00:19:44.756 --> 00:19:47.526 A:middle
Now remember that I said
in the beginning that all

00:19:47.526 --> 00:19:49.856 A:middle
of my examples involve
scheduling events

00:19:49.856 --> 00:19:51.206 A:middle
that play as soon as possible.

00:19:51.826 --> 00:19:54.346 A:middle
I can also schedule events
to play in the future.

00:19:54.796 --> 00:19:56.336 A:middle
So here's an example of that.

00:19:56.796 --> 00:19:59.276 A:middle
In this case I'm just
going to schedule a buffer

00:19:59.276 --> 00:20:01.936 A:middle
to play 10 seconds
in the future.


WEBVTT
X-TIMESTAMP-MAP=MPEGTS:181083,LOCAL:00:00:00.000

00:19:59.276 --> 00:20:01.936 A:middle
to play 10 seconds
in the future.

00:20:02.096 --> 00:20:04.186 A:middle
So I create an AVAudioTime
object

00:20:04.836 --> 00:20:08.566 A:middle
that has a relative sample
time 10 seconds in the future,

00:20:08.986 --> 00:20:11.716 A:middle
and I use the buffer sampleRate
as my reference point.

00:20:12.846 --> 00:20:16.196 A:middle
I can now schedule the buffer
with this AVAudioTime object

00:20:16.196 --> 00:20:18.536 A:middle
and call play on the player

00:20:18.536 --> 00:20:24.706 A:middle
and my buffer gets played
10 seconds in the future.

00:20:24.796 --> 00:20:27.856 A:middle
So we've talked about
player nodes

00:20:28.076 --> 00:20:29.796 A:middle
and how you can use a player

00:20:30.346 --> 00:20:33.016 A:middle
to push your audio data
on the render thread.

00:20:33.966 --> 00:20:35.386 A:middle
Well if you wanted to pull data

00:20:35.386 --> 00:20:38.016 A:middle
from the render thread
how do you do that?

00:20:38.486 --> 00:20:40.546 A:middle
You use a node tap.

00:20:40.546 --> 00:20:43.336 A:middle
And here's some reasons for
why you may want to do that.

00:20:43.526 --> 00:20:45.796 A:middle
Let's say you want to capture
the output of the microphone

00:20:46.626 --> 00:20:51.796 A:middle
and save that data to disk, or
if you have a music application

00:20:51.946 --> 00:20:53.796 A:middle
and you want to record
a live performance

00:20:54.576 --> 00:20:56.216 A:middle
or if you have a
game and you want

00:20:56.216 --> 00:20:57.836 A:middle
to capture the output
mix of the game.

00:20:58.456 --> 00:21:00.976 A:middle
You can do all of
that using a node tap.


WEBVTT
X-TIMESTAMP-MAP=MPEGTS:181083,LOCAL:00:00:00.000

00:20:58.456 --> 00:21:00.976 A:middle
You can do all of
that using a node tap.

00:21:01.286 --> 00:21:05.096 A:middle
And what that is, is essentially
a tap that you install

00:21:05.096 --> 00:21:06.736 A:middle
on the output bus of a node.

00:21:07.726 --> 00:21:11.176 A:middle
So the data that's captured
by the tap is returned back

00:21:11.176 --> 00:21:14.146 A:middle
to your application
via the callback log.

00:21:14.716 --> 00:21:16.686 A:middle
So going back to a
familiar diagram,

00:21:17.086 --> 00:21:18.706 A:middle
I have two players
that's connected

00:21:18.706 --> 00:21:19.856 A:middle
to the engines main mixer.

00:21:19.856 --> 00:21:24.946 A:middle
And I want to tap the output of
the mixer so I can install a tap

00:21:24.946 --> 00:21:27.886 A:middle
on the mixer and the tap
will start pulling data

00:21:27.886 --> 00:21:28.826 A:middle
from the render thread.

00:21:29.726 --> 00:21:32.636 A:middle
I can then go ahead, the
tap will then go ahead

00:21:32.806 --> 00:21:36.696 A:middle
and create a buffer object,
stuff that data into the buffer

00:21:37.456 --> 00:21:38.846 A:middle
and return that back

00:21:38.846 --> 00:21:41.016 A:middle
to the application
via a callback block.

00:21:41.966 --> 00:21:45.186 A:middle
In code it's just
one function call.

00:21:45.796 --> 00:21:53.426 A:middle
You install a tap on the mixer's
output bus 0 with a buffer size

00:21:53.426 --> 00:21:59.766 A:middle
of 4096 frames and the mixer's
output format for that bus.


WEBVTT
X-TIMESTAMP-MAP=MPEGTS:181083,LOCAL:00:00:00.000

00:22:00.106 --> 00:22:03.716 A:middle
Within the block I have an
AVAudioPCMBuffer that contains

00:22:03.946 --> 00:22:05.036 A:middle
that much amount of data.

00:22:05.636 --> 00:22:07.606 A:middle
And I can do whatever I
need to do with that data.

00:22:08.546 --> 00:22:10.336 A:middle
Alright, so to quickly
summarize,

00:22:10.726 --> 00:22:12.426 A:middle
you have an active
render thread.

00:22:12.686 --> 00:22:14.986 A:middle
You use player nodes
to push your audio data

00:22:15.246 --> 00:22:18.616 A:middle
on the render thread and use
node taps to pull audio data

00:22:18.826 --> 00:22:20.916 A:middle
from the render thread.

00:22:21.246 --> 00:22:23.216 A:middle
Let's now switch gears and talk

00:22:23.216 --> 00:22:25.436 A:middle
about a new node
called the input node.

00:22:26.496 --> 00:22:29.776 A:middle
The input node receives
data from the input hardware

00:22:29.886 --> 00:22:31.896 A:middle
and it's parallel
to the output node.

00:22:32.776 --> 00:22:35.836 A:middle
With the input node you cannot
create a standalone instance.

00:22:36.056 --> 00:22:40.776 A:middle
You have to get the
instance from the engine.

00:22:40.776 --> 00:22:44.396 A:middle
When you've connected the
input node in an active chain

00:22:44.396 --> 00:22:49.206 A:middle
and the engine is running, data
is pulled from the input node.

00:22:50.346 --> 00:22:52.076 A:middle
So let's go back to
a familiar diagram.

00:22:52.586 --> 00:22:55.496 A:middle
I've connected the input
node to the mixer nodes

00:22:55.946 --> 00:22:57.616 A:middle
and that's connected
to the output node.

00:22:58.226 --> 00:23:01.126 A:middle
So when I start the engine,
this is an active chain


WEBVTT
X-TIMESTAMP-MAP=MPEGTS:181083,LOCAL:00:00:00.000

00:22:58.226 --> 00:23:01.126 A:middle
So when I start the engine,
this is an active chain

00:23:01.516 --> 00:23:03.956 A:middle
and data is pulled
from the input node.

00:23:04.806 --> 00:23:07.676 A:middle
So, if I'm receiving
data from the input node

00:23:07.746 --> 00:23:11.736 A:middle
and the engine is running and
I want to stop receiving data

00:23:11.736 --> 00:23:14.056 A:middle
at a certain point,
how do I do that?

00:23:14.486 --> 00:23:15.186 A:middle
It's very simple.

00:23:15.586 --> 00:23:18.236 A:middle
All you have to do-oh I'm sorry.

00:23:18.236 --> 00:23:18.936 A:middle
I raced ahead.

00:23:18.936 --> 00:23:20.786 A:middle
Let's look at a code example

00:23:20.946 --> 00:23:22.446 A:middle
of how you can connect
the input node.

00:23:23.416 --> 00:23:25.416 A:middle
So I get the input
node from the engine.

00:23:26.126 --> 00:23:28.016 A:middle
Just make a connection
to any other node

00:23:28.526 --> 00:23:30.076 A:middle
with the input node's
hardware format

00:23:31.596 --> 00:23:32.846 A:middle
and then start the engine.

00:23:33.396 --> 00:23:34.736 A:middle
This creates an active
render thread

00:23:34.736 --> 00:23:36.456 A:middle
and the input nodes
pull for data.

00:23:37.426 --> 00:23:38.836 A:middle
So like I was saying earlier,

00:23:38.906 --> 00:23:42.646 A:middle
if you have an input node that's
being pulled and you don't want

00:23:42.646 --> 00:23:45.856 A:middle
to receive data anymore from
the input node, what do you do?

00:23:46.266 --> 00:23:47.856 A:middle
Just disconnect the input node.

00:23:48.656 --> 00:23:51.496 A:middle
So the input node will no
longer be in an active chain

00:23:52.416 --> 00:23:53.756 A:middle
and it won't be pulled for data.

00:23:54.786 --> 00:23:57.276 A:middle
In order to do that, it's
just one line of code.

00:23:57.896 --> 00:24:00.776 A:middle
Using the engine, you
disconnect the node output


WEBVTT
X-TIMESTAMP-MAP=MPEGTS:181083,LOCAL:00:00:00.000

00:23:57.896 --> 00:24:00.776 A:middle
Using the engine, you
disconnect the node output

00:24:00.776 --> 00:24:02.396 A:middle
of the input node.

00:24:02.396 --> 00:24:08.456 A:middle
Now if you want to capture
data from the input node,

00:24:08.756 --> 00:24:11.296 A:middle
you can install a node tap
and we've talked about that.

00:24:12.086 --> 00:24:15.216 A:middle
But what's interesting about
this particular example is,

00:24:15.216 --> 00:24:18.186 A:middle
if I wanted to work with
just the input node,

00:24:18.416 --> 00:24:22.346 A:middle
say just capture data from the
microphone and maybe examine it,

00:24:22.606 --> 00:24:25.096 A:middle
analyze it in real time or
maybe write it out to file,

00:24:25.136 --> 00:24:29.136 A:middle
I can directly install
a tap on the input node.

00:24:29.786 --> 00:24:32.976 A:middle
And the tap will do the work of
pulling the input node for data,

00:24:33.526 --> 00:24:35.866 A:middle
stuffing it in buffers
and then returning

00:24:35.866 --> 00:24:37.076 A:middle
that back to the application.

00:24:37.696 --> 00:24:39.796 A:middle
Once you have that data you
can do whatever you need

00:24:39.796 --> 00:24:41.006 A:middle
to do with it.

00:24:42.626 --> 00:24:46.696 A:middle
And let's now talk about
the last type of nodes

00:24:46.816 --> 00:24:49.046 A:middle
in this section, effect nodes.

00:24:50.156 --> 00:24:52.666 A:middle
Effect nodes are nodes
that process data.

00:24:53.226 --> 00:24:56.326 A:middle
So, depending on the type of
effect, they take some amount

00:24:56.326 --> 00:24:59.356 A:middle
of data in, they process
it and push that data out.


WEBVTT
X-TIMESTAMP-MAP=MPEGTS:181083,LOCAL:00:00:00.000

00:25:00.736 --> 00:25:03.066 A:middle
We have two main
categories of effects.

00:25:03.226 --> 00:25:07.666 A:middle
You have AVAudioUnitEffects
and AVAudioUnitTimeEffects.

00:25:08.676 --> 00:25:10.586 A:middle
So what's the difference
between the two?

00:25:11.456 --> 00:25:16.286 A:middle
AVAudioUnitEffects require the
same amount of data on input

00:25:16.286 --> 00:25:19.396 A:middle
as the amount of data they're
being asked to provide.

00:25:20.136 --> 00:25:23.296 A:middle
So let's take the example
of a distortion effect.

00:25:24.016 --> 00:25:28.636 A:middle
If a distortion node has
to provide 24ms of output,

00:25:29.206 --> 00:25:31.646 A:middle
all it needs is 24ms of input

00:25:31.876 --> 00:25:33.776 A:middle
that it then processes
and pushes out.

00:25:33.776 --> 00:25:38.696 A:middle
As opposed to that, TimeEffects
don't have that constraint.

00:25:39.666 --> 00:25:42.146 A:middle
So let's say that you have a
TimeEffect that's doing some

00:25:42.146 --> 00:25:43.246 A:middle
amount of time stretching.

00:25:43.986 --> 00:25:47.366 A:middle
If it is being asked to
provide 24ms of output,

00:25:47.646 --> 00:25:50.196 A:middle
it may require 48ms of input.

00:25:51.346 --> 00:25:53.206 A:middle
So that brings me
to my second point.

00:25:53.756 --> 00:25:57.386 A:middle
It is for that reason why you
cannot connect a TimeEffect

00:25:57.716 --> 00:25:59.326 A:middle
directly with the input node.


WEBVTT
X-TIMESTAMP-MAP=MPEGTS:181083,LOCAL:00:00:00.000

00:26:00.226 --> 00:26:02.556 A:middle
Because, when you have
the input node running

00:26:02.556 --> 00:26:07.536 A:middle
in real-time it cannot provide
data that it doesn't have.

00:26:07.756 --> 00:26:09.066 A:middle
As opposed to that,

00:26:09.146 --> 00:26:12.186 A:middle
with AVAudioUnitEffects you
can connect them anywhere

00:26:12.186 --> 00:26:12.666 A:middle
in the chain.

00:26:13.006 --> 00:26:14.516 A:middle
So you can use them with players

00:26:14.826 --> 00:26:17.526 A:middle
or you can use them
with the input node.

00:26:19.256 --> 00:26:21.556 A:middle
These are the list of effects

00:26:21.616 --> 00:26:22.976 A:middle
that we currently
have available.

00:26:29.076 --> 00:26:32.276 A:middle
So on the effects side
we have the Delay,

00:26:32.276 --> 00:26:34.026 A:middle
Distortion, EQ and Reverb.

00:26:34.326 --> 00:26:36.736 A:middle
If you're a musician you're
probably already familiar

00:26:36.736 --> 00:26:39.156 A:middle
with these effects so you
can use them in real time

00:26:39.186 --> 00:26:40.076 A:middle
or use them in the player.

00:26:40.816 --> 00:26:43.346 A:middle
And on the TimeEffect side,

00:26:43.636 --> 00:26:45.676 A:middle
we have the Varispeed
and the TimePitch.

00:26:46.146 --> 00:26:49.136 A:middle
And these effects are useful
in cases where you want

00:26:49.136 --> 00:26:52.116 A:middle
to manipulate the amount of time
stretching or maybe the pitch

00:26:52.116 --> 00:26:53.096 A:middle
of the source content.

00:26:53.796 --> 00:26:56.476 A:middle
So let's say that you have a
speech file that you're playing

00:26:56.606 --> 00:27:00.296 A:middle
and you want to pitch the voice
up to sound like a chipmunk.


WEBVTT
X-TIMESTAMP-MAP=MPEGTS:181083,LOCAL:00:00:00.000

00:26:56.606 --> 00:27:00.296 A:middle
and you want to pitch the voice
up to sound like a chipmunk.

00:27:01.086 --> 00:27:05.386 A:middle
Well you can do that using
one of the TimeEffects.

00:27:05.666 --> 00:27:07.416 A:middle
So let's now look at an example

00:27:07.596 --> 00:27:09.226 A:middle
of how you can use
one of these effects.

00:27:10.086 --> 00:27:13.316 A:middle
In this example I'm going
to make use of the EQ.

00:27:13.726 --> 00:27:17.556 A:middle
But note that over here I've
connected the EQ directly

00:27:18.046 --> 00:27:19.076 A:middle
to the output node.

00:27:20.126 --> 00:27:24.226 A:middle
In all of my prior examples
I was connecting nodes

00:27:24.226 --> 00:27:26.936 A:middle
to the mixers, to the
engine's mixer node.

00:27:27.246 --> 00:27:28.936 A:middle
But I don't always
have to do that.

00:27:29.546 --> 00:27:32.386 A:middle
If I just have one chain of data

00:27:33.066 --> 00:27:35.536 A:middle
in my application then I
can just directly connect it

00:27:35.536 --> 00:27:37.926 A:middle
to the output node, which
is what I've done here.

00:27:40.356 --> 00:27:45.136 A:middle
So this is a multiband EQ
and I specify the number

00:27:45.356 --> 00:27:48.106 A:middle
of bands I'm going to use when
I create an instance of the EQ.

00:27:48.876 --> 00:27:51.126 A:middle
So over here, I'm
going to use two bands

00:27:51.546 --> 00:27:53.046 A:middle
so I create an EQ
with two bands.

00:27:53.916 --> 00:27:56.896 A:middle
I can then go ahead and get
access to each of the bands

00:27:56.896 --> 00:27:59.136 A:middle
and set up the different
filter parameters.

00:27:59.586 --> 00:28:04.466 A:middle
Connecting the EQ
is no different


WEBVTT
X-TIMESTAMP-MAP=MPEGTS:181083,LOCAL:00:00:00.000

00:27:59.586 --> 00:28:04.466 A:middle
Connecting the EQ
is no different

00:28:04.566 --> 00:28:06.376 A:middle
than what we've already seen.

00:28:06.836 --> 00:28:09.106 A:middle
I can connect the
player to the EQ

00:28:09.166 --> 00:28:12.296 A:middle
with the file's processing
format and connect the EQ

00:28:12.296 --> 00:28:17.766 A:middle
to the engine's output node with
the same format and that's it.

00:28:18.136 --> 00:28:20.696 A:middle
So with all of this information,
let's look at a demo

00:28:20.786 --> 00:28:23.596 A:middle
that makes use of some of the
nodes that we've talked about.

00:28:24.306 --> 00:28:26.806 A:middle
Okay, so I'm going to
explain what I have here.

00:28:27.666 --> 00:28:30.396 A:middle
Over here I have two
player nodes and each

00:28:30.396 --> 00:28:33.236 A:middle
of these players is going
to be fed by a separate,

00:28:33.336 --> 00:28:34.816 A:middle
by separate looping buffers.

00:28:36.496 --> 00:28:39.576 A:middle
Each of the players are
connected to separate effects

00:28:40.396 --> 00:28:43.826 A:middle
and each of these effects are
connected to separate inputs

00:28:43.886 --> 00:28:45.196 A:middle
of the engine's main mixer.

00:28:45.816 --> 00:28:50.506 A:middle
I have control over the output
volume of the main mixer

00:28:51.526 --> 00:28:54.826 A:middle
and down here I have
a transport control,

00:28:55.316 --> 00:28:57.686 A:middle
which essentially
controls a node tap

00:28:58.056 --> 00:28:59.906 A:middle
that I've installed
on the main mixer.


WEBVTT
X-TIMESTAMP-MAP=MPEGTS:181083,LOCAL:00:00:00.000

00:29:00.846 --> 00:29:04.526 A:middle
So when I hit Record, a tap
gets installed and I capture

00:29:04.526 --> 00:29:06.486 A:middle
that data and save it to a file.

00:29:07.116 --> 00:29:08.896 A:middle
And then when I hit
Play I'm just going

00:29:08.896 --> 00:29:09.856 A:middle
to play that file back.

00:29:10.726 --> 00:29:12.286 A:middle
So let's listen to what
this sounds like [music].

00:29:14.596 --> 00:29:18.336 A:middle
So here I'm playing the drums.

00:29:18.806 --> 00:29:23.286 A:middle
I can change the volume
and the pan of each player,

00:29:26.756 --> 00:29:27.816 A:middle
so you can hear that effect.

00:29:28.776 --> 00:29:31.076 A:middle
I'm now going to go ahead and
play the reverb a little bit.

00:29:31.276 --> 00:29:36.556 A:middle
It sounds a little too wet, so
I'm going to keep it about here.

00:29:37.566 --> 00:29:38.726 A:middle
Let me start the other player.

00:29:39.516 --> 00:29:44.546 A:middle
[ Music ]

00:29:45.046 --> 00:29:48.686 A:middle
Okay, so what I thought I'd
do now is use the node tap

00:29:49.126 --> 00:29:53.506 A:middle
to maybe capture a little live
performance, and any changes

00:29:53.556 --> 00:29:56.126 A:middle
that I make to any of the
nodes here should be captured

00:29:56.126 --> 00:29:57.146 A:middle
in that performance.

00:29:57.146 --> 00:29:59.566 A:middle
So when we go back and listen
to that we should hear that.


WEBVTT
X-TIMESTAMP-MAP=MPEGTS:181083,LOCAL:00:00:00.000

00:30:00.386 --> 00:30:01.846 A:middle
So let me do that.

00:30:02.516 --> 00:30:21.636 A:middle
[ Music ]

00:30:22.136 --> 00:30:26.296 A:middle
Okay, so I'm going to stop my
recording, stop my delay player.

00:30:28.006 --> 00:30:31.276 A:middle
And let's go back and
listen to the recording.

00:30:32.516 --> 00:30:48.516 A:middle
[ Music ]

00:30:49.016 --> 00:30:52.000 A:middle
[ Silence ]

00:30:52.046 --> 00:30:54.576 A:middle
And that's a preview of
AVAudioEngine in action.

00:30:55.966 --> 00:30:57.116 A:middle
Let's go back to slides.

00:30:57.996 --> 00:31:01.216 A:middle
Alright, so two of the
settings that I was changing


WEBVTT
X-TIMESTAMP-MAP=MPEGTS:181083,LOCAL:00:00:00.000

00:30:57.996 --> 00:31:01.216 A:middle
Alright, so two of the
settings that I was changing

00:31:02.966 --> 00:31:06.036 A:middle
with the players were the volume
and the pan for each player.

00:31:06.616 --> 00:31:11.786 A:middle
But these are actually
settings of the input mixer bus

00:31:12.116 --> 00:31:13.606 A:middle
that the player is connected to.

00:31:14.636 --> 00:31:18.066 A:middle
So the way we've exposed
mixer input bus settings

00:31:18.656 --> 00:31:21.926 A:middle
in the audio engine is
through a protocol called the

00:31:21.926 --> 00:31:23.366 A:middle
AVAudioMixing protocol.

00:31:24.286 --> 00:31:27.936 A:middle
Source nodes conform to this
protocol so the player node

00:31:27.936 --> 00:31:29.816 A:middle
and the input node do that.

00:31:30.196 --> 00:31:33.046 A:middle
And settings, like
volume, you can change

00:31:33.046 --> 00:31:38.076 A:middle
by just doing player.volume=.5
or player.band=minus 1.

00:31:39.776 --> 00:31:43.616 A:middle
When a source node is in an
active connection with the mixer

00:31:43.876 --> 00:31:46.416 A:middle
and you make changes
to the protocols,

00:31:46.416 --> 00:31:49.476 A:middle
different properties they
take effect immediately.

00:31:50.316 --> 00:31:53.266 A:middle
However, if a source node
is not connected to a mixer

00:31:53.666 --> 00:31:55.806 A:middle
and you make changes to
the protocol's properties,

00:31:56.596 --> 00:32:00.546 A:middle
those changes are cached in the
source node and then applied


WEBVTT
X-TIMESTAMP-MAP=MPEGTS:181083,LOCAL:00:00:00.000

00:31:56.596 --> 00:32:00.546 A:middle
those changes are cached in the
source node and then applied

00:32:00.616 --> 00:32:03.016 A:middle
when you make a physical
connection to a mixer.

00:32:03.666 --> 00:32:06.516 A:middle
So these are the
mixing properties

00:32:06.516 --> 00:32:07.546 A:middle
that we have available.

00:32:08.216 --> 00:32:10.776 A:middle
Under the common mixing
properties we just have volume

00:32:10.776 --> 00:32:11.156 A:middle
right now.

00:32:11.926 --> 00:32:14.256 A:middle
Under the stereo mixing
properties, we have pan

00:32:15.216 --> 00:32:17.556 A:middle
and we have a number
of 3D mixing properties

00:32:17.746 --> 00:32:19.436 A:middle
that we're going to look
at in the next section.

00:32:19.846 --> 00:32:22.626 A:middle
So in the form of a diagram,

00:32:23.106 --> 00:32:26.326 A:middle
let's say that I have Player
1 connected to Mixer 1

00:32:27.076 --> 00:32:30.686 A:middle
and I go ahead and set player
to start pan to minus 1,

00:32:31.016 --> 00:32:34.366 A:middle
hard pan it to the left and
player 1's volume to .5.

00:32:35.546 --> 00:32:39.676 A:middle
So these mixing settings are
now associated with Player 1.

00:32:39.886 --> 00:32:41.576 A:middle
And because Player
1 is connected

00:32:41.576 --> 00:32:44.966 A:middle
to Mixer 1 they also get
applied on the mixer.

00:32:45.326 --> 00:32:49.236 A:middle
If I were to disconnect Player
1 and connect it to Mixer 2,

00:32:49.706 --> 00:32:52.656 A:middle
these mixing settings
travel along with Player 1

00:32:53.056 --> 00:32:54.896 A:middle
and get applied to Mixer 2.

00:32:55.516 --> 00:32:59.576 A:middle
So in this sense, we've
been able to carry settings

00:32:59.576 --> 00:33:02.906 A:middle
that belong to the input
bus of a mixer along


WEBVTT
X-TIMESTAMP-MAP=MPEGTS:181083,LOCAL:00:00:00.000

00:32:59.576 --> 00:33:02.906 A:middle
that belong to the input
bus of a mixer along

00:33:02.906 --> 00:33:04.346 A:middle
with the source node itself.

00:33:04.926 --> 00:33:10.236 A:middle
Alright, so let's now
move onto the next section

00:33:10.436 --> 00:33:11.836 A:middle
on gaming and 3D audio.

00:33:14.606 --> 00:33:17.226 A:middle
So in games, typically
you have several types

00:33:17.226 --> 00:33:18.156 A:middle
of sounds that you play.

00:33:18.156 --> 00:33:22.956 A:middle
You have short sounds, and we've
seen AudioServices, which is one

00:33:22.956 --> 00:33:25.086 A:middle
of our C-APIs get used for that.

00:33:25.836 --> 00:33:29.476 A:middle
For playing music we see
AVAudioPlayer getting used

00:33:29.476 --> 00:33:30.036 A:middle
a lot.

00:33:30.266 --> 00:33:32.196 A:middle
And for sounds that
need to be spatialized,

00:33:32.516 --> 00:33:34.396 A:middle
OpenAL is the API of choice.

00:33:35.796 --> 00:33:38.836 A:middle
Now while each of these
APIs work really well

00:33:38.836 --> 00:33:42.086 A:middle
for what they were designed
for, if your application has

00:33:42.116 --> 00:33:46.356 A:middle
to make use of all of them, then
one of the biggest tradeoffs is

00:33:46.356 --> 00:33:48.156 A:middle
that you have to
familiarize yourself

00:33:48.156 --> 00:33:51.156 A:middle
with the nomenclature
associated with each API.

00:33:52.266 --> 00:33:55.406 A:middle
In addition, with AudioServices
you don't have a latency

00:33:55.406 --> 00:33:57.156 A:middle
guarantee of when
your sound will play.

00:33:57.866 --> 00:34:00.316 A:middle
With AVAudioPlayer
you can't play sounds


WEBVTT
X-TIMESTAMP-MAP=MPEGTS:181083,LOCAL:00:00:00.000

00:33:57.866 --> 00:34:00.316 A:middle
With AVAudioPlayer
you can't play sounds

00:34:00.316 --> 00:34:01.496 A:middle
that you have in buffers.

00:34:01.976 --> 00:34:05.486 A:middle
And with OpenAL, you can't play
sounds directly from a file

00:34:05.706 --> 00:34:07.536 A:middle
or play compressed data.

00:34:08.016 --> 00:34:10.556 A:middle
With our knowledge
of AVAudioEngine,

00:34:10.616 --> 00:34:12.716 A:middle
we have to go back
and cover cases one

00:34:12.716 --> 00:34:15.146 A:middle
and two; we can easily do so.

00:34:15.906 --> 00:34:17.775 A:middle
For short sounds we
can just load them

00:34:17.775 --> 00:34:20.946 A:middle
into AVAudioBuffer objects
and schedule them on a player.

00:34:22.005 --> 00:34:25.706 A:middle
For music you can just create
an AVAudioFile log object

00:34:25.795 --> 00:34:27.876 A:middle
and schedule that
directly on a player.

00:34:28.815 --> 00:34:31.386 A:middle
So how do you play sounds
that need to be spatialized?

00:34:31.746 --> 00:34:32.626 A:middle
We'll look at that now.

00:34:33.536 --> 00:34:36.946 A:middle
I'd like to introduce a new
node called the environment node

00:34:37.406 --> 00:34:39.726 A:middle
and this is essentially
a 3D mixer.

00:34:40.946 --> 00:34:44.775 A:middle
So when you create an instance
of the environment node.

00:34:44.775 --> 00:34:48.706 A:middle
You have a 3D space and you
get a listener that's implicit

00:34:48.746 --> 00:34:49.926 A:middle
to that 3D space.

00:34:50.976 --> 00:34:53.025 A:middle
All of the source
nodes that connect

00:34:53.656 --> 00:34:58.026 A:middle
to the environment node act
as sources in this 3D space.

00:34:59.366 --> 00:35:01.416 A:middle
So the environment
has some attributes


WEBVTT
X-TIMESTAMP-MAP=MPEGTS:181083,LOCAL:00:00:00.000

00:34:59.366 --> 00:35:01.416 A:middle
So the environment
has some attributes

00:35:01.506 --> 00:35:03.856 A:middle
that you can set directly
on the environment node.

00:35:04.496 --> 00:35:07.136 A:middle
And then each of these
sources have some attributes

00:35:07.226 --> 00:35:10.536 A:middle
and you can set that using
the AVAudioMixing protocol's

00:35:10.596 --> 00:35:11.636 A:middle
3D properties.

00:35:13.086 --> 00:35:16.596 A:middle
Now in terms of data formats,
I just wanted to point out that

00:35:16.596 --> 00:35:18.186 A:middle
when you're working with
the environment node,

00:35:18.666 --> 00:35:23.116 A:middle
all of the sources need to have
a mono data format in order

00:35:23.116 --> 00:35:24.726 A:middle
for that audio to
be spatialized.

00:35:25.636 --> 00:35:27.846 A:middle
If the sources have
a stereo data format,

00:35:28.236 --> 00:35:30.176 A:middle
then that data is passed through

00:35:30.626 --> 00:35:33.526 A:middle
and currently the environment
node doesn't support a data

00:35:33.526 --> 00:35:36.166 A:middle
format greater than
two channels on input.

00:35:36.326 --> 00:35:40.216 A:middle
So as a diagram, this
is what it looks like.

00:35:40.586 --> 00:35:43.086 A:middle
I've created an instance
of an environment node

00:35:43.236 --> 00:35:45.226 A:middle
which means I now
have a 3D space

00:35:45.996 --> 00:35:47.446 A:middle
and I have an implicit listener.

00:35:48.866 --> 00:35:50.496 A:middle
I now create two player nodes.

00:35:50.496 --> 00:35:54.106 A:middle
Who are going to act as
sources in my 3D space

00:35:54.676 --> 00:35:57.846 A:middle
and using the AVAudioMixing
protocol I can set all

00:35:57.846 --> 00:35:59.196 A:middle
of the source attributes.

00:35:59.766 --> 00:36:04.326 A:middle
So what makes things
sound 3D or virtual 3D?


WEBVTT
X-TIMESTAMP-MAP=MPEGTS:181083,LOCAL:00:00:00.000

00:35:59.766 --> 00:36:04.326 A:middle
So what makes things
sound 3D or virtual 3D?

00:36:04.786 --> 00:36:07.436 A:middle
Well, we have a number of
attributes and some belong

00:36:07.436 --> 00:36:09.966 A:middle
to the sources, others
belong to the environment.

00:36:10.486 --> 00:36:12.736 A:middle
Let's walk through each of
the source attributes first.

00:36:13.826 --> 00:36:17.476 A:middle
So every source has a
position in this 3D space.

00:36:17.896 --> 00:36:20.746 A:middle
And right now it's specified
using the right-handed cartesian

00:36:20.746 --> 00:36:25.126 A:middle
coordinate system that
right positive Y is up

00:36:25.126 --> 00:36:26.846 A:middle
and positive Z is
towards the listener.

00:36:28.076 --> 00:36:30.006 A:middle
Now with respect
to the listener,

00:36:30.706 --> 00:36:33.486 A:middle
the listener uses
some spatial cues

00:36:33.596 --> 00:36:35.886 A:middle
to localize the position
of the source.

00:36:36.646 --> 00:36:38.836 A:middle
There's an inter-aural
time difference,

00:36:39.086 --> 00:36:43.396 A:middle
just a slight time difference
for the sound made by the source

00:36:43.486 --> 00:36:45.466 A:middle
to get to each one of the
listeners in those years.

00:36:45.826 --> 00:36:48.326 A:middle
There's also an inter-aural
level difference.

00:36:49.066 --> 00:36:52.576 A:middle
In addition, your head has the
effect of doing some filtering

00:36:53.096 --> 00:36:55.766 A:middle
and you also have some
filtering here with the ears,

00:36:55.816 --> 00:36:56.786 A:middle
depending on the ears.

00:36:58.156 --> 00:37:01.926 A:middle
So we have several rendering
algorithms and each one


WEBVTT
X-TIMESTAMP-MAP=MPEGTS:181083,LOCAL:00:00:00.000

00:36:58.156 --> 00:37:01.926 A:middle
So we have several rendering
algorithms and each one

00:37:01.926 --> 00:37:04.216 A:middle
of them model these
spatial cues differently.

00:37:05.396 --> 00:37:08.456 A:middle
The thing is that we've exposed
this as a source property.

00:37:08.956 --> 00:37:11.806 A:middle
So you can pick a rendering
algorithm per source

00:37:12.286 --> 00:37:15.636 A:middle
and some algorithms may sound
better depending on the type

00:37:15.636 --> 00:37:17.736 A:middle
of content your source
is playing

00:37:18.476 --> 00:37:21.686 A:middle
and also they differ
in terms of CPU cost.

00:37:22.146 --> 00:37:25.816 A:middle
So you may want to pick a
more expensive algorithm

00:37:26.036 --> 00:37:27.466 A:middle
for an important source

00:37:27.866 --> 00:37:30.336 A:middle
and a cheaper algorithm
for a regular source.

00:37:30.856 --> 00:37:36.686 A:middle
The next two properties,
obstruction and occlusion,

00:37:36.936 --> 00:37:40.356 A:middle
deal with the filtering of
sound if there are obstacles

00:37:40.356 --> 00:37:42.066 A:middle
between the source and listener.

00:37:43.406 --> 00:37:47.216 A:middle
So in this case, I have the
source, that's the monster,

00:37:48.086 --> 00:37:50.126 A:middle
and the listener, that's
the handsome prince,

00:37:50.586 --> 00:37:53.236 A:middle
and there is a column between
the source and the listener.

00:37:53.326 --> 00:37:58.116 A:middle
So the direct path of sound is
muffled whereas the reflected

00:37:58.176 --> 00:38:00.746 A:middle
paths across the walls are clear


WEBVTT
X-TIMESTAMP-MAP=MPEGTS:181083,LOCAL:00:00:00.000

00:37:58.176 --> 00:38:00.746 A:middle
paths across the walls are clear

00:38:01.006 --> 00:38:02.626 A:middle
and this is modeled
by obstruction.

00:38:03.516 --> 00:38:08.096 A:middle
On the other hand, if the
source and the listener are

00:38:08.096 --> 00:38:11.256 A:middle
on different spaces, so
right now that's a wall

00:38:11.256 --> 00:38:12.556 A:middle
between the source
and the listener.

00:38:13.276 --> 00:38:14.866 A:middle
Both the direct part of sound

00:38:15.296 --> 00:38:17.576 A:middle
and the reflective parts
of sound are muffled.

00:38:18.126 --> 00:38:22.226 A:middle
Let's now move on
to the listener,

00:38:22.516 --> 00:38:23.866 A:middle
the environment attributes.

00:38:25.276 --> 00:38:27.706 A:middle
So every environment
has an implicit listener

00:38:27.706 --> 00:38:30.936 A:middle
and the listener has a
position and an orientation.

00:38:31.526 --> 00:38:34.266 A:middle
The position is specified using
the same coordinate system.

00:38:34.816 --> 00:38:38.226 A:middle
And for the orientation, you
can specify using either two

00:38:38.226 --> 00:38:40.366 A:middle
vectors, a front
and an up vector,

00:38:41.406 --> 00:38:45.876 A:middle
or three angles yaw,
pitch and draw.

00:38:46.646 --> 00:38:49.446 A:middle
You also have distance
attenuation in the environment,

00:38:49.916 --> 00:38:51.736 A:middle
which is just the
attenuation of sound

00:38:52.276 --> 00:38:54.656 A:middle
as a source moves away
from the listener.

00:38:55.446 --> 00:38:58.326 A:middle
So in this graph there are
two points of interest.

00:38:58.796 --> 00:39:01.626 A:middle
There's the reference
distance, which is the distance


WEBVTT
X-TIMESTAMP-MAP=MPEGTS:181083,LOCAL:00:00:00.000

00:38:58.796 --> 00:39:01.626 A:middle
There's the reference
distance, which is the distance

00:39:01.626 --> 00:39:05.236 A:middle
above which we start applying
some amount of attenuation.

00:39:05.886 --> 00:39:09.216 A:middle
There's also the maximum
distance, which is the point

00:39:09.216 --> 00:39:10.836 A:middle
above which the amount

00:39:10.836 --> 00:39:12.996 A:middle
of attenuation being
applied is capped.

00:39:13.936 --> 00:39:15.706 A:middle
So all of the exciting
stuff happens

00:39:15.706 --> 00:39:18.366 A:middle
between the reference distance
and the maximum distance.

00:39:19.156 --> 00:39:22.126 A:middle
And in that region we have three
curves that you can pick from.

00:39:22.746 --> 00:39:25.796 A:middle
So, in the form of code,
this is what it looks like.

00:39:26.126 --> 00:39:28.786 A:middle
All you need to do is get the
distance attenuation parameters

00:39:28.786 --> 00:39:32.426 A:middle
object from the environment
and then you can go ahead

00:39:32.476 --> 00:39:33.826 A:middle
and tweak all the settings.

00:39:34.386 --> 00:39:39.766 A:middle
Now every environment
also has reverberation

00:39:39.986 --> 00:39:41.596 A:middle
which is just a simulation

00:39:41.596 --> 00:39:43.856 A:middle
of the sound reflections
within that space.

00:39:44.846 --> 00:39:49.666 A:middle
The environment node has a
built-in reverb and you can pick

00:39:50.286 --> 00:39:52.246 A:middle
from a selection
of factory presets.

00:39:53.166 --> 00:39:56.536 A:middle
Now once you pick the type
of reverb you want to use,

00:39:57.296 --> 00:40:00.306 A:middle
you can set a blend
amount for each source


WEBVTT
X-TIMESTAMP-MAP=MPEGTS:181083,LOCAL:00:00:00.000

00:39:57.296 --> 00:40:00.306 A:middle
you can set a blend
amount for each source

00:40:01.186 --> 00:40:04.266 A:middle
and that just affects
the amount of each source

00:40:04.266 --> 00:40:05.966 A:middle
that you'll here
in the reverb mix.

00:40:06.496 --> 00:40:08.256 A:middle
So for some sources,
you may want them

00:40:08.256 --> 00:40:11.226 A:middle
to sound completely dry, so you
set the blend amount to zero.

00:40:11.636 --> 00:40:14.286 A:middle
And other sources you may
want to sound more ambient

00:40:14.426 --> 00:40:16.026 A:middle
so you can turn up
the blend amount.

00:40:17.396 --> 00:40:20.076 A:middle
We also have a single
filter that applies

00:40:20.136 --> 00:40:21.346 A:middle
to the output of the reverb.

00:40:21.786 --> 00:40:24.566 A:middle
So let's say that you pick
one of the factory presets

00:40:24.906 --> 00:40:27.116 A:middle
and you want it to sound
maybe a little brighter.

00:40:27.726 --> 00:40:29.466 A:middle
You can do that using
the filter.

00:40:30.726 --> 00:40:32.316 A:middle
In code, this is
what it looks like.

00:40:32.646 --> 00:40:35.376 A:middle
I get the ReverbParameters
object from the environment.

00:40:35.986 --> 00:40:37.576 A:middle
In this case, I'm enabling it

00:40:37.746 --> 00:40:41.796 A:middle
and then I load a factory
preset, LargeHall preset.

00:40:42.276 --> 00:40:44.826 A:middle
And using the AVAudioMixing
protocol,

00:40:44.826 --> 00:40:47.896 A:middle
I set the source's
reverbBlend to 0.2.

00:40:48.816 --> 00:40:51.276 A:middle
So now we've talked about
two types of mixers.

00:40:51.516 --> 00:40:54.816 A:middle
You have the 2D mixer and
you have the 3D mixer.

00:40:55.636 --> 00:40:58.506 A:middle
And source nodes, that is
the player or the input node,

00:40:59.096 --> 00:41:02.836 A:middle
talk to these mixers using
the AVAudioMixing protocol.


WEBVTT
X-TIMESTAMP-MAP=MPEGTS:181083,LOCAL:00:00:00.000

00:40:59.096 --> 00:41:02.836 A:middle
talk to these mixers using
the AVAudioMixing protocol.

00:41:03.896 --> 00:41:05.826 A:middle
So I just wanted
to point out that

00:41:05.826 --> 00:41:08.566 A:middle
when a source node is
connected to a 2D mixer,

00:41:08.996 --> 00:41:12.106 A:middle
then all of the common and
the 2D mixing properties

00:41:12.166 --> 00:41:12.766 A:middle
take effect.

00:41:13.716 --> 00:41:16.656 A:middle
When a source node is
connected to a 3D mixer,

00:41:17.156 --> 00:41:20.696 A:middle
then all of the common and
the 3D mixing properties

00:41:20.696 --> 00:41:21.256 A:middle
take effect.

00:41:22.336 --> 00:41:24.646 A:middle
Let's look at what
that looks like here.

00:41:26.126 --> 00:41:27.866 A:middle
So let's say that
I have Player 1

00:41:28.316 --> 00:41:30.906 A:middle
who is connected
to the 2D mixer.

00:41:31.776 --> 00:41:36.246 A:middle
I set the pan to be -1
and volume to be .5.

00:41:36.996 --> 00:41:40.206 A:middle
Note that pan is a
2D mixing property

00:41:40.466 --> 00:41:42.496 A:middle
but volume is a common
mixing property.

00:41:43.136 --> 00:41:45.646 A:middle
But in this case both of
them will take effect,

00:41:46.036 --> 00:41:47.846 A:middle
because the mixer
node implements both

00:41:47.846 --> 00:41:48.716 A:middle
of these properties.

00:41:49.556 --> 00:41:53.026 A:middle
If I disconnect Player 1
from the mixer and connect it

00:41:53.026 --> 00:41:57.836 A:middle
to the environment node, the
pan property will now be cached.

00:41:57.946 --> 00:42:00.956 A:middle
It doesn't take effect because
it's a 2D mixing property.


WEBVTT
X-TIMESTAMP-MAP=MPEGTS:181083,LOCAL:00:00:00.000

00:41:57.946 --> 00:42:00.956 A:middle
It doesn't take effect because
it's a 2D mixing property.

00:42:01.136 --> 00:42:03.046 A:middle
It doesn't apply to
the environment node.

00:42:04.036 --> 00:42:07.106 A:middle
Volume, on the other hand,
will continue to take effect,

00:42:07.486 --> 00:42:10.276 A:middle
because it's a common mixing
property and it's implemented

00:42:10.276 --> 00:42:11.216 A:middle
by the environment node.

00:42:11.706 --> 00:42:13.806 A:middle
So with all of that
information let's look

00:42:13.886 --> 00:42:15.986 A:middle
at a sample gaming setup.

00:42:16.786 --> 00:42:20.016 A:middle
This is just one of many
ways that you can do this

00:42:20.016 --> 00:42:21.336 A:middle
and this is just a suggestion.

00:42:21.676 --> 00:42:23.476 A:middle
It really all depends
on your application.

00:42:24.436 --> 00:42:28.316 A:middle
But in this case, I
have two 3D sources.

00:42:28.646 --> 00:42:31.366 A:middle
So, I'm going to use a
player to play some sounds

00:42:31.366 --> 00:42:34.336 A:middle
that will be spatialized
and also live input.

00:42:34.946 --> 00:42:37.466 A:middle
So let's say that the user
is chatting and then you want

00:42:37.466 --> 00:42:39.576 A:middle
to spatialize that
in a 3D environment.

00:42:40.116 --> 00:42:42.186 A:middle
I can connect the player
node and the input node

00:42:42.496 --> 00:42:43.456 A:middle
to the environment node.

00:42:43.876 --> 00:42:46.136 A:middle
And that's connected out
to the engine's main mixer.

00:42:47.066 --> 00:42:50.526 A:middle
I can now have a second
player that I'm going

00:42:50.526 --> 00:42:52.456 A:middle
to dedicate to playing music.

00:42:53.216 --> 00:42:56.776 A:middle
So this player is going to play
music and I'm going to run it

00:42:56.816 --> 00:42:59.086 A:middle
through an EQ and connect
that to the main mixer.

00:42:59.746 --> 00:43:02.346 A:middle
Let's say that I present
some UI for the users


WEBVTT
X-TIMESTAMP-MAP=MPEGTS:181083,LOCAL:00:00:00.000

00:42:59.746 --> 00:43:02.346 A:middle
Let's say that I present
some UI for the users

00:43:02.346 --> 00:43:04.166 A:middle
so that he can tweak
the EQ settings,

00:43:04.826 --> 00:43:06.456 A:middle
maybe to make the
music sound better.

00:43:06.456 --> 00:43:10.926 A:middle
I have a third player
now that I'm going

00:43:10.926 --> 00:43:13.626 A:middle
to dedicate only to
UI sound effects.

00:43:14.016 --> 00:43:17.336 A:middle
So maybe the sounds that are
made as I navigate through menus

00:43:17.846 --> 00:43:20.726 A:middle
or if my game avatar has
picked up a bonus item,

00:43:20.926 --> 00:43:25.406 A:middle
etc. So the UI player
is connected directly

00:43:25.526 --> 00:43:26.666 A:middle
to the engine's main mixer.

00:43:27.586 --> 00:43:30.746 A:middle
This is what the overall
picture looks like.

00:43:33.256 --> 00:43:36.576 A:middle
So given all of this information
let's now look at a demo

00:43:36.576 --> 00:43:38.016 A:middle
of the environment node.

00:43:39.516 --> 00:43:50.586 A:middle
[ Balls popping ]

00:43:51.086 --> 00:43:53.366 A:middle
So I want to explain
what's happening over here.

00:43:53.366 --> 00:43:56.916 A:middle
In this demo, I am using
SceneKit for the graphics

00:43:56.986 --> 00:43:59.146 A:middle
and SceneKit also comes
with a physics engine.

00:43:59.276 --> 00:44:01.306 A:middle
So this works nicely
with AVAudioEngine.


WEBVTT
X-TIMESTAMP-MAP=MPEGTS:181083,LOCAL:00:00:00.000

00:43:59.276 --> 00:44:01.306 A:middle
So this works nicely
with AVAudioEngine.

00:44:01.686 --> 00:44:05.226 A:middle
So I basically have two types
of sounds that I'm playing;

00:44:05.656 --> 00:44:07.956 A:middle
that's the "fffuh"
sound that plays

00:44:08.136 --> 00:44:10.916 A:middle
and that's before
any ball is launched.

00:44:11.596 --> 00:44:13.906 A:middle
So to do that I use
a player node

00:44:14.646 --> 00:44:19.196 A:middle
and I have the long sound effect
in a buffer and I schedule

00:44:19.196 --> 00:44:20.576 A:middle
that buffer on the player node.

00:44:21.076 --> 00:44:24.146 A:middle
But I make use of the
completion handler to know

00:44:24.146 --> 00:44:26.066 A:middle
when the player has
consumed the buffer.

00:44:26.786 --> 00:44:29.506 A:middle
So, when the player lets me know
that it's done with the buffer,

00:44:29.506 --> 00:44:32.946 A:middle
I go ahead and now
create a SceneKit node.

00:44:33.246 --> 00:44:37.496 A:middle
That's a ball and I also
create an AVAudioPlayer node,

00:44:38.266 --> 00:44:41.756 A:middle
attach it to the
engine and connect

00:44:41.876 --> 00:44:43.416 A:middle
that to the environment node.

00:44:43.936 --> 00:44:48.176 A:middle
So I'm tying a player, a
dedicated player to each ball.

00:44:49.456 --> 00:44:53.046 A:middle
Now the ball is launched into
the world and as it goes about

00:44:53.046 --> 00:44:56.166 A:middle
and collides with other
surfaces, for every collision

00:44:56.166 --> 00:44:59.946 A:middle
that happens SceneKit's
physics engine lets me know

00:44:59.946 --> 00:45:02.326 A:middle
that a collision has happened
with some other surface.


WEBVTT
X-TIMESTAMP-MAP=MPEGTS:181083,LOCAL:00:00:00.000

00:44:59.946 --> 00:45:02.326 A:middle
that a collision has happened
with some other surface.

00:45:02.666 --> 00:45:07.406 A:middle
And I get the point of
collision and also the impulse.

00:45:08.656 --> 00:45:11.836 A:middle
So using that, I can go and dig

00:45:11.836 --> 00:45:16.036 A:middle
up the player node that's
tied to the SceneKit node.

00:45:16.036 --> 00:45:19.816 A:middle
I can set the position
on the player based

00:45:19.816 --> 00:45:23.486 A:middle
on where the collision
happened, calculate a volume

00:45:23.706 --> 00:45:26.126 A:middle
for the collision sound
based on the impulse

00:45:26.996 --> 00:45:28.246 A:middle
and then just play the sound.

00:45:29.076 --> 00:45:33.166 A:middle
But you can see now
how, in this setup,

00:45:33.346 --> 00:45:36.926 A:middle
for every ball that's
born into this world,

00:45:37.016 --> 00:45:39.106 A:middle
a new player node
is also created.

00:45:39.516 --> 00:45:41.296 A:middle
So the number of
players is growing

00:45:41.296 --> 00:45:44.866 A:middle
and I'm dynamically attaching it
to the engine and connecting it

00:45:44.866 --> 00:45:45.836 A:middle
to the environment node.

00:45:46.366 --> 00:45:47.976 A:middle
So this setup is very flexible.

00:45:48.516 --> 00:45:52.546 A:middle
[ Balls popping ]

00:45:53.046 --> 00:45:54.296 A:middle
Alright, so let's
get back to slides.

00:45:55.126 --> 00:45:56.666 A:middle
That brings us to
the end of our talk.

00:45:56.936 --> 00:45:59.376 A:middle
So let's quickly summarize all
the things we've seen today.


WEBVTT
X-TIMESTAMP-MAP=MPEGTS:181083,LOCAL:00:00:00.000

00:46:00.586 --> 00:46:02.516 A:middle
We started off with
talking about an engine

00:46:03.076 --> 00:46:06.016 A:middle
and how you can create different
nodes, attach them to the engine

00:46:06.116 --> 00:46:08.016 A:middle
and then use the engine
to make connections

00:46:08.556 --> 00:46:09.806 A:middle
between each of these nodes.

00:46:10.126 --> 00:46:13.396 A:middle
We then looked at the
different types of nodes:

00:46:13.656 --> 00:46:16.536 A:middle
the destination node,
which is the output node.

00:46:17.536 --> 00:46:19.656 A:middle
And we talked about
two source nodes,

00:46:20.126 --> 00:46:21.976 A:middle
the player node and
the input node.

00:46:22.936 --> 00:46:25.576 A:middle
The player is the node you use

00:46:25.576 --> 00:46:27.796 A:middle
to push audio data
on the render thread.

00:46:28.546 --> 00:46:32.316 A:middle
We looked at two types of
mixer nodes, the 2D Mixer

00:46:32.676 --> 00:46:37.596 A:middle
and the 3D Mixer and
how source nodes talk

00:46:37.596 --> 00:46:40.806 A:middle
to these mixers using the
AVAudioMixing protocol.

00:46:41.886 --> 00:46:45.506 A:middle
We then looked at effect nodes
and two types of effect nodes:

00:46:45.586 --> 00:46:48.486 A:middle
the AVAudioEffects and
AVAudioUnitTime effects.

00:46:49.416 --> 00:46:53.596 A:middle
Finally we talked
about node taps

00:46:53.596 --> 00:46:55.906 A:middle
and that's how you pull
data from the render thread.

00:46:56.486 --> 00:46:58.206 A:middle
So I just wanted to point

00:46:58.206 --> 00:47:01.026 A:middle
out that node taps are also
a useful debugging tool.


WEBVTT
X-TIMESTAMP-MAP=MPEGTS:181083,LOCAL:00:00:00.000

00:46:58.206 --> 00:47:01.026 A:middle
out that node taps are also
a useful debugging tool.

00:47:01.666 --> 00:47:03.776 A:middle
Let's say that you have
a number of connections

00:47:03.896 --> 00:47:06.966 A:middle
in your application and things
don't sound the way you expect

00:47:06.966 --> 00:47:07.566 A:middle
them to sound.

00:47:08.326 --> 00:47:11.876 A:middle
What you can do is install
node taps at different points

00:47:11.876 --> 00:47:15.016 A:middle
in your chain on different nodes
and just examine the output

00:47:15.016 --> 00:47:16.106 A:middle
of each of these nodes.

00:47:16.596 --> 00:47:19.336 A:middle
And using that you can drill
down and where the problem is.

00:47:19.476 --> 00:47:22.246 A:middle
So in that sense node taps
are a useful debugging tool.

00:47:22.716 --> 00:47:25.856 A:middle
So that brings us to
the end of our session.

00:47:26.506 --> 00:47:29.126 A:middle
I just want to say that
this is the first version

00:47:29.126 --> 00:47:31.836 A:middle
of AVAudioEngine and we
are very excited about it.

00:47:32.536 --> 00:47:34.016 A:middle
So, we'd love to
hear what you think.

00:47:34.506 --> 00:47:36.626 A:middle
Please try it out and
give us your feedback.

00:47:37.736 --> 00:47:40.256 A:middle
If you have any further
questions at a later point,

00:47:40.476 --> 00:47:42.916 A:middle
you can contact Filip,
who's our Graphics

00:47:42.916 --> 00:47:45.666 A:middle
and Game Technologies
Evangelist.

