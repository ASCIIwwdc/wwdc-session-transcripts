WEBVTT

00:00:00.506 --> 00:00:05.500 A:middle
[ Music ]

00:00:09.516 --> 00:00:12.500 A:middle
[ Applause ]

00:00:14.096 --> 00:00:14.596 A:middle
&gt;&gt; Good morning.

00:00:15.486 --> 00:00:16.606 A:middle
My name's Dan Klingler.

00:00:17.156 --> 00:00:18.656 A:middle
And, I'm a software engineer on

00:00:18.656 --> 00:00:19.926 A:middle
the audio team here at Apple.

00:00:19.926 --> 00:00:22.266 A:middle
And, today I'm really excited to

00:00:22.266 --> 00:00:24.016 A:middle
talk to you about training sound

00:00:24.016 --> 00:00:26.036 A:middle
classification models in Create

00:00:26.036 --> 00:00:26.276 A:middle
ML.

00:00:28.766 --> 00:00:30.906 A:middle
So, before we start, we might

00:00:30.906 --> 00:00:32.766 A:middle
want to ask the question what is

00:00:32.766 --> 00:00:34.806 A:middle
sound classification, and how

00:00:34.806 --> 00:00:36.266 A:middle
might it be useful in your

00:00:36.266 --> 00:00:36.876 A:middle
application?

00:00:38.086 --> 00:00:41.036 A:middle
Sound classification is the task

00:00:41.036 --> 00:00:43.166 A:middle
of taking a sound, and placing

00:00:43.166 --> 00:00:44.966 A:middle
it into one of many categories.

00:00:45.846 --> 00:00:47.386 A:middle
But, if you think about it,

00:00:47.806 --> 00:00:49.056 A:middle
there are many different ways

00:00:49.056 --> 00:00:50.416 A:middle
that we can categorize sound.

00:00:51.616 --> 00:00:53.956 A:middle
One way might be to think about

00:00:53.956 --> 00:00:55.486 A:middle
the object that makes the sound.

00:00:55.996 --> 00:00:57.686 A:middle
So, in this example, we have the

00:00:57.686 --> 00:00:59.456 A:middle
sound of guitar, or the sound of

00:00:59.456 --> 00:01:00.056 A:middle
drums.

00:00:59.456 --> 00:01:00.056 A:middle
drums.

00:01:00.336 --> 00:01:02.106 A:middle
And, it's really the acoustical

00:01:02.106 --> 00:01:03.496 A:middle
properties of that object which

00:01:03.496 --> 00:01:05.135 A:middle
are different, and allow us as

00:01:05.135 --> 00:01:06.686 A:middle
humans to disambiguate these

00:01:06.686 --> 00:01:07.266 A:middle
sounds.

00:01:09.236 --> 00:01:10.766 A:middle
But, a second way we could think

00:01:10.766 --> 00:01:13.156 A:middle
about sound classification is

00:01:13.156 --> 00:01:15.216 A:middle
where the sound comes from, and

00:01:15.216 --> 00:01:16.436 A:middle
if you've ever gone on a hike,

00:01:16.996 --> 00:01:18.456 A:middle
or been in the middle of a busy

00:01:18.456 --> 00:01:20.986 A:middle
city, you'll understand that the

00:01:20.986 --> 00:01:22.566 A:middle
texture of the sound around you

00:01:22.846 --> 00:01:25.026 A:middle
is very different, even though

00:01:25.026 --> 00:01:26.616 A:middle
there's not one particular sound

00:01:26.616 --> 00:01:27.916 A:middle
that stands out, necessarily.

00:01:31.306 --> 00:01:32.386 A:middle
And, a third way we could think

00:01:32.386 --> 00:01:34.996 A:middle
about sound classification, is

00:01:34.996 --> 00:01:36.526 A:middle
by looking at the attributes of

00:01:36.526 --> 00:01:37.906 A:middle
the sound, or the properties of

00:01:37.906 --> 00:01:38.376 A:middle
the sound.

00:01:38.926 --> 00:01:41.516 A:middle
And so in this example, a baby's

00:01:41.516 --> 00:01:43.976 A:middle
laugh versus a baby's cry.

00:01:44.026 --> 00:01:45.756 A:middle
Both come from the same source,

00:01:46.346 --> 00:01:47.606 A:middle
but the properties of the sound

00:01:47.606 --> 00:01:48.486 A:middle
are very different.

00:01:48.656 --> 00:01:50.406 A:middle
And, this allows us to tell the

00:01:50.406 --> 00:01:51.726 A:middle
difference between these sounds.

00:01:53.336 --> 00:01:56.856 A:middle
Now, as app developers, you all

00:01:56.856 --> 00:01:58.406 A:middle
have different apps, and you

00:01:58.406 --> 00:01:59.686 A:middle
might have your different use

00:01:59.686 --> 00:02:01.386 A:middle
case for sound classification.

00:01:59.686 --> 00:02:01.386 A:middle
case for sound classification.

00:02:02.716 --> 00:02:04.456 A:middle
And, wouldn't it be great if you

00:02:04.456 --> 00:02:05.946 A:middle
could train your own model,

00:02:06.346 --> 00:02:07.696 A:middle
tailor-made for your own

00:02:07.746 --> 00:02:08.976 A:middle
application's use case?

00:02:11.486 --> 00:02:13.626 A:middle
And, you can do that today using

00:02:13.626 --> 00:02:15.176 A:middle
the Create ML app, which is

00:02:15.176 --> 00:02:16.366 A:middle
built right into Xcode.

00:02:16.856 --> 00:02:18.806 A:middle
This is the simplest way to

00:02:18.806 --> 00:02:20.696 A:middle
train a sound classifier model.

00:02:22.556 --> 00:02:24.846 A:middle
To train a sound classifier,

00:02:25.596 --> 00:02:27.386 A:middle
you'll first provide labeled

00:02:27.386 --> 00:02:29.726 A:middle
audio data to Create ML, in the

00:02:29.726 --> 00:02:31.056 A:middle
form of audio files.

00:02:32.736 --> 00:02:35.236 A:middle
Then, Create ML will train a

00:02:35.236 --> 00:02:37.096 A:middle
sound classifier model on your

00:02:37.096 --> 00:02:38.616 A:middle
custom data.

00:02:39.356 --> 00:02:41.236 A:middle
And then, you can take that

00:02:41.236 --> 00:02:43.126 A:middle
sound classifier, and use it

00:02:43.226 --> 00:02:44.376 A:middle
right in your application.

00:02:45.256 --> 00:02:46.926 A:middle
And, I'd love to show you this

00:02:46.926 --> 00:02:49.336 A:middle
process in action today, with a

00:02:49.336 --> 00:02:49.736 A:middle
demo.

00:02:52.516 --> 00:02:55.500 A:middle
[ Applause ]

00:03:00.376 --> 00:03:02.226 A:middle
So, to start, I'm going to

00:03:02.226 --> 00:03:04.236 A:middle
launch the Create ML app, which

00:03:04.236 --> 00:03:05.556 A:middle
you can find bundled with your

00:03:05.556 --> 00:03:06.546 A:middle
Xcode installation.

00:03:08.936 --> 00:03:10.196 A:middle
We're going to be creating a new

00:03:10.196 --> 00:03:13.366 A:middle
document, and select Sound from

00:03:13.366 --> 00:03:14.246 A:middle
the template chooser.

00:03:17.816 --> 00:03:20.316 A:middle
We'll click Next, and name our

00:03:20.316 --> 00:03:22.636 A:middle
project MySoundClassifier.

00:03:23.006 --> 00:03:25.406 A:middle
Let's save this project to our

00:03:25.406 --> 00:03:26.436 A:middle
Documents directory.

00:03:27.696 --> 00:03:30.236 A:middle
Once the Create ML app launches,

00:03:30.446 --> 00:03:32.576 A:middle
you'll see this home screen, and

00:03:32.576 --> 00:03:33.946 A:middle
the Input tab is selected on the

00:03:33.946 --> 00:03:34.336 A:middle
left.

00:03:36.036 --> 00:03:37.106 A:middle
This is where we'll provide our

00:03:37.106 --> 00:03:38.846 A:middle
training data to the Create ML

00:03:39.326 --> 00:03:40.346 A:middle
app, in order to train our

00:03:40.346 --> 00:03:41.096 A:middle
custom model.

00:03:43.236 --> 00:03:45.086 A:middle
You'll see some other tabs

00:03:45.086 --> 00:03:45.996 A:middle
across the top here.

00:03:46.476 --> 00:03:48.156 A:middle
Like Training, Validation, and

00:03:48.156 --> 00:03:48.636 A:middle
Testing.

00:03:49.126 --> 00:03:50.656 A:middle
And, these allow us to view some

00:03:50.656 --> 00:03:52.516 A:middle
statistics on the accuracy of

00:03:52.516 --> 00:03:53.886 A:middle
our model for each of these

00:03:53.886 --> 00:03:54.776 A:middle
stages of training.

00:03:55.996 --> 00:03:58.726 A:middle
And, finally, the Output tab is

00:03:58.726 --> 00:03:59.846 A:middle
where we'll expect to find our

00:03:59.846 --> 00:04:01.296 A:middle
model after it's been trained.

00:03:59.846 --> 00:04:01.296 A:middle
model after it's been trained.

00:04:02.046 --> 00:04:03.606 A:middle
And, we can also interact with

00:04:03.606 --> 00:04:04.916 A:middle
our model in real time.

00:04:06.356 --> 00:04:07.866 A:middle
Now, today, I'm going to be

00:04:07.866 --> 00:04:09.246 A:middle
training a musical instrument

00:04:09.246 --> 00:04:09.956 A:middle
classifier.

00:04:09.956 --> 00:04:11.376 A:middle
And, I've brought some

00:04:11.376 --> 00:04:13.246 A:middle
instruments with me that we can

00:04:13.696 --> 00:04:13.806 A:middle
try.

00:04:14.476 --> 00:04:16.245 A:middle
I have a TrainingData directory

00:04:16.805 --> 00:04:18.406 A:middle
that we can open, and take a

00:04:18.406 --> 00:04:19.706 A:middle
look at some of the sound files

00:04:19.706 --> 00:04:21.046 A:middle
I've collected.

00:04:22.055 --> 00:04:25.056 A:middle
These contain recordings from an

00:04:25.056 --> 00:04:27.526 A:middle
acoustic guitar, for example, or

00:04:27.526 --> 00:04:29.376 A:middle
cowbell, or shaker.

00:04:30.056 --> 00:04:33.756 A:middle
To train our model, all we need

00:04:33.756 --> 00:04:35.886 A:middle
to do is drag this directory

00:04:36.266 --> 00:04:37.456 A:middle
straight into Create ML.

00:04:38.136 --> 00:04:41.636 A:middle
Create ML has figured out that

00:04:41.636 --> 00:04:43.386 A:middle
we have a total of 49 sound

00:04:43.386 --> 00:04:44.346 A:middle
files we're going to be using

00:04:44.346 --> 00:04:44.696 A:middle
today.

00:04:44.696 --> 00:04:47.286 A:middle
And, that spans 7 different

00:04:47.286 --> 00:04:47.876 A:middle
classes.

00:04:50.276 --> 00:04:52.086 A:middle
All we need to do is press the

00:04:52.086 --> 00:04:53.816 A:middle
Train button, and our model will

00:04:53.816 --> 00:04:54.516 A:middle
begin training.

00:04:55.766 --> 00:04:57.696 A:middle
Now, the first thing Create ML

00:04:57.696 --> 00:04:58.596 A:middle
is going to be doing when

00:04:58.596 --> 00:05:00.586 A:middle
training this model is walking

00:04:58.596 --> 00:05:00.586 A:middle
training this model is walking

00:05:00.586 --> 00:05:02.036 A:middle
through each of the sound files

00:05:02.036 --> 00:05:03.916 A:middle
we provided, and extracting

00:05:03.916 --> 00:05:06.176 A:middle
audio features across the entire

00:05:06.176 --> 00:05:07.006 A:middle
file.

00:05:07.376 --> 00:05:08.776 A:middle
And, once it's collected all

00:05:08.776 --> 00:05:10.526 A:middle
these audio features, it will

00:05:10.526 --> 00:05:11.766 A:middle
begin the process you're seeing

00:05:11.766 --> 00:05:13.706 A:middle
now, which is where the model

00:05:13.706 --> 00:05:14.616 A:middle
weights are updating

00:05:14.616 --> 00:05:15.236 A:middle
iteratively.

00:05:17.266 --> 00:05:18.206 A:middle
As the model weights are

00:05:18.206 --> 00:05:19.896 A:middle
updating, you can see that the

00:05:19.896 --> 00:05:22.126 A:middle
performance is increasing, and

00:05:22.126 --> 00:05:24.276 A:middle
our accuracy is moving towards

00:05:24.276 --> 00:05:26.356 A:middle
100%, which is a good sign that

00:05:26.356 --> 00:05:27.416 A:middle
our model is converging.

00:05:28.906 --> 00:05:30.026 A:middle
Now, if you think about the

00:05:30.026 --> 00:05:31.356 A:middle
sounds we've collected today,

00:05:31.396 --> 00:05:33.456 A:middle
they're fairly distinct, like

00:05:33.456 --> 00:05:35.176 A:middle
cowbell and acoustic guitar

00:05:35.176 --> 00:05:36.086 A:middle
sound fairly different.

00:05:36.316 --> 00:05:37.616 A:middle
And so, this particular model

00:05:37.616 --> 00:05:38.976 A:middle
we've trained is able to do a

00:05:38.976 --> 00:05:40.356 A:middle
really good job with the sounds,

00:05:40.356 --> 00:05:42.296 A:middle
as you can see, on both the

00:05:42.296 --> 00:05:43.906 A:middle
training and the validation

00:05:43.906 --> 00:05:44.396 A:middle
sets.

00:05:44.936 --> 00:05:48.736 A:middle
The testing pane is a good place

00:05:48.796 --> 00:05:50.726 A:middle
to provide a large data set that

00:05:50.726 --> 00:05:52.676 A:middle
you might have for benchmarking.

00:05:53.866 --> 00:05:55.976 A:middle
The Create ML app allows you to

00:05:55.976 --> 00:05:57.556 A:middle
train multiple models at the

00:05:57.556 --> 00:05:58.286 A:middle
same time.

00:05:58.916 --> 00:06:00.256 A:middle
And, potentially provide

00:05:58.916 --> 00:06:00.256 A:middle
And, potentially provide

00:06:00.316 --> 00:06:01.636 A:middle
different sets of training data.

00:06:01.716 --> 00:06:03.626 A:middle
And so, the testing pane is a

00:06:03.626 --> 00:06:05.176 A:middle
great place if you want to

00:06:05.176 --> 00:06:06.776 A:middle
provide a common benchmark for

00:06:06.776 --> 00:06:07.706 A:middle
all those different model

00:06:07.706 --> 00:06:08.876 A:middle
configurations you're training.

00:06:10.636 --> 00:06:12.186 A:middle
Finally, as we make our way to

00:06:12.186 --> 00:06:14.346 A:middle
the Output tab, you'll see a UI

00:06:14.446 --> 00:06:16.196 A:middle
that shows how we can interact

00:06:16.196 --> 00:06:16.926 A:middle
with our model.

00:06:17.846 --> 00:06:19.556 A:middle
Now, I've collected one other

00:06:19.556 --> 00:06:21.376 A:middle
file that I didn't include in my

00:06:21.376 --> 00:06:22.066 A:middle
training set.

00:06:22.066 --> 00:06:24.606 A:middle
And, I've placed this in the

00:06:24.606 --> 00:06:25.866 A:middle
TestingData directory.

00:06:25.866 --> 00:06:28.376 A:middle
When I drag that directory into

00:06:28.376 --> 00:06:30.206 A:middle
the UI, you can see it

00:06:30.206 --> 00:06:31.706 A:middle
recognizes a file called

00:06:31.706 --> 00:06:33.606 A:middle
classification test.

00:06:34.876 --> 00:06:36.616 A:middle
As we scroll through this file,

00:06:37.276 --> 00:06:39.126 A:middle
it appears that Create ML has

00:06:39.126 --> 00:06:40.806 A:middle
classified the first second or

00:06:40.806 --> 00:06:42.376 A:middle
so of this file as background

00:06:42.376 --> 00:06:42.756 A:middle
noise.

00:06:43.876 --> 00:06:45.276 A:middle
Then, speech for the next couple

00:06:45.276 --> 00:06:48.246 A:middle
seconds, and finally shaker.

00:06:50.506 --> 00:06:52.706 A:middle
But, let's find out if we agree

00:06:52.706 --> 00:06:53.956 A:middle
with this classification.

00:06:54.256 --> 00:06:55.606 A:middle
And, we can listen to this file

00:06:55.606 --> 00:06:58.626 A:middle
right here in the UI.

00:06:58.816 --> 00:06:59.976 A:middle
Test, 1, 2, 3.

00:07:00.516 --> 00:07:04.516 A:middle
[ Shaker Playing ]

00:07:05.516 --> 00:07:10.366 A:middle
[ Applause ]

00:07:10.866 --> 00:07:12.306 A:middle
So, it seems at least on the

00:07:12.306 --> 00:07:14.436 A:middle
file that we've collected, that

00:07:14.436 --> 00:07:15.386 A:middle
this model seems to be

00:07:15.386 --> 00:07:16.516 A:middle
performing reasonably.

00:07:17.286 --> 00:07:19.146 A:middle
Now, what would be even better,

00:07:19.726 --> 00:07:20.756 A:middle
is if we could interact with

00:07:20.756 --> 00:07:21.676 A:middle
this model live.

00:07:22.606 --> 00:07:24.306 A:middle
And, to do that, we've added a

00:07:24.306 --> 00:07:26.806 A:middle
button here that has Record

00:07:26.806 --> 00:07:27.426 A:middle
Microphone.

00:07:28.316 --> 00:07:30.796 A:middle
And, once I begin recording, my

00:07:30.796 --> 00:07:32.176 A:middle
Mac will begin feeding the

00:07:32.176 --> 00:07:34.026 A:middle
built-in microphone data into

00:07:34.026 --> 00:07:34.966 A:middle
the model we've just trained.

00:07:35.516 --> 00:07:43.436 A:middle
[ Applause ]

00:07:43.936 --> 00:07:45.626 A:middle
So, what you can see is, anytime

00:07:45.626 --> 00:07:47.286 A:middle
I'm speaking, the model's

00:07:47.286 --> 00:07:48.796 A:middle
recognizing speech with high

00:07:48.796 --> 00:07:49.496 A:middle
confidence.

00:07:49.976 --> 00:07:51.696 A:middle
And, as I quiet down, you can

00:07:51.696 --> 00:07:53.526 A:middle
see the model settle back down

00:07:53.676 --> 00:07:56.636 A:middle
into a background state.

00:07:57.056 --> 00:07:58.206 A:middle
And, I brought a few instruments

00:07:58.206 --> 00:07:59.716 A:middle
with me so that we can play

00:07:59.716 --> 00:08:00.966 A:middle
along and see if the model can

00:07:59.716 --> 00:08:00.966 A:middle
along and see if the model can

00:08:00.966 --> 00:08:01.676 A:middle
recognize them.

00:08:02.556 --> 00:08:03.726 A:middle
Let's start with a shaker.

00:08:04.516 --> 00:08:06.516 A:middle
[ Playing Shaker ]

00:08:07.516 --> 00:08:11.706 A:middle
[ Applause ]

00:08:12.206 --> 00:08:13.826 A:middle
I've also brought my trusty

00:08:13.826 --> 00:08:13.976 A:middle
cowbell.

00:08:14.516 --> 00:08:19.546 A:middle
[ Playing Cowbell ]

00:08:20.046 --> 00:08:22.266 A:middle
&gt;&gt; More cowbell!

00:08:22.686 --> 00:08:23.836 A:middle
&gt;&gt; Well, the people have spoken.

00:08:23.836 --> 00:08:24.496 A:middle
More cowbell.

00:08:24.496 --> 00:08:24.966 A:middle
There you have it.

00:08:25.016 --> 00:08:27.016 A:middle
[ Playing Cowbell ]

00:08:27.516 --> 00:08:30.096 A:middle
[ Applause ]

00:08:30.596 --> 00:08:32.155 A:middle
And, I also brought my acoustic

00:08:32.155 --> 00:08:34.366 A:middle
guitar with me here today, so we

00:08:34.366 --> 00:08:35.645 A:middle
can try some of this as well.

00:08:38.666 --> 00:08:39.515 A:middle
I can start with some

00:08:39.515 --> 00:08:40.496 A:middle
single-note lines.

00:08:41.515 --> 00:08:46.636 A:middle
[ Playing Guitar ]

00:08:47.136 --> 00:08:48.436 A:middle
And then, we can try some chords

00:08:48.436 --> 00:08:48.976 A:middle
as well.

00:08:49.516 --> 00:08:55.500 A:middle
[ Playing Guitar ]

00:08:57.516 --> 00:09:05.216 A:middle
[ Applause ]

00:08:57.516 --> 00:09:05.216 A:middle
[ Applause ]

00:09:05.716 --> 00:09:07.206 A:middle
So, that seemed to be working

00:09:07.206 --> 00:09:08.246 A:middle
pretty well, and I think that's

00:09:08.246 --> 00:09:09.216 A:middle
something we can work with.

00:09:10.196 --> 00:09:11.556 A:middle
So, I can stop the recording

00:09:11.556 --> 00:09:11.856 A:middle
now.

00:09:12.526 --> 00:09:14.386 A:middle
And, in the Create ML app, I'm

00:09:14.386 --> 00:09:15.956 A:middle
able to scroll back through this

00:09:15.956 --> 00:09:17.596 A:middle
recording, and take a look at

00:09:17.596 --> 00:09:18.726 A:middle
any of the segments that we've

00:09:18.726 --> 00:09:19.926 A:middle
been analyzing so far.

00:09:21.096 --> 00:09:22.386 A:middle
This might be a great place to

00:09:22.386 --> 00:09:23.456 A:middle
check if there are any

00:09:23.456 --> 00:09:24.966 A:middle
anomalies, or things that it

00:09:24.966 --> 00:09:26.836 A:middle
didn't get correct, and maybe we

00:09:26.836 --> 00:09:28.356 A:middle
can clip some parts of this file

00:09:28.356 --> 00:09:29.936 A:middle
to use, as part of our training

00:09:29.936 --> 00:09:31.376 A:middle
set to improve the performance

00:09:31.376 --> 00:09:32.306 A:middle
of our model.

00:09:33.376 --> 00:09:35.136 A:middle
And, finally, when we're happy

00:09:35.136 --> 00:09:36.356 A:middle
that our model is performing the

00:09:36.356 --> 00:09:38.336 A:middle
way we'd like, we can simply

00:09:38.336 --> 00:09:40.126 A:middle
drag this model to our desktop,

00:09:40.416 --> 00:09:41.506 A:middle
where we can integrate it into

00:09:41.506 --> 00:09:42.316 A:middle
our application.

00:09:43.556 --> 00:09:44.796 A:middle
And, that's training a sound

00:09:44.826 --> 00:09:47.316 A:middle
classifier in the Create ML app

00:09:47.316 --> 00:09:48.836 A:middle
in under a minute, with zero

00:09:48.836 --> 00:09:49.416 A:middle
lines of code.

00:09:50.516 --> 00:09:56.500 A:middle
[ Applause ]

00:09:59.106 --> 00:10:00.736 A:middle
So, you saw during the demo,

00:09:59.106 --> 00:10:00.736 A:middle
So, you saw during the demo,

00:10:01.726 --> 00:10:02.936 A:middle
there's some things to consider

00:10:02.996 --> 00:10:04.046 A:middle
when collecting your training

00:10:04.046 --> 00:10:04.386 A:middle
data.

00:10:05.766 --> 00:10:06.486 A:middle
And, the first thing you'll

00:10:06.486 --> 00:10:08.466 A:middle
notice is how I collected this

00:10:08.526 --> 00:10:09.706 A:middle
data in directories.

00:10:11.696 --> 00:10:12.946 A:middle
All the sounds that come from a

00:10:12.946 --> 00:10:15.206 A:middle
guitar are placed in the Guitar

00:10:15.206 --> 00:10:15.746 A:middle
directory.

00:10:16.476 --> 00:10:19.296 A:middle
And, likewise with a file like

00:10:19.296 --> 00:10:20.386 A:middle
Drums or Background.

00:10:21.226 --> 00:10:22.556 A:middle
Now, let's talk about the

00:10:22.556 --> 00:10:23.836 A:middle
background class for a minute.

00:10:25.916 --> 00:10:26.786 A:middle
Even though we're training a

00:10:26.786 --> 00:10:28.406 A:middle
musical instrument classifier,

00:10:28.816 --> 00:10:30.136 A:middle
you still need to consider what

00:10:30.136 --> 00:10:31.976 A:middle
might happen if there's not any

00:10:31.976 --> 00:10:33.056 A:middle
musical instruments being

00:10:33.056 --> 00:10:35.276 A:middle
played, and if you only trained

00:10:35.276 --> 00:10:36.416 A:middle
your model on musical

00:10:36.416 --> 00:10:38.406 A:middle
instruments, but then fed it

00:10:38.406 --> 00:10:39.186 A:middle
background data.

00:10:39.716 --> 00:10:40.876 A:middle
That's data it's never seen

00:10:40.876 --> 00:10:41.306 A:middle
before.

00:10:41.476 --> 00:10:43.276 A:middle
And so, make sure that when

00:10:43.276 --> 00:10:44.066 A:middle
you're training a sound

00:10:44.066 --> 00:10:45.716 A:middle
classifier, if you expect your

00:10:45.716 --> 00:10:47.816 A:middle
model to work in situations

00:10:48.106 --> 00:10:49.436 A:middle
where there's background noise,

00:10:50.046 --> 00:10:51.856 A:middle
to provide that as part of the

00:10:51.856 --> 00:10:52.916 A:middle
class as well.

00:10:55.866 --> 00:10:57.506 A:middle
Now, suppose you had a file that

00:10:57.506 --> 00:10:58.826 A:middle
was called sounds.

00:10:59.686 --> 00:11:01.176 A:middle
And, the file started at the

00:10:59.686 --> 00:11:01.176 A:middle
And, the file started at the

00:11:01.176 --> 00:11:03.686 A:middle
beginning with drums, and then

00:11:03.686 --> 00:11:04.866 A:middle
transitioned to background

00:11:04.866 --> 00:11:07.286 A:middle
noise, and then finally ended

00:11:07.666 --> 00:11:08.626 A:middle
with guitar.

00:11:09.906 --> 00:11:13.236 A:middle
This file, as is, is not going

00:11:13.236 --> 00:11:15.606 A:middle
to be useful for dragging

00:11:15.606 --> 00:11:16.886 A:middle
directly in the Create ML app.

00:11:17.006 --> 00:11:18.896 A:middle
And, that's because this sound

00:11:18.896 --> 00:11:20.676 A:middle
contains multiple sound classes

00:11:20.716 --> 00:11:21.456 A:middle
in one file.

00:11:23.016 --> 00:11:25.246 A:middle
Remember, you have to use

00:11:25.246 --> 00:11:26.756 A:middle
labeled directories to train

00:11:26.756 --> 00:11:28.726 A:middle
your model, and so the best

00:11:28.726 --> 00:11:29.946 A:middle
thing to do in this situation

00:11:29.946 --> 00:11:31.666 A:middle
would be to split this file into

00:11:31.666 --> 00:11:33.776 A:middle
three, and name them drums,

00:11:33.776 --> 00:11:35.116 A:middle
guitar, and background.

00:11:35.736 --> 00:11:38.126 A:middle
This is going to have a lot

00:11:38.126 --> 00:11:39.406 A:middle
better performance when training

00:11:39.406 --> 00:11:40.936 A:middle
your model, if you split your

00:11:40.936 --> 00:11:41.686 A:middle
files this way.

00:11:44.196 --> 00:11:46.856 A:middle
A few other considerations when

00:11:46.856 --> 00:11:47.856 A:middle
collecting audio data.

00:11:49.046 --> 00:11:51.656 A:middle
First, we want to insure that

00:11:51.656 --> 00:11:52.806 A:middle
the data you're collecting

00:11:53.066 --> 00:11:54.536 A:middle
matches a real-world audio

00:11:54.536 --> 00:11:55.216 A:middle
environment.

00:11:55.786 --> 00:11:59.346 A:middle
So, remember that if your app is

00:11:59.346 --> 00:12:01.006 A:middle
intended to work in a variety of

00:11:59.346 --> 00:12:01.006 A:middle
intended to work in a variety of

00:12:01.006 --> 00:12:03.136 A:middle
rooms or acoustic scenarios, you

00:12:03.136 --> 00:12:04.646 A:middle
can either collect data in those

00:12:04.646 --> 00:12:06.986 A:middle
acoustic scenarios, or consider

00:12:06.986 --> 00:12:08.486 A:middle
even simulating those rooms,

00:12:08.666 --> 00:12:09.656 A:middle
using a technique called

00:12:09.746 --> 00:12:10.356 A:middle
convolution.

00:12:12.856 --> 00:12:14.776 A:middle
Another important thing to

00:12:14.776 --> 00:12:16.416 A:middle
consider is the on-device

00:12:16.416 --> 00:12:17.466 A:middle
microphone processing.

00:12:17.996 --> 00:12:20.446 A:middle
You might check out AV audio

00:12:20.626 --> 00:12:22.926 A:middle
session modes to select

00:12:23.156 --> 00:12:24.726 A:middle
different modes of microphone

00:12:24.726 --> 00:12:26.066 A:middle
processing in your application,

00:12:26.516 --> 00:12:27.906 A:middle
and select the one which is best

00:12:27.906 --> 00:12:29.516 A:middle
suited for your app, or

00:12:29.516 --> 00:12:31.066 A:middle
potentially matches the training

00:12:31.066 --> 00:12:33.966 A:middle
data you've collected.

00:12:34.426 --> 00:12:36.406 A:middle
And a final point, is to be

00:12:36.406 --> 00:12:38.516 A:middle
aware of the model architecture.

00:12:39.226 --> 00:12:40.616 A:middle
So, this is the sound classifier

00:12:40.616 --> 00:12:42.876 A:middle
model, and it can do pretty well

00:12:42.876 --> 00:12:44.396 A:middle
at classifying varieties of

00:12:44.396 --> 00:12:45.016 A:middle
sounds.

00:12:45.576 --> 00:12:46.666 A:middle
But, this is not something that

00:12:46.666 --> 00:12:48.136 A:middle
would be suitable for, say,

00:12:48.396 --> 00:12:49.576 A:middle
training a full-on speech

00:12:49.576 --> 00:12:50.226 A:middle
recognizer.

00:12:50.676 --> 00:12:52.096 A:middle
There are better tools for that

00:12:52.096 --> 00:12:52.436 A:middle
job.

00:12:52.526 --> 00:12:53.496 A:middle
So, make sure you're always

00:12:53.496 --> 00:12:54.476 A:middle
using the right tool for the

00:12:54.476 --> 00:12:54.756 A:middle
job.

00:12:57.246 --> 00:13:00.166 A:middle
So, now you have this ML model,

00:12:57.246 --> 00:13:00.166 A:middle
So, now you have this ML model,

00:13:00.166 --> 00:13:01.796 A:middle
and let's talk about how you can

00:13:01.796 --> 00:13:02.846 A:middle
integrate it into your

00:13:02.846 --> 00:13:03.476 A:middle
application.

00:13:05.336 --> 00:13:07.136 A:middle
And, to make it as easy as

00:13:07.136 --> 00:13:08.836 A:middle
possible to run sound

00:13:08.876 --> 00:13:10.056 A:middle
classification models in your

00:13:10.056 --> 00:13:11.926 A:middle
app, we're also releasing a new

00:13:11.926 --> 00:13:14.456 A:middle
framework called SoundAnalysis.

00:13:15.366 --> 00:13:17.996 A:middle
SoundAnalysis is a new

00:13:17.996 --> 00:13:19.536 A:middle
high-level framework for

00:13:19.536 --> 00:13:20.526 A:middle
analyzing sound.

00:13:21.596 --> 00:13:25.576 A:middle
It uses Core ML models, and it

00:13:25.576 --> 00:13:27.376 A:middle
handles common audio operations

00:13:27.376 --> 00:13:29.296 A:middle
internally, such as channel

00:13:29.296 --> 00:13:31.306 A:middle
mapping, sample rate conversion,

00:13:31.716 --> 00:13:32.526 A:middle
or reblocking.

00:13:35.136 --> 00:13:36.306 A:middle
And, let's take a look under the

00:13:36.306 --> 00:13:38.436 A:middle
hood to see how SoundAnalysis

00:13:38.436 --> 00:13:38.946 A:middle
works.

00:13:39.276 --> 00:13:41.716 A:middle
Now, the top section represents

00:13:41.716 --> 00:13:42.566 A:middle
your application.

00:13:43.086 --> 00:13:44.436 A:middle
And, the bottom represents

00:13:44.506 --> 00:13:45.696 A:middle
what's happening under the hood

00:13:45.776 --> 00:13:46.946 A:middle
in SoundAnalysis.

00:13:47.186 --> 00:13:50.346 A:middle
The first thing you'll do is

00:13:50.346 --> 00:13:51.616 A:middle
provide the model you just

00:13:51.616 --> 00:13:53.596 A:middle
trained using Create ML to

00:13:53.596 --> 00:13:54.846 A:middle
SoundAnalysis framework.

00:13:56.656 --> 00:13:59.826 A:middle
Then, your application will

00:13:59.826 --> 00:14:01.756 A:middle
provide some audio that needs to

00:13:59.826 --> 00:14:01.756 A:middle
provide some audio that needs to

00:14:01.756 --> 00:14:02.456 A:middle
be analyzed.

00:14:04.176 --> 00:14:06.596 A:middle
This audio will first hit a

00:14:06.596 --> 00:14:08.356 A:middle
channel-mapping step, which

00:14:08.356 --> 00:14:09.996 A:middle
ensures that if your model

00:14:09.996 --> 00:14:12.236 A:middle
expects one channel of audio,

00:14:12.346 --> 00:14:14.506 A:middle
like ours did here, that that's

00:14:14.506 --> 00:14:15.636 A:middle
what's delivered to the model,

00:14:16.086 --> 00:14:17.736 A:middle
even as a client, if you're

00:14:17.736 --> 00:14:19.576 A:middle
delivering stereo data, for

00:14:19.576 --> 00:14:19.976 A:middle
example.

00:14:22.476 --> 00:14:23.986 A:middle
The next step that happens is

00:14:23.986 --> 00:14:25.296 A:middle
called sample rate conversion.

00:14:26.066 --> 00:14:28.066 A:middle
The model we trained natively

00:14:28.066 --> 00:14:30.456 A:middle
operates on 16 kilohertz audio

00:14:30.456 --> 00:14:30.846 A:middle
data.

00:14:31.216 --> 00:14:33.196 A:middle
And, this ensures that the audio

00:14:33.196 --> 00:14:34.766 A:middle
that you provide gets converted

00:14:35.036 --> 00:14:36.386 A:middle
to match the rate the model

00:14:36.386 --> 00:14:36.976 A:middle
expects.

00:14:41.326 --> 00:14:42.366 A:middle
The final step that

00:14:42.366 --> 00:14:43.986 A:middle
SoundAnalysis performs is an

00:14:43.986 --> 00:14:45.316 A:middle
audio buffering operation.

00:14:46.796 --> 00:14:47.966 A:middle
Most of the models we're working

00:14:47.966 --> 00:14:50.276 A:middle
with today require a fixed

00:14:50.276 --> 00:14:52.236 A:middle
amount of audio data to process

00:14:52.236 --> 00:14:53.106 A:middle
an analysis chunk.

00:14:54.006 --> 00:14:56.916 A:middle
And, oftentimes, the audio that

00:14:56.916 --> 00:14:58.596 A:middle
you have as a client might be

00:14:58.596 --> 00:15:00.516 A:middle
coming in at arbitrary sized

00:14:58.596 --> 00:15:00.516 A:middle
coming in at arbitrary sized

00:15:00.516 --> 00:15:01.006 A:middle
buffers.

00:15:01.136 --> 00:15:02.496 A:middle
And, it's a lot of work to

00:15:02.496 --> 00:15:04.096 A:middle
implement an efficient ring

00:15:04.096 --> 00:15:05.666 A:middle
buffer that makes sure to

00:15:05.666 --> 00:15:07.506 A:middle
deliver the correct size chunks

00:15:07.506 --> 00:15:08.796 A:middle
of audio to your model.

00:15:09.386 --> 00:15:11.966 A:middle
And so, this step ensures that

00:15:11.966 --> 00:15:13.456 A:middle
if the model expects around 1

00:15:13.456 --> 00:15:15.046 A:middle
second of audio data, that that

00:15:15.046 --> 00:15:16.876 A:middle
will always be what's delivered

00:15:16.876 --> 00:15:17.406 A:middle
to the model.

00:15:18.396 --> 00:15:20.726 A:middle
And then, finally, after the

00:15:20.726 --> 00:15:22.306 A:middle
data's delivered to the model,

00:15:22.586 --> 00:15:23.466 A:middle
your app will receive a

00:15:23.466 --> 00:15:25.786 A:middle
callback, containing the top

00:15:25.786 --> 00:15:27.536 A:middle
classification results for that

00:15:27.536 --> 00:15:28.176 A:middle
piece of audio.

00:15:28.996 --> 00:15:31.286 A:middle
Now, the good thing is, you

00:15:31.286 --> 00:15:32.816 A:middle
don't really have to know any of

00:15:32.816 --> 00:15:33.276 A:middle
this.

00:15:33.866 --> 00:15:35.486 A:middle
Just remember to take your

00:15:35.486 --> 00:15:36.996 A:middle
audio, provide it to

00:15:36.996 --> 00:15:38.626 A:middle
SoundAnalysis framework, and

00:15:38.626 --> 00:15:40.166 A:middle
then handle the results in your

00:15:40.166 --> 00:15:40.776 A:middle
application.

00:15:43.696 --> 00:15:45.356 A:middle
So, let's talk a little bit more

00:15:45.356 --> 00:15:46.876 A:middle
about the results you'll expect

00:15:46.876 --> 00:15:48.276 A:middle
to get from SoundAnalysis.

00:15:49.616 --> 00:15:51.496 A:middle
Audio is a stream, and it

00:15:51.496 --> 00:15:53.216 A:middle
doesn't always have a beginning

00:15:53.216 --> 00:15:54.516 A:middle
and end, like images do.

00:15:54.966 --> 00:15:56.986 A:middle
And, for this reason, the

00:15:56.986 --> 00:15:58.206 A:middle
results that we're working with

00:15:58.416 --> 00:15:59.506 A:middle
might look a little different.

00:16:00.196 --> 00:16:02.416 A:middle
Your results contain a time

00:16:02.416 --> 00:16:04.566 A:middle
range, and this corresponds to

00:16:04.606 --> 00:16:05.906 A:middle
the block of audio that was

00:16:05.906 --> 00:16:07.336 A:middle
analyzed for that result.

00:16:08.396 --> 00:16:10.756 A:middle
In this example, the block size

00:16:10.756 --> 00:16:11.716 A:middle
is specific to model

00:16:11.716 --> 00:16:13.506 A:middle
architecture, and is around 1

00:16:13.506 --> 00:16:14.786 A:middle
second, as you can see.

00:16:17.046 --> 00:16:18.496 A:middle
As you continue providing audio

00:16:18.496 --> 00:16:20.016 A:middle
data to the model, you'll

00:16:20.016 --> 00:16:21.566 A:middle
continue to receive results

00:16:21.826 --> 00:16:22.776 A:middle
containing the top

00:16:22.776 --> 00:16:24.516 A:middle
classifications for that block

00:16:24.516 --> 00:16:25.756 A:middle
of audio that you analyzed.

00:16:27.326 --> 00:16:29.586 A:middle
Now, you might notice that this

00:16:29.586 --> 00:16:31.126 A:middle
second result has overlapped the

00:16:31.126 --> 00:16:33.126 A:middle
previous results by about 50%.

00:16:33.726 --> 00:16:35.166 A:middle
And, this is actually by design.

00:16:36.636 --> 00:16:38.066 A:middle
You want to make sure that every

00:16:38.266 --> 00:16:39.196 A:middle
piece of audio that you're

00:16:39.196 --> 00:16:40.956 A:middle
providing has the opportunity to

00:16:40.956 --> 00:16:42.536 A:middle
fall near the middle of an

00:16:42.536 --> 00:16:43.476 A:middle
analysis window.

00:16:43.916 --> 00:16:45.726 A:middle
Otherwise, it might fall between

00:16:45.726 --> 00:16:48.066 A:middle
two analysis windows, and the

00:16:48.066 --> 00:16:49.576 A:middle
model performance might not be

00:16:49.576 --> 00:16:50.096 A:middle
as good.

00:16:51.096 --> 00:16:53.126 A:middle
And so, the default is 50%

00:16:53.126 --> 00:16:54.696 A:middle
overlap on the analysis,

00:16:55.096 --> 00:16:56.476 A:middle
although it's configurable in

00:16:56.506 --> 00:16:57.906 A:middle
the API, if you have a use case

00:16:57.906 --> 00:16:59.146 A:middle
that requires otherwise.

00:17:00.336 --> 00:17:02.216 A:middle
And, as you continue providing

00:17:02.216 --> 00:17:04.715 A:middle
audio data, you'll continue to

00:17:04.715 --> 00:17:05.726 A:middle
receive results.

00:17:05.886 --> 00:17:08.566 A:middle
And, you can continue pushing

00:17:08.566 --> 00:17:09.996 A:middle
this data, and getting results

00:17:09.996 --> 00:17:10.656 A:middle
for as long as the audio stream

00:17:10.656 --> 00:17:10.976 A:middle
is active.

00:17:15.876 --> 00:17:18.175 A:middle
Now, let's take a quick look at

00:17:18.175 --> 00:17:19.766 A:middle
the API provided by

00:17:19.766 --> 00:17:21.016 A:middle
SoundAnalysis Framework.

00:17:23.356 --> 00:17:24.846 A:middle
Let's say we have an audio file,

00:17:24.846 --> 00:17:27.106 A:middle
and we want to analyze it using

00:17:27.106 --> 00:17:28.215 A:middle
the classifier we've just

00:17:28.215 --> 00:17:29.016 A:middle
trained here today.

00:17:30.526 --> 00:17:32.516 A:middle
To start, we'll create an audio

00:17:32.516 --> 00:17:34.736 A:middle
file analyzer, and provide the

00:17:34.736 --> 00:17:36.126 A:middle
URL to the file we'd like to

00:17:36.126 --> 00:17:36.806 A:middle
analyze.

00:17:38.476 --> 00:17:40.506 A:middle
Then, we'll create a

00:17:40.506 --> 00:17:42.716 A:middle
classifySoundRequest, and then

00:17:42.716 --> 00:17:44.756 A:middle
instantiate MySoundClassifier,

00:17:45.136 --> 00:17:46.196 A:middle
which is the model we trained

00:17:46.196 --> 00:17:46.396 A:middle
here.

00:17:49.406 --> 00:17:51.536 A:middle
Then, we'll add this request to

00:17:51.536 --> 00:17:53.796 A:middle
our analyzer, and provide an

00:17:53.796 --> 00:17:55.376 A:middle
observer, which will handle the

00:17:55.376 --> 00:17:56.876 A:middle
results that the model will

00:17:56.876 --> 00:17:57.496 A:middle
produce.

00:17:59.156 --> 00:18:02.366 A:middle
Finally, we'll analyze the file,

00:17:59.156 --> 00:18:02.366 A:middle
Finally, we'll analyze the file,

00:18:02.816 --> 00:18:03.966 A:middle
which will start scanning

00:18:03.966 --> 00:18:05.336 A:middle
through the file and producing

00:18:05.336 --> 00:18:06.016 A:middle
the results.

00:18:09.146 --> 00:18:10.476 A:middle
Now, on your application side,

00:18:11.266 --> 00:18:12.486 A:middle
you'll need to make sure that

00:18:12.486 --> 00:18:14.296 A:middle
one of your classes implements

00:18:14.576 --> 00:18:16.706 A:middle
the SNResultsObserving protocol.

00:18:17.806 --> 00:18:18.866 A:middle
This is how you'll receive

00:18:18.866 --> 00:18:20.106 A:middle
results from the framework.

00:18:21.796 --> 00:18:23.076 A:middle
The first method you might

00:18:23.076 --> 00:18:25.466 A:middle
implement is request didProduce

00:18:25.466 --> 00:18:25.866 A:middle
result.

00:18:26.526 --> 00:18:28.876 A:middle
This method will be called many

00:18:28.876 --> 00:18:29.986 A:middle
times, potentially.

00:18:30.656 --> 00:18:32.976 A:middle
Once for each new observation

00:18:32.976 --> 00:18:33.786 A:middle
that's available.

00:18:34.796 --> 00:18:37.516 A:middle
You might consider grabbing the

00:18:37.666 --> 00:18:39.536 A:middle
top classification result, and

00:18:39.536 --> 00:18:40.886 A:middle
the time range associated with

00:18:40.886 --> 00:18:41.026 A:middle
it.

00:18:41.346 --> 00:18:42.436 A:middle
And, this is where the logic

00:18:42.466 --> 00:18:44.356 A:middle
would go in your application, to

00:18:44.356 --> 00:18:46.766 A:middle
handle the sound classification

00:18:46.766 --> 00:18:46.976 A:middle
event.

00:18:49.596 --> 00:18:50.726 A:middle
Another method you'll be

00:18:50.726 --> 00:18:52.216 A:middle
interested in is request

00:18:52.386 --> 00:18:53.546 A:middle
didFailWithError.

00:18:54.216 --> 00:18:55.586 A:middle
If analysis fails for any

00:18:55.586 --> 00:18:57.106 A:middle
reason, this method will be

00:18:57.106 --> 00:18:57.526 A:middle
called.

00:18:57.526 --> 00:18:59.176 A:middle
And then, you shouldn't expect

00:18:59.176 --> 00:19:00.766 A:middle
to receive any more results from

00:18:59.176 --> 00:19:00.766 A:middle
to receive any more results from

00:19:00.766 --> 00:19:01.496 A:middle
this analyzer.

00:19:02.796 --> 00:19:04.926 A:middle
Or, if the stream completes

00:19:04.926 --> 00:19:06.656 A:middle
successfully, at the end of the

00:19:06.656 --> 00:19:08.486 A:middle
file, for example, you'll

00:19:08.486 --> 00:19:11.246 A:middle
receive the request didComplete.

00:19:13.506 --> 00:19:15.546 A:middle
So, let's recap what you've seen

00:19:15.546 --> 00:19:15.926 A:middle
today.

00:19:17.656 --> 00:19:18.926 A:middle
You saw how you can train a

00:19:18.926 --> 00:19:20.636 A:middle
sound classifier in Create ML

00:19:21.326 --> 00:19:24.596 A:middle
using your own audio data.

00:19:25.656 --> 00:19:27.636 A:middle
And, take that model and run it

00:19:27.636 --> 00:19:29.866 A:middle
on-device using SoundAnalysis

00:19:29.866 --> 00:19:30.216 A:middle
framework.

00:19:32.186 --> 00:19:34.886 A:middle
For more information, check out

00:19:34.886 --> 00:19:36.486 A:middle
our sound classification article

00:19:36.846 --> 00:19:38.496 A:middle
on developer.apple.com.

00:19:39.106 --> 00:19:40.346 A:middle
And, there you'll find an

00:19:40.346 --> 00:19:42.246 A:middle
example of how to perform sound

00:19:42.246 --> 00:19:43.956 A:middle
classification on your device's

00:19:44.006 --> 00:19:45.866 A:middle
built-in microphone, using AV

00:19:45.866 --> 00:19:47.816 A:middle
Audio Engine, just like the

00:19:47.816 --> 00:19:49.466 A:middle
musical instrument demo you saw

00:19:49.466 --> 00:19:49.886 A:middle
earlier.

00:19:53.046 --> 00:19:54.506 A:middle
Thank you all for listening, and

00:19:54.506 --> 00:19:56.186 A:middle
I can't wait to see how you use

00:19:56.186 --> 00:19:57.556 A:middle
sound classification in your

00:19:57.556 --> 00:19:57.976 A:middle
applications.

00:19:58.516 --> 00:20:04.500 A:middle
[ Applause ]
