WEBVTT

00:00:01.516 --> 00:00:04.500 A:middle
[ Music ]

00:00:08.516 --> 00:00:17.546 A:middle
[ Applause ]

00:00:18.046 --> 00:00:19.896 A:middle
&gt;&gt; Hello. Welcome everyone.

00:00:20.656 --> 00:00:22.786 A:middle
My name is Gaurav and today

00:00:22.786 --> 00:00:24.306 A:middle
we're going to talk about What's

00:00:24.396 --> 00:00:27.456 A:middle
New in Machine Learning.

00:00:29.696 --> 00:00:31.316 A:middle
Machine Learning if used by

00:00:31.406 --> 00:00:32.326 A:middle
thousands of apps.

00:00:33.356 --> 00:00:35.556 A:middle
The apps that you are making are

00:00:35.556 --> 00:00:36.436 A:middle
amazing.

00:00:37.006 --> 00:00:38.896 A:middle
They're touching every aspect of

00:00:38.946 --> 00:00:39.846 A:middle
a user's life.

00:00:40.456 --> 00:00:45.086 A:middle
In hospitals, doctors are using

00:00:45.086 --> 00:00:47.326 A:middle
apps such as Butterfly iQ to do

00:00:47.326 --> 00:00:50.356 A:middle
medical diagnostics in real

00:00:52.096 --> 00:00:52.206 A:middle
time.

00:00:52.416 --> 00:00:54.866 A:middle
In sports, coaches are using

00:00:54.866 --> 00:00:57.186 A:middle
apps such as HomeCourt to train

00:00:57.186 --> 00:00:57.646 A:middle
their players.

00:00:58.576 --> 00:01:02.896 A:middle
In creativity, apps such as

00:00:58.576 --> 00:01:02.896 A:middle
In creativity, apps such as

00:01:02.896 --> 00:01:04.936 A:middle
Pixelmator Pro are helping other

00:01:04.936 --> 00:01:06.366 A:middle
users to augment their

00:01:06.366 --> 00:01:07.686 A:middle
creativity using ML.

00:01:08.746 --> 00:01:10.366 A:middle
These are just a few examples,

00:01:10.576 --> 00:01:12.446 A:middle
and we would like all of these

00:01:12.446 --> 00:01:14.746 A:middle
apps you guys should also make

00:01:14.746 --> 00:01:17.396 A:middle
these kind of apps.

00:01:17.656 --> 00:01:19.446 A:middle
So now the question is what's

00:01:19.486 --> 00:01:19.686 A:middle
new?

00:01:20.556 --> 00:01:24.736 A:middle
Let me begin by saying a lot.

00:01:25.576 --> 00:01:28.896 A:middle
We have so much material that we

00:01:28.896 --> 00:01:30.746 A:middle
can hardly cover it one session

00:01:30.746 --> 00:01:31.556 A:middle
or two sessions.

00:01:32.126 --> 00:01:35.746 A:middle
We need ten sessions to cover

00:01:35.746 --> 00:01:36.856 A:middle
the entire material.

00:01:38.316 --> 00:01:39.606 A:middle
We also have a session

00:01:39.776 --> 00:01:41.616 A:middle
intersecting the design in

00:01:41.616 --> 00:01:43.216 A:middle
machine learning.

00:01:44.556 --> 00:01:47.216 A:middle
We also have Daily Labs where

00:01:47.216 --> 00:01:49.496 A:middle
you can meet and discuss your

00:01:49.496 --> 00:01:51.006 A:middle
ideas with Apple Machine

00:01:51.006 --> 00:01:51.746 A:middle
Learning engineers.

00:01:53.116 --> 00:01:55.156 A:middle
All of us are here to remove any

00:01:55.156 --> 00:01:56.956 A:middle
roadblocks that you might be

00:01:56.956 --> 00:01:58.966 A:middle
facing while integrating Machine

00:01:59.026 --> 00:02:00.326 A:middle
Learning in your app.

00:01:59.026 --> 00:02:00.326 A:middle
Learning in your app.

00:02:02.876 --> 00:02:04.776 A:middle
In all of these sessions, you

00:02:04.776 --> 00:02:06.366 A:middle
will see that we follow simple

00:02:06.366 --> 00:02:07.016 A:middle
principles.

00:02:08.356 --> 00:02:11.246 A:middle
We want Machine Learning to be

00:02:11.246 --> 00:02:12.976 A:middle
as easy to use as possible.

00:02:12.976 --> 00:02:14.336 A:middle
We are laser focused on doing

00:02:15.006 --> 00:02:15.106 A:middle
that.

00:02:15.736 --> 00:02:17.766 A:middle
We want to make it flexible so

00:02:17.766 --> 00:02:20.466 A:middle
you can do it by variety of

00:02:21.436 --> 00:02:22.926 A:middle
tasks, and we want to make it

00:02:22.976 --> 00:02:25.506 A:middle
powerful so you can run state of

00:02:25.506 --> 00:02:27.246 A:middle
the art Machine Learning models

00:02:27.336 --> 00:02:28.136 A:middle
on your devices.

00:02:28.876 --> 00:02:32.976 A:middle
It is truly Machine Learning for

00:02:32.976 --> 00:02:35.036 A:middle
everyone whether you are a

00:02:35.036 --> 00:02:36.606 A:middle
researcher or someone new to

00:02:36.606 --> 00:02:37.286 A:middle
Machine Learning.

00:02:38.776 --> 00:02:40.046 A:middle
With create in a lab you can

00:02:40.046 --> 00:02:41.236 A:middle
make state of the art,

00:02:41.236 --> 00:02:42.816 A:middle
task-focused ML models.

00:02:44.526 --> 00:02:45.846 A:middle
The next big pillar of our

00:02:45.846 --> 00:02:47.286 A:middle
offering is Domain APIs.

00:02:49.256 --> 00:02:50.746 A:middle
Domain APIs allow you to

00:02:50.746 --> 00:02:52.106 A:middle
leverage Apple's built-in

00:02:52.106 --> 00:02:53.346 A:middle
intelligence and models.

00:02:54.266 --> 00:02:55.506 A:middle
So you don't have to worry about

00:02:55.506 --> 00:02:56.916 A:middle
collecting the data and building

00:02:56.916 --> 00:02:57.426 A:middle
the model.

00:02:57.646 --> 00:03:00.296 A:middle
Simple call the API and be done.

00:02:57.646 --> 00:03:00.296 A:middle
Simple call the API and be done.

00:03:02.696 --> 00:03:04.326 A:middle
This year we are significantly

00:03:04.356 --> 00:03:05.806 A:middle
expanding our Domain APIs.

00:03:06.256 --> 00:03:08.676 A:middle
We have Domain APIs in Vision,

00:03:09.106 --> 00:03:13.196 A:middle
Text, Speech and Sound.

00:03:13.406 --> 00:03:15.226 A:middle
Now let's take a sneak peek of

00:03:15.276 --> 00:03:16.386 A:middle
some of these APIs.

00:03:17.146 --> 00:03:19.746 A:middle
Let's start with Vision.

00:03:20.976 --> 00:03:22.906 A:middle
Vision allows you to reason

00:03:23.016 --> 00:03:24.716 A:middle
about the content of the image.

00:03:26.206 --> 00:03:27.386 A:middle
One of the new features this

00:03:27.386 --> 00:03:29.036 A:middle
year is Image Saliency.

00:03:30.096 --> 00:03:31.576 A:middle
Image Saliency can help you

00:03:31.576 --> 00:03:33.536 A:middle
identify the most relevant

00:03:33.536 --> 00:03:35.256 A:middle
region in your image.

00:03:36.106 --> 00:03:39.466 A:middle
In this case, the region around

00:03:39.636 --> 00:03:40.146 A:middle
the person.

00:03:41.936 --> 00:03:43.056 A:middle
You can use it to generate

00:03:43.056 --> 00:03:44.996 A:middle
thumbnails or to make image

00:03:44.996 --> 00:03:49.196 A:middle
cropping, generate memories,

00:03:49.196 --> 00:03:49.976 A:middle
guide camera et cetera.

00:03:50.046 --> 00:03:53.786 A:middle
Another big feature we are

00:03:53.786 --> 00:03:55.406 A:middle
introducing this year is Text

00:03:55.466 --> 00:03:56.126 A:middle
Recognition.

00:03:57.666 --> 00:03:59.516 A:middle
You can now take a picture of

00:03:59.516 --> 00:04:01.476 A:middle
the document, perform

00:03:59.516 --> 00:04:01.476 A:middle
the document, perform

00:04:01.476 --> 00:04:03.556 A:middle
perspective correction, lighting

00:04:03.556 --> 00:04:05.566 A:middle
correction and reorganize the

00:04:05.616 --> 00:04:06.976 A:middle
text on the device.

00:04:07.516 --> 00:04:13.986 A:middle
[ Applause ]

00:04:14.486 --> 00:04:18.106 A:middle
This is huge, but that's not

00:04:18.106 --> 00:04:18.666 A:middle
all.

00:04:18.666 --> 00:04:19.796 A:middle
We have Image inbuilt

00:04:19.796 --> 00:04:22.016 A:middle
classifier, human detector, pet

00:04:22.016 --> 00:04:24.036 A:middle
detector, and we are going to

00:04:24.036 --> 00:04:26.166 A:middle
cover them in detail in our two

00:04:26.166 --> 00:04:27.046 A:middle
Vision sessions.

00:04:31.036 --> 00:04:33.106 A:middle
Next domain is Natural Language.

00:04:33.656 --> 00:04:36.356 A:middle
Just like you use Vision to

00:04:36.356 --> 00:04:38.376 A:middle
reason about images you can use

00:04:38.376 --> 00:04:41.046 A:middle
Natural Language to reason about

00:04:42.056 --> 00:04:43.076 A:middle
the text.

00:04:43.256 --> 00:04:44.906 A:middle
New this year is inbuilt

00:04:44.906 --> 00:04:46.166 A:middle
Sentiment Analysis.

00:04:46.166 --> 00:04:48.626 A:middle
So you can use to analyze the

00:04:48.686 --> 00:04:50.616 A:middle
sentiment of the text in real

00:04:50.616 --> 00:04:52.636 A:middle
time on the device in a privacy

00:04:52.636 --> 00:04:53.216 A:middle
friendly way.

00:04:53.836 --> 00:04:55.296 A:middle
So, for example, if somebody

00:04:55.296 --> 00:04:56.486 A:middle
types something like this I was

00:04:56.566 --> 00:04:57.926 A:middle
so excited about the season

00:04:57.926 --> 00:04:59.406 A:middle
finale that's a positive

00:05:00.256 --> 00:05:02.656 A:middle
sentiment, but it was a bit

00:05:02.656 --> 00:05:04.386 A:middle
disappointing at the end it's a

00:05:04.386 --> 00:05:05.406 A:middle
negative sentiment.

00:05:06.056 --> 00:05:08.656 A:middle
So you can provide this kind of

00:05:08.656 --> 00:05:10.006 A:middle
feedback in real time.

00:05:10.526 --> 00:05:14.516 A:middle
For the first time we are also

00:05:14.516 --> 00:05:16.826 A:middle
exposing inbuilt Word

00:05:16.826 --> 00:05:17.386 A:middle
Embeddings.

00:05:18.606 --> 00:05:20.096 A:middle
Word Embeddings can allow you to

00:05:20.166 --> 00:05:21.666 A:middle
find semantically similar words.

00:05:21.666 --> 00:05:23.426 A:middle
So, for example, the word

00:05:23.426 --> 00:05:25.246 A:middle
thunderstorm is very close to

00:05:25.246 --> 00:05:27.496 A:middle
cloudy but it is very far away

00:05:27.496 --> 00:05:28.406 A:middle
from shoes and boots.

00:05:28.896 --> 00:05:30.486 A:middle
One of the big use case of Word

00:05:30.486 --> 00:05:31.486 A:middle
Embeddings is semantic search

00:05:31.486 --> 00:05:33.636 A:middle
and we are going to give you an

00:05:34.246 --> 00:05:35.246 A:middle
example soon.

00:05:35.456 --> 00:05:36.546 A:middle
Natural Language will be

00:05:36.546 --> 00:05:38.446 A:middle
discussed in detail in Advances

00:05:38.446 --> 00:05:39.666 A:middle
in Natural Language Framework

00:05:39.716 --> 00:05:40.086 A:middle
session.

00:05:40.636 --> 00:05:45.836 A:middle
The third way user interacts

00:05:45.836 --> 00:05:47.476 A:middle
with your app is through speech

00:05:47.826 --> 00:05:48.426 A:middle
and sound.

00:05:49.566 --> 00:05:51.076 A:middle
Now we have an on-device speech

00:05:51.106 --> 00:05:52.496 A:middle
support so you can transcribe

00:05:52.546 --> 00:05:54.986 A:middle
the text on the device so you no

00:05:54.986 --> 00:05:56.226 A:middle
longer have to rely on the

00:05:56.226 --> 00:05:59.266 A:middle
network connection, and we also

00:05:59.266 --> 00:06:01.086 A:middle
have new Voice Analytics API

00:05:59.266 --> 00:06:01.086 A:middle
have new Voice Analytics API

00:06:01.436 --> 00:06:03.066 A:middle
that can tell you not only what

00:06:03.066 --> 00:06:05.926 A:middle
is spoken but how it is spoken.

00:06:05.926 --> 00:06:07.066 A:middle
So you can differentiate between

00:06:07.066 --> 00:06:08.486 A:middle
a normal voice and a high

00:06:08.576 --> 00:06:09.986 A:middle
jittery voice.

00:06:11.016 --> 00:06:12.916 A:middle
We also have a brand-new Sound

00:06:12.916 --> 00:06:14.396 A:middle
Analysis Framework, that we will

00:06:14.396 --> 00:06:15.936 A:middle
discuss in Create ML session.

00:06:16.416 --> 00:06:21.496 A:middle
There's a lot more in each of

00:06:21.496 --> 00:06:24.146 A:middle
these domains and another thing

00:06:24.146 --> 00:06:25.606 A:middle
you can do to combine these

00:06:25.606 --> 00:06:27.396 A:middle
domains almost seamlessly.

00:06:28.216 --> 00:06:29.246 A:middle
Let me show you an example.

00:06:30.026 --> 00:06:33.176 A:middle
Let's just say you want to build

00:06:33.176 --> 00:06:34.616 A:middle
a feature that does Semantic

00:06:34.616 --> 00:06:35.566 A:middle
Search on Images.

00:06:35.676 --> 00:06:36.896 A:middle
That's a very complex feature.

00:06:36.896 --> 00:06:38.746 A:middle
So if a user searches for

00:06:38.746 --> 00:06:41.056 A:middle
thunderstorm, you want to

00:06:41.056 --> 00:06:42.366 A:middle
provide them the results not

00:06:42.366 --> 00:06:43.886 A:middle
only for thunderstorm but also

00:06:44.026 --> 00:06:46.716 A:middle
for sky and cloudy.

00:06:46.836 --> 00:06:49.756 A:middle
Now, you can combine Vision with

00:06:49.756 --> 00:06:51.336 A:middle
Natural Language to implement

00:06:51.336 --> 00:06:53.376 A:middle
this feature in very few lines

00:06:53.376 --> 00:06:53.806 A:middle
of code.

00:06:55.176 --> 00:06:56.206 A:middle
This is how you will do it.

00:06:56.206 --> 00:06:58.146 A:middle
You will run your Image

00:06:58.146 --> 00:07:02.056 A:middle
Classifier on images or you can

00:06:58.146 --> 00:07:02.056 A:middle
Classifier on images or you can

00:07:02.056 --> 00:07:03.606 A:middle
have them use inbuilt Image

00:07:03.606 --> 00:07:05.286 A:middle
Classifier to generate the tags.

00:07:06.016 --> 00:07:08.886 A:middle
When the user types the word

00:07:08.936 --> 00:07:10.356 A:middle
something like thunderstorm, you

00:07:10.356 --> 00:07:11.926 A:middle
can use Word Embeddings to

00:07:11.926 --> 00:07:15.226 A:middle
generate similar words and find

00:07:15.226 --> 00:07:16.576 A:middle
the images that matches these

00:07:16.646 --> 00:07:16.896 A:middle
tags.

00:07:17.626 --> 00:07:21.896 A:middle
And that's not all.

00:07:21.896 --> 00:07:23.756 A:middle
You can also combine the custom

00:07:23.756 --> 00:07:24.936 A:middle
models that you made using

00:07:24.936 --> 00:07:26.746 A:middle
Create ML with Domain APIs, and

00:07:27.036 --> 00:07:28.646 A:middle
we are going to show an example

00:07:28.646 --> 00:07:30.686 A:middle
of that in our Creating Great

00:07:30.686 --> 00:07:32.496 A:middle
Apps Using Core ML and ARKit

00:07:32.606 --> 00:07:32.996 A:middle
session.

00:07:33.546 --> 00:07:37.426 A:middle
So to summarize, Domain APIs

00:07:37.426 --> 00:07:38.766 A:middle
allow you to leverage Apple's

00:07:38.896 --> 00:07:40.576 A:middle
built-in intelligence and model

00:07:40.856 --> 00:07:42.586 A:middle
using API so you don't have to

00:07:42.586 --> 00:07:43.856 A:middle
collect the data and make the

00:07:43.856 --> 00:07:44.266 A:middle
models.

00:07:44.736 --> 00:07:45.776 A:middle
And this year we have a

00:07:45.866 --> 00:07:48.046 A:middle
significant expansion in our

00:07:48.046 --> 00:07:49.276 A:middle
Vision Natural Language and

00:07:49.276 --> 00:07:49.976 A:middle
Speech and Sound APIs.

00:07:56.336 --> 00:07:58.176 A:middle
Now let's talk about Core ML 3,

00:07:58.526 --> 00:08:00.136 A:middle
the third big pillar of our

00:07:58.526 --> 00:08:00.136 A:middle
the third big pillar of our

00:08:00.846 --> 00:08:01.016 A:middle
offering.

00:08:03.536 --> 00:08:06.106 A:middle
Core ML is now supported across

00:08:06.196 --> 00:08:10.156 A:middle
all our platforms.

00:08:10.216 --> 00:08:11.396 A:middle
All the work is done on the

00:08:11.396 --> 00:08:13.046 A:middle
device so user's privacy is

00:08:13.076 --> 00:08:13.936 A:middle
maintained.

00:08:14.716 --> 00:08:16.686 A:middle
Core ML is hardware accelerated

00:08:16.686 --> 00:08:18.466 A:middle
so you can do, you can use it

00:08:18.466 --> 00:08:19.726 A:middle
for realtime Machine Learning

00:08:20.646 --> 00:08:22.586 A:middle
and you don't need a server and

00:08:22.586 --> 00:08:23.656 A:middle
it always available.

00:08:24.356 --> 00:08:29.666 A:middle
Core ML has always supported a

00:08:29.666 --> 00:08:31.056 A:middle
wide variety of Machine Learning

00:08:31.056 --> 00:08:33.006 A:middle
models ranging from classical

00:08:33.006 --> 00:08:34.566 A:middle
generalized linear models, tree

00:08:34.566 --> 00:08:35.976 A:middle
ensembles and support vector

00:08:35.976 --> 00:08:38.275 A:middle
machines as well as neural

00:08:38.275 --> 00:08:40.395 A:middle
networks such as Convolution

00:08:40.395 --> 00:08:41.946 A:middle
Neural Network and Recurrent

00:08:41.946 --> 00:08:43.155 A:middle
Neural Networks.

00:08:46.236 --> 00:08:48.596 A:middle
New in Core ML is model

00:08:48.596 --> 00:08:50.146 A:middle
flexibility and model

00:08:50.146 --> 00:08:51.046 A:middle
personalization.

00:08:51.626 --> 00:08:53.196 A:middle
So let's take a look at both of

00:08:53.926 --> 00:08:54.016 A:middle
them.

00:08:55.956 --> 00:08:58.226 A:middle
Core ML has expanded support for

00:08:58.226 --> 00:09:01.036 A:middle
Data Neural Networks and we have

00:08:58.226 --> 00:09:01.036 A:middle
Data Neural Networks and we have

00:09:01.036 --> 00:09:03.686 A:middle
added a support for more than

00:09:03.686 --> 00:09:06.106 A:middle
100+ Neural Network layers.

00:09:07.356 --> 00:09:09.046 A:middle
This means that you can bring

00:09:09.046 --> 00:09:11.186 A:middle
almost, you can bring the most

00:09:11.186 --> 00:09:13.456 A:middle
cutting-edge Machine Learning

00:09:13.456 --> 00:09:15.486 A:middle
models into your app such as

00:09:15.486 --> 00:09:16.926 A:middle
ELMo, BERT, Wavenet.

00:09:17.816 --> 00:09:18.586 A:middle
What does that mean?

00:09:20.136 --> 00:09:21.716 A:middle
Let's just say you have an app

00:09:21.716 --> 00:09:22.906 A:middle
and you want to integrate a

00:09:22.906 --> 00:09:24.056 A:middle
state of the art Question and

00:09:24.056 --> 00:09:26.476 A:middle
Answer system in your app so

00:09:26.476 --> 00:09:28.156 A:middle
that when a user asks a question

00:09:28.156 --> 00:09:29.556 A:middle
how many sessions will there be

00:09:29.556 --> 00:09:30.606 A:middle
at WWDC this year?

00:09:31.196 --> 00:09:32.896 A:middle
You can do it by using BERT

00:09:32.966 --> 00:09:33.336 A:middle
model.

00:09:34.536 --> 00:09:35.796 A:middle
So the model can analyze the

00:09:35.796 --> 00:09:38.116 A:middle
statement and gives feedback the

00:09:38.146 --> 00:09:43.956 A:middle
result over 100 is the answer.

00:09:43.956 --> 00:09:45.286 A:middle
Besides Natural Language you can

00:09:45.286 --> 00:09:47.756 A:middle
also run the latest advances in

00:09:47.756 --> 00:09:49.166 A:middle
Vision such as Instance

00:09:49.166 --> 00:09:51.666 A:middle
Segmentation as well as latest

00:09:51.666 --> 00:09:53.256 A:middle
advances in Audio Generation.

00:09:56.876 --> 00:09:58.606 A:middle
And to take full advantage of

00:09:58.606 --> 00:10:00.216 A:middle
this expanded support of Core

00:09:58.606 --> 00:10:00.216 A:middle
this expanded support of Core

00:10:00.266 --> 00:10:03.146 A:middle
ML, we are updating our

00:10:03.146 --> 00:10:03.556 A:middle
converters.

00:10:03.556 --> 00:10:04.986 A:middle
So we will have a brand new

00:10:04.986 --> 00:10:06.276 A:middle
TensorFlow Core ML converter and

00:10:06.276 --> 00:10:08.676 A:middle
ONNX support ML converter will

00:10:08.676 --> 00:10:09.566 A:middle
be coming soon.

00:10:10.176 --> 00:10:14.756 A:middle
We are also updating our Model

00:10:14.756 --> 00:10:16.326 A:middle
Gallery importing some of these

00:10:16.366 --> 00:10:18.726 A:middle
research models on our Model

00:10:18.726 --> 00:10:20.596 A:middle
Gallery so you can start using

00:10:20.596 --> 00:10:21.566 A:middle
them immediately.

00:10:22.686 --> 00:10:24.466 A:middle
So Core ML 3 with this new model

00:10:24.466 --> 00:10:26.016 A:middle
representation and embedded

00:10:26.056 --> 00:10:27.666 A:middle
converters with Model Gallery

00:10:28.196 --> 00:10:29.526 A:middle
can help you bring the cutting

00:10:29.526 --> 00:10:31.316 A:middle
edge ML research into your app.

00:10:32.216 --> 00:10:33.476 A:middle
Now a big feature, Model

00:10:33.476 --> 00:10:34.456 A:middle
Personalization.

00:10:35.146 --> 00:10:36.836 A:middle
So let me explain what it is.

00:10:37.336 --> 00:10:39.086 A:middle
So up to now you have seen, you

00:10:39.086 --> 00:10:41.106 A:middle
have used Create ML to build the

00:10:41.106 --> 00:10:44.006 A:middle
models and Core ML to deploy the

00:10:44.006 --> 00:10:47.616 A:middle
models, but new in Core ML 3 is

00:10:47.846 --> 00:10:49.806 A:middle
On-Device Model Personalization.

00:10:50.266 --> 00:10:52.726 A:middle
You can fine tune the model on

00:10:52.726 --> 00:10:53.236 A:middle
the device.

00:10:53.966 --> 00:10:58.196 A:middle
And we use this kind of

00:10:58.196 --> 00:11:00.466 A:middle
technology in Face ID and

00:10:58.196 --> 00:11:00.466 A:middle
technology in Face ID and

00:11:00.466 --> 00:11:01.276 A:middle
setting Watch Face.

00:11:02.026 --> 00:11:05.536 A:middle
So this is how it happens.

00:11:05.756 --> 00:11:08.506 A:middle
Today you have data, you make an

00:11:08.566 --> 00:11:11.526 A:middle
ML model, you ship it to your

00:11:11.736 --> 00:11:13.776 A:middle
app and all of your users

00:11:13.776 --> 00:11:15.136 A:middle
download the same model.

00:11:15.706 --> 00:11:19.716 A:middle
And this is great because now

00:11:20.196 --> 00:11:21.596 A:middle
users don't have to upload their

00:11:21.596 --> 00:11:21.986 A:middle
portals.

00:11:22.346 --> 00:11:24.166 A:middle
You can directly take photos,

00:11:24.646 --> 00:11:25.956 A:middle
run through the model on the

00:11:25.956 --> 00:11:28.466 A:middle
device and get inference such as

00:11:31.516 --> 00:11:31.616 A:middle
dog.

00:11:31.836 --> 00:11:34.126 A:middle
But what if you are trying to

00:11:34.126 --> 00:11:35.496 A:middle
deal with a concept that is

00:11:35.496 --> 00:11:36.526 A:middle
unique to each user?

00:11:37.096 --> 00:11:38.826 A:middle
Say the concept of My Dog.

00:11:39.696 --> 00:11:41.006 A:middle
You might want users to find

00:11:41.006 --> 00:11:42.866 A:middle
pictures of their own dog in the

00:11:42.916 --> 00:11:44.456 A:middle
photo library and not all dog

00:11:44.456 --> 00:11:47.216 A:middle
photos they've taken.

00:11:47.336 --> 00:11:48.696 A:middle
And each user's dog looks

00:11:48.696 --> 00:11:49.156 A:middle
different.

00:11:49.156 --> 00:11:50.426 A:middle
For example, someone may have

00:11:50.486 --> 00:11:51.706 A:middle
Golden Retriever, someone may

00:11:52.196 --> 00:11:54.386 A:middle
have this Bulldog, ah, and this

00:11:54.386 --> 00:11:56.976 A:middle
is my crazy dog.

00:11:58.976 --> 00:12:01.546 A:middle
So, how do we do that?

00:11:58.976 --> 00:12:01.546 A:middle
So, how do we do that?

00:12:02.746 --> 00:12:05.106 A:middle
So what we want is for user 1 we

00:12:05.106 --> 00:12:06.936 A:middle
need image classifier that takes

00:12:06.936 --> 00:12:08.806 A:middle
this dog and says it's their

00:12:08.806 --> 00:12:09.076 A:middle
dog.

00:12:10.286 --> 00:12:12.326 A:middle
For user 2, that's the English

00:12:12.326 --> 00:12:14.906 A:middle
Bulldog and this the third dog.

00:12:18.576 --> 00:12:21.286 A:middle
So in order to do that, one

00:12:21.286 --> 00:12:23.196 A:middle
approach would be to use Server

00:12:23.196 --> 00:12:23.886 A:middle
Based Approach.

00:12:23.916 --> 00:12:26.126 A:middle
So you may ask each of your

00:12:26.636 --> 00:12:30.366 A:middle
users to upload the photos in

00:12:31.836 --> 00:12:32.476 A:middle
the Cloud.

00:12:32.476 --> 00:12:34.496 A:middle
Server generates model for each

00:12:34.496 --> 00:12:37.526 A:middle
of the users and then sends it

00:12:39.056 --> 00:12:39.466 A:middle
back.

00:12:39.466 --> 00:12:41.826 A:middle
Unfortunately, this approach has

00:12:41.826 --> 00:12:42.606 A:middle
privacy concerns.

00:12:42.606 --> 00:12:45.006 A:middle
Your user may not be very happy

00:12:45.266 --> 00:12:47.236 A:middle
uploading their photo to your

00:12:47.236 --> 00:12:48.496 A:middle
server and you may also not be

00:12:48.496 --> 00:12:49.516 A:middle
okay taking their photo.

00:12:50.606 --> 00:12:51.866 A:middle
You have to setup the server so

00:12:51.916 --> 00:12:53.296 A:middle
there is some cost involved.

00:12:53.846 --> 00:12:56.346 A:middle
Let us say you have a million

00:12:56.346 --> 00:12:58.036 A:middle
users, which we sincerely hope

00:12:58.036 --> 00:12:59.396 A:middle
you do, you have to make

00:12:59.396 --> 00:13:00.776 A:middle
millions such models and keep

00:12:59.396 --> 00:13:00.776 A:middle
millions such models and keep

00:13:00.836 --> 00:13:02.966 A:middle
track of them over time.

00:13:05.176 --> 00:13:07.356 A:middle
With Core ML 3 you can do more

00:13:07.356 --> 00:13:09.186 A:middle
personalization on the device.

00:13:09.396 --> 00:13:11.776 A:middle
So if you have a training data

00:13:11.776 --> 00:13:13.886 A:middle
on the device, you can just

00:13:13.886 --> 00:13:15.906 A:middle
simply use it to fine tune the

00:13:15.906 --> 00:13:17.146 A:middle
model on the device.

00:13:17.946 --> 00:13:23.366 A:middle
So previously you take LMH and

00:13:23.366 --> 00:13:27.156 A:middle
you do the inference and now you

00:13:27.156 --> 00:13:28.936 A:middle
can take the label data feedback

00:13:28.936 --> 00:13:30.396 A:middle
from the user, provide some kind

00:13:30.396 --> 00:13:32.386 A:middle
of training data and fine tune

00:13:32.386 --> 00:13:34.276 A:middle
the model on the device.

00:13:35.516 --> 00:13:42.226 A:middle
[ Applause ]

00:13:42.726 --> 00:13:44.256 A:middle
You have a personalized model

00:13:44.406 --> 00:13:46.006 A:middle
for each user.

00:13:48.176 --> 00:13:50.826 A:middle
We respect user's privacy and

00:13:50.826 --> 00:13:52.696 A:middle
you don't need to put server.

00:13:53.386 --> 00:13:56.386 A:middle
So Core ML 3 supports On-Device

00:13:56.386 --> 00:13:58.116 A:middle
Personalization for Neural

00:13:58.936 --> 00:13:59.326 A:middle
Networks.

00:13:59.786 --> 00:14:01.146 A:middle
We're also supporting Nearest

00:13:59.786 --> 00:14:01.146 A:middle
We're also supporting Nearest

00:14:01.276 --> 00:14:03.346 A:middle
Neighbor and you can do it in

00:14:03.346 --> 00:14:05.246 A:middle
the background at night.

00:14:08.536 --> 00:14:10.276 A:middle
So to summarize and end our

00:14:10.356 --> 00:14:13.046 A:middle
session we saw Create ML a brand

00:14:13.046 --> 00:14:16.006 A:middle
new app to build ML models, we

00:14:16.006 --> 00:14:17.556 A:middle
have a significant expansion in

00:14:17.556 --> 00:14:21.126 A:middle
our Domain APIs and Core ML is

00:14:21.196 --> 00:14:22.586 A:middle
much more flexible and now

00:14:22.586 --> 00:14:23.706 A:middle
supports On-Device

00:14:23.706 --> 00:14:24.666 A:middle
Personalization.

00:14:25.206 --> 00:14:29.016 A:middle
You can find more information on

00:14:29.016 --> 00:14:30.796 A:middle
our Developer Website Session

00:14:30.796 --> 00:14:33.966 A:middle
209 and we hope you are as

00:14:33.996 --> 00:14:35.636 A:middle
excited about these technologies

00:14:35.636 --> 00:14:36.176 A:middle
as we are.

00:14:36.566 --> 00:14:37.876 A:middle
I look forward to seeing you in

00:14:37.876 --> 00:14:38.226 A:middle
the labs.

00:14:38.366 --> 00:14:38.706 A:middle
Thank you.

00:14:39.516 --> 00:14:43.500 A:middle
[ Applause ]
