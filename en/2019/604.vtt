WEBVTT

00:00:00.506 --> 00:00:05.460 A:middle
[ Music ]

00:00:07.096 --> 00:00:07.976 A:middle
&gt;&gt; Good afternoon.

00:00:08.516 --> 00:00:14.716 A:middle
[ Applause ]

00:00:15.216 --> 00:00:16.085 A:middle
Hello, everyone.

00:00:16.346 --> 00:00:18.076 A:middle
And welcome to our session on

00:00:18.076 --> 00:00:19.636 A:middle
introducing ARKit 3.

00:00:20.046 --> 00:00:21.286 A:middle
My name is Andreas.

00:00:21.606 --> 00:00:23.106 A:middle
I'm an engineer on the ARKit

00:00:23.146 --> 00:00:23.466 A:middle
team.

00:00:23.926 --> 00:00:25.196 A:middle
And I couldn't be more excited

00:00:25.196 --> 00:00:26.896 A:middle
to be here today to tell you all

00:00:26.896 --> 00:00:29.666 A:middle
about the third major release of

00:00:29.666 --> 00:00:30.016 A:middle
ARKit.

00:00:32.046 --> 00:00:33.926 A:middle
When we introduced ARKit in

00:00:33.926 --> 00:00:37.196 A:middle
2017, we made iOS the largest AR

00:00:37.196 --> 00:00:39.976 A:middle
platform in the world, bringing

00:00:39.976 --> 00:00:42.116 A:middle
AR to hundreds of millions of

00:00:42.216 --> 00:00:43.076 A:middle
iOS devices.

00:00:43.236 --> 00:00:45.496 A:middle
And this is really significant

00:00:45.496 --> 00:00:47.516 A:middle
for you because that's the wide

00:00:47.516 --> 00:00:49.506 A:middle
audience that you can reach with

00:00:49.506 --> 00:00:50.966 A:middle
the apps and the games you

00:00:50.966 --> 00:00:52.896 A:middle
write.

00:00:53.176 --> 00:00:54.626 A:middle
Our mission has been from the

00:00:54.626 --> 00:00:55.946 A:middle
beginning to make it really

00:00:56.066 --> 00:00:58.056 A:middle
simple for you to write your

00:00:58.056 --> 00:01:00.106 A:middle
first augmented reality app even

00:00:58.056 --> 00:01:00.106 A:middle
first augmented reality app even

00:01:00.106 --> 00:01:01.186 A:middle
if you're a new developer.

00:01:01.186 --> 00:01:03.626 A:middle
But we also want to give you the

00:01:03.696 --> 00:01:05.906 A:middle
tools at hand that you need to

00:01:05.906 --> 00:01:07.726 A:middle
create really advanced and

00:01:07.726 --> 00:01:09.126 A:middle
sophisticated experiences.

00:01:09.126 --> 00:01:12.116 A:middle
So if you look at the App Store

00:01:12.116 --> 00:01:14.716 A:middle
today, we can see that you did

00:01:14.716 --> 00:01:15.896 A:middle
an amazing job.

00:01:15.896 --> 00:01:17.216 A:middle
You built great apps and great

00:01:17.216 --> 00:01:17.606 A:middle
games.

00:01:18.126 --> 00:01:19.426 A:middle
So let's just have a look at a

00:01:19.426 --> 00:01:22.956 A:middle
few of them now.

00:01:22.956 --> 00:01:25.086 A:middle
Bringing your game idea into AR

00:01:25.086 --> 00:01:26.866 A:middle
can make them so much more

00:01:26.866 --> 00:01:28.626 A:middle
engaging and physical, like

00:01:28.626 --> 00:01:29.426 A:middle
Angry Birds.

00:01:29.776 --> 00:01:31.226 A:middle
You can now play it in AR,

00:01:31.226 --> 00:01:32.446 A:middle
actually work around those

00:01:32.446 --> 00:01:35.146 A:middle
structures and identify the best

00:01:35.146 --> 00:01:38.076 A:middle
spot to shoot and then have to

00:01:38.076 --> 00:01:39.436 A:middle
slingshot right into your own

00:01:39.436 --> 00:01:41.616 A:middle
hand and fire off those angry

00:01:41.676 --> 00:01:41.926 A:middle
birds.

00:01:45.196 --> 00:01:47.426 A:middle
ARKit also works really well for

00:01:47.426 --> 00:01:48.806 A:middle
large scale use cases.

00:01:49.416 --> 00:01:51.476 A:middle
iScape is a tool for outdoor

00:01:51.476 --> 00:01:52.236 A:middle
landscaping.

00:01:52.236 --> 00:01:54.856 A:middle
We can place bushes and trees

00:01:54.856 --> 00:01:56.116 A:middle
and watch your next garden

00:01:56.116 --> 00:01:57.776 A:middle
remodeling project come to life

00:01:57.776 --> 00:02:00.426 A:middle
in AR right in your own garden

00:01:57.776 --> 00:02:00.426 A:middle
in AR right in your own garden

00:02:00.506 --> 00:02:02.976 A:middle
or backyard.

00:02:03.056 --> 00:02:05.706 A:middle
Last year, with ARKit 2, we

00:02:05.706 --> 00:02:08.746 A:middle
introduced USDZ, a new 3D file

00:02:08.746 --> 00:02:11.766 A:middle
format for exchanging formats

00:02:11.766 --> 00:02:13.106 A:middle
right made for AR.

00:02:14.126 --> 00:02:16.116 A:middle
Wayfair uses this to place

00:02:16.116 --> 00:02:17.876 A:middle
virtual furniture right in your

00:02:17.876 --> 00:02:18.136 A:middle
home.

00:02:18.136 --> 00:02:21.266 A:middle
But leveraging ARKit's advanced

00:02:21.366 --> 00:02:22.806 A:middle
scene understanding features

00:02:22.806 --> 00:02:24.226 A:middle
like environment texturing,

00:02:24.396 --> 00:02:25.646 A:middle
these objects really blend

00:02:25.646 --> 00:02:27.626 A:middle
nicely with your living room.

00:02:27.776 --> 00:02:31.236 A:middle
And Lego is making use of

00:02:31.236 --> 00:02:33.026 A:middle
ARKit's 3D object detection

00:02:33.026 --> 00:02:33.836 A:middle
capabilities.

00:02:35.326 --> 00:02:37.006 A:middle
It finds your physical Lego set

00:02:37.006 --> 00:02:38.696 A:middle
and enhances it in AR.

00:02:38.696 --> 00:02:40.706 A:middle
And thanks to ARKit's multi-user

00:02:40.706 --> 00:02:42.456 A:middle
support, you can even play

00:02:42.456 --> 00:02:44.906 A:middle
together with your friends.

00:02:45.396 --> 00:02:47.486 A:middle
So these are just a few examples

00:02:47.486 --> 00:02:48.686 A:middle
of what you have created.

00:02:49.476 --> 00:02:52.366 A:middle
ARKit helps you to take care all

00:02:52.786 --> 00:02:54.296 A:middle
of the technical details, do the

00:02:54.296 --> 00:02:56.266 A:middle
heavy-lifting for you to make

00:02:56.266 --> 00:02:57.886 A:middle
augmented reality work so that

00:02:58.016 --> 00:02:59.886 A:middle
you can really focus on creating

00:03:00.186 --> 00:03:01.676 A:middle
the great experiences around

00:03:01.916 --> 00:03:02.216 A:middle
them.

00:03:03.016 --> 00:03:04.986 A:middle
Let's do a quick recap of the

00:03:05.186 --> 00:03:06.256 A:middle
three main pillars of

00:03:06.256 --> 00:03:08.036 A:middle
functionality that ARKit

00:03:08.166 --> 00:03:09.046 A:middle
provides to you.

00:03:10.806 --> 00:03:12.376 A:middle
First, there is tracking.

00:03:12.376 --> 00:03:16.206 A:middle
Tracking determines where your

00:03:16.206 --> 00:03:17.886 A:middle
device is with regard to the

00:03:17.886 --> 00:03:19.886 A:middle
environment so that virtual

00:03:19.966 --> 00:03:21.596 A:middle
content can be positioned

00:03:21.706 --> 00:03:24.026 A:middle
accurately and updated correctly

00:03:24.026 --> 00:03:25.516 A:middle
on top of the camera image in

00:03:25.516 --> 00:03:26.046 A:middle
real time.

00:03:27.186 --> 00:03:28.876 A:middle
This creates then the illusion

00:03:29.136 --> 00:03:30.876 A:middle
that virtual content is actually

00:03:30.996 --> 00:03:32.736 A:middle
placed in the real world.

00:03:33.906 --> 00:03:35.576 A:middle
ARKit also provides you with

00:03:35.636 --> 00:03:37.116 A:middle
different tracking technologies

00:03:37.116 --> 00:03:38.766 A:middle
such as road World Tracking,

00:03:39.136 --> 00:03:42.306 A:middle
face tracking or image tracking.

00:03:42.996 --> 00:03:44.736 A:middle
On top of tracking, we have

00:03:44.886 --> 00:03:45.766 A:middle
scene understanding.

00:03:47.606 --> 00:03:49.036 A:middle
With scene understanding, you

00:03:49.036 --> 00:03:51.696 A:middle
can identify surfaces, images,

00:03:51.826 --> 00:03:54.266 A:middle
and 3D objects in the scene and

00:03:54.266 --> 00:03:56.006 A:middle
attach virtual content right on

00:03:56.076 --> 00:03:56.566 A:middle
top of them.

00:03:58.196 --> 00:03:59.696 A:middle
Scene understanding also learns

00:03:59.696 --> 00:04:01.416 A:middle
about the lighting and even

00:03:59.696 --> 00:04:01.416 A:middle
about the lighting and even

00:04:01.456 --> 00:04:03.556 A:middle
textures in the environment to

00:04:03.556 --> 00:04:04.976 A:middle
help make your content look

00:04:05.346 --> 00:04:06.226 A:middle
really realistic.

00:04:06.486 --> 00:04:09.016 A:middle
And finally, rendering.

00:04:09.146 --> 00:04:10.836 A:middle
It brings your 3D content to

00:04:10.836 --> 00:04:11.266 A:middle
life.

00:04:12.806 --> 00:04:14.066 A:middle
We've been supporting different

00:04:14.066 --> 00:04:15.686 A:middle
renderers like SceneKit,

00:04:15.986 --> 00:04:17.406 A:middle
SpriteKit, and Metal.

00:04:17.446 --> 00:04:19.906 A:middle
And now, this year also

00:04:20.166 --> 00:04:22.176 A:middle
RealityKit, designed from the

00:04:22.176 --> 00:04:24.226 A:middle
ground up with augmented reality

00:04:24.226 --> 00:04:24.666 A:middle
in mind.

00:04:27.416 --> 00:04:29.046 A:middle
So with this year's release of

00:04:29.046 --> 00:04:30.886 A:middle
ARKit, they're making a huge

00:04:30.886 --> 00:04:31.536 A:middle
leap forward.

00:04:32.416 --> 00:04:34.526 A:middle
Not only are your experiences

00:04:34.766 --> 00:04:36.236 A:middle
going to look even better and

00:04:36.316 --> 00:04:38.616 A:middle
feel more natural, you will also

00:04:38.616 --> 00:04:40.886 A:middle
be able to create entirely new

00:04:40.886 --> 00:04:43.396 A:middle
experiences for use cases that

00:04:43.396 --> 00:04:44.936 A:middle
haven't been possible before.

00:04:45.056 --> 00:04:47.646 A:middle
Thanks to many new features

00:04:47.756 --> 00:04:49.706 A:middle
we're bringing with ARKit 3,

00:04:50.796 --> 00:04:53.116 A:middle
like people occlusion, motion

00:04:53.116 --> 00:04:55.286 A:middle
capture, collaborative sessions,

00:04:55.826 --> 00:04:57.396 A:middle
simultaneous use of the front

00:04:57.396 --> 00:04:59.246 A:middle
and back camera, tracking

00:04:59.246 --> 00:05:00.836 A:middle
multiple faces, and many more.

00:04:59.246 --> 00:05:00.836 A:middle
multiple faces, and many more.

00:05:01.616 --> 00:05:02.966 A:middle
We've got a lot to cover so

00:05:02.966 --> 00:05:05.586 A:middle
let's dive right in and we're

00:05:05.586 --> 00:05:07.986 A:middle
starting with people occlusion.

00:05:07.986 --> 00:05:11.806 A:middle
Let's have a look at this scene

00:05:11.856 --> 00:05:12.066 A:middle
here.

00:05:13.356 --> 00:05:14.406 A:middle
So in order to create a

00:05:14.406 --> 00:05:16.846 A:middle
convincing AR experience, it's

00:05:16.846 --> 00:05:18.586 A:middle
important to position virtual

00:05:18.736 --> 00:05:21.226 A:middle
content accurately and also to

00:05:21.226 --> 00:05:22.536 A:middle
match the real world lighting.

00:05:23.626 --> 00:05:25.076 A:middle
So let's bring in a virtual

00:05:25.406 --> 00:05:26.966 A:middle
espresso machine here and put it

00:05:26.966 --> 00:05:27.746 A:middle
right on the table.

00:05:29.146 --> 00:05:31.066 A:middle
But wait, when people are in the

00:05:31.176 --> 00:05:32.616 A:middle
frame, as in this example, it

00:05:32.646 --> 00:05:34.356 A:middle
can easily break the illusion,

00:05:34.996 --> 00:05:35.956 A:middle
because you would expect the

00:05:36.086 --> 00:05:37.996 A:middle
person on the front to actually

00:05:38.136 --> 00:05:39.356 A:middle
cover the espresso machine.

00:05:41.266 --> 00:05:43.186 A:middle
So with ARKit 3 and people

00:05:43.306 --> 00:05:45.336 A:middle
occlusion, you can now solve

00:05:45.406 --> 00:05:45.996 A:middle
that problem.

00:05:47.106 --> 00:05:49.106 A:middle
[ Applause ]

00:05:49.236 --> 00:05:49.526 A:middle
Thank you.

00:05:51.766 --> 00:05:53.326 A:middle
So let's have a look at how this

00:05:53.326 --> 00:05:53.666 A:middle
is done.

00:05:55.546 --> 00:05:57.356 A:middle
Virtual content by default is

00:05:57.356 --> 00:05:59.256 A:middle
rendered on top of the camera

00:05:59.256 --> 00:05:59.736 A:middle
image.

00:05:59.826 --> 00:06:03.096 A:middle
As you can see here, for pure

00:05:59.826 --> 00:06:03.096 A:middle
As you can see here, for pure

00:06:03.236 --> 00:06:04.726 A:middle
tabletop experience, this is

00:06:04.726 --> 00:06:06.296 A:middle
fine, but if any people in the

00:06:06.296 --> 00:06:07.626 A:middle
frame are in front of that

00:06:07.626 --> 00:06:09.806 A:middle
object, the augmentation doesn't

00:06:09.806 --> 00:06:10.816 A:middle
look correct anymore.

00:06:12.046 --> 00:06:14.566 A:middle
So what ARKit 3 now does for you

00:06:15.456 --> 00:06:17.396 A:middle
is, thanks to machine learning,

00:06:17.676 --> 00:06:19.316 A:middle
recognized people present in the

00:06:19.316 --> 00:06:21.266 A:middle
frame and then create a separate

00:06:21.266 --> 00:06:23.506 A:middle
layer that has only the pixels

00:06:24.026 --> 00:06:25.176 A:middle
containing these people.

00:06:25.556 --> 00:06:26.976 A:middle
We call that segmentation.

00:06:28.286 --> 00:06:30.226 A:middle
Then we can render that layer on

00:06:30.226 --> 00:06:31.216 A:middle
top of everything else.

00:06:31.456 --> 00:06:33.806 A:middle
Let's have a look at the

00:06:33.806 --> 00:06:34.776 A:middle
composite image.

00:06:35.416 --> 00:06:36.696 A:middle
It's looking much better now.

00:06:37.676 --> 00:06:38.786 A:middle
But if you look closely, it's

00:06:38.786 --> 00:06:39.996 A:middle
still not entirely correct.

00:06:41.206 --> 00:06:42.836 A:middle
So the person the front is now

00:06:43.146 --> 00:06:45.106 A:middle
occluding the espresso machine.

00:06:46.246 --> 00:06:47.506 A:middle
But if you zoom in here, then

00:06:47.506 --> 00:06:49.086 A:middle
you can see that also the person

00:06:49.086 --> 00:06:51.066 A:middle
in the back is rendered on top

00:06:51.066 --> 00:06:53.206 A:middle
of the virtual object, although

00:06:53.306 --> 00:06:54.876 A:middle
she is actually standing behind

00:06:54.876 --> 00:06:55.376 A:middle
the table.

00:06:55.856 --> 00:06:57.616 A:middle
So the virtual model in that

00:06:57.706 --> 00:06:59.756 A:middle
case should occlude her, not the

00:06:59.756 --> 00:07:00.406 A:middle
other way around.

00:06:59.756 --> 00:07:00.406 A:middle
other way around.

00:07:00.406 --> 00:07:04.446 A:middle
Now, that happened because we

00:07:04.446 --> 00:07:06.396 A:middle
did not take the distance of

00:07:06.516 --> 00:07:07.886 A:middle
people from the camera into

00:07:08.036 --> 00:07:08.316 A:middle
account.

00:07:10.786 --> 00:07:12.816 A:middle
When ARKit 3 uses advanced

00:07:12.816 --> 00:07:14.246 A:middle
machine learning to do an

00:07:14.246 --> 00:07:15.716 A:middle
additional depth estimation

00:07:15.716 --> 00:07:18.446 A:middle
step, with this estimate, how

00:07:18.446 --> 00:07:19.886 A:middle
far does segmented people are

00:07:19.886 --> 00:07:22.306 A:middle
away from the camera, we can now

00:07:22.346 --> 00:07:23.786 A:middle
correct the rendering order and

00:07:23.786 --> 00:07:26.016 A:middle
make sure to render only people

00:07:26.016 --> 00:07:27.656 A:middle
upfront if they are actually

00:07:27.686 --> 00:07:28.586 A:middle
closer to the camera.

00:07:28.746 --> 00:07:31.286 A:middle
And thanks to the power of Apple

00:07:31.286 --> 00:07:33.606 A:middle
Neural Engine, we are able to do

00:07:33.606 --> 00:07:35.826 A:middle
this on every frame in real

00:07:35.966 --> 00:07:36.206 A:middle
time.

00:07:36.206 --> 00:07:40.736 A:middle
So let's have a look at the

00:07:40.736 --> 00:07:41.876 A:middle
composite image now.

00:07:42.636 --> 00:07:43.436 A:middle
You see that people are

00:07:43.536 --> 00:07:44.946 A:middle
occluding the virtual content

00:07:45.246 --> 00:07:47.186 A:middle
just as you would expect

00:07:47.186 --> 00:07:49.016 A:middle
resulting in a convincing AR

00:07:49.016 --> 00:07:49.676 A:middle
experience.

00:07:50.356 --> 00:07:52.356 A:middle
[ Applause ]

00:07:52.696 --> 00:07:53.316 A:middle
That is great.

00:07:53.566 --> 00:07:53.976 A:middle
Thank you.

00:07:58.146 --> 00:08:00.946 A:middle
So people occlusion enables

00:07:58.146 --> 00:08:00.946 A:middle
So people occlusion enables

00:08:00.946 --> 00:08:02.346 A:middle
virtual content to be rendered

00:08:02.376 --> 00:08:03.186 A:middle
behind people.

00:08:04.296 --> 00:08:05.856 A:middle
It works also for multiple

00:08:05.906 --> 00:08:08.276 A:middle
people in the scene, and it even

00:08:08.276 --> 00:08:09.726 A:middle
works if people are only

00:08:09.796 --> 00:08:11.936 A:middle
partially visible like in the

00:08:11.936 --> 00:08:13.696 A:middle
example before the woman behind

00:08:13.696 --> 00:08:15.766 A:middle
the table actually was not

00:08:15.766 --> 00:08:17.806 A:middle
visible with the full body but

00:08:17.806 --> 00:08:18.726 A:middle
it still is working.

00:08:19.816 --> 00:08:21.196 A:middle
Now, this is really significant

00:08:21.196 --> 00:08:23.166 A:middle
because it does not only make

00:08:23.166 --> 00:08:25.226 A:middle
your experiences look way more

00:08:25.226 --> 00:08:27.686 A:middle
realistic than before, it also

00:08:27.976 --> 00:08:29.356 A:middle
means for you that you can now

00:08:29.356 --> 00:08:31.386 A:middle
create experiences that haven't

00:08:31.386 --> 00:08:32.556 A:middle
been impossible before.

00:08:33.446 --> 00:08:34.566 A:middle
For example, think of a

00:08:34.566 --> 00:08:36.655 A:middle
multiplayer game where you have

00:08:36.966 --> 00:08:38.556 A:middle
people in the frame together

00:08:38.856 --> 00:08:40.666 A:middle
with your virtual content.

00:08:42.976 --> 00:08:44.786 A:middle
People occlusion is integrated

00:08:44.876 --> 00:08:48.106 A:middle
right in ARView and ARSCNView as

00:08:48.106 --> 00:08:48.396 A:middle
well.

00:08:48.396 --> 00:08:51.616 A:middle
And thanks to depth estimation,

00:08:51.736 --> 00:08:53.326 A:middle
we can provide you with an

00:08:53.326 --> 00:08:55.456 A:middle
approximation of the distance of

00:08:55.516 --> 00:08:57.506 A:middle
the people detected with regard

00:08:57.706 --> 00:09:00.436 A:middle
to the camera.

00:08:57.706 --> 00:09:00.436 A:middle
to the camera.

00:09:00.436 --> 00:09:02.106 A:middle
We're using Apple Neural Engine

00:09:02.556 --> 00:09:03.266 A:middle
to do that work.

00:09:03.796 --> 00:09:05.236 A:middle
So people occlusion will work on

00:09:05.236 --> 00:09:08.076 A:middle
devices with an A12 processor or

00:09:08.076 --> 00:09:08.346 A:middle
later.

00:09:08.346 --> 00:09:12.706 A:middle
So let's have a look at how to

00:09:12.706 --> 00:09:13.806 A:middle
turn this on in API.

00:09:14.716 --> 00:09:16.996 A:middle
We have a new property on

00:09:17.146 --> 00:09:18.966 A:middle
ARConfiguration called

00:09:18.966 --> 00:09:20.106 A:middle
FrameSemantics.

00:09:21.256 --> 00:09:23.086 A:middle
This will give you different

00:09:23.146 --> 00:09:25.036 A:middle
kinds of semantic information of

00:09:25.096 --> 00:09:26.256 A:middle
what's in the current frame.

00:09:27.686 --> 00:09:29.416 A:middle
You can also check if a certain

00:09:29.416 --> 00:09:31.336 A:middle
semantic is available on the

00:09:31.516 --> 00:09:33.616 A:middle
specific configuration or device

00:09:34.246 --> 00:09:36.326 A:middle
with an additional method on the

00:09:36.326 --> 00:09:37.176 A:middle
ARConfiguration.

00:09:38.646 --> 00:09:40.306 A:middle
Specific for people occlusion,

00:09:40.426 --> 00:09:41.936 A:middle
there are two methods available

00:09:41.936 --> 00:09:46.096 A:middle
that you can use.

00:09:46.316 --> 00:09:47.756 A:middle
One option is person

00:09:47.756 --> 00:09:48.476 A:middle
segmentation.

00:09:49.186 --> 00:09:51.096 A:middle
This will-- you provide just

00:09:51.356 --> 00:09:52.986 A:middle
with the segmentation of people

00:09:53.436 --> 00:09:55.146 A:middle
rendered on top of the camera

00:09:55.146 --> 00:09:55.606 A:middle
image.

00:09:56.826 --> 00:09:58.096 A:middle
That's the best choice if you

00:09:58.096 --> 00:09:59.826 A:middle
know that people will always be

00:09:59.826 --> 00:10:01.736 A:middle
standing upfront and your

00:09:59.826 --> 00:10:01.736 A:middle
standing upfront and your

00:10:01.736 --> 00:10:03.676 A:middle
virtual content will always be

00:10:03.856 --> 00:10:04.826 A:middle
behind those people.

00:10:05.476 --> 00:10:07.386 A:middle
For example, a green screen use

00:10:07.386 --> 00:10:09.026 A:middle
case just that you don't need a

00:10:09.026 --> 00:10:10.346 A:middle
green screen anymore now.

00:10:10.616 --> 00:10:13.486 A:middle
And the other option is person

00:10:13.486 --> 00:10:15.316 A:middle
segmentation with depth.

00:10:16.016 --> 00:10:17.266 A:middle
This will provide you with

00:10:17.406 --> 00:10:19.666 A:middle
additional depth estimation of

00:10:19.816 --> 00:10:21.616 A:middle
how far those people are away

00:10:21.616 --> 00:10:22.326 A:middle
from the camera.

00:10:23.266 --> 00:10:24.866 A:middle
That's the best choice if people

00:10:24.866 --> 00:10:26.546 A:middle
might be visible together with

00:10:26.586 --> 00:10:29.236 A:middle
virtual content either behind or

00:10:29.236 --> 00:10:33.286 A:middle
in front of that content.

00:10:33.386 --> 00:10:34.876 A:middle
If you do your own rendering

00:10:35.046 --> 00:10:37.506 A:middle
using Metal or for advanced used

00:10:37.506 --> 00:10:39.616 A:middle
cases, you can also directly

00:10:39.616 --> 00:10:42.106 A:middle
access the pixel buffers with

00:10:42.106 --> 00:10:43.576 A:middle
the segmentation and the

00:10:43.576 --> 00:10:45.996 A:middle
estimated depth data on every

00:10:45.996 --> 00:10:46.516 A:middle
ARFrame.

00:10:47.756 --> 00:10:50.446 A:middle
So now, let me show you how

00:10:50.446 --> 00:10:52.196 A:middle
people occlusion works in a live

00:10:52.286 --> 00:10:52.976 A:middle
demo.

00:10:56.516 --> 00:11:02.056 A:middle
[ Applause ]

00:10:56.516 --> 00:11:02.056 A:middle
[ Applause ]

00:11:02.556 --> 00:11:04.356 A:middle
So here in Xcode, I have a small

00:11:04.356 --> 00:11:06.706 A:middle
sample project using the new

00:11:06.706 --> 00:11:07.656 A:middle
RealityKit API.

00:11:08.816 --> 00:11:10.256 A:middle
And let me quickly walk you

00:11:10.256 --> 00:11:11.456 A:middle
through what it does.

00:11:12.806 --> 00:11:15.296 A:middle
So in our viewDidLoad method,

00:11:15.636 --> 00:11:17.666 A:middle
we're creating an AnchorEntity

00:11:18.456 --> 00:11:19.926 A:middle
looking for horizontal planes

00:11:20.486 --> 00:11:21.936 A:middle
and we're adding this anchor

00:11:21.936 --> 00:11:25.146 A:middle
entity to the scene.

00:11:25.146 --> 00:11:27.386 A:middle
Then, we're retrieving a URL of

00:11:27.386 --> 00:11:30.816 A:middle
a model to load and load it

00:11:31.186 --> 00:11:33.136 A:middle
using ModelEntity's asynchronous

00:11:33.136 --> 00:11:34.536 A:middle
mode loading API.

00:11:34.536 --> 00:11:38.386 A:middle
We add the entity as a child of

00:11:38.386 --> 00:11:42.466 A:middle
our anchor, and also install

00:11:42.466 --> 00:11:44.216 A:middle
gestures so that I will be able

00:11:44.266 --> 00:11:46.276 A:middle
to drag the object around on the

00:11:46.326 --> 00:11:46.736 A:middle
plane.

00:11:47.976 --> 00:11:49.996 A:middle
So what this does, thanks to

00:11:49.996 --> 00:11:51.886 A:middle
RealityKit, is automatically

00:11:52.006 --> 00:11:53.126 A:middle
setup a World Tracking

00:11:53.126 --> 00:11:55.406 A:middle
configuration because we know

00:11:55.406 --> 00:11:57.476 A:middle
that we need World Tracking for

00:11:57.646 --> 00:11:58.536 A:middle
plane estimation.

00:11:59.116 --> 00:12:00.916 A:middle
And then, as soon as a plane is

00:11:59.116 --> 00:12:00.916 A:middle
And then, as soon as a plane is

00:12:00.956 --> 00:12:02.366 A:middle
detected, the content will

00:12:02.366 --> 00:12:03.586 A:middle
automatically be placed.

00:12:04.636 --> 00:12:07.596 A:middle
Now, this is not using people

00:12:07.596 --> 00:12:08.296 A:middle
occlusion yet.

00:12:09.516 --> 00:12:11.386 A:middle
But I have already a stop to

00:12:11.386 --> 00:12:12.106 A:middle
turn this on.

00:12:12.866 --> 00:12:13.266 A:middle
It's called

00:12:13.266 --> 00:12:14.986 A:middle
togglePeopleOcclusion and what I

00:12:14.986 --> 00:12:16.556 A:middle
want to implement is a method

00:12:16.786 --> 00:12:18.326 A:middle
that lets me switch people

00:12:18.326 --> 00:12:19.946 A:middle
occlusion on and off when the

00:12:19.946 --> 00:12:21.136 A:middle
user taps the screen.

00:12:22.256 --> 00:12:23.986 A:middle
So let's go ahead and implement

00:12:23.986 --> 00:12:24.376 A:middle
it now.

00:12:25.046 --> 00:12:28.306 A:middle
So the first thing I'm going to

00:12:28.436 --> 00:12:31.756 A:middle
do is actually check if my World

00:12:31.756 --> 00:12:34.076 A:middle
Tracking configuration supports

00:12:34.656 --> 00:12:35.946 A:middle
the person segmentation with

00:12:36.046 --> 00:12:37.296 A:middle
depth frame semantics.

00:12:38.176 --> 00:12:39.406 A:middle
It's recommended that you do

00:12:39.406 --> 00:12:42.776 A:middle
that always because if the code

00:12:42.776 --> 00:12:44.846 A:middle
is run on a device that it does

00:12:44.846 --> 00:12:47.166 A:middle
not have Apple Neural Engine and

00:12:47.286 --> 00:12:48.426 A:middle
this frame semantic is not

00:12:48.426 --> 00:12:50.646 A:middle
supported, we want to gracefully

00:12:50.646 --> 00:12:53.396 A:middle
handle that case.

00:12:53.606 --> 00:12:56.066 A:middle
So if that's the case, we would

00:12:56.066 --> 00:12:57.186 A:middle
just display a message to the

00:12:57.186 --> 00:12:58.506 A:middle
user that people occlusion is

00:12:58.546 --> 00:13:00.496 A:middle
not available on that device.

00:12:58.546 --> 00:13:00.496 A:middle
not available on that device.

00:13:03.456 --> 00:13:05.186 A:middle
And let's actually move on and

00:13:05.186 --> 00:13:06.826 A:middle
implement the toggle.

00:13:08.056 --> 00:13:09.386 A:middle
I'm going to do a switch

00:13:09.386 --> 00:13:11.436 A:middle
statement on the frameSemantics

00:13:11.506 --> 00:13:13.026 A:middle
property of our configuration

00:13:13.026 --> 00:13:13.276 A:middle
here.

00:13:13.916 --> 00:13:14.496 A:middle
And if

00:13:14.546 --> 00:13:16.586 A:middle
personSegmentationWithDepth is

00:13:16.586 --> 00:13:19.036 A:middle
part of the frameSemantics, then

00:13:19.036 --> 00:13:22.116 A:middle
we remove it and tell the user

00:13:22.116 --> 00:13:23.306 A:middle
that people occlusion is now

00:13:23.406 --> 00:13:23.786 A:middle
turned off.

00:13:24.446 --> 00:13:28.486 A:middle
And I just need to implement the

00:13:28.486 --> 00:13:29.436 A:middle
other case as well.

00:13:30.046 --> 00:13:30.986 A:middle
If you don't have person

00:13:30.986 --> 00:13:32.756 A:middle
segmentation enable then, we

00:13:32.756 --> 00:13:35.416 A:middle
insert the frameSemantics into

00:13:36.356 --> 00:13:38.096 A:middle
different semantics property and

00:13:38.096 --> 00:13:39.696 A:middle
display a message that we now

00:13:39.786 --> 00:13:42.766 A:middle
turn people occlusion on.

00:13:43.346 --> 00:13:45.856 A:middle
Now, I need to rerun the updated

00:13:45.886 --> 00:13:47.326 A:middle
configuration on my session.

00:13:48.516 --> 00:13:51.086 A:middle
So let me retrieve the session

00:13:51.806 --> 00:13:55.136 A:middle
from the ARView and call run

00:13:55.676 --> 00:13:57.196 A:middle
with a configuration that I've

00:13:57.256 --> 00:13:58.826 A:middle
just updated here.

00:14:00.036 --> 00:14:01.786 A:middle
So now, the implementation of my

00:14:01.786 --> 00:14:03.936 A:middle
togglePeopleOcclusion method is

00:14:03.936 --> 00:14:06.446 A:middle
finished, now I need to make

00:14:06.446 --> 00:14:08.116 A:middle
sure that it's actually called

00:14:08.776 --> 00:14:10.146 A:middle
when the user taps the screen.

00:14:11.336 --> 00:14:12.696 A:middle
I already installed a tap

00:14:12.696 --> 00:14:14.786 A:middle
gesture recognizer and here in

00:14:14.786 --> 00:14:18.476 A:middle
my onTap method, I just call

00:14:19.796 --> 00:14:20.736 A:middle
togglePeopleOcclusion.

00:14:21.396 --> 00:14:24.266 A:middle
And that's all I need to do.

00:14:25.376 --> 00:14:27.236 A:middle
And now, let me go ahead and

00:14:28.556 --> 00:14:31.466 A:middle
build that code and run it on my

00:14:31.466 --> 00:14:33.086 A:middle
device here.

00:14:34.346 --> 00:14:36.876 A:middle
And we already see that the

00:14:36.926 --> 00:14:39.166 A:middle
plane was detected and the

00:14:39.166 --> 00:14:40.066 A:middle
content placed.

00:14:40.066 --> 00:14:41.856 A:middle
I can also move it around,

00:14:41.976 --> 00:14:43.856 A:middle
thanks to the gestures that I've

00:14:43.856 --> 00:14:44.226 A:middle
added.

00:14:45.026 --> 00:14:46.886 A:middle
You see that RealityKit has

00:14:46.886 --> 00:14:48.966 A:middle
added a nice grounding shadow.

00:14:49.726 --> 00:14:52.806 A:middle
And now, let's actually have a

00:14:52.806 --> 00:14:53.806 A:middle
look at people occlusion.

00:14:53.946 --> 00:14:54.966 A:middle
Right now, it's still turned

00:14:54.966 --> 00:14:55.176 A:middle
off.

00:14:55.176 --> 00:14:57.366 A:middle
So if I bring in my hand now,

00:14:57.486 --> 00:14:58.676 A:middle
you see that the content is

00:14:58.676 --> 00:15:00.106 A:middle
always rendered on top.

00:14:58.676 --> 00:15:00.106 A:middle
always rendered on top.

00:15:00.426 --> 00:15:01.736 A:middle
This is the behavior that you

00:15:01.736 --> 00:15:04.616 A:middle
know from ARKit 2.

00:15:04.856 --> 00:15:06.856 A:middle
Now, let me turn it on and bring

00:15:06.856 --> 00:15:07.736 A:middle
in my hand again.

00:15:07.736 --> 00:15:10.156 A:middle
And you'll see that now

00:15:10.206 --> 00:15:10.966 A:middle
[applause] the virtual object is

00:15:11.176 --> 00:15:12.866 A:middle
actually covered.

00:15:14.516 --> 00:15:18.466 A:middle
[ Applause ]

00:15:18.966 --> 00:15:21.646 A:middle
And that's people occlusion in

00:15:21.646 --> 00:15:22.236 A:middle
the ARKit 3.

00:15:27.786 --> 00:15:32.276 A:middle
[Applause] Thank you.

00:15:32.276 --> 00:15:34.656 A:middle
So, let's talk about another

00:15:34.656 --> 00:15:36.666 A:middle
exciting new feature of ARKit 3

00:15:37.116 --> 00:15:38.616 A:middle
which is motion capture.

00:15:39.746 --> 00:15:42.736 A:middle
With motion capture, you can

00:15:42.786 --> 00:15:45.056 A:middle
track the body of a person which

00:15:45.056 --> 00:15:46.696 A:middle
can then for example be mapped

00:15:46.696 --> 00:15:48.666 A:middle
to a virtual character in real

00:15:48.706 --> 00:15:49.046 A:middle
time.

00:15:49.966 --> 00:15:51.156 A:middle
Now, this could only be done

00:15:51.156 --> 00:15:52.626 A:middle
with external setup and special

00:15:52.626 --> 00:15:53.776 A:middle
equipment before.

00:15:54.466 --> 00:15:56.636 A:middle
Now, with the ARKit 3, it just

00:15:56.706 --> 00:15:58.566 A:middle
takes few lines of code and

00:15:58.566 --> 00:16:00.456 A:middle
works right on your iPad or

00:15:58.566 --> 00:16:00.456 A:middle
works right on your iPad or

00:16:00.456 --> 00:16:00.806 A:middle
iPhone.

00:16:03.476 --> 00:16:06.086 A:middle
So, motion capture let's you

00:16:06.126 --> 00:16:08.876 A:middle
track a human body both in 2D

00:16:09.156 --> 00:16:10.026 A:middle
and in 3D.

00:16:10.586 --> 00:16:12.976 A:middle
And it provides you with a

00:16:12.976 --> 00:16:15.186 A:middle
skeleton representation of that

00:16:15.186 --> 00:16:15.676 A:middle
person.

00:16:16.286 --> 00:16:19.786 A:middle
This, for example, enables to

00:16:19.786 --> 00:16:21.256 A:middle
drive a virtual character.

00:16:22.036 --> 00:16:23.796 A:middle
And this is made possible by

00:16:23.796 --> 00:16:24.856 A:middle
advanced machine learning

00:16:24.856 --> 00:16:26.716 A:middle
algorithms running on Apple

00:16:26.716 --> 00:16:27.426 A:middle
Neural Engine.

00:16:28.116 --> 00:16:29.586 A:middle
So it's available on devices

00:16:29.586 --> 00:16:31.496 A:middle
with an A12 or later processor.

00:16:32.656 --> 00:16:34.606 A:middle
Let's look at 2D body detection

00:16:34.606 --> 00:16:35.076 A:middle
first.

00:16:35.676 --> 00:16:39.286 A:middle
How to turn this on?

00:16:40.336 --> 00:16:42.186 A:middle
We have a new frame semantics

00:16:42.186 --> 00:16:43.866 A:middle
option called bodyDetection.

00:16:44.746 --> 00:16:46.396 A:middle
This is supported on the World

00:16:46.396 --> 00:16:48.666 A:middle
Tracking configuration and on

00:16:48.666 --> 00:16:50.206 A:middle
image and orientation tracking

00:16:50.206 --> 00:16:51.416 A:middle
configurations as well.

00:16:52.276 --> 00:16:53.506 A:middle
So you'll simply add this to

00:16:53.506 --> 00:16:55.336 A:middle
your frame semantics and call

00:16:55.336 --> 00:16:56.036 A:middle
run on the session.

00:16:57.636 --> 00:16:58.486 A:middle
Now, let's have a look at the

00:16:58.486 --> 00:17:02.696 A:middle
data we will be getting back.

00:16:58.486 --> 00:17:02.696 A:middle
data we will be getting back.

00:17:02.696 --> 00:17:05.116 A:middle
Every ARFrame delivers an object

00:17:05.165 --> 00:17:07.945 A:middle
of type ARBody2D in the

00:17:07.945 --> 00:17:10.596 A:middle
detectedBody property if a

00:17:10.596 --> 00:17:12.506 A:middle
person was detected.

00:17:13.376 --> 00:17:15.425 A:middle
This object contains a 2D

00:17:15.425 --> 00:17:15.996 A:middle
skeleton.

00:17:17.455 --> 00:17:19.106 A:middle
And the skeleton will provide

00:17:19.106 --> 00:17:21.016 A:middle
you with all the joint landmarks

00:17:21.076 --> 00:17:22.896 A:middle
in normalized image space.

00:17:23.876 --> 00:17:24.896 A:middle
They are being returned in a

00:17:24.896 --> 00:17:26.896 A:middle
flat hierarchy in an array

00:17:27.026 --> 00:17:28.246 A:middle
because this is most efficient

00:17:28.286 --> 00:17:29.036 A:middle
for processing.

00:17:29.846 --> 00:17:31.386 A:middle
But you will also be getting a

00:17:31.386 --> 00:17:32.486 A:middle
skeleton definition.

00:17:33.406 --> 00:17:34.706 A:middle
And thanks to the skeleton

00:17:34.706 --> 00:17:36.416 A:middle
definition, you have all the

00:17:36.416 --> 00:17:38.786 A:middle
information how to interpret the

00:17:38.786 --> 00:17:39.656 A:middle
skeleton data.

00:17:40.446 --> 00:17:41.826 A:middle
In particular, it contains

00:17:41.826 --> 00:17:43.356 A:middle
information about the hierarchy

00:17:43.356 --> 00:17:45.426 A:middle
of joints, for example, the fact

00:17:45.676 --> 00:17:47.466 A:middle
that the hand joint is a child

00:17:47.466 --> 00:17:48.386 A:middle
of the elbow joint.

00:17:49.786 --> 00:17:51.176 A:middle
And you will also be provided

00:17:51.266 --> 00:17:53.486 A:middle
with names for joints that can

00:17:53.486 --> 00:17:55.266 A:middle
then be used for easier access.

00:17:55.936 --> 00:17:57.796 A:middle
So let's have a look at how this

00:17:57.826 --> 00:17:58.256 A:middle
looks like.

00:17:59.056 --> 00:18:00.406 A:middle
Here's a person we detected in

00:17:59.056 --> 00:18:00.406 A:middle
Here's a person we detected in

00:18:00.406 --> 00:18:01.006 A:middle
our frame.

00:18:01.926 --> 00:18:03.526 A:middle
And that's the 2D skeleton

00:18:03.526 --> 00:18:05.426 A:middle
provided by ARKit.

00:18:06.016 --> 00:18:08.256 A:middle
As mentioned before, important

00:18:08.256 --> 00:18:10.056 A:middle
joints are named to make it easy

00:18:10.056 --> 00:18:11.986 A:middle
for you to find out the position

00:18:11.986 --> 00:18:13.036 A:middle
of a particular one you're

00:18:13.036 --> 00:18:14.846 A:middle
interested in, for example, the

00:18:14.846 --> 00:18:17.446 A:middle
head or the right hand.

00:18:18.636 --> 00:18:19.996 A:middle
So this was 2D.

00:18:20.626 --> 00:18:22.646 A:middle
Now, let's have a look at 3D

00:18:22.646 --> 00:18:24.166 A:middle
motion capture.

00:18:25.216 --> 00:18:26.766 A:middle
3D motion capture let's you

00:18:26.766 --> 00:18:29.286 A:middle
track a human body in 3D space

00:18:29.936 --> 00:18:31.046 A:middle
and it provides you with a

00:18:31.266 --> 00:18:32.496 A:middle
three-dimensional skeleton

00:18:32.496 --> 00:18:33.376 A:middle
representation.

00:18:34.656 --> 00:18:36.496 A:middle
It also provides you scale

00:18:36.496 --> 00:18:38.516 A:middle
estimation to let you determine

00:18:38.516 --> 00:18:40.016 A:middle
the size of the person that is

00:18:40.076 --> 00:18:40.696 A:middle
being tracked.

00:18:41.746 --> 00:18:43.866 A:middle
And the 3D skeleton is anchored

00:18:44.186 --> 00:18:45.256 A:middle
in world coordinates.

00:18:46.516 --> 00:18:48.186 A:middle
Let's see how to use this in

00:18:48.186 --> 00:18:48.496 A:middle
API.

00:18:52.006 --> 00:18:53.276 A:middle
We are introducing a new

00:18:53.336 --> 00:18:55.226 A:middle
configuration called

00:18:55.226 --> 00:18:56.806 A:middle
ARBodyTrackingConfiguration.

00:18:58.426 --> 00:19:00.346 A:middle
So this lets you use 3D body

00:18:58.426 --> 00:19:00.346 A:middle
So this lets you use 3D body

00:19:00.346 --> 00:19:02.856 A:middle
tracking but it also provides

00:19:03.186 --> 00:19:05.476 A:middle
the 2D body detection that we

00:19:05.476 --> 00:19:06.476 A:middle
have seen before.

00:19:06.916 --> 00:19:09.346 A:middle
So the frame semantics is turned

00:19:09.346 --> 00:19:10.706 A:middle
on by default in that

00:19:10.706 --> 00:19:11.486 A:middle
configuration.

00:19:12.686 --> 00:19:14.626 A:middle
In addition, this configuration

00:19:14.686 --> 00:19:16.236 A:middle
also tracks the device's

00:19:16.296 --> 00:19:18.666 A:middle
position and orientation and

00:19:18.666 --> 00:19:20.166 A:middle
provides selected World Tracking

00:19:20.166 --> 00:19:21.866 A:middle
features such as plane

00:19:21.866 --> 00:19:23.926 A:middle
estimation or image detection.

00:19:24.746 --> 00:19:26.756 A:middle
So with that, you have even more

00:19:26.756 --> 00:19:28.666 A:middle
possibilities in what you can do

00:19:29.316 --> 00:19:31.226 A:middle
using body tracking in your AR

00:19:32.016 --> 00:19:32.086 A:middle
app.

00:19:33.516 --> 00:19:35.716 A:middle
In order to set it up, you

00:19:35.716 --> 00:19:37.426 A:middle
simply create the body tracking

00:19:37.426 --> 00:19:39.846 A:middle
configuration and run it on your

00:19:39.846 --> 00:19:40.256 A:middle
session.

00:19:41.396 --> 00:19:43.096 A:middle
Note that we also have API to

00:19:43.146 --> 00:19:45.076 A:middle
check if that configuration is

00:19:45.076 --> 00:19:49.136 A:middle
supported on the current device.

00:19:49.306 --> 00:19:51.746 A:middle
So, now, when ARKit is running

00:19:52.076 --> 00:19:54.026 A:middle
and detects a person, it will

00:19:54.026 --> 00:19:57.286 A:middle
add a new type of anchor, an

00:19:57.286 --> 00:19:58.026 A:middle
ARBodyAnchor.

00:20:00.096 --> 00:20:02.366 A:middle
This will be provided to you in

00:20:02.366 --> 00:20:03.546 A:middle
the session that that anchor's

00:20:03.546 --> 00:20:05.186 A:middle
called back just like any other

00:20:05.186 --> 00:20:06.436 A:middle
anchor types that you know.

00:20:07.786 --> 00:20:09.526 A:middle
And just like any anchor, it

00:20:09.526 --> 00:20:11.796 A:middle
also has a transform providing

00:20:11.796 --> 00:20:13.066 A:middle
you with a position and

00:20:13.066 --> 00:20:15.136 A:middle
orientation of the detected

00:20:15.206 --> 00:20:17.306 A:middle
person in world coordinates.

00:20:17.856 --> 00:20:19.576 A:middle
In addition, you will be getting

00:20:19.686 --> 00:20:22.296 A:middle
the scale factor and a reference

00:20:22.676 --> 00:20:23.826 A:middle
to the 3D skeleton.

00:20:24.446 --> 00:20:28.636 A:middle
Let's have a look at how this

00:20:28.636 --> 00:20:29.176 A:middle
looks like.

00:20:29.846 --> 00:20:31.056 A:middle
You see already that it's much

00:20:31.056 --> 00:20:32.916 A:middle
more detailed than the 2D

00:20:32.916 --> 00:20:33.406 A:middle
skeleton.

00:20:34.436 --> 00:20:35.626 A:middle
So the yellow joints are the

00:20:35.626 --> 00:20:37.036 A:middle
ones that will be delivered to

00:20:37.036 --> 00:20:38.976 A:middle
the user with motion capture

00:20:38.976 --> 00:20:39.316 A:middle
data.

00:20:40.396 --> 00:20:42.516 A:middle
The white ones are leaf joints

00:20:42.876 --> 00:20:44.326 A:middle
that our additionally available

00:20:44.326 --> 00:20:45.046 A:middle
in the skeleton.

00:20:46.096 --> 00:20:47.936 A:middle
These are not actively tracked

00:20:48.246 --> 00:20:49.806 A:middle
so the transform is static with

00:20:49.806 --> 00:20:51.156 A:middle
regard to the tracked parent.

00:20:51.846 --> 00:20:53.486 A:middle
But of course, you can direct

00:20:53.486 --> 00:20:55.666 A:middle
the access each of those joints

00:20:56.006 --> 00:20:58.016 A:middle
and retrieve their road

00:20:58.016 --> 00:20:58.766 A:middle
coordinates.

00:21:00.016 --> 00:21:01.606 A:middle
Again, we also have labels for

00:21:01.606 --> 00:21:04.056 A:middle
the most important once, an API

00:21:04.326 --> 00:21:06.066 A:middle
to query them by name so that

00:21:06.066 --> 00:21:08.826 A:middle
you can easily find out about a

00:21:08.826 --> 00:21:10.076 A:middle
particular joint you're

00:21:10.076 --> 00:21:11.456 A:middle
interested in.

00:21:12.016 --> 00:21:13.686 A:middle
Now, I'm sure that you can come

00:21:13.686 --> 00:21:15.486 A:middle
up with a lot of great use cases

00:21:15.746 --> 00:21:18.376 A:middle
for this new API, but I want to

00:21:18.376 --> 00:21:20.026 A:middle
talk about one particular use

00:21:20.056 --> 00:21:21.866 A:middle
case that might be interesting

00:21:21.866 --> 00:21:24.256 A:middle
for many of you, which is

00:21:24.346 --> 00:21:26.036 A:middle
animating 3D characters.

00:21:27.696 --> 00:21:30.256 A:middle
By using ARKit in combination

00:21:30.256 --> 00:21:32.736 A:middle
with RealityKit, you can drive a

00:21:32.736 --> 00:21:34.986 A:middle
model based on the 3D skeleton

00:21:34.986 --> 00:21:36.646 A:middle
pose and it's really simple to

00:21:36.646 --> 00:21:36.836 A:middle
do.

00:21:37.936 --> 00:21:41.486 A:middle
All you need is a rigged mesh.

00:21:42.586 --> 00:21:43.916 A:middle
You can find an example for that

00:21:43.916 --> 00:21:45.506 A:middle
in one of our sample apps that

00:21:45.506 --> 00:21:46.646 A:middle
you can download on the session

00:21:46.646 --> 00:21:48.606 A:middle
home page, but of course, you're

00:21:48.606 --> 00:21:50.586 A:middle
also free to make your own in a

00:21:50.586 --> 00:21:51.886 A:middle
content creation tool of your

00:21:51.886 --> 00:21:52.296 A:middle
choice.

00:21:52.826 --> 00:21:55.836 A:middle
Let's see how easy it is to do

00:21:55.836 --> 00:21:56.576 A:middle
that in code.

00:21:56.856 --> 00:21:58.406 A:middle
It's built right in RealityKit

00:21:58.406 --> 00:21:58.756 A:middle
API.

00:21:59.736 --> 00:22:01.036 A:middle
And the major class we will be

00:21:59.736 --> 00:22:01.036 A:middle
And the major class we will be

00:22:01.036 --> 00:22:03.686 A:middle
using is the BodyTrackedEntity.

00:22:04.296 --> 00:22:08.596 A:middle
So here is a code example using

00:22:08.596 --> 00:22:09.486 A:middle
RealityKit API.

00:22:09.486 --> 00:22:12.056 A:middle
The first thing you're going to

00:22:12.056 --> 00:22:14.676 A:middle
do is create an AnchorEntity of

00:22:14.736 --> 00:22:16.966 A:middle
type body and add this anchor to

00:22:16.966 --> 00:22:17.376 A:middle
the scene.

00:22:18.786 --> 00:22:21.016 A:middle
Next, you're going to load the

00:22:21.016 --> 00:22:21.736 A:middle
model.

00:22:22.036 --> 00:22:23.496 A:middle
In our case, it's called robot.

00:22:24.606 --> 00:22:26.136 A:middle
We're using the asynchronous

00:22:26.136 --> 00:22:27.386 A:middle
loading API for that.

00:22:27.826 --> 00:22:29.076 A:middle
And in the completion handler,

00:22:29.516 --> 00:22:30.776 A:middle
you will be getting the

00:22:30.776 --> 00:22:32.806 A:middle
BodyTrackedEntity that we now

00:22:32.866 --> 00:22:35.076 A:middle
just need to add as a child to

00:22:35.076 --> 00:22:35.906 A:middle
our bodyAnchor.

00:22:36.896 --> 00:22:38.196 A:middle
And that's already all you need

00:22:38.196 --> 00:22:39.126 A:middle
to do.

00:22:39.666 --> 00:22:42.516 A:middle
So as soon as ARKit now adds the

00:22:42.516 --> 00:22:44.536 A:middle
AR body anchor to the session,

00:22:45.846 --> 00:22:48.176 A:middle
the 3D pose of the skeleton will

00:22:48.176 --> 00:22:50.386 A:middle
automatically be applied to the

00:22:50.386 --> 00:22:52.276 A:middle
virtual model in real time.

00:22:53.466 --> 00:22:54.916 A:middle
And that's how simple it is to

00:22:54.916 --> 00:22:56.896 A:middle
do motion capture with ARKit 3.

00:22:58.316 --> 00:23:03.936 A:middle
[Applause] Thank you.

00:22:58.316 --> 00:23:03.936 A:middle
[Applause] Thank you.

00:23:03.936 --> 00:23:06.436 A:middle
So, now, let's talk about

00:23:06.436 --> 00:23:09.226 A:middle
simultaneous front and back

00:23:10.116 --> 00:23:10.526 A:middle
camera.

00:23:10.526 --> 00:23:12.166 A:middle
ARKit lets you to World Tracking

00:23:12.166 --> 00:23:14.316 A:middle
with the back-facing camera and

00:23:14.316 --> 00:23:16.056 A:middle
face tracking with the 2Depth

00:23:16.056 --> 00:23:17.326 A:middle
camera system on the front.

00:23:18.176 --> 00:23:20.116 A:middle
Now, one really highly requested

00:23:20.116 --> 00:23:22.386 A:middle
feature from you was to enable

00:23:22.386 --> 00:23:24.896 A:middle
user experiences with the front

00:23:24.896 --> 00:23:26.456 A:middle
and the back camera together.

00:23:27.316 --> 00:23:29.706 A:middle
Now, with ARKit 3, you can do

00:23:30.506 --> 00:23:30.666 A:middle
that.

00:23:31.956 --> 00:23:33.426 A:middle
So with this new feature, you

00:23:33.426 --> 00:23:35.596 A:middle
can build AR experiences using

00:23:35.676 --> 00:23:37.696 A:middle
both cameras at the same time.

00:23:37.996 --> 00:23:39.336 A:middle
And what this means for you is,

00:23:39.866 --> 00:23:41.666 A:middle
that you can now build two new

00:23:41.716 --> 00:23:42.866 A:middle
types of use cases.

00:23:43.896 --> 00:23:45.976 A:middle
First, you can create World

00:23:45.976 --> 00:23:47.106 A:middle
Tracking experiences.

00:23:47.106 --> 00:23:48.816 A:middle
So, using the back-facing

00:23:48.886 --> 00:23:51.156 A:middle
camera, but also benefit from

00:23:51.156 --> 00:23:53.446 A:middle
face data captured with the

00:23:53.446 --> 00:23:54.096 A:middle
front camera.

00:23:55.106 --> 00:23:56.746 A:middle
And you can create Face Tracking

00:23:56.746 --> 00:23:59.086 A:middle
experiences that make use of the

00:23:59.216 --> 00:24:00.826 A:middle
full device orientation and

00:23:59.216 --> 00:24:00.826 A:middle
full device orientation and

00:24:00.826 --> 00:24:02.496 A:middle
position in 6 degrees of

00:24:02.586 --> 00:24:02.956 A:middle
freedom.

00:24:03.406 --> 00:24:06.896 A:middle
All of this is supported on A12

00:24:06.896 --> 00:24:08.546 A:middle
devices and later.

00:24:08.806 --> 00:24:10.956 A:middle
Let's see an example.

00:24:11.426 --> 00:24:12.586 A:middle
So here we're running World

00:24:12.586 --> 00:24:14.196 A:middle
Tracking with plane estimation

00:24:14.806 --> 00:24:16.836 A:middle
but we also placed a face mesh

00:24:16.836 --> 00:24:18.656 A:middle
on top of the plane and are

00:24:18.656 --> 00:24:20.586 A:middle
updating it in real time with

00:24:20.636 --> 00:24:23.186 A:middle
the facial expressions captured

00:24:23.186 --> 00:24:24.216 A:middle
through the front camera.

00:24:24.216 --> 00:24:27.436 A:middle
So let' see how to use

00:24:27.516 --> 00:24:29.266 A:middle
concurrent front and back camera

00:24:30.296 --> 00:24:31.336 A:middle
in API.

00:24:31.546 --> 00:24:33.026 A:middle
First, let's create a World

00:24:33.026 --> 00:24:34.066 A:middle
Tracking configuration.

00:24:34.416 --> 00:24:36.416 A:middle
Now, the configuration that I

00:24:36.506 --> 00:24:38.706 A:middle
choose determines which camera

00:24:38.706 --> 00:24:40.406 A:middle
stream is actually displayed on

00:24:40.406 --> 00:24:40.926 A:middle
the screen.

00:24:41.206 --> 00:24:43.196 A:middle
So in that case, it would be the

00:24:43.196 --> 00:24:44.256 A:middle
back-facing camera.

00:24:45.666 --> 00:24:47.756 A:middle
Now I'm turning on the new

00:24:47.916 --> 00:24:50.386 A:middle
userFaceTrackingEnabled property

00:24:50.586 --> 00:24:52.046 A:middle
and run the session.

00:24:52.366 --> 00:24:57.046 A:middle
This will cause that I receive

00:24:57.176 --> 00:24:58.156 A:middle
face anchors.

00:24:58.276 --> 00:25:01.326 A:middle
And I can then use any

00:24:58.276 --> 00:25:01.326 A:middle
And I can then use any

00:25:01.326 --> 00:25:02.876 A:middle
information from that anchors

00:25:03.066 --> 00:25:05.496 A:middle
like the face mesh, land shapes,

00:25:05.926 --> 00:25:08.666 A:middle
or the anchors transform itself.

00:25:09.236 --> 00:25:11.496 A:middle
Now, note, since we are working

00:25:11.666 --> 00:25:13.646 A:middle
with world coordinates here, the

00:25:13.646 --> 00:25:15.256 A:middle
user face transfer will be

00:25:15.256 --> 00:25:17.646 A:middle
placed behind the camera, which

00:25:17.646 --> 00:25:19.196 A:middle
means that in order to visualize

00:25:19.256 --> 00:25:20.486 A:middle
the face, you would need to

00:25:20.616 --> 00:25:22.156 A:middle
translate it to a location

00:25:22.156 --> 00:25:25.636 A:middle
somewhere in front of the

00:25:25.846 --> 00:25:26.366 A:middle
camera.

00:25:26.366 --> 00:25:28.316 A:middle
Now, let's also look at the face

00:25:28.366 --> 00:25:29.486 A:middle
tracking configuration.

00:25:30.366 --> 00:25:31.746 A:middle
You create your face tracking

00:25:31.746 --> 00:25:33.646 A:middle
configuration just as you would

00:25:33.686 --> 00:25:35.676 A:middle
do it always and set

00:25:36.056 --> 00:25:37.496 A:middle
worldTrackingEnabled to true.

00:25:38.226 --> 00:25:40.276 A:middle
And then, after you run the

00:25:40.276 --> 00:25:42.876 A:middle
configuration, you can access in

00:25:42.876 --> 00:25:45.216 A:middle
every frame, for example, in the

00:25:45.216 --> 00:25:46.696 A:middle
session that update frame

00:25:46.856 --> 00:25:50.386 A:middle
callback the transform of the

00:25:50.386 --> 00:25:51.586 A:middle
current camera position.

00:25:52.216 --> 00:25:54.506 A:middle
And you can then use that for

00:25:54.616 --> 00:25:55.746 A:middle
whatever use case you have in

00:25:55.746 --> 00:25:56.116 A:middle
mind.

00:25:56.326 --> 00:25:58.626 A:middle
And that's simultaneous front

00:25:58.626 --> 00:26:00.076 A:middle
and back camera in ARKit 3.

00:25:58.626 --> 00:26:00.076 A:middle
and back camera in ARKit 3.

00:26:01.036 --> 00:26:02.376 A:middle
We think that you will be able

00:26:02.526 --> 00:26:04.516 A:middle
to enable many great new use

00:26:04.516 --> 00:26:06.126 A:middle
cases with this new API.

00:26:07.281 --> 00:26:09.281 A:middle
[ Applause ]

00:26:09.546 --> 00:26:09.746 A:middle
Thank you.

00:26:10.421 --> 00:26:12.421 A:middle
[ Applause ]

00:26:12.826 --> 00:26:14.446 A:middle
And now, let me hand it over

00:26:14.446 --> 00:26:16.506 A:middle
Thomas who will tell you all

00:26:16.506 --> 00:26:17.816 A:middle
about collaborative sessions.

00:26:20.516 --> 00:26:22.636 A:middle
[ Applause ]

00:26:23.136 --> 00:26:23.426 A:middle
&gt;&gt; Thank you.

00:26:24.366 --> 00:26:25.056 A:middle
Thank you, Andreas.

00:26:25.416 --> 00:26:26.386 A:middle
Good afternoon, everyone.

00:26:26.736 --> 00:26:28.156 A:middle
My name is Thomas and I'm part

00:26:28.156 --> 00:26:28.876 A:middle
of the ARKit team.

00:26:29.506 --> 00:26:30.526 A:middle
So let's talk about

00:26:30.526 --> 00:26:31.686 A:middle
collaborative sessions.

00:26:32.216 --> 00:26:35.276 A:middle
In ARKit 2, you could create

00:26:35.666 --> 00:26:37.786 A:middle
multiuser experiences with the

00:26:37.786 --> 00:26:39.606 A:middle
ability to save and load world

00:26:39.606 --> 00:26:40.086 A:middle
maps.

00:26:40.726 --> 00:26:42.046 A:middle
You had to save a map on one

00:26:42.046 --> 00:26:43.686 A:middle
device and send it to another

00:26:43.686 --> 00:26:45.756 A:middle
one in order for your users to

00:26:45.756 --> 00:26:47.036 A:middle
jump into the same experience

00:26:47.036 --> 00:26:47.276 A:middle
again.

00:26:48.586 --> 00:26:50.556 A:middle
It was a one map -- one-time map

00:26:50.556 --> 00:26:52.026 A:middle
sharing experience, and after

00:26:52.026 --> 00:26:54.306 A:middle
that point, most of the users

00:26:54.306 --> 00:26:55.186 A:middle
wouldn't be in the same--

00:26:55.296 --> 00:26:56.616 A:middle
wouldn't share the same

00:26:56.616 --> 00:26:57.506 A:middle
information anymore.

00:26:58.006 --> 00:26:59.786 A:middle
Well, with collaborative

00:26:59.786 --> 00:27:02.816 A:middle
sessions in ARKit 3, we now

00:26:59.786 --> 00:27:02.816 A:middle
sessions in ARKit 3, we now

00:27:02.816 --> 00:27:04.416 A:middle
continuously share your mapping

00:27:04.416 --> 00:27:05.886 A:middle
information across the network.

00:27:07.226 --> 00:27:09.016 A:middle
This allows you to create ad hoc

00:27:09.016 --> 00:27:11.366 A:middle
multiuser experiences where your

00:27:11.366 --> 00:27:13.986 A:middle
users are more easily accessing

00:27:13.986 --> 00:27:14.686 A:middle
the same session.

00:27:15.896 --> 00:27:18.076 A:middle
Additionally, we also allow you

00:27:18.826 --> 00:27:20.626 A:middle
to share or we actually share

00:27:20.626 --> 00:27:22.556 A:middle
ARAnchors across all devices.

00:27:23.186 --> 00:27:24.226 A:middle
All those anchors are

00:27:24.226 --> 00:27:26.016 A:middle
identifiable with anchor's

00:27:26.016 --> 00:27:28.216 A:middle
session IDs on those ones.

00:27:29.536 --> 00:27:31.386 A:middle
Note that at this point, most--

00:27:31.386 --> 00:27:32.846 A:middle
all the coordinate systems are

00:27:32.846 --> 00:27:34.016 A:middle
independent from each others

00:27:34.016 --> 00:27:35.116 A:middle
even though we still share the

00:27:35.116 --> 00:27:37.206 A:middle
information under the hood.

00:27:37.406 --> 00:27:38.976 A:middle
Let me show you how this works.

00:27:41.566 --> 00:27:44.156 A:middle
So in this video here, we can

00:27:44.156 --> 00:27:44.956 A:middle
see two users.

00:27:45.016 --> 00:27:46.246 A:middle
Pay attention to the colors.

00:27:46.246 --> 00:27:47.656 A:middle
One user will be showing feature

00:27:47.656 --> 00:27:49.826 A:middle
points in green and another user

00:27:49.826 --> 00:27:51.016 A:middle
will be showing feature points

00:27:51.016 --> 00:27:52.896 A:middle
in red.

00:27:53.086 --> 00:27:53.976 A:middle
As they move around the

00:27:53.976 --> 00:27:56.976 A:middle
environment, they're starting to

00:27:56.976 --> 00:27:59.556 A:middle
map the environment and add more

00:27:59.556 --> 00:28:00.986 A:middle
feature points do it.

00:27:59.556 --> 00:28:00.986 A:middle
feature points do it.

00:28:01.976 --> 00:28:03.746 A:middle
At this point, and this is the

00:28:03.746 --> 00:28:05.316 A:middle
internal representation of their

00:28:05.316 --> 00:28:07.186 A:middle
internal maps, they don't know

00:28:07.186 --> 00:28:08.246 A:middle
about each other's maps.

00:28:08.506 --> 00:28:13.616 A:middle
As they move around, they gather

00:28:13.616 --> 00:28:14.576 A:middle
more feature points.

00:28:17.796 --> 00:28:19.026 A:middle
When they gather more feature

00:28:19.026 --> 00:28:20.776 A:middle
points in the scene and you can

00:28:20.776 --> 00:28:21.876 A:middle
see the internal maps, and pay

00:28:21.876 --> 00:28:23.426 A:middle
attention to the color and their

00:28:23.426 --> 00:28:25.416 A:middle
final matching point, then those

00:28:25.416 --> 00:28:27.256 A:middle
internal map will then merge

00:28:27.256 --> 00:28:28.806 A:middle
into each others and will only

00:28:28.806 --> 00:28:30.716 A:middle
form one map only, which means

00:28:30.716 --> 00:28:32.366 A:middle
that each users will now

00:28:32.366 --> 00:28:34.996 A:middle
understand each others and scene

00:28:34.996 --> 00:28:36.056 A:middle
understanding.

00:28:36.056 --> 00:28:40.206 A:middle
As they move around, they map

00:28:40.206 --> 00:28:41.346 A:middle
even more information.

00:28:42.606 --> 00:28:43.786 A:middle
And they continue sharing that

00:28:43.786 --> 00:28:45.106 A:middle
under the hood.

00:28:46.316 --> 00:28:48.766 A:middle
Additionally, ARKit 3 provides

00:28:48.766 --> 00:28:51.016 A:middle
you with like AR participant

00:28:51.016 --> 00:28:52.596 A:middle
anchors that allows you to

00:28:53.056 --> 00:28:54.786 A:middle
understand where another user is

00:28:54.786 --> 00:28:55.906 A:middle
in real time in your

00:28:55.906 --> 00:28:56.386 A:middle
environment.

00:28:57.226 --> 00:28:59.106 A:middle
This is really handy if you want

00:28:59.106 --> 00:29:00.646 A:middle
to display for example an icon

00:28:59.106 --> 00:29:00.646 A:middle
to display for example an icon

00:29:00.646 --> 00:29:01.796 A:middle
or something to represent that

00:29:01.796 --> 00:29:02.066 A:middle
user.

00:29:02.616 --> 00:29:07.436 A:middle
As mentioned earlier, ARKit 3

00:29:07.436 --> 00:29:09.076 A:middle
also share ARAnchors under the

00:29:09.076 --> 00:29:11.656 A:middle
hood, meaning that if you share

00:29:11.656 --> 00:29:13.406 A:middle
or add an anchor on one device,

00:29:13.406 --> 00:29:15.026 A:middle
it will automatically show up on

00:29:15.026 --> 00:29:15.696 A:middle
the other device.

00:29:17.106 --> 00:29:18.126 A:middle
Let's now have a look at how it

00:29:18.126 --> 00:29:18.846 A:middle
works in code.

00:29:20.576 --> 00:29:22.066 A:middle
As Andreas mentioned earlier,

00:29:22.096 --> 00:29:23.596 A:middle
ARKit is really well integrated

00:29:23.596 --> 00:29:24.406 A:middle
with RealityKit.

00:29:25.386 --> 00:29:26.876 A:middle
If you want to enable the

00:29:26.876 --> 00:29:27.916 A:middle
collaborative session with

00:29:27.916 --> 00:29:29.366 A:middle
RealityKit, it's pretty simple.

00:29:30.276 --> 00:29:31.556 A:middle
You first need to setup your

00:29:31.556 --> 00:29:33.126 A:middle
Multipeer Connectivity session.

00:29:33.506 --> 00:29:36.326 A:middle
Multipeer Connectivity framework

00:29:36.326 --> 00:29:37.346 A:middle
is an Apple framework that

00:29:37.346 --> 00:29:38.676 A:middle
allows you for discovery and

00:29:38.676 --> 00:29:40.196 A:middle
peer-to-peer connections.

00:29:40.196 --> 00:29:43.126 A:middle
Then, you need to pass this

00:29:43.126 --> 00:29:45.116 A:middle
Multipeer Connectivity session

00:29:45.116 --> 00:29:47.416 A:middle
to the AR scenes view

00:29:47.746 --> 00:29:48.826 A:middle
synchronization service.

00:29:49.316 --> 00:29:53.806 A:middle
And finally, as every ARKit

00:29:53.806 --> 00:29:55.456 A:middle
experiences, you have to setup

00:29:55.506 --> 00:29:55.786 A:middle
your

00:29:55.786 --> 00:29:57.286 A:middle
ARWorldTrackingConfiguration,

00:29:57.436 --> 00:29:59.576 A:middle
set the isCollaborationEnabled

00:29:59.576 --> 00:30:01.566 A:middle
flag to true and run that

00:29:59.576 --> 00:30:01.566 A:middle
flag to true and run that

00:30:01.566 --> 00:30:02.706 A:middle
configuration on the session.

00:30:03.236 --> 00:30:03.836 A:middle
And that's it.

00:30:06.696 --> 00:30:08.196 A:middle
So what's going to happen then?

00:30:09.416 --> 00:30:11.046 A:middle
So ARKit when sending up the

00:30:11.046 --> 00:30:13.286 A:middle
isCollaborationEnabled flag to

00:30:13.796 --> 00:30:16.336 A:middle
true will essentially -- and

00:30:16.336 --> 00:30:17.466 A:middle
running that configuration on

00:30:17.466 --> 00:30:18.836 A:middle
the session -- will essentially

00:30:18.836 --> 00:30:22.506 A:middle
create a new method on the

00:30:22.506 --> 00:30:24.006 A:middle
ARSessionDelegate for you to be

00:30:24.006 --> 00:30:24.936 A:middle
transferring that data.

00:30:25.576 --> 00:30:27.016 A:middle
In the RealityKit use case,

00:30:27.016 --> 00:30:28.686 A:middle
we'll take care of it, but if

00:30:28.686 --> 00:30:29.906 A:middle
you're using ARKit in another

00:30:29.906 --> 00:30:31.646 A:middle
renderer, then we'll-- you'll

00:30:31.646 --> 00:30:33.056 A:middle
have to send that data across

00:30:33.056 --> 00:30:33.596 A:middle
the network.

00:30:35.576 --> 00:30:36.916 A:middle
This data is called AR

00:30:36.916 --> 00:30:37.756 A:middle
collaboration data.

00:30:39.006 --> 00:30:41.036 A:middle
ARKit can create at any point in

00:30:41.036 --> 00:30:43.226 A:middle
time an AR collaboration data

00:30:43.226 --> 00:30:45.106 A:middle
package that you then have to

00:30:45.106 --> 00:30:47.526 A:middle
forward again to other users.

00:30:47.886 --> 00:30:49.396 A:middle
This is not limited to only two

00:30:49.396 --> 00:30:49.886 A:middle
users.

00:30:49.886 --> 00:30:51.916 A:middle
You can have a large amount of

00:30:51.916 --> 00:30:53.456 A:middle
users in that session.

00:30:54.676 --> 00:30:57.676 A:middle
During that process, ARKit will

00:30:57.676 --> 00:30:59.196 A:middle
generate additional AR

00:30:59.196 --> 00:31:00.746 A:middle
collaboration data that you will

00:30:59.196 --> 00:31:00.746 A:middle
collaboration data that you will

00:31:00.746 --> 00:31:02.776 A:middle
have to forward to other devices

00:31:02.776 --> 00:31:03.816 A:middle
and broadcast that data.

00:31:07.876 --> 00:31:09.406 A:middle
Let's see how that works in

00:31:09.906 --> 00:31:10.006 A:middle
code.

00:31:11.296 --> 00:31:13.236 A:middle
So you first need to setup your

00:31:13.236 --> 00:31:14.736 A:middle
multipeer connectivity in this

00:31:14.736 --> 00:31:17.236 A:middle
example, or you can also setup

00:31:17.236 --> 00:31:18.606 A:middle
any framework-- any network

00:31:18.606 --> 00:31:19.716 A:middle
framework of your choice and

00:31:19.716 --> 00:31:20.856 A:middle
make sure that your devices are

00:31:20.856 --> 00:31:21.866 A:middle
sharing the same session.

00:31:22.336 --> 00:31:25.676 A:middle
When they do, then you need to

00:31:25.676 --> 00:31:26.366 A:middle
enable the

00:31:26.366 --> 00:31:27.826 A:middle
ARWorldTrackingConfiguration

00:31:27.826 --> 00:31:29.686 A:middle
with this isCollaborationEnabled

00:31:29.686 --> 00:31:30.396 A:middle
flag to true.

00:31:31.706 --> 00:31:33.466 A:middle
When this is the case, you need

00:31:33.466 --> 00:31:34.436 A:middle
to run-- then run the

00:31:34.436 --> 00:31:35.216 A:middle
configuration.

00:31:36.296 --> 00:31:38.856 A:middle
At this point, you will then

00:31:38.856 --> 00:31:40.196 A:middle
have a new method available

00:31:40.196 --> 00:31:41.806 A:middle
under delegate for you where you

00:31:41.806 --> 00:31:43.356 A:middle
will be receiving some

00:31:43.356 --> 00:31:44.296 A:middle
collaboration data.

00:31:44.776 --> 00:31:49.066 A:middle
Upon receiving that data, you

00:31:49.066 --> 00:31:50.296 A:middle
need to make sure to broadcast

00:31:50.296 --> 00:31:52.406 A:middle
it on the network to other users

00:31:52.726 --> 00:31:53.546 A:middle
that are also in this

00:31:53.576 --> 00:31:54.496 A:middle
collaboration session.

00:31:54.976 --> 00:31:58.376 A:middle
Upon the reception of that data

00:31:58.826 --> 00:32:00.516 A:middle
on the other devices, you need

00:31:58.826 --> 00:32:00.516 A:middle
on the other devices, you need

00:32:00.516 --> 00:32:02.936 A:middle
to update URL session so that it

00:32:02.936 --> 00:32:04.016 A:middle
knows about this new data.

00:32:04.406 --> 00:32:05.566 A:middle
And that's it.

00:32:07.436 --> 00:32:09.266 A:middle
This collaboration session data

00:32:10.276 --> 00:32:12.456 A:middle
automatically exchange all the

00:32:12.456 --> 00:32:14.226 A:middle
user created ARAnchors.

00:32:15.566 --> 00:32:18.126 A:middle
Each anchors is identifiable by

00:32:18.126 --> 00:32:20.506 A:middle
a session ID so that you can

00:32:20.506 --> 00:32:21.706 A:middle
make sure to understand from

00:32:21.706 --> 00:32:23.686 A:middle
which device or which AR session

00:32:24.136 --> 00:32:25.316 A:middle
the anchor is coming from.

00:32:25.836 --> 00:32:28.426 A:middle
As mentioned earlier, the

00:32:28.426 --> 00:32:30.386 A:middle
ARParticipantAnchor represents

00:32:30.386 --> 00:32:32.626 A:middle
in real time the participant

00:32:32.626 --> 00:32:34.586 A:middle
position which can be very handy

00:32:34.586 --> 00:32:35.706 A:middle
in some of your use cases.

00:32:39.296 --> 00:32:40.536 A:middle
So this is how you create

00:32:40.536 --> 00:32:41.436 A:middle
collaborative sessions.

00:32:42.516 --> 00:32:48.546 A:middle
[ Applause ]

00:32:49.046 --> 00:32:50.906 A:middle
Let's now talk about coaching.

00:32:51.636 --> 00:32:53.316 A:middle
When you create an experience,

00:32:53.556 --> 00:32:55.586 A:middle
an AR experience, coaching is

00:32:55.586 --> 00:32:56.436 A:middle
really important.

00:32:57.056 --> 00:32:58.056 A:middle
You really want to guide your

00:32:58.056 --> 00:33:00.336 A:middle
users whether they are new or

00:32:58.056 --> 00:33:00.336 A:middle
users whether they are new or

00:33:00.336 --> 00:33:02.546 A:middle
returning users into your AR

00:33:02.546 --> 00:33:03.176 A:middle
experience.

00:33:04.036 --> 00:33:05.256 A:middle
It's not a trivial process.

00:33:05.656 --> 00:33:07.216 A:middle
And sometimes, it's hard for you

00:33:07.216 --> 00:33:09.256 A:middle
to understand or even to guide

00:33:09.256 --> 00:33:11.766 A:middle
the user to that new experience.

00:33:13.136 --> 00:33:14.626 A:middle
Throughout that process, you

00:33:14.626 --> 00:33:15.726 A:middle
have to react to certain

00:33:15.726 --> 00:33:16.656 A:middle
tracking events.

00:33:16.846 --> 00:33:17.956 A:middle
Sometimes the tracking gets

00:33:17.956 --> 00:33:19.216 A:middle
limited because the user moves

00:33:19.216 --> 00:33:21.836 A:middle
way too fast.

00:33:21.836 --> 00:33:23.266 A:middle
So far, we've been providing you

00:33:23.266 --> 00:33:25.336 A:middle
with human interface guideline

00:33:25.976 --> 00:33:28.276 A:middle
that allowed you to provide some

00:33:28.276 --> 00:33:29.536 A:middle
guidelines for the onboarding

00:33:29.536 --> 00:33:30.166 A:middle
experiences.

00:33:31.736 --> 00:33:33.346 A:middle
Well this year, we're embedding

00:33:33.346 --> 00:33:34.836 A:middle
that in the UI view

00:33:38.206 --> 00:33:39.686 A:middle
and we call it the AR Coaching

00:33:39.686 --> 00:33:39.906 A:middle
View.

00:33:40.406 --> 00:33:43.746 A:middle
This is a built-in overlay that

00:33:43.746 --> 00:33:45.186 A:middle
you can directly embed in your

00:33:45.186 --> 00:33:46.106 A:middle
AR applications.

00:33:46.746 --> 00:33:48.386 A:middle
It guides your users to a really

00:33:48.386 --> 00:33:49.646 A:middle
good tracking experience.

00:33:50.176 --> 00:33:53.146 A:middle
It provides a consistent design

00:33:53.676 --> 00:33:55.096 A:middle
throughout your applications so

00:33:55.096 --> 00:33:56.406 A:middle
that your users are very

00:33:56.406 --> 00:33:57.846 A:middle
familiar with it.

00:33:58.496 --> 00:33:59.886 A:middle
You actually may have seen that

00:33:59.886 --> 00:34:00.726 A:middle
design before.

00:33:59.886 --> 00:34:00.726 A:middle
design before.

00:34:00.946 --> 00:34:02.836 A:middle
We have it AR Quick Look and in

00:34:03.346 --> 00:34:03.496 A:middle
Measure.

00:34:05.296 --> 00:34:07.906 A:middle
This new UI overlay

00:34:07.906 --> 00:34:09.906 A:middle
automatically activates and

00:34:09.906 --> 00:34:11.446 A:middle
deactivate base on the different

00:34:11.446 --> 00:34:12.246 A:middle
tracking events.

00:34:13.036 --> 00:34:14.826 A:middle
And you can also adjust certain

00:34:14.826 --> 00:34:15.576 A:middle
coaching goals.

00:34:16.255 --> 00:34:17.146 A:middle
Let's have a look at some of

00:34:17.146 --> 00:34:17.686 A:middle
those overlays.

00:34:17.686 --> 00:34:21.235 A:middle
So in the AR Coaching View, we

00:34:21.985 --> 00:34:23.206 A:middle
have multiple overlays.

00:34:24.036 --> 00:34:25.846 A:middle
The onboarding UI provides the

00:34:25.846 --> 00:34:28.065 A:middle
users the ability to understand

00:34:28.065 --> 00:34:29.235 A:middle
what you're looking for, and in

00:34:29.235 --> 00:34:30.246 A:middle
this case, surfaces.

00:34:30.835 --> 00:34:32.106 A:middle
Most of the time your experience

00:34:32.106 --> 00:34:33.335 A:middle
require a surface to place

00:34:33.335 --> 00:34:34.235 A:middle
content on to it.

00:34:34.235 --> 00:34:36.556 A:middle
So if you enable plane detection

00:34:36.556 --> 00:34:37.956 A:middle
on your configuration, then this

00:34:37.956 --> 00:34:39.146 A:middle
overlay will automatically show

00:34:39.146 --> 00:34:39.946 A:middle
up.

00:34:41.335 --> 00:34:42.846 A:middle
Secondly, we have another

00:34:42.846 --> 00:34:45.576 A:middle
overlay that provides the user

00:34:45.576 --> 00:34:46.936 A:middle
with the ability to understand

00:34:46.936 --> 00:34:48.005 A:middle
that they have to move around a

00:34:48.005 --> 00:34:49.126 A:middle
little bit more to gather

00:34:49.126 --> 00:34:50.746 A:middle
additional features so that

00:34:50.746 --> 00:34:52.565 A:middle
tracking can works best.

00:34:53.216 --> 00:34:55.426 A:middle
And then finally, we have

00:34:55.426 --> 00:34:57.536 A:middle
another overlay which helps your

00:34:57.536 --> 00:34:59.156 A:middle
user relocalize against certain

00:34:59.156 --> 00:35:00.766 A:middle
environments in case of your

00:34:59.156 --> 00:35:00.766 A:middle
environments in case of your

00:35:00.766 --> 00:35:03.116 A:middle
lost tracking for example, or if

00:35:03.116 --> 00:35:03.936 A:middle
the app went into the

00:35:03.936 --> 00:35:04.446 A:middle
background.

00:35:05.066 --> 00:35:08.306 A:middle
Let's look at one example.

00:35:11.346 --> 00:35:12.876 A:middle
So, in this example, we're

00:35:12.876 --> 00:35:14.646 A:middle
asking a user to move the device

00:35:14.646 --> 00:35:16.466 A:middle
around to find a new plane, and

00:35:16.466 --> 00:35:17.886 A:middle
as soon as the user is moving

00:35:17.886 --> 00:35:19.566 A:middle
around and gathering more

00:35:19.566 --> 00:35:21.636 A:middle
feature, then the content can be

00:35:21.706 --> 00:35:23.446 A:middle
placed and the view deactivates

00:35:23.446 --> 00:35:24.116 A:middle
automatically.

00:35:24.596 --> 00:35:25.586 A:middle
So you don't have to do

00:35:25.586 --> 00:35:26.086 A:middle
anything.

00:35:26.086 --> 00:35:26.836 A:middle
Everything is handled

00:35:26.836 --> 00:35:27.526 A:middle
automatically.

00:35:28.516 --> 00:35:32.966 A:middle
[ Applause ]

00:35:33.466 --> 00:35:36.166 A:middle
Let's have a look at how you can

00:35:36.166 --> 00:35:36.646 A:middle
set that up.

00:35:37.376 --> 00:35:38.886 A:middle
Again, this is really easy.

00:35:39.636 --> 00:35:41.686 A:middle
As is it's a simple UI view, you

00:35:41.686 --> 00:35:43.306 A:middle
have to set it up as a child of

00:35:43.346 --> 00:35:44.506 A:middle
another UI view.

00:35:44.766 --> 00:35:46.386 A:middle
Ideally, you set it as a child

00:35:46.386 --> 00:35:47.826 A:middle
of the AR view.

00:35:48.436 --> 00:35:50.676 A:middle
Then, you need to connect this

00:35:50.676 --> 00:35:53.036 A:middle
session to the coaching view so

00:35:53.036 --> 00:35:54.076 A:middle
that the coaching view knows

00:35:54.076 --> 00:35:57.386 A:middle
what events to react to.

00:35:57.386 --> 00:35:58.296 A:middle
Or you need to connect the

00:35:58.296 --> 00:36:00.866 A:middle
session provider outlet of the

00:35:58.296 --> 00:36:00.866 A:middle
session provider outlet of the

00:36:00.866 --> 00:36:02.016 A:middle
coaching view to the session

00:36:02.016 --> 00:36:04.326 A:middle
provider itself if you're using

00:36:04.326 --> 00:36:07.556 A:middle
a storyboard for example.

00:36:07.626 --> 00:36:09.416 A:middle
Optionally, you can set a bunch

00:36:09.416 --> 00:36:10.556 A:middle
of delegates if you want to

00:36:10.556 --> 00:36:12.336 A:middle
react to certain events that the

00:36:12.336 --> 00:36:13.156 A:middle
view is giving you.

00:36:13.686 --> 00:36:18.146 A:middle
And finally, you can also

00:36:18.146 --> 00:36:19.706 A:middle
provide a set of specific

00:36:19.706 --> 00:36:21.606 A:middle
coaching goals if you want to

00:36:21.606 --> 00:36:23.166 A:middle
disable certain functionalities.

00:36:23.686 --> 00:36:26.596 A:middle
Let's look at some of those

00:36:26.596 --> 00:36:26.976 A:middle
delegates.

00:36:30.296 --> 00:36:32.666 A:middle
So we have three new methods on

00:36:32.666 --> 00:36:33.806 A:middle
the AR Coaching View--

00:36:33.806 --> 00:36:35.346 A:middle
CoachingOverlayViewDelegate.

00:36:35.826 --> 00:36:37.146 A:middle
Two of them can react to

00:36:37.146 --> 00:36:38.516 A:middle
activation and deactivation.

00:36:38.516 --> 00:36:40.476 A:middle
So you can choose if you want to

00:36:40.476 --> 00:36:41.636 A:middle
still enable that throughout the

00:36:41.636 --> 00:36:43.016 A:middle
experience or if you think, for

00:36:43.016 --> 00:36:45.376 A:middle
example, that if a user had that

00:36:45.466 --> 00:36:47.306 A:middle
once, it doesn't need to know

00:36:47.306 --> 00:36:47.696 A:middle
anymore.

00:36:49.106 --> 00:36:50.916 A:middle
Additionally, you can react to

00:36:50.916 --> 00:36:52.866 A:middle
certain relocalization abort

00:36:52.866 --> 00:36:56.946 A:middle
requests by default the UI view,

00:36:56.946 --> 00:36:58.506 A:middle
the coaching view will actually

00:36:58.506 --> 00:37:01.106 A:middle
give you or give your users a

00:36:58.506 --> 00:37:01.106 A:middle
give you or give your users a

00:37:01.106 --> 00:37:02.946 A:middle
new UI bottom that-- where they

00:37:03.066 --> 00:37:04.266 A:middle
can actually relocalize and

00:37:04.266 --> 00:37:05.636 A:middle
restart the session or reset the

00:37:05.636 --> 00:37:06.786 A:middle
tracking for example.

00:37:07.806 --> 00:37:09.676 A:middle
So this new view is really handy

00:37:09.756 --> 00:37:11.746 A:middle
for your applications so that

00:37:11.746 --> 00:37:12.596 A:middle
you can make sure that you've

00:37:12.596 --> 00:37:14.006 A:middle
got that consistent design and

00:37:14.006 --> 00:37:14.676 A:middle
help your users.

00:37:15.306 --> 00:37:18.196 A:middle
Let's now talk about Face

00:37:18.196 --> 00:37:18.596 A:middle
Tracking.

00:37:19.316 --> 00:37:21.996 A:middle
In ARKit 1, we enabled Face

00:37:21.996 --> 00:37:23.306 A:middle
Tracking with the ability to

00:37:23.306 --> 00:37:24.386 A:middle
track one face.

00:37:25.396 --> 00:37:27.456 A:middle
While in ARKit 2, we have the

00:37:27.456 --> 00:37:28.786 A:middle
ability to the multi-face

00:37:28.786 --> 00:37:30.186 A:middle
tracking up to three faces

00:37:30.216 --> 00:37:31.516 A:middle
concurrently.

00:37:32.536 --> 00:37:36.476 A:middle
Additionally, you can also make

00:37:36.476 --> 00:37:38.196 A:middle
sure to recognize the person

00:37:38.196 --> 00:37:39.576 A:middle
when he leaves the frame and

00:37:39.576 --> 00:37:41.466 A:middle
comes back again giving you the

00:37:41.466 --> 00:37:43.656 A:middle
same face anchor ID again.

00:37:47.516 --> 00:37:52.546 A:middle
[ Applause ]

00:37:53.046 --> 00:37:55.796 A:middle
So multi-Face Tracking tracks up

00:37:55.796 --> 00:37:59.036 A:middle
to three faces concurrently and

00:37:59.186 --> 00:38:00.826 A:middle
provides you with a persistent

00:37:59.186 --> 00:38:00.826 A:middle
provides you with a persistent

00:38:00.826 --> 00:38:02.986 A:middle
face anchor ID so that you can

00:38:02.986 --> 00:38:04.496 A:middle
make sure to recognize one user

00:38:04.496 --> 00:38:05.376 A:middle
throughout the session.

00:38:06.016 --> 00:38:07.616 A:middle
If you restart a new session,

00:38:08.436 --> 00:38:09.846 A:middle
then this ID disappears and a

00:38:09.846 --> 00:38:11.206 A:middle
new one comes up.

00:38:12.536 --> 00:38:14.266 A:middle
To enable this, this is really

00:38:14.266 --> 00:38:14.776 A:middle
easy.

00:38:15.446 --> 00:38:17.036 A:middle
We have two new properties on

00:38:17.036 --> 00:38:18.856 A:middle
the ARFaceTrackingConfiguration.

00:38:20.206 --> 00:38:21.386 A:middle
The first one allows you to

00:38:21.386 --> 00:38:23.756 A:middle
query how many multiple faces

00:38:23.756 --> 00:38:25.386 A:middle
can be tracked concurrently in

00:38:25.386 --> 00:38:26.646 A:middle
one session on that specific

00:38:26.646 --> 00:38:27.286 A:middle
device.

00:38:27.846 --> 00:38:29.416 A:middle
And the other one allows you to

00:38:29.416 --> 00:38:31.046 A:middle
set the number of track faces

00:38:31.096 --> 00:38:31.766 A:middle
that you want to track

00:38:31.766 --> 00:38:32.356 A:middle
concurrently.

00:38:32.856 --> 00:38:35.976 A:middle
And that's Multi-Face Tracking.

00:38:36.516 --> 00:38:40.956 A:middle
[ Applause ]

00:38:41.456 --> 00:38:43.096 A:middle
Let's now talk about a new

00:38:43.246 --> 00:38:45.306 A:middle
tracking configuration that we

00:38:45.806 --> 00:38:47.506 A:middle
called ARPositional

00:38:47.506 --> 00:38:48.556 A:middle
TrackingConfiguration.

00:38:49.486 --> 00:38:50.606 A:middle
So this new tracking

00:38:50.606 --> 00:38:53.416 A:middle
configuration is intended for

00:38:53.416 --> 00:38:54.906 A:middle
tracking only use cases.

00:38:55.846 --> 00:38:58.096 A:middle
You often had a use case where

00:38:58.096 --> 00:39:00.066 A:middle
it didn't really need the camera

00:38:58.096 --> 00:39:00.066 A:middle
it didn't really need the camera

00:39:00.096 --> 00:39:01.416 A:middle
backdrop to be rendered for

00:39:01.416 --> 00:39:01.846 A:middle
example.

00:39:03.206 --> 00:39:04.786 A:middle
Well, this is made for that use

00:39:04.826 --> 00:39:05.146 A:middle
case.

00:39:06.376 --> 00:39:07.816 A:middle
We can achieve a low power

00:39:07.816 --> 00:39:09.956 A:middle
consumption with the ability to

00:39:09.956 --> 00:39:13.006 A:middle
lower the capture frame rate and

00:39:13.006 --> 00:39:15.426 A:middle
also the camera resolution by

00:39:15.426 --> 00:39:16.596 A:middle
still keeping your rendering

00:39:16.596 --> 00:39:18.546 A:middle
rate at 60 hertz.

00:39:21.956 --> 00:39:24.536 A:middle
Next, let's talk about some of

00:39:24.566 --> 00:39:25.376 A:middle
the scene understanding

00:39:25.376 --> 00:39:27.626 A:middle
improvements we've made this

00:39:28.476 --> 00:39:28.636 A:middle
year.

00:39:29.966 --> 00:39:31.166 A:middle
Image detection and image

00:39:31.166 --> 00:39:32.216 A:middle
tracking has been around for

00:39:32.216 --> 00:39:33.366 A:middle
some time now.

00:39:34.086 --> 00:39:35.816 A:middle
We can now this year detect up

00:39:35.816 --> 00:39:38.056 A:middle
to 100 images at the same time.

00:39:38.616 --> 00:39:42.226 A:middle
We also provide you with the

00:39:42.316 --> 00:39:44.316 A:middle
ability to detect the scale of

00:39:44.316 --> 00:39:46.306 A:middle
an printed image for example.

00:39:47.386 --> 00:39:49.016 A:middle
Oftentimes, when new application

00:39:49.016 --> 00:39:51.346 A:middle
require a user to use an image

00:39:51.346 --> 00:39:52.696 A:middle
to place content and scale that

00:39:52.696 --> 00:39:55.216 A:middle
content accordingly, the image

00:39:55.216 --> 00:39:56.226 A:middle
might be printed with a

00:39:56.226 --> 00:39:57.456 A:middle
different size for example or

00:39:57.456 --> 00:39:58.726 A:middle
different paper size.

00:39:59.426 --> 00:40:01.086 A:middle
With this automatic scale

00:39:59.426 --> 00:40:01.086 A:middle
With this automatic scale

00:40:01.086 --> 00:40:03.516 A:middle
estimation, you can now detect

00:40:03.516 --> 00:40:05.986 A:middle
the physical size and adjust the

00:40:05.986 --> 00:40:06.736 A:middle
scale accordingly.

00:40:06.736 --> 00:40:10.786 A:middle
We also have the ability to

00:40:10.786 --> 00:40:14.356 A:middle
query at run time the quality of

00:40:14.356 --> 00:40:15.666 A:middle
an image that you're passing to

00:40:15.666 --> 00:40:17.196 A:middle
ARKit when you want to create a

00:40:17.196 --> 00:40:19.096 A:middle
new AR reference image.

00:40:21.516 --> 00:40:23.376 A:middle
We've also made improvements to

00:40:23.376 --> 00:40:24.806 A:middle
our object detection algorithms.

00:40:25.966 --> 00:40:27.756 A:middle
With machine learning, we can

00:40:27.756 --> 00:40:29.536 A:middle
enhance that object detection

00:40:29.536 --> 00:40:32.656 A:middle
algorithms and provide you with

00:40:32.656 --> 00:40:34.896 A:middle
a faster recognition, and also

00:40:34.896 --> 00:40:36.626 A:middle
in more robust environments, in

00:40:36.626 --> 00:40:37.466 A:middle
more-- in different

00:40:37.466 --> 00:40:38.006 A:middle
environments.

00:40:38.326 --> 00:40:39.976 A:middle
Oftentimes, you had to scan a

00:40:40.286 --> 00:40:41.396 A:middle
specific object in an

00:40:41.396 --> 00:40:42.346 A:middle
environment so that it works

00:40:42.346 --> 00:40:43.456 A:middle
perfectly in another one.

00:40:44.226 --> 00:40:45.116 A:middle
Now, this is a bit more

00:40:45.116 --> 00:40:45.586 A:middle
flexible.

00:40:48.456 --> 00:40:51.166 A:middle
Finally, another area of scene

00:40:51.166 --> 00:40:52.246 A:middle
understanding which is really

00:40:52.246 --> 00:40:54.616 A:middle
important is plane estimation.

00:40:55.376 --> 00:40:56.536 A:middle
Oftentimes, you need plane

00:40:56.536 --> 00:40:58.706 A:middle
estimation to place content.

00:40:58.856 --> 00:40:59.926 A:middle
Well, with machine learning,

00:41:00.476 --> 00:41:01.686 A:middle
we're actually making that even

00:41:01.686 --> 00:41:04.006 A:middle
more accurate and we're making

00:41:04.006 --> 00:41:05.956 A:middle
that even more robust to detect

00:41:05.956 --> 00:41:06.996 A:middle
planes and faster.

00:41:07.566 --> 00:41:10.356 A:middle
Let's have a look at an example.

00:41:10.986 --> 00:41:16.406 A:middle
With machine learning, not only

00:41:16.406 --> 00:41:18.756 A:middle
we can extend those planes on

00:41:18.756 --> 00:41:19.956 A:middle
the ground when features are

00:41:19.956 --> 00:41:21.886 A:middle
detected but the flow is

00:41:21.886 --> 00:41:23.466 A:middle
actually going even further.

00:41:23.746 --> 00:41:26.056 A:middle
But we can also with the ability

00:41:26.056 --> 00:41:27.526 A:middle
to detect-- we also have the

00:41:27.526 --> 00:41:29.676 A:middle
ability detect walls on the side

00:41:30.026 --> 00:41:31.276 A:middle
when no feature points our

00:41:31.276 --> 00:41:31.676 A:middle
present.

00:41:32.166 --> 00:41:33.576 A:middle
And this is thanks to machine

00:41:34.076 --> 00:41:34.276 A:middle
learning.

00:41:36.516 --> 00:41:41.376 A:middle
[ Applause ]

00:41:41.876 --> 00:41:44.056 A:middle
As you can see here-- As you

00:41:44.056 --> 00:41:45.146 A:middle
can-- You saw on the previous

00:41:45.146 --> 00:41:46.646 A:middle
video, we had a couple of

00:41:46.696 --> 00:41:48.326 A:middle
classifications on the planes.

00:41:49.026 --> 00:41:50.166 A:middle
Well, this is done again with

00:41:50.166 --> 00:41:50.886 A:middle
machine learning.

00:41:50.996 --> 00:41:52.336 A:middle
And last year, we introduced

00:41:52.816 --> 00:41:54.446 A:middle
five different classifications,

00:41:55.136 --> 00:41:56.956 A:middle
wall, floor, ceiling, table, and

00:41:56.956 --> 00:41:57.366 A:middle
seat.

00:41:58.466 --> 00:41:59.806 A:middle
Well, this year, we're adding

00:41:59.936 --> 00:42:01.026 A:middle
two additional ones.

00:41:59.936 --> 00:42:01.026 A:middle
two additional ones.

00:42:01.406 --> 00:42:03.106 A:middle
We are adding the ability to

00:42:03.106 --> 00:42:05.886 A:middle
detect doors and windows.

00:42:07.296 --> 00:42:09.406 A:middle
As mentioned earlier, plane

00:42:09.406 --> 00:42:10.676 A:middle
classification is really

00:42:10.676 --> 00:42:12.526 A:middle
important-- Or plane estimation

00:42:12.526 --> 00:42:13.566 A:middle
is really important to place

00:42:13.566 --> 00:42:14.696 A:middle
content on your-- in the world.

00:42:14.696 --> 00:42:16.586 A:middle
This is actually ideal for

00:42:16.586 --> 00:42:17.256 A:middle
object placement.

00:42:17.526 --> 00:42:18.716 A:middle
You always your objects to be

00:42:18.716 --> 00:42:19.566 A:middle
placed on a surface.

00:42:20.896 --> 00:42:21.916 A:middle
Well, this year with the new

00:42:21.916 --> 00:42:25.126 A:middle
raycasting API, you can now even

00:42:25.126 --> 00:42:26.556 A:middle
easier place your content like

00:42:26.746 --> 00:42:28.976 A:middle
more precisely and is even more

00:42:28.976 --> 00:42:29.506 A:middle
flexible.

00:42:30.866 --> 00:42:32.486 A:middle
It supports any kind of surface

00:42:32.486 --> 00:42:32.956 A:middle
alignment.

00:42:33.286 --> 00:42:34.566 A:middle
So you're not always bound to

00:42:34.566 --> 00:42:36.116 A:middle
vertical and horizontal anymore.

00:42:38.156 --> 00:42:40.036 A:middle
But also, you can track your

00:42:40.036 --> 00:42:40.966 A:middle
raycast all the time.

00:42:42.716 --> 00:42:44.466 A:middle
Meaning that as ARKit-- or as

00:42:44.466 --> 00:42:45.486 A:middle
you move your device around,

00:42:45.486 --> 00:42:47.066 A:middle
ARKit detects more information

00:42:47.066 --> 00:42:49.456 A:middle
about the environment, it can

00:42:49.666 --> 00:42:51.486 A:middle
accurately place your object on

00:42:51.486 --> 00:42:52.986 A:middle
top of the physical surface such

00:42:52.986 --> 00:42:54.026 A:middle
as those planes are evolving.

00:42:54.546 --> 00:42:57.016 A:middle
Let's see how you can enable

00:42:57.016 --> 00:42:57.806 A:middle
that in ARKit.

00:43:00.336 --> 00:43:01.906 A:middle
Well, it sounds we're creating a

00:43:01.906 --> 00:43:02.816 A:middle
raycast query.

00:43:03.526 --> 00:43:05.806 A:middle
A raycast query has three

00:43:05.806 --> 00:43:06.466 A:middle
parameters.

00:43:06.596 --> 00:43:08.096 A:middle
The first one decides from where

00:43:08.096 --> 00:43:08.956 A:middle
you want to perform that

00:43:08.956 --> 00:43:09.536 A:middle
raycast.

00:43:10.106 --> 00:43:11.246 A:middle
In this example, we're doing

00:43:11.246 --> 00:43:12.266 A:middle
this from the screenCenter.

00:43:12.266 --> 00:43:15.896 A:middle
Then you need to tell it like

00:43:16.126 --> 00:43:18.686 A:middle
what you want to allow in order

00:43:18.686 --> 00:43:20.606 A:middle
to place that content or get the

00:43:20.606 --> 00:43:21.366 A:middle
transforms back.

00:43:21.936 --> 00:43:25.406 A:middle
And additionally, you need tell

00:43:25.406 --> 00:43:26.566 A:middle
it which alignment you want.

00:43:26.746 --> 00:43:29.296 A:middle
It can be horizontal, vertical,

00:43:29.566 --> 00:43:30.686 A:middle
or any.

00:43:32.616 --> 00:43:34.756 A:middle
Then you need to pass that query

00:43:35.076 --> 00:43:36.616 A:middle
on to the trackedRaycast method

00:43:36.616 --> 00:43:38.206 A:middle
on your AR session.

00:43:40.056 --> 00:43:41.976 A:middle
This method has a callback that

00:43:41.976 --> 00:43:43.836 A:middle
will allow you to react to the

00:43:43.836 --> 00:43:45.406 A:middle
new transform and result giving

00:43:45.406 --> 00:43:47.746 A:middle
to you with that raycast so that

00:43:47.746 --> 00:43:49.026 A:middle
you can adjust your content or

00:43:49.026 --> 00:43:49.996 A:middle
your anchors accordingly.

00:43:50.456 --> 00:43:53.436 A:middle
And then finally, when you are

00:43:53.436 --> 00:43:55.146 A:middle
done with this raycast, you can

00:43:55.146 --> 00:43:55.696 A:middle
just stop it.

00:43:56.346 --> 00:43:58.966 A:middle
And those are some of the

00:43:58.966 --> 00:44:00.236 A:middle
scene-- Those are some of the

00:43:58.966 --> 00:44:00.236 A:middle
scene-- Those are some of the

00:44:00.236 --> 00:44:01.856 A:middle
raycasting improvements that

00:44:01.886 --> 00:44:02.466 A:middle
we've done this year.

00:44:07.256 --> 00:44:08.666 A:middle
Let's move on with some of the

00:44:08.666 --> 00:44:10.366 A:middle
visual coherence enhancement

00:44:10.416 --> 00:44:11.346 A:middle
we've made.

00:44:12.216 --> 00:44:16.456 A:middle
So this year, we have this new

00:44:16.456 --> 00:44:18.696 A:middle
ARView that allows you to

00:44:18.696 --> 00:44:19.986 A:middle
activate and deactivate

00:44:20.216 --> 00:44:22.116 A:middle
different render options on

00:44:22.116 --> 00:44:22.456 A:middle
demand.

00:44:23.396 --> 00:44:24.766 A:middle
It also can also be

00:44:25.056 --> 00:44:26.876 A:middle
automatically deactivated and

00:44:26.876 --> 00:44:28.556 A:middle
activated based on your device

00:44:28.556 --> 00:44:29.226 A:middle
capability.

00:44:30.076 --> 00:44:31.636 A:middle
Let's look at an example of

00:44:31.636 --> 00:44:31.936 A:middle
this.

00:44:32.266 --> 00:44:34.826 A:middle
You've probably seen that video

00:44:34.826 --> 00:44:35.496 A:middle
before.

00:44:35.496 --> 00:44:37.016 A:middle
Look at how the Quadrocopter

00:44:37.016 --> 00:44:38.426 A:middle
actually moves around and how

00:44:38.426 --> 00:44:39.646 A:middle
the objects are moving as well

00:44:39.646 --> 00:44:42.066 A:middle
on the surface and how real all

00:44:42.066 --> 00:44:43.646 A:middle
of these looks like.

00:44:44.276 --> 00:44:45.716 A:middle
When everything disappears, you

00:44:47.256 --> 00:44:49.156 A:middle
actually don't even realize that

00:44:49.156 --> 00:44:50.176 A:middle
those objects were virtual.

00:44:50.666 --> 00:44:52.726 A:middle
Let's look at some of those

00:44:52.726 --> 00:44:54.866 A:middle
visual coherence enhancements

00:44:54.866 --> 00:44:55.196 A:middle
we've made.

00:44:55.736 --> 00:44:57.956 A:middle
Let's look again at the

00:44:57.956 --> 00:44:59.396 A:middle
beginning of that video and

00:44:59.396 --> 00:45:00.936 A:middle
let's pause for a second when

00:44:59.396 --> 00:45:00.936 A:middle
let's pause for a second when

00:45:00.936 --> 00:45:02.316 A:middle
those balls are rolling on the

00:45:02.796 --> 00:45:02.986 A:middle
table.

00:45:04.376 --> 00:45:05.926 A:middle
Here you can see the depth of

00:45:06.006 --> 00:45:08.256 A:middle
field effect which is a new

00:45:08.256 --> 00:45:09.736 A:middle
feature of RealityKit.

00:45:11.226 --> 00:45:13.216 A:middle
Your AR experience are designed

00:45:13.216 --> 00:45:15.226 A:middle
for, you know, small and big

00:45:15.226 --> 00:45:15.746 A:middle
rooms.

00:45:16.316 --> 00:45:19.196 A:middle
The camera on your iOS device

00:45:19.196 --> 00:45:20.696 A:middle
always adjust their focus to the

00:45:20.696 --> 00:45:23.266 A:middle
environment, while the depth of

00:45:23.266 --> 00:45:25.436 A:middle
field feature allows to adjust

00:45:25.436 --> 00:45:26.776 A:middle
the focus on the virtual

00:45:26.776 --> 00:45:27.886 A:middle
contents so that it matches

00:45:27.946 --> 00:45:29.596 A:middle
perfectly with your physical

00:45:29.596 --> 00:45:31.626 A:middle
one, so that the object blends

00:45:31.676 --> 00:45:36.306 A:middle
perfectly in the environment.

00:45:36.306 --> 00:45:37.416 A:middle
Additionally, when you move the

00:45:37.416 --> 00:45:39.026 A:middle
camera quickly or when the

00:45:39.026 --> 00:45:41.566 A:middle
object moves quickly, you can

00:45:41.566 --> 00:45:43.356 A:middle
see that blurriness occurring.

00:45:44.496 --> 00:45:46.516 A:middle
Well-- And then most of the time

00:45:46.516 --> 00:45:48.626 A:middle
when you have a regular renderer

00:45:48.626 --> 00:45:50.986 A:middle
and no motion blur effect, then

00:45:50.986 --> 00:45:52.896 A:middle
your virtual content stands out.

00:45:54.346 --> 00:45:55.386 A:middle
And it doesn't really blend

00:45:55.386 --> 00:45:56.436 A:middle
nicely in the environment.

00:45:57.096 --> 00:45:58.356 A:middle
Well, thanks to VIO camera

00:45:58.356 --> 00:46:01.226 A:middle
motion and the sense of

00:45:58.356 --> 00:46:01.226 A:middle
motion and the sense of

00:46:01.226 --> 00:46:03.386 A:middle
parameters, then we can

00:46:03.386 --> 00:46:06.116 A:middle
synthesize the motion blur.

00:46:06.436 --> 00:46:07.746 A:middle
And we can apply it on the

00:46:07.746 --> 00:46:10.506 A:middle
visual object so that it blends

00:46:10.586 --> 00:46:11.696 A:middle
perfectly in your environment.

00:46:11.696 --> 00:46:15.506 A:middle
This is a variable on ARSCNView

00:46:15.506 --> 00:46:16.596 A:middle
and on the ARView as well.

00:46:16.716 --> 00:46:20.446 A:middle
Let's look again at this example

00:46:20.446 --> 00:46:21.686 A:middle
where everything looks really

00:46:21.686 --> 00:46:21.876 A:middle
good.

00:46:55.616 --> 00:46:56.996 A:middle
Two additional APIs that we're

00:46:57.056 --> 00:46:58.646 A:middle
making available for visual

00:46:58.646 --> 00:47:00.306 A:middle
coherence enhancement this year

00:46:58.646 --> 00:47:00.306 A:middle
coherence enhancement this year

00:47:00.616 --> 00:47:03.176 A:middle
are HDR environment textures and

00:47:03.176 --> 00:47:03.756 A:middle
camera grain.

00:47:06.286 --> 00:47:07.996 A:middle
When you place your content, you

00:47:07.996 --> 00:47:08.956 A:middle
really want that content to

00:47:08.956 --> 00:47:10.026 A:middle
reflect the real world.

00:47:11.296 --> 00:47:13.206 A:middle
Well, with high-dynamic-range,

00:47:13.986 --> 00:47:16.226 A:middle
you can capture even in bright

00:47:16.226 --> 00:47:17.236 A:middle
light environment those

00:47:17.356 --> 00:47:18.746 A:middle
highlights or higher highlights

00:47:19.376 --> 00:47:20.576 A:middle
that makes your contents more

00:47:20.576 --> 00:47:21.076 A:middle
vibrant.

00:47:22.626 --> 00:47:23.846 A:middle
Well, with ARKit this year, we

00:47:23.846 --> 00:47:25.386 A:middle
can actually request for those

00:47:25.386 --> 00:47:26.666 A:middle
HDR environment textures so that

00:47:26.666 --> 00:47:28.576 A:middle
your content looks even better.

00:47:29.896 --> 00:47:31.916 A:middle
Additionally, we also have a

00:47:31.916 --> 00:47:32.846 A:middle
camera grain API.

00:47:33.786 --> 00:47:36.236 A:middle
You'll probably notice when you

00:47:36.236 --> 00:47:38.026 A:middle
have an AR experience in a very

00:47:38.026 --> 00:47:39.506 A:middle
low light environment how your

00:47:39.806 --> 00:47:41.856 A:middle
other content looks really shiny

00:47:42.176 --> 00:47:43.046 A:middle
compared to the camera.

00:47:44.906 --> 00:47:46.476 A:middle
Every camera produce some grain.

00:47:46.786 --> 00:47:48.056 A:middle
And especially in low lights,

00:47:48.126 --> 00:47:51.216 A:middle
this grain can be a bit heavier.

00:47:51.856 --> 00:47:53.336 A:middle
Where with this new camera grain

00:47:53.336 --> 00:47:55.906 A:middle
API, we can make sure to apply

00:47:56.716 --> 00:47:58.686 A:middle
the same grain patterns on your

00:47:58.686 --> 00:48:00.576 A:middle
virtual content so that it

00:47:58.686 --> 00:48:00.576 A:middle
virtual content so that it

00:48:00.576 --> 00:48:02.386 A:middle
blends nicely and doesn't stand

00:48:03.316 --> 00:48:03.386 A:middle
out.

00:48:04.516 --> 00:48:06.296 A:middle
So those are some of the visual

00:48:06.296 --> 00:48:07.746 A:middle
coherence enhancement for this

00:48:07.746 --> 00:48:07.916 A:middle
year.

00:48:07.916 --> 00:48:10.966 A:middle
But we didn't stop there.

00:48:11.776 --> 00:48:13.156 A:middle
I think there's one feature that

00:48:13.156 --> 00:48:13.866 A:middle
a lot of you have been

00:48:13.866 --> 00:48:14.456 A:middle
requesting.

00:48:16.146 --> 00:48:17.076 A:middle
When you develop an AR

00:48:17.076 --> 00:48:19.076 A:middle
experience, you'll always have

00:48:19.116 --> 00:48:21.506 A:middle
or often in order to prototype

00:48:21.506 --> 00:48:23.236 A:middle
or to test your experience go to

00:48:23.236 --> 00:48:24.266 A:middle
a certain location.

00:48:26.356 --> 00:48:28.386 A:middle
And most of the time when you go

00:48:28.386 --> 00:48:29.386 A:middle
there, you'd come back to your

00:48:29.386 --> 00:48:30.186 A:middle
desk, you develop your

00:48:30.186 --> 00:48:31.146 A:middle
experience, you want to come

00:48:31.146 --> 00:48:31.696 A:middle
back again.

00:48:31.866 --> 00:48:34.216 A:middle
Well, this year with the Reality

00:48:34.216 --> 00:48:36.876 A:middle
Composer app, you can now record

00:48:36.966 --> 00:48:38.976 A:middle
an experience or a sequence.

00:48:40.076 --> 00:48:41.216 A:middle
Meaning that you can go to your

00:48:41.266 --> 00:48:42.936 A:middle
favorite place so where-- the

00:48:43.006 --> 00:48:44.556 A:middle
place where your experience will

00:48:44.556 --> 00:48:47.366 A:middle
be happening to capture the

00:48:47.366 --> 00:48:48.206 A:middle
environment.

00:48:48.706 --> 00:48:50.376 A:middle
ARKit will make sure to save the

00:48:50.376 --> 00:48:52.596 A:middle
sensor data alongside the video

00:48:52.596 --> 00:48:56.436 A:middle
stream into a movie file

00:48:56.436 --> 00:48:58.906 A:middle
container so that you can take

00:48:58.996 --> 00:49:02.946 A:middle
it with you and put it in Xcode.

00:48:58.996 --> 00:49:02.946 A:middle
it with you and put it in Xcode.

00:49:03.076 --> 00:49:06.256 A:middle
At that point, the Xcode scheme

00:49:06.256 --> 00:49:08.156 A:middle
settings now have like one new

00:49:08.156 --> 00:49:10.126 A:middle
additional feature or a new

00:49:10.226 --> 00:49:12.256 A:middle
field that allows you to select

00:49:12.256 --> 00:49:13.456 A:middle
that file.

00:49:16.106 --> 00:49:18.336 A:middle
When that file is selected and

00:49:18.336 --> 00:49:19.976 A:middle
then you press Run on the device

00:49:19.976 --> 00:49:23.906 A:middle
that is attached to Xcode, then

00:49:24.046 --> 00:49:25.656 A:middle
you can replay that experience

00:49:25.656 --> 00:49:26.316 A:middle
at your desk.

00:49:26.936 --> 00:49:28.636 A:middle
This is ideal for prototyping,

00:49:29.256 --> 00:49:31.336 A:middle
and even better for tweaking

00:49:31.336 --> 00:49:32.736 A:middle
your AR configuration with

00:49:32.736 --> 00:49:34.136 A:middle
different parameters and trying

00:49:34.136 --> 00:49:35.366 A:middle
to see how the experience looks

00:49:35.366 --> 00:49:35.526 A:middle
like.

00:49:36.076 --> 00:49:37.676 A:middle
You can even react to certain

00:49:37.676 --> 00:49:37.976 A:middle
tracking--

00:49:38.516 --> 00:49:46.456 A:middle
[ Applause ]

00:49:46.956 --> 00:49:48.376 A:middle
So this is great.

00:49:48.376 --> 00:49:50.136 A:middle
I think you've got a lot of

00:49:50.136 --> 00:49:51.776 A:middle
different tools this year in

00:49:51.776 --> 00:49:54.456 A:middle
ARKit 3 where you can enhance

00:49:54.456 --> 00:49:56.296 A:middle
your multiuser experiences with

00:49:56.356 --> 00:49:57.556 A:middle
collaborative sessions,

00:49:57.996 --> 00:49:59.246 A:middle
multiple-face tracking.

00:50:00.096 --> 00:50:02.356 A:middle
You can improve the realism of

00:50:02.406 --> 00:50:03.986 A:middle
all your AR apps with

00:50:03.986 --> 00:50:06.746 A:middle
RealityKit's new coherence

00:50:06.746 --> 00:50:08.676 A:middle
effect, and also the new visual

00:50:08.676 --> 00:50:09.616 A:middle
effects on the

00:50:09.616 --> 00:50:10.926 A:middle
ARWorldTrackingConfiguration.

00:50:12.396 --> 00:50:13.916 A:middle
You can enable new use cases

00:50:14.996 --> 00:50:18.086 A:middle
with the new motion capture, and

00:50:18.086 --> 00:50:19.706 A:middle
also the simultaneous face and

00:50:19.706 --> 00:50:20.396 A:middle
back camera.

00:50:21.536 --> 00:50:23.396 A:middle
And, of course, there are lots

00:50:23.396 --> 00:50:25.086 A:middle
of improvements under the hood

00:50:25.356 --> 00:50:26.416 A:middle
on existing features.

00:50:26.756 --> 00:50:28.526 A:middle
As an example, with object

00:50:28.526 --> 00:50:29.676 A:middle
detection and machine learning.

00:50:30.266 --> 00:50:33.456 A:middle
And least-- last but not the

00:50:33.456 --> 00:50:36.676 A:middle
least, the record and replay

00:50:36.676 --> 00:50:38.446 A:middle
workflow I think will make your

00:50:38.446 --> 00:50:39.966 A:middle
design and your experience

00:50:39.966 --> 00:50:41.246 A:middle
prototyping even better than

00:50:41.246 --> 00:50:41.716 A:middle
before.

00:50:41.716 --> 00:50:44.956 A:middle
So I'm really looking forward

00:50:44.956 --> 00:50:46.296 A:middle
for you to go and download our

00:50:46.296 --> 00:50:47.526 A:middle
samples on the website.

00:50:49.036 --> 00:50:50.556 A:middle
We also have a couple of labs,

00:50:50.946 --> 00:50:52.476 A:middle
one tomorrow and one on

00:50:52.576 --> 00:50:54.416 A:middle
Thursday, where I hope you will

00:50:54.416 --> 00:50:55.466 A:middle
be coming and asking us the

00:50:55.526 --> 00:50:57.756 A:middle
questions you have or just to

00:50:57.756 --> 00:50:58.016 A:middle
chat.

00:50:59.166 --> 00:51:01.136 A:middle
And then we also have two

00:50:59.166 --> 00:51:01.136 A:middle
And then we also have two

00:51:01.136 --> 00:51:03.016 A:middle
in-depth sessions, one of which

00:51:03.016 --> 00:51:04.856 A:middle
is around bringing people into

00:51:04.856 --> 00:51:07.046 A:middle
AR which we'll talk more about

00:51:07.046 --> 00:51:08.166 A:middle
people occlusion and motion

00:51:08.166 --> 00:51:08.616 A:middle
capture.

00:51:09.086 --> 00:51:10.676 A:middle
And the second one is about

00:51:10.746 --> 00:51:12.146 A:middle
collaborative AR experiences.

00:51:12.806 --> 00:51:15.416 A:middle
I hope you enjoy the rest of the

00:51:15.416 --> 00:51:15.946 A:middle
conference.

00:51:17.336 --> 00:51:18.096 A:middle
Have a great day.

00:51:18.416 --> 00:51:18.616 A:middle
Bye.

00:51:19.516 --> 00:51:22.500 A:middle
[ Applause ]
