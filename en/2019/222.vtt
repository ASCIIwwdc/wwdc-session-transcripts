WEBVTT

00:00:00.506 --> 00:00:04.500 A:middle
[ Music ]

00:00:07.516 --> 00:00:14.196 A:middle
[ Applause ]

00:00:14.696 --> 00:00:15.336 A:middle
&gt;&gt; Good morning.

00:00:15.676 --> 00:00:16.976 A:middle
My name is Brittany Weinert, and

00:00:16.976 --> 00:00:17.946 A:middle
I'm a software engineer on the

00:00:17.946 --> 00:00:18.866 A:middle
Vision Framework Team.

00:00:19.566 --> 00:00:20.596 A:middle
This year the Vision Team has a

00:00:20.596 --> 00:00:22.086 A:middle
lot of exciting new updates that

00:00:22.086 --> 00:00:23.096 A:middle
we think you're all going to

00:00:23.096 --> 00:00:23.316 A:middle
love.

00:00:23.916 --> 00:00:24.846 A:middle
Because we have so much new

00:00:24.846 --> 00:00:26.076 A:middle
stuff to cover, we're going to

00:00:26.076 --> 00:00:27.106 A:middle
dive right into the new

00:00:27.106 --> 00:00:27.516 A:middle
features.

00:00:27.876 --> 00:00:29.256 A:middle
If you're completely new to

00:00:29.256 --> 00:00:30.706 A:middle
Vision, don't worry.

00:00:30.706 --> 00:00:31.466 A:middle
You should still be able to

00:00:31.466 --> 00:00:33.096 A:middle
follow along, and our hope is

00:00:33.096 --> 00:00:34.216 A:middle
that the new capabilities that

00:00:34.216 --> 00:00:35.956 A:middle
we introduce today will motivate

00:00:35.956 --> 00:00:37.366 A:middle
you to learn about Vision and to

00:00:37.366 --> 00:00:39.606 A:middle
use it in your apps.

00:00:39.836 --> 00:00:40.906 A:middle
Today well be covering four

00:00:40.906 --> 00:00:44.206 A:middle
completely new topics, saliency,

00:00:44.416 --> 00:00:46.066 A:middle
image classification, Image

00:00:46.066 --> 00:00:47.556 A:middle
Similarity, and face quality.

00:00:47.556 --> 00:00:49.456 A:middle
We also have some technology

00:00:49.456 --> 00:00:50.796 A:middle
upgrades for the Object Tracker

00:00:50.796 --> 00:00:52.826 A:middle
and Face Landmarks as well as

00:00:52.826 --> 00:00:54.666 A:middle
new detectors and improved Core

00:00:54.666 --> 00:00:55.196 A:middle
ML support.

00:00:55.196 --> 00:00:58.486 A:middle
Today, I'm going to be talking

00:00:58.486 --> 00:00:59.276 A:middle
about saliency.

00:00:59.736 --> 00:01:00.836 A:middle
Let's start with a definition.

00:00:59.736 --> 00:01:00.836 A:middle
Let's start with a definition.

00:01:01.386 --> 00:01:02.806 A:middle
I'm about to show you a photo,

00:01:02.806 --> 00:01:06.426 A:middle
and I want you to pay attention

00:01:06.426 --> 00:01:07.586 A:middle
to where your eyes are first

00:01:07.586 --> 00:01:08.036 A:middle
drawn.

00:01:08.526 --> 00:01:13.046 A:middle
When you first saw this photo of

00:01:13.046 --> 00:01:14.126 A:middle
the three puffins sitting on a

00:01:14.126 --> 00:01:15.696 A:middle
cliff, did you notice what stood

00:01:15.696 --> 00:01:16.366 A:middle
out to you first?

00:01:17.326 --> 00:01:19.956 A:middle
According to our models, most of

00:01:19.956 --> 00:01:21.226 A:middle
you looked at the puffins faces

00:01:21.226 --> 00:01:21.476 A:middle
first.

00:01:22.436 --> 00:01:23.556 A:middle
This is saliency.

00:01:24.006 --> 00:01:26.366 A:middle
There are two types of saliency,

00:01:26.766 --> 00:01:28.156 A:middle
attention based and objectness

00:01:28.216 --> 00:01:28.566 A:middle
based.

00:01:29.086 --> 00:01:30.976 A:middle
The overlay that you saw on the

00:01:30.976 --> 00:01:32.566 A:middle
puffin image just now called the

00:01:32.566 --> 00:01:34.226 A:middle
heatmap was generated by

00:01:34.226 --> 00:01:35.686 A:middle
attention based saliency.

00:01:36.146 --> 00:01:37.216 A:middle
But before we get into more

00:01:37.216 --> 00:01:38.556 A:middle
visual examples, I want to go

00:01:38.556 --> 00:01:39.696 A:middle
over the basics of each

00:01:39.696 --> 00:01:40.246 A:middle
algorithm.

00:01:41.776 --> 00:01:45.186 A:middle
Attention based saliency is a

00:01:45.186 --> 00:01:47.786 A:middle
human aspected saliency, and by

00:01:47.786 --> 00:01:49.436 A:middle
this, I mean that the attention

00:01:49.436 --> 00:01:51.006 A:middle
based saliency models were

00:01:51.006 --> 00:01:53.566 A:middle
generated by where people looked

00:01:53.906 --> 00:01:55.006 A:middle
when they were shown a series of

00:01:55.006 --> 00:01:55.446 A:middle
images.

00:01:56.326 --> 00:01:57.646 A:middle
This means that the heatmap

00:01:57.646 --> 00:02:00.026 A:middle
reflects and highlights where

00:01:57.646 --> 00:02:00.026 A:middle
reflects and highlights where

00:02:00.026 --> 00:02:00.966 A:middle
people first look when they're

00:02:00.966 --> 00:02:01.646 A:middle
shown an image.

00:02:02.676 --> 00:02:04.556 A:middle
Objectness based saliency on the

00:02:04.556 --> 00:02:06.006 A:middle
other hand was trained on

00:02:06.056 --> 00:02:07.976 A:middle
subject segmentation in an image

00:02:08.515 --> 00:02:10.286 A:middle
with the goal to highlight the

00:02:10.286 --> 00:02:12.086 A:middle
foreground objects or the

00:02:12.086 --> 00:02:13.176 A:middle
subjects of an image.

00:02:13.766 --> 00:02:15.996 A:middle
So, in the heatmap, the subjects

00:02:15.996 --> 00:02:17.196 A:middle
or foreground objects should be

00:02:17.196 --> 00:02:17.656 A:middle
highlighted.

00:02:18.226 --> 00:02:19.916 A:middle
Let's look at some examples now.

00:02:20.496 --> 00:02:23.566 A:middle
So, here are the puffins from

00:02:23.566 --> 00:02:23.946 A:middle
earlier.

00:02:24.636 --> 00:02:26.956 A:middle
Here's the attention based

00:02:26.956 --> 00:02:28.436 A:middle
heatmap overlaid on the image,

00:02:29.446 --> 00:02:30.916 A:middle
and here's the objectness based

00:02:32.316 --> 00:02:33.046 A:middle
heatmap.

00:02:33.046 --> 00:02:34.806 A:middle
As I said, people tend to look

00:02:34.806 --> 00:02:36.056 A:middle
at the puffins' faces first, so

00:02:36.346 --> 00:02:37.686 A:middle
the area around the puffins'

00:02:37.686 --> 00:02:38.996 A:middle
heads is very salient for the

00:02:38.996 --> 00:02:39.956 A:middle
attention based heatmap.

00:02:40.896 --> 00:02:42.286 A:middle
For objectness, we're just

00:02:42.286 --> 00:02:43.676 A:middle
trying to pick up the subjects,

00:02:43.676 --> 00:02:45.246 A:middle
and in this case, it's the three

00:02:45.246 --> 00:02:45.636 A:middle
puffins.

00:02:45.786 --> 00:02:46.846 A:middle
So, all the puffins are

00:02:46.846 --> 00:02:47.266 A:middle
highlighted.

00:02:48.486 --> 00:02:50.206 A:middle
Let's look at how saliency works

00:02:50.206 --> 00:02:51.206 A:middle
with images of people.

00:02:51.746 --> 00:02:56.556 A:middle
For attention based saliency,

00:02:57.046 --> 00:02:58.656 A:middle
the areas around peoples' faces

00:02:58.656 --> 00:02:59.926 A:middle
tend to be the most salient,

00:03:00.536 --> 00:03:02.146 A:middle
unsurprisingly because we tend

00:03:02.146 --> 00:03:04.146 A:middle
to look at people's faces first.

00:03:04.846 --> 00:03:06.616 A:middle
For objectness based saliency,

00:03:06.616 --> 00:03:08.196 A:middle
if the person is the subject of

00:03:08.196 --> 00:03:09.646 A:middle
the image, the entire person

00:03:09.646 --> 00:03:10.386 A:middle
should be highlighted.

00:03:12.596 --> 00:03:14.896 A:middle
So, attention based saliency

00:03:14.896 --> 00:03:16.266 A:middle
though is the more complicated

00:03:16.266 --> 00:03:17.846 A:middle
of the two saliencies, I'd say,

00:03:18.436 --> 00:03:19.926 A:middle
because it is determined by a

00:03:19.926 --> 00:03:21.386 A:middle
number of very human factors.

00:03:22.226 --> 00:03:23.846 A:middle
And the number, the main factors

00:03:23.846 --> 00:03:25.126 A:middle
that determine attention based

00:03:25.126 --> 00:03:26.236 A:middle
saliency and what's salient or

00:03:26.236 --> 00:03:28.426 A:middle
not, is contrast, faces,

00:03:28.916 --> 00:03:31.046 A:middle
subjects, horizons, and light.

00:03:32.416 --> 00:03:33.666 A:middle
But interestingly enough, it can

00:03:33.666 --> 00:03:35.516 A:middle
also be affected by perceived

00:03:35.516 --> 00:03:36.026 A:middle
motion.

00:03:36.556 --> 00:03:39.816 A:middle
In this example, the umbrella

00:03:39.816 --> 00:03:42.436 A:middle
colors really pop, so the area

00:03:42.436 --> 00:03:43.716 A:middle
around the umbrella is salient,

00:03:44.026 --> 00:03:45.636 A:middle
but the road is also salient

00:03:46.006 --> 00:03:47.666 A:middle
because our eyes try to track

00:03:47.666 --> 00:03:48.796 A:middle
where the umbrella is headed.

00:03:50.706 --> 00:03:52.216 A:middle
For objectness based saliency,

00:03:52.216 --> 00:03:53.406 A:middle
we just pick up on the umbrella

00:03:55.016 --> 00:03:55.096 A:middle
guy.

00:03:55.316 --> 00:03:56.876 A:middle
So, I could do this all day and

00:03:56.876 --> 00:03:58.456 A:middle
show you more examples, but

00:03:58.456 --> 00:03:59.456 A:middle
honestly, the best way to

00:03:59.456 --> 00:04:00.966 A:middle
understand saliency is to try it

00:03:59.456 --> 00:04:00.966 A:middle
understand saliency is to try it

00:04:00.966 --> 00:04:01.766 A:middle
out for yourself.

00:04:02.476 --> 00:04:03.776 A:middle
I encourage everybody to

00:04:03.776 --> 00:04:05.726 A:middle
download the Saliency app and

00:04:05.726 --> 00:04:06.656 A:middle
try it on their own photo

00:04:06.656 --> 00:04:07.136 A:middle
libraries.

00:04:07.796 --> 00:04:10.536 A:middle
So, let's get into what's

00:04:10.536 --> 00:04:12.236 A:middle
returned from the saliency

00:04:12.236 --> 00:04:15.726 A:middle
request, mainly the heatmap.

00:04:16.055 --> 00:04:17.375 A:middle
So, the images that I've been

00:04:17.375 --> 00:04:19.596 A:middle
showing to you up until now, the

00:04:19.596 --> 00:04:22.356 A:middle
heatmap has been scaled,

00:04:22.356 --> 00:04:25.996 A:middle
overlaid, and colorized and put

00:04:25.996 --> 00:04:27.536 A:middle
onto the image, but in

00:04:27.536 --> 00:04:29.656 A:middle
actuality, the heatmap is a very

00:04:29.656 --> 00:04:32.206 A:middle
small CV pixel buffer that's

00:04:32.206 --> 00:04:34.186 A:middle
made up of Floats in the range

00:04:34.186 --> 00:04:37.246 A:middle
of 0 to 1, 0 designating

00:04:37.526 --> 00:04:39.356 A:middle
nonsalient and 1 being most

00:04:39.356 --> 00:04:39.746 A:middle
salient.

00:04:40.256 --> 00:04:44.066 A:middle
And there's extra code that

00:04:44.066 --> 00:04:45.206 A:middle
you'd have to do to get the

00:04:45.206 --> 00:04:46.526 A:middle
exact same effect like you see

00:04:46.526 --> 00:04:46.746 A:middle
here.

00:04:47.526 --> 00:04:48.896 A:middle
But let's go into how to

00:04:48.946 --> 00:04:50.926 A:middle
formulate a request at the very

00:04:50.926 --> 00:04:51.716 A:middle
basic level.

00:04:53.256 --> 00:04:56.686 A:middle
Okay. So, first we start out

00:04:56.686 --> 00:04:58.776 A:middle
with a VNImageRequestHandler to

00:04:58.776 --> 00:05:00.206 A:middle
handle a single image.

00:04:58.776 --> 00:05:00.206 A:middle
handle a single image.

00:05:01.266 --> 00:05:03.126 A:middle
Next, you choose the algorithm

00:05:03.126 --> 00:05:04.346 A:middle
that you want to run, in this

00:05:04.346 --> 00:05:06.516 A:middle
case, AttentionBasedSaliency,

00:05:06.566 --> 00:05:09.256 A:middle
and set the revision if you

00:05:09.256 --> 00:05:10.586 A:middle
always want to be using the same

00:05:10.586 --> 00:05:10.976 A:middle
algorithm.

00:05:12.976 --> 00:05:14.896 A:middle
Next, you call perform request,

00:05:15.196 --> 00:05:17.466 A:middle
like you usually would, and if

00:05:17.466 --> 00:05:19.476 A:middle
it's successful, the results

00:05:19.476 --> 00:05:20.996 A:middle
property on the request should

00:05:20.996 --> 00:05:22.456 A:middle
be populated with a

00:05:22.456 --> 00:05:24.266 A:middle
VNSaliencyImageObservation.

00:05:25.326 --> 00:05:28.396 A:middle
To access the heatmap, you call

00:05:28.396 --> 00:05:30.086 A:middle
the pixelBuffer property on the

00:05:30.086 --> 00:05:32.906 A:middle
VNSaliencyImageObservation like

00:05:35.256 --> 00:05:35.356 A:middle
so.

00:05:35.596 --> 00:05:37.516 A:middle
If you wanted to do objectness

00:05:37.516 --> 00:05:39.316 A:middle
based saliency, all you would

00:05:39.316 --> 00:05:41.096 A:middle
have to do is change the request

00:05:41.246 --> 00:05:43.786 A:middle
name and the revision to be

00:05:43.786 --> 00:05:44.246 A:middle
objectness.

00:05:45.426 --> 00:05:46.676 A:middle
So, for attention, it's

00:05:46.736 --> 00:05:48.206 A:middle
VNGenerateAttentionBased

00:05:48.206 --> 00:05:50.106 A:middle
SaliencyImageRequest and for

00:05:50.106 --> 00:05:50.836 A:middle
objectness, it's

00:05:50.836 --> 00:05:52.386 A:middle
VNGenerateObjectnessBased

00:05:52.386 --> 00:05:53.076 A:middle
SaliencyRequest.

00:05:53.796 --> 00:05:56.856 A:middle
So, let's get into another tool

00:05:56.856 --> 00:05:58.226 A:middle
other than at heatmap, the

00:05:58.226 --> 00:05:58.856 A:middle
bounding box.

00:05:59.476 --> 00:06:03.196 A:middle
The bounding boxes encapsulate

00:05:59.476 --> 00:06:03.196 A:middle
The bounding boxes encapsulate

00:06:03.196 --> 00:06:04.536 A:middle
all the salient regions in an

00:06:04.536 --> 00:06:04.796 A:middle
image.

00:06:05.156 --> 00:06:06.726 A:middle
For attention based saliency,

00:06:06.726 --> 00:06:08.006 A:middle
you should always have one

00:06:08.006 --> 00:06:09.866 A:middle
bounding box, and for objectness

00:06:09.866 --> 00:06:11.116 A:middle
based saliency, you can have up

00:06:11.116 --> 00:06:12.616 A:middle
to three bounding boxes.

00:06:13.726 --> 00:06:15.826 A:middle
The bounding boxes are in

00:06:15.826 --> 00:06:17.336 A:middle
normalized coordinate space with

00:06:17.336 --> 00:06:18.546 A:middle
respect to the image, the

00:06:18.546 --> 00:06:21.726 A:middle
original image, and the lower

00:06:21.726 --> 00:06:23.316 A:middle
left-hand corner is the origin

00:06:23.316 --> 00:06:25.066 A:middle
point, much like bounding boxes

00:06:25.066 --> 00:06:26.826 A:middle
returned by other algorithms in

00:06:26.826 --> 00:06:27.186 A:middle
Vision.

00:06:27.956 --> 00:06:30.716 A:middle
So, I wrote up a small method to

00:06:30.716 --> 00:06:32.266 A:middle
show how to access the bounding

00:06:32.266 --> 00:06:33.146 A:middle
boxes and use them.

00:06:33.916 --> 00:06:35.346 A:middle
Here we have a

00:06:35.346 --> 00:06:37.386 A:middle
VNSaliencyImageObservation, and

00:06:37.826 --> 00:06:40.156 A:middle
all you have to do is access the

00:06:40.156 --> 00:06:41.706 A:middle
salientObjects property on that

00:06:41.706 --> 00:06:43.876 A:middle
observation, and you should get

00:06:44.036 --> 00:06:46.456 A:middle
a list of bounding boxes, and

00:06:46.456 --> 00:06:48.296 A:middle
you can access them like so.

00:06:48.296 --> 00:06:50.566 A:middle
Okay. So, now that you know how

00:06:50.566 --> 00:06:53.136 A:middle
to formulate a request and now

00:06:53.136 --> 00:06:54.906 A:middle
that you know what saliency is,

00:06:55.686 --> 00:06:57.306 A:middle
let's get into some of the use

00:06:57.306 --> 00:06:57.736 A:middle
cases.

00:06:58.346 --> 00:07:02.606 A:middle
First, for a bit of fun, you can

00:06:58.346 --> 00:07:02.606 A:middle
First, for a bit of fun, you can

00:07:02.816 --> 00:07:05.816 A:middle
use saliency as a graphical mask

00:07:06.076 --> 00:07:07.056 A:middle
to edit your photos with.

00:07:07.056 --> 00:07:09.496 A:middle
So, here you have the heatmaps.

00:07:10.216 --> 00:07:14.306 A:middle
On the left-hand side, I've

00:07:14.306 --> 00:07:15.886 A:middle
desaturated all the nonsalient

00:07:15.886 --> 00:07:17.716 A:middle
regions, and on the right-hand

00:07:17.716 --> 00:07:19.416 A:middle
side, I've added a Gaussian blur

00:07:19.416 --> 00:07:20.636 A:middle
to all the nonsalient regions.

00:07:21.006 --> 00:07:22.336 A:middle
It really makes the subjects

00:07:22.336 --> 00:07:22.576 A:middle
pop.

00:07:25.536 --> 00:07:27.246 A:middle
Another use case of saliency is

00:07:27.246 --> 00:07:28.396 A:middle
you can enhance your photo

00:07:28.396 --> 00:07:29.156 A:middle
viewing experience.

00:07:30.106 --> 00:07:31.666 A:middle
So, let's say that you're at

00:07:31.666 --> 00:07:32.036 A:middle
home.

00:07:32.036 --> 00:07:33.846 A:middle
You're sitting on the couch, and

00:07:33.946 --> 00:07:35.236 A:middle
either your TV or your computer

00:07:35.236 --> 00:07:38.216 A:middle
has gone into standby mode, and

00:07:38.216 --> 00:07:39.066 A:middle
it's going through your photo

00:07:39.066 --> 00:07:39.906 A:middle
library.

00:07:40.336 --> 00:07:41.706 A:middle
A lot of times, these

00:07:41.706 --> 00:07:44.446 A:middle
photo-showing algorithms can be

00:07:44.446 --> 00:07:45.306 A:middle
a little bit awkward.

00:07:45.306 --> 00:07:47.006 A:middle
They zoom into seemingly random

00:07:47.006 --> 00:07:50.506 A:middle
parts of the image, and it's not

00:07:50.506 --> 00:07:51.346 A:middle
always what you expect.

00:07:52.086 --> 00:07:53.546 A:middle
But with saliency, you always

00:07:53.546 --> 00:07:56.026 A:middle
know where the subjects are, so

00:07:56.026 --> 00:07:57.036 A:middle
you can get a more

00:07:57.136 --> 00:07:59.366 A:middle
documentary-like effect like

00:08:02.476 --> 00:08:02.576 A:middle
this.

00:08:02.766 --> 00:08:04.616 A:middle
Finally, saliency works really

00:08:04.616 --> 00:08:05.656 A:middle
great with other vision

00:08:05.656 --> 00:08:06.206 A:middle
algorithms.

00:08:07.486 --> 00:08:09.246 A:middle
Let's say we have an image, and

00:08:09.246 --> 00:08:11.236 A:middle
we want to classify the objects

00:08:11.236 --> 00:08:11.806 A:middle
in the image.

00:08:12.966 --> 00:08:14.426 A:middle
We can run objectness based

00:08:14.426 --> 00:08:16.226 A:middle
saliency to pick up on the

00:08:16.226 --> 00:08:19.196 A:middle
objects in the image, crop the

00:08:19.196 --> 00:08:21.266 A:middle
image to the bounding boxes

00:08:21.266 --> 00:08:22.456 A:middle
returned by objectness based

00:08:22.456 --> 00:08:25.006 A:middle
saliency, and run these crops

00:08:25.366 --> 00:08:26.816 A:middle
through the algorithm through a

00:08:26.886 --> 00:08:28.736 A:middle
image classification algorithm

00:08:29.106 --> 00:08:31.286 A:middle
to find out what the objects

00:08:31.286 --> 00:08:31.536 A:middle
are.

00:08:32.316 --> 00:08:33.996 A:middle
So, not only do you know where

00:08:34.285 --> 00:08:35.576 A:middle
they are in the image because of

00:08:35.576 --> 00:08:36.885 A:middle
the bounding boxes, but it

00:08:36.885 --> 00:08:39.905 A:middle
allows you the hone in on what

00:08:39.905 --> 00:08:42.035 A:middle
the objects are by just picking

00:08:42.035 --> 00:08:43.366 A:middle
out the crops that have those

00:08:43.366 --> 00:08:44.976 A:middle
objects in it.

00:08:45.286 --> 00:08:46.756 A:middle
Now, you can already classify

00:08:46.756 --> 00:08:48.996 A:middle
things with Core ML, but this

00:08:48.996 --> 00:08:50.606 A:middle
year, Vision has new image

00:08:50.606 --> 00:08:52.296 A:middle
classification technique that

00:08:52.296 --> 00:08:54.126 A:middle
Rohan will now present to you.

00:08:55.516 --> 00:09:02.736 A:middle
[ Applause ]

00:08:55.516 --> 00:09:02.736 A:middle
[ Applause ]

00:09:03.236 --> 00:09:04.696 A:middle
&gt;&gt; Good morning.

00:09:05.136 --> 00:09:06.806 A:middle
My name is Rohan Chandra, and

00:09:06.806 --> 00:09:08.176 A:middle
I'm a researcher on the Vision

00:09:08.176 --> 00:09:08.476 A:middle
Team.

00:09:09.266 --> 00:09:10.846 A:middle
Today, I'm going to be talking

00:09:10.846 --> 00:09:12.106 A:middle
about some of the new image

00:09:12.106 --> 00:09:13.476 A:middle
classification requests we're

00:09:13.476 --> 00:09:14.926 A:middle
introducing to the Vision API

00:09:15.126 --> 00:09:15.606 A:middle
this year.

00:09:16.806 --> 00:09:18.856 A:middle
Now, image classification as a

00:09:18.856 --> 00:09:20.626 A:middle
task is fundamentally meant to

00:09:20.626 --> 00:09:22.096 A:middle
answer the question, what are

00:09:22.096 --> 00:09:23.406 A:middle
the objects that appear in my

00:09:23.406 --> 00:09:23.916 A:middle
image.

00:09:25.066 --> 00:09:26.546 A:middle
Many of you will already be

00:09:26.546 --> 00:09:27.386 A:middle
familiar with image

00:09:27.386 --> 00:09:28.206 A:middle
classification.

00:09:28.626 --> 00:09:30.146 A:middle
You may have used Create ML or

00:09:30.146 --> 00:09:31.776 A:middle
Core ML to train your own

00:09:31.776 --> 00:09:33.306 A:middle
classification networks on your

00:09:33.356 --> 00:09:35.366 A:middle
own data as we showed in the

00:09:35.366 --> 00:09:36.846 A:middle
Vision with Core ML talk last

00:09:36.846 --> 00:09:37.046 A:middle
year.

00:09:38.156 --> 00:09:39.306 A:middle
Others of you may have been

00:09:39.356 --> 00:09:40.456 A:middle
interested in image

00:09:40.456 --> 00:09:41.636 A:middle
classification but felt you

00:09:41.636 --> 00:09:43.246 A:middle
lacked the resources or the

00:09:43.246 --> 00:09:44.696 A:middle
expertise to develop your own

00:09:44.696 --> 00:09:45.116 A:middle
networks.

00:09:45.896 --> 00:09:48.716 A:middle
In practice, developing a

00:09:48.716 --> 00:09:50.026 A:middle
large-scale classification

00:09:50.026 --> 00:09:51.576 A:middle
network from scratch can take

00:09:51.676 --> 00:09:53.076 A:middle
millions of images to annotate,

00:09:53.596 --> 00:09:55.246 A:middle
thousands of hours to train, and

00:09:55.246 --> 00:09:56.406 A:middle
very specialized domain

00:09:56.406 --> 00:09:57.566 A:middle
expertise to develop.

00:09:58.736 --> 00:10:00.606 A:middle
We here at Apple have already

00:09:58.736 --> 00:10:00.606 A:middle
We here at Apple have already

00:10:00.606 --> 00:10:02.296 A:middle
gone through this process, and

00:10:02.336 --> 00:10:03.646 A:middle
so we wanted to share our

00:10:03.646 --> 00:10:05.206 A:middle
large-scale, on-device

00:10:05.256 --> 00:10:06.706 A:middle
classification network with you

00:10:07.176 --> 00:10:08.206 A:middle
so that you can leverage this

00:10:08.206 --> 00:10:09.936 A:middle
technology without needing to

00:10:09.936 --> 00:10:11.396 A:middle
invest a huge amount of time or

00:10:11.396 --> 00:10:12.816 A:middle
resources into developing it

00:10:12.816 --> 00:10:13.306 A:middle
yourself.

00:10:14.296 --> 00:10:16.116 A:middle
We've also strived to put tools

00:10:16.116 --> 00:10:17.346 A:middle
in the API to help you

00:10:17.346 --> 00:10:19.216 A:middle
contextualize and understand the

00:10:19.216 --> 00:10:20.646 A:middle
results in a way that makes

00:10:20.646 --> 00:10:21.806 A:middle
sense for your application.

00:10:22.986 --> 00:10:24.116 A:middle
Now, the network we're talking

00:10:24.116 --> 00:10:25.796 A:middle
about exposing here is in fact

00:10:25.846 --> 00:10:27.326 A:middle
the same network we ourselves

00:10:27.326 --> 00:10:28.686 A:middle
use to power the photo search

00:10:28.686 --> 00:10:29.306 A:middle
experience.

00:10:30.096 --> 00:10:31.136 A:middle
This is a network we've

00:10:31.136 --> 00:10:32.646 A:middle
developed specifically to run

00:10:32.646 --> 00:10:34.256 A:middle
efficiently on device without

00:10:34.256 --> 00:10:35.516 A:middle
requiring any service side

00:10:35.576 --> 00:10:36.236 A:middle
processing.

00:10:36.956 --> 00:10:38.446 A:middle
We've also developed it to

00:10:38.446 --> 00:10:39.746 A:middle
identify over a thousand

00:10:39.746 --> 00:10:41.136 A:middle
different categories of objects.

00:10:42.516 --> 00:10:44.396 A:middle
Now, it's also important to note

00:10:44.516 --> 00:10:45.776 A:middle
that this is a multi-label

00:10:45.776 --> 00:10:47.876 A:middle
network capable of identifying

00:10:48.236 --> 00:10:49.936 A:middle
multiple objects in a single

00:10:49.936 --> 00:10:51.986 A:middle
image, in contrast to more

00:10:51.986 --> 00:10:53.976 A:middle
typical mono-label networks that

00:10:53.976 --> 00:10:55.496 A:middle
try to focus on identifying a

00:10:55.496 --> 00:10:57.666 A:middle
single large central object in

00:10:57.666 --> 00:10:58.226 A:middle
an image.

00:10:59.586 --> 00:11:01.076 A:middle
Now, as I talk about this new

00:10:59.586 --> 00:11:01.076 A:middle
Now, as I talk about this new

00:11:01.076 --> 00:11:03.226 A:middle
classification API, I think one

00:11:03.226 --> 00:11:04.186 A:middle
of the first questions that

00:11:04.186 --> 00:11:05.786 A:middle
comes to mind is what are the

00:11:05.786 --> 00:11:06.856 A:middle
objects it can actually

00:11:06.856 --> 00:11:07.426 A:middle
identify?

00:11:08.336 --> 00:11:09.916 A:middle
Well, the set of objects that a

00:11:09.916 --> 00:11:11.536 A:middle
classifier can predict is known

00:11:11.536 --> 00:11:12.426 A:middle
as the taxonomy.

00:11:13.436 --> 00:11:15.106 A:middle
The taxonomy has a hierarchical

00:11:15.106 --> 00:11:16.456 A:middle
structure with directional

00:11:16.456 --> 00:11:17.936 A:middle
relationships between classes.

00:11:19.006 --> 00:11:20.606 A:middle
These relationships are based

00:11:20.606 --> 00:11:22.226 A:middle
upon shared semantic meaning.

00:11:22.916 --> 00:11:24.866 A:middle
For instance, a class like dog

00:11:24.966 --> 00:11:26.436 A:middle
might have children like Beagle,

00:11:26.616 --> 00:11:28.176 A:middle
Poodle, Husky, and other

00:11:28.176 --> 00:11:29.156 A:middle
sub-breeds of dogs.

00:11:30.256 --> 00:11:31.866 A:middle
In this sense, a parent class

00:11:31.896 --> 00:11:33.226 A:middle
tends to be more general while

00:11:33.226 --> 00:11:34.866 A:middle
child classes are more specific

00:11:34.866 --> 00:11:36.106 A:middle
instances of their parent.

00:11:37.046 --> 00:11:38.466 A:middle
You can of course see the entire

00:11:38.466 --> 00:11:39.816 A:middle
taxonomy using

00:11:40.006 --> 00:11:40.926 A:middle
ImageRequest.known

00:11:40.926 --> 00:11:41.756 A:middle
Classifications.

00:11:43.136 --> 00:11:44.416 A:middle
Now, when we constructed the

00:11:44.416 --> 00:11:46.276 A:middle
taxonomy, we had a few specific

00:11:46.276 --> 00:11:47.226 A:middle
rules that we applied.

00:11:48.656 --> 00:11:49.816 A:middle
The first is that the classes

00:11:49.856 --> 00:11:51.516 A:middle
must be visually identifiable.

00:11:52.836 --> 00:11:54.456 A:middle
That is, we avoid more abstract

00:11:54.456 --> 00:11:55.756 A:middle
concepts like holiday or

00:11:55.756 --> 00:11:56.306 A:middle
festival.

00:11:57.426 --> 00:11:59.196 A:middle
We also avoid any classes that

00:11:59.196 --> 00:11:59.876 A:middle
might be considered

00:11:59.876 --> 00:12:01.746 A:middle
controversial or offensive as

00:11:59.876 --> 00:12:01.746 A:middle
controversial or offensive as

00:12:01.746 --> 00:12:03.066 A:middle
well as those to do with proper

00:12:03.066 --> 00:12:04.866 A:middle
names, nouns, excuse me,

00:12:04.866 --> 00:12:06.256 A:middle
adjectives, or basic shapes.

00:12:07.346 --> 00:12:09.266 A:middle
Finally, we omit occupations,

00:12:09.526 --> 00:12:10.686 A:middle
and this might seem odd at

00:12:10.686 --> 00:12:11.076 A:middle
first.

00:12:11.716 --> 00:12:12.856 A:middle
But consider the range of

00:12:12.856 --> 00:12:14.266 A:middle
answers you'd get if we asked

00:12:14.266 --> 00:12:15.226 A:middle
something like what does an

00:12:15.226 --> 00:12:16.216 A:middle
engineer look like.

00:12:16.556 --> 00:12:18.496 A:middle
There probably isn't a single

00:12:18.496 --> 00:12:19.546 A:middle
concise description you could

00:12:19.546 --> 00:12:20.856 A:middle
give that would apply to every

00:12:20.856 --> 00:12:22.416 A:middle
engineer aside from sleep

00:12:22.416 --> 00:12:23.716 A:middle
deprived and usually glued to a

00:12:23.716 --> 00:12:24.476 A:middle
computer screen.

00:12:25.556 --> 00:12:26.646 A:middle
Let's take a look at the code

00:12:26.646 --> 00:12:27.816 A:middle
you need to use in order to

00:12:27.816 --> 00:12:28.706 A:middle
classify an image.

00:12:28.706 --> 00:12:31.636 A:middle
So, as usual, you form an

00:12:31.636 --> 00:12:32.896 A:middle
ImageRequestHandler to your

00:12:32.896 --> 00:12:33.526 A:middle
source image.

00:12:34.196 --> 00:12:35.026 A:middle
You then perform the

00:12:35.026 --> 00:12:36.836 A:middle
VNClassifyImageRequest and

00:12:36.836 --> 00:12:38.066 A:middle
retrieve your observations.

00:12:38.746 --> 00:12:40.036 A:middle
Now, in this case, you actually

00:12:40.036 --> 00:12:41.406 A:middle
get an array of observations,

00:12:41.696 --> 00:12:42.976 A:middle
one for every class in the

00:12:42.976 --> 00:12:44.976 A:middle
taxonomy and its associated

00:12:44.976 --> 00:12:45.456 A:middle
confidence.

00:12:46.366 --> 00:12:47.876 A:middle
In a mono-label problem, you'd

00:12:47.876 --> 00:12:48.946 A:middle
probably expect that these

00:12:48.946 --> 00:12:50.906 A:middle
probabilities sum up to 1, but

00:12:50.906 --> 00:12:51.896 A:middle
this is a multi-label

00:12:51.936 --> 00:12:53.876 A:middle
classification network, and each

00:12:53.876 --> 00:12:55.376 A:middle
prediction is an independent

00:12:55.376 --> 00:12:56.756 A:middle
confidence associated with a

00:12:56.756 --> 00:12:57.416 A:middle
particular class.

00:12:58.416 --> 00:13:00.096 A:middle
As such, they won't sum to 1,

00:12:58.416 --> 00:13:00.096 A:middle
As such, they won't sum to 1,

00:13:00.406 --> 00:13:01.466 A:middle
and they're meant to be compared

00:13:01.466 --> 00:13:02.956 A:middle
within the same class, not

00:13:02.956 --> 00:13:04.256 A:middle
across different classes.

00:13:04.496 --> 00:13:06.196 A:middle
So we can't simply take the max

00:13:06.196 --> 00:13:07.486 A:middle
amongst them in order to

00:13:07.486 --> 00:13:08.816 A:middle
determine our final prediction.

00:13:09.696 --> 00:13:11.166 A:middle
You might be wondering then, how

00:13:11.166 --> 00:13:12.806 A:middle
do I deal with so many classes

00:13:12.856 --> 00:13:13.756 A:middle
and so many numbers.

00:13:14.516 --> 00:13:15.836 A:middle
Well, there are a few key tools

00:13:15.836 --> 00:13:17.066 A:middle
in the API that we've

00:13:17.066 --> 00:13:18.186 A:middle
implemented to help you make

00:13:18.256 --> 00:13:19.086 A:middle
sense of the result.

00:13:20.376 --> 00:13:22.026 A:middle
Now, in order to talk about

00:13:22.086 --> 00:13:23.916 A:middle
these tools in the API, we first

00:13:23.916 --> 00:13:25.586 A:middle
need to define some basic terms.

00:13:26.266 --> 00:13:28.716 A:middle
The first is when you get a

00:13:28.716 --> 00:13:30.576 A:middle
confidence for a class, we

00:13:30.576 --> 00:13:31.936 A:middle
typically compare that to a

00:13:31.936 --> 00:13:33.606 A:middle
class-specific threshold, which

00:13:33.606 --> 00:13:34.866 A:middle
we refer to as an operating

00:13:34.866 --> 00:13:35.186 A:middle
point.

00:13:35.976 --> 00:13:37.976 A:middle
If the class confidence is above

00:13:37.976 --> 00:13:39.466 A:middle
the threshold, then we say that

00:13:39.466 --> 00:13:40.796 A:middle
class is present in the image.

00:13:41.266 --> 00:13:42.996 A:middle
If the class confidence is below

00:13:42.996 --> 00:13:44.516 A:middle
the class threshold, then we say

00:13:44.516 --> 00:13:46.316 A:middle
that object is not present in

00:13:46.316 --> 00:13:46.756 A:middle
the image.

00:13:47.656 --> 00:13:49.316 A:middle
In this sense, we want to pick

00:13:49.316 --> 00:13:50.866 A:middle
thresholds such that objects

00:13:50.866 --> 00:13:52.346 A:middle
with the target class typically

00:13:52.346 --> 00:13:53.456 A:middle
have a confidence higher than

00:13:53.456 --> 00:13:55.646 A:middle
the threshold, and images

00:13:55.736 --> 00:13:56.886 A:middle
without the target class

00:13:57.026 --> 00:13:58.556 A:middle
typically have a score lower

00:13:58.556 --> 00:13:59.406 A:middle
than the threshold.

00:14:00.376 --> 00:14:02.216 A:middle
However, machine learning is not

00:14:02.216 --> 00:14:03.866 A:middle
infallible, and there will be

00:14:03.866 --> 00:14:05.636 A:middle
instances where the network is

00:14:05.636 --> 00:14:07.136 A:middle
unsure and the confidence is

00:14:07.136 --> 00:14:08.276 A:middle
proportionally lower.

00:14:09.056 --> 00:14:10.496 A:middle
This can happen when objects are

00:14:10.536 --> 00:14:12.156 A:middle
office gated, appear in odd

00:14:12.156 --> 00:14:14.006 A:middle
lighting or at odd angles, for

00:14:14.006 --> 00:14:14.506 A:middle
instance.

00:14:15.176 --> 00:14:15.866 A:middle
So how do we pick our

00:14:15.866 --> 00:14:16.426 A:middle
thresholds?

00:14:17.596 --> 00:14:18.306 A:middle
Well, there are essentially

00:14:18.436 --> 00:14:19.756 A:middle
three different regimes we can

00:14:19.756 --> 00:14:20.926 A:middle
be in depending on our choice of

00:14:20.926 --> 00:14:22.266 A:middle
threshold that yield three

00:14:22.266 --> 00:14:23.326 A:middle
different kinds of searches.

00:14:24.416 --> 00:14:25.426 A:middle
To make this a little more

00:14:25.426 --> 00:14:26.816 A:middle
concrete, let's say I have a

00:14:26.876 --> 00:14:28.696 A:middle
library of images for which I've

00:14:28.806 --> 00:14:30.346 A:middle
already performed classification

00:14:30.386 --> 00:14:31.246 A:middle
and stored the results.

00:14:32.026 --> 00:14:33.296 A:middle
Let's say in this particular

00:14:33.296 --> 00:14:35.016 A:middle
case I'm looking for images of

00:14:35.016 --> 00:14:35.866 A:middle
motorcycles.

00:14:36.726 --> 00:14:37.626 A:middle
Now, I want to pick my

00:14:37.626 --> 00:14:39.466 A:middle
thresholds such that images with

00:14:39.466 --> 00:14:40.776 A:middle
motorcycles typically have a

00:14:40.776 --> 00:14:41.826 A:middle
confidence higher than this

00:14:41.876 --> 00:14:43.906 A:middle
threshold and images without

00:14:43.906 --> 00:14:45.306 A:middle
motorcycles typically have a

00:14:45.306 --> 00:14:46.946 A:middle
score lower than this threshold.

00:14:47.596 --> 00:14:49.186 A:middle
So, what happens if I just pick

00:14:49.186 --> 00:14:50.006 A:middle
a low threshold.

00:14:50.826 --> 00:14:52.286 A:middle
As you can see behind me, when I

00:14:52.286 --> 00:14:54.246 A:middle
apply this low threshold, I do

00:14:54.246 --> 00:14:55.356 A:middle
in fact get my motorcycle

00:14:55.356 --> 00:14:56.926 A:middle
images, but I'm also getting

00:14:56.926 --> 00:14:58.696 A:middle
these images of mopeds in the

00:14:58.696 --> 00:14:59.246 A:middle
bottom right.

00:14:59.246 --> 00:15:01.106 A:middle
And if my users are motorcycle

00:14:59.246 --> 00:15:01.106 A:middle
And if my users are motorcycle

00:15:01.106 --> 00:15:02.106 A:middle
enthusiasts, they might be a

00:15:02.106 --> 00:15:03.386 A:middle
little annoyed with that result.

00:15:04.516 --> 00:15:06.186 A:middle
When we talk about a search that

00:15:06.186 --> 00:15:07.806 A:middle
tries to maximize the percentage

00:15:07.806 --> 00:15:09.346 A:middle
of the target class retrieved

00:15:09.346 --> 00:15:11.156 A:middle
amongst the entire library, and

00:15:11.246 --> 00:15:12.656 A:middle
isn't as concerned with these

00:15:12.656 --> 00:15:14.196 A:middle
missed predictions where we say

00:15:14.196 --> 00:15:15.596 A:middle
the motorcycle is present when

00:15:15.596 --> 00:15:16.266 A:middle
it actually isn't.

00:15:16.656 --> 00:15:18.036 A:middle
We are typically talking about a

00:15:18.036 --> 00:15:19.076 A:middle
high recall search.

00:15:20.156 --> 00:15:22.236 A:middle
Now, I could maximize recall by

00:15:22.236 --> 00:15:23.806 A:middle
simply returning as many images

00:15:23.806 --> 00:15:25.086 A:middle
as possible, but I would get a

00:15:25.176 --> 00:15:26.526 A:middle
huge number of these false

00:15:26.526 --> 00:15:27.516 A:middle
predictions where I say my

00:15:27.516 --> 00:15:28.806 A:middle
target class is present when it

00:15:28.806 --> 00:15:30.436 A:middle
actually isn't, and so we need

00:15:30.436 --> 00:15:31.666 A:middle
to find a more balanced point of

00:15:31.666 --> 00:15:32.676 A:middle
recall to operate at.

00:15:33.586 --> 00:15:34.956 A:middle
Let's take a look at how I need

00:15:34.956 --> 00:15:36.596 A:middle
to change my code in order to

00:15:36.596 --> 00:15:38.256 A:middle
perform this high recall search.

00:15:39.716 --> 00:15:41.566 A:middle
So, here I have the same code

00:15:41.566 --> 00:15:43.466 A:middle
snippet as before, but this time

00:15:43.606 --> 00:15:45.516 A:middle
I'm performing a filtering with

00:15:45.516 --> 00:15:47.206 A:middle
hasMinimumPrecision and a

00:15:47.206 --> 00:15:48.666 A:middle
specific recall value.

00:15:49.496 --> 00:15:51.526 A:middle
For each observation in my array

00:15:51.526 --> 00:15:53.646 A:middle
of observations, the filter only

00:15:53.646 --> 00:15:55.166 A:middle
retains it if the confidence

00:15:55.166 --> 00:15:56.336 A:middle
associated with the class

00:15:56.656 --> 00:15:57.946 A:middle
achieves the level of recall

00:15:57.946 --> 00:15:58.896 A:middle
that I specified.

00:15:59.646 --> 00:16:01.526 A:middle
Now, the actual operating point

00:15:59.646 --> 00:16:01.526 A:middle
Now, the actual operating point

00:16:01.526 --> 00:16:02.856 A:middle
needed to determine this is

00:16:02.856 --> 00:16:04.196 A:middle
going to be different for every

00:16:04.196 --> 00:16:06.106 A:middle
class, and it's something we've

00:16:06.106 --> 00:16:07.756 A:middle
determined based on our internal

00:16:07.756 --> 00:16:08.986 A:middle
tests of how the network

00:16:08.986 --> 00:16:10.476 A:middle
performs on every class in the

00:16:10.476 --> 00:16:11.116 A:middle
taxonomy.

00:16:12.086 --> 00:16:13.736 A:middle
However, the filter handles this

00:16:13.736 --> 00:16:14.696 A:middle
for you automatically.

00:16:15.026 --> 00:16:16.756 A:middle
All you need to do is specify

00:16:16.756 --> 00:16:18.016 A:middle
the level of recall you want to

00:16:18.016 --> 00:16:18.696 A:middle
operate at.

00:16:19.736 --> 00:16:20.686 A:middle
So, we talked about a high

00:16:20.686 --> 00:16:22.426 A:middle
recall search here, but what if

00:16:22.426 --> 00:16:24.196 A:middle
I have an application that can't

00:16:24.196 --> 00:16:25.646 A:middle
tolerate these false predictions

00:16:25.646 --> 00:16:26.786 A:middle
where I'm saying motorcycles are

00:16:26.786 --> 00:16:27.696 A:middle
present when they're not.

00:16:28.106 --> 00:16:29.746 A:middle
That is, I want to be absolutely

00:16:29.746 --> 00:16:31.456 A:middle
sure that the images I retrieve

00:16:31.626 --> 00:16:32.566 A:middle
actually do contain a

00:16:32.566 --> 00:16:33.246 A:middle
motorcycle.

00:16:34.026 --> 00:16:35.326 A:middle
Well, let's come back to our

00:16:35.326 --> 00:16:36.956 A:middle
library of images then and see

00:16:36.956 --> 00:16:38.236 A:middle
what would happen if we applied

00:16:38.266 --> 00:16:39.186 A:middle
the higher threshold.

00:16:39.956 --> 00:16:41.406 A:middle
As you can see behind me, when I

00:16:41.406 --> 00:16:43.186 A:middle
apply my high threshold, I do in

00:16:43.186 --> 00:16:44.776 A:middle
fact only get motorcycle images,

00:16:45.106 --> 00:16:46.506 A:middle
but I get far fewer images

00:16:46.506 --> 00:16:47.166 A:middle
overall.

00:16:48.536 --> 00:16:50.176 A:middle
When we talk about a search that

00:16:50.176 --> 00:16:51.816 A:middle
tries to maximize the percentage

00:16:51.876 --> 00:16:53.506 A:middle
of the target class amongst the

00:16:53.506 --> 00:16:55.586 A:middle
retrieved images and isn't as

00:16:55.586 --> 00:16:57.146 A:middle
concerned with overlooking some

00:16:57.146 --> 00:16:58.486 A:middle
of the more ambiguous images

00:16:58.536 --> 00:16:59.696 A:middle
that actually do contain the

00:16:59.696 --> 00:17:01.216 A:middle
target class, we are typically

00:16:59.696 --> 00:17:01.216 A:middle
target class, we are typically

00:17:01.216 --> 00:17:03.296 A:middle
talking about a high precision

00:17:03.296 --> 00:17:03.736 A:middle
search.

00:17:04.445 --> 00:17:06.296 A:middle
Again, like with high recall, we

00:17:06.296 --> 00:17:07.316 A:middle
need to find a more balanced

00:17:07.316 --> 00:17:08.656 A:middle
operating point where I have an

00:17:08.656 --> 00:17:10.435 A:middle
acceptable likelihood about my

00:17:10.435 --> 00:17:11.486 A:middle
target class appearing in my

00:17:11.486 --> 00:17:12.886 A:middle
results, but I'm not getting too

00:17:12.986 --> 00:17:13.756 A:middle
few images.

00:17:14.816 --> 00:17:16.116 A:middle
So, let's take a look at how I

00:17:16.116 --> 00:17:17.816 A:middle
need to modify my code in order

00:17:17.816 --> 00:17:19.175 A:middle
to perform this high precision

00:17:19.175 --> 00:17:19.586 A:middle
search.

00:17:20.175 --> 00:17:22.546 A:middle
So here's the same code snippet,

00:17:22.715 --> 00:17:24.215 A:middle
but this time my filtering is

00:17:24.215 --> 00:17:26.366 A:middle
done with hasMinimumRecall and a

00:17:26.366 --> 00:17:27.935 A:middle
precision value I've specified.

00:17:28.806 --> 00:17:30.636 A:middle
Again, I only retain the

00:17:30.636 --> 00:17:32.186 A:middle
observation if the confidence

00:17:32.186 --> 00:17:33.786 A:middle
associated with it achieves the

00:17:33.786 --> 00:17:34.736 A:middle
level of precision that I

00:17:34.736 --> 00:17:35.406 A:middle
specified.

00:17:36.166 --> 00:17:37.546 A:middle
The actual threshold needed for

00:17:37.546 --> 00:17:38.806 A:middle
this is going to be different

00:17:38.806 --> 00:17:40.286 A:middle
for every class, but the filter

00:17:40.286 --> 00:17:41.646 A:middle
handles that for me

00:17:41.646 --> 00:17:42.256 A:middle
automatically.

00:17:42.626 --> 00:17:44.056 A:middle
All I need to do is tell it the

00:17:44.056 --> 00:17:45.026 A:middle
level of precision I want to

00:17:45.026 --> 00:17:45.616 A:middle
operate at.

00:17:46.936 --> 00:17:48.576 A:middle
So we've talked about two

00:17:48.576 --> 00:17:49.846 A:middle
different extremes here, one of

00:17:49.906 --> 00:17:51.236 A:middle
high recall and one of high

00:17:51.236 --> 00:17:53.516 A:middle
precision, but in practice, it

00:17:53.516 --> 00:17:55.216 A:middle
can be better to find a balanced

00:17:55.416 --> 00:17:57.076 A:middle
tradeoff between the two.

00:17:58.096 --> 00:17:59.496 A:middle
So, let's see how we can go

00:17:59.496 --> 00:18:00.896 A:middle
about doing that, and in order

00:17:59.496 --> 00:18:00.896 A:middle
about doing that, and in order

00:18:00.896 --> 00:18:01.966 A:middle
to understand what's happening,

00:18:02.246 --> 00:18:03.266 A:middle
I first need to introduce

00:18:03.396 --> 00:18:04.836 A:middle
something known as the precision

00:18:04.836 --> 00:18:05.646 A:middle
and recall curve.

00:18:06.346 --> 00:18:08.976 A:middle
So, in practice, there is a

00:18:08.976 --> 00:18:10.406 A:middle
tradeoff to be made where

00:18:10.406 --> 00:18:11.986 A:middle
increasing one of precision and

00:18:11.986 --> 00:18:13.846 A:middle
recall can lead to a decrease in

00:18:13.846 --> 00:18:14.196 A:middle
the other.

00:18:14.646 --> 00:18:16.376 A:middle
I can represent this tradeoff as

00:18:16.376 --> 00:18:17.956 A:middle
a graph, where for each

00:18:18.026 --> 00:18:19.586 A:middle
operating point I can compute

00:18:19.586 --> 00:18:20.936 A:middle
the corresponding precision and

00:18:20.936 --> 00:18:21.336 A:middle
recall.

00:18:22.016 --> 00:18:23.476 A:middle
For instance, at the operating

00:18:23.506 --> 00:18:25.156 A:middle
point at where I achieve a

00:18:25.156 --> 00:18:27.116 A:middle
recall of 0.7, I find that I get

00:18:27.116 --> 00:18:28.456 A:middle
a corresponding precision of

00:18:28.456 --> 00:18:29.436 A:middle
0.74.

00:18:29.436 --> 00:18:31.886 A:middle
I can compute this for a

00:18:31.886 --> 00:18:33.486 A:middle
multitude of operating points in

00:18:33.486 --> 00:18:35.016 A:middle
order to form my full curve.

00:18:36.246 --> 00:18:37.956 A:middle
As I said before, I want to find

00:18:37.956 --> 00:18:39.846 A:middle
a balance point along this curve

00:18:40.056 --> 00:18:41.056 A:middle
that achieves the level of

00:18:41.056 --> 00:18:42.286 A:middle
recall and precision that makes

00:18:42.286 --> 00:18:43.536 A:middle
sense for my application.

00:18:44.486 --> 00:18:45.726 A:middle
So let's see how I need to

00:18:45.756 --> 00:18:47.366 A:middle
change my code in order to

00:18:47.366 --> 00:18:48.636 A:middle
accomplish it and how the

00:18:48.636 --> 00:18:50.376 A:middle
precision and recall curve plays

00:18:51.106 --> 00:18:52.146 A:middle
into that.

00:18:52.286 --> 00:18:54.276 A:middle
So here I have a filtering with

00:18:54.276 --> 00:18:55.896 A:middle
hasMinimumPrecision where I'm

00:18:55.896 --> 00:18:57.546 A:middle
specifying the minimum precision

00:18:57.546 --> 00:18:57.976 A:middle
and a recall value.

00:18:58.416 --> 00:19:00.746 A:middle
When I specify a

00:18:58.416 --> 00:19:00.746 A:middle
When I specify a

00:19:00.746 --> 00:19:02.486 A:middle
MinimumPrecision, I'm actually

00:19:02.486 --> 00:19:04.286 A:middle
selecting an area along the

00:19:04.286 --> 00:19:05.566 A:middle
graph that I want to operate

00:19:05.566 --> 00:19:05.976 A:middle
within.

00:19:06.866 --> 00:19:08.236 A:middle
When I select a recall point

00:19:08.286 --> 00:19:10.226 A:middle
with forRecall, I'm choosing a

00:19:10.296 --> 00:19:11.896 A:middle
point along the curve that will

00:19:11.896 --> 00:19:13.006 A:middle
be my operating point.

00:19:13.876 --> 00:19:15.516 A:middle
Now, if the operating point is

00:19:15.576 --> 00:19:16.666 A:middle
in the valid region that I

00:19:16.666 --> 00:19:18.166 A:middle
selected, then that is the

00:19:18.166 --> 00:19:19.616 A:middle
threshold that the filter will

00:19:19.616 --> 00:19:20.916 A:middle
apply when looking at that

00:19:20.916 --> 00:19:21.626 A:middle
particular class.

00:19:22.586 --> 00:19:24.076 A:middle
If the operating point is not in

00:19:24.076 --> 00:19:25.636 A:middle
the valid region, then there is

00:19:25.636 --> 00:19:27.046 A:middle
no operating point that meets

00:19:27.046 --> 00:19:28.536 A:middle
the constraints I stated, and

00:19:28.536 --> 00:19:29.606 A:middle
the class will always be

00:19:29.606 --> 00:19:30.816 A:middle
filtered out of my results.

00:19:31.856 --> 00:19:33.556 A:middle
In this sense, all you need to

00:19:33.556 --> 00:19:35.166 A:middle
do is provide the level of

00:19:35.166 --> 00:19:36.476 A:middle
precision and recall that you

00:19:36.476 --> 00:19:37.686 A:middle
want to operate at, and the

00:19:37.686 --> 00:19:38.626 A:middle
filter will determine the

00:19:38.626 --> 00:19:39.916 A:middle
necessary thresholds for you

00:19:40.076 --> 00:19:40.796 A:middle
automatically.

00:19:41.386 --> 00:19:44.906 A:middle
So, to summarize, the

00:19:44.906 --> 00:19:46.226 A:middle
observation I get back when

00:19:46.226 --> 00:19:47.656 A:middle
performing image classification

00:19:47.986 --> 00:19:49.106 A:middle
is actually an array of

00:19:49.106 --> 00:19:50.656 A:middle
observations, one for every

00:19:50.656 --> 00:19:51.736 A:middle
class in the taxonomy.

00:19:52.876 --> 00:19:54.106 A:middle
Because this is a multi-label

00:19:54.206 --> 00:19:55.566 A:middle
problem, the confidences will

00:19:55.566 --> 00:19:56.456 A:middle
not sum to 1.

00:19:57.166 --> 00:19:58.716 A:middle
Instead, we have independent

00:19:58.716 --> 00:20:00.376 A:middle
confidence values, one for every

00:19:58.716 --> 00:20:00.376 A:middle
confidence values, one for every

00:20:00.376 --> 00:20:02.616 A:middle
class between 0 to 1, and we

00:20:02.616 --> 00:20:04.096 A:middle
need to understand precision and

00:20:04.096 --> 00:20:05.656 A:middle
recall and how they apply to our

00:20:05.656 --> 00:20:07.676 A:middle
specific use case in order to

00:20:07.676 --> 00:20:08.606 A:middle
apply a filtering with

00:20:08.606 --> 00:20:10.226 A:middle
hasMinimumPrecision or

00:20:10.226 --> 00:20:11.856 A:middle
hasMinimumRecall that makes

00:20:11.856 --> 00:20:13.166 A:middle
sense for our application.

00:20:13.606 --> 00:20:15.846 A:middle
So, that concludes the portion

00:20:16.146 --> 00:20:17.496 A:middle
on image classification.

00:20:17.986 --> 00:20:19.416 A:middle
I'd like to switch gears and

00:20:19.416 --> 00:20:20.676 A:middle
talk about a related topic,

00:20:21.226 --> 00:20:22.866 A:middle
Image -- excuse me.

00:20:23.176 --> 00:20:23.976 A:middle
Image Similarity.

00:20:26.216 --> 00:20:27.216 A:middle
When we talk about Image

00:20:27.216 --> 00:20:29.076 A:middle
Similarity, what we really mean

00:20:29.076 --> 00:20:30.406 A:middle
is a method to describe the

00:20:30.406 --> 00:20:32.476 A:middle
content of an image and another

00:20:32.476 --> 00:20:34.286 A:middle
method to compare those

00:20:34.316 --> 00:20:34.956 A:middle
descriptions.

00:20:36.116 --> 00:20:38.106 A:middle
The most basic way in which I

00:20:38.106 --> 00:20:39.676 A:middle
can describe the contents of an

00:20:39.676 --> 00:20:41.436 A:middle
image is using the source pixels

00:20:41.436 --> 00:20:41.986 A:middle
themselves.

00:20:43.536 --> 00:20:45.286 A:middle
That is, I can search for other

00:20:45.286 --> 00:20:47.076 A:middle
images that have close to or

00:20:47.076 --> 00:20:49.196 A:middle
exactly the same pixel values

00:20:49.256 --> 00:20:50.076 A:middle
and retrieve them.

00:20:51.146 --> 00:20:52.136 A:middle
If I did a search in this

00:20:52.136 --> 00:20:54.046 A:middle
fashion, however, it's extremely

00:20:54.046 --> 00:20:55.886 A:middle
fragile, and it's easily fooled

00:20:55.986 --> 00:20:57.646 A:middle
by small changes like rotations

00:20:57.916 --> 00:20:59.466 A:middle
or lighting augmentations that

00:20:59.466 --> 00:21:00.656 A:middle
drastically change the pixel

00:20:59.466 --> 00:21:00.656 A:middle
drastically change the pixel

00:21:00.656 --> 00:21:02.256 A:middle
values but not the semantic

00:21:02.256 --> 00:21:03.376 A:middle
content in the image.

00:21:04.266 --> 00:21:05.536 A:middle
What I really want is a more

00:21:05.536 --> 00:21:07.246 A:middle
high-level description of what

00:21:07.246 --> 00:21:08.836 A:middle
the content of the image is,

00:21:08.836 --> 00:21:09.926 A:middle
perhaps something like natural

00:21:09.926 --> 00:21:10.426 A:middle
language.

00:21:10.936 --> 00:21:13.126 A:middle
I could make use of the image

00:21:13.126 --> 00:21:14.696 A:middle
classification API I was

00:21:14.696 --> 00:21:16.426 A:middle
describing previously in order

00:21:16.426 --> 00:21:17.636 A:middle
to extract a set of words that

00:21:17.636 --> 00:21:18.626 A:middle
describe my image.

00:21:19.446 --> 00:21:20.936 A:middle
I could then retrieve other

00:21:20.936 --> 00:21:22.426 A:middle
images with a similar set of

00:21:22.426 --> 00:21:23.316 A:middle
classifications.

00:21:23.756 --> 00:21:25.106 A:middle
I might even combine this with

00:21:25.106 --> 00:21:26.256 A:middle
something like word vectors to

00:21:26.326 --> 00:21:27.716 A:middle
account for similar but not

00:21:27.716 --> 00:21:29.386 A:middle
exactly matching words like cat

00:21:29.386 --> 00:21:30.356 A:middle
and kitten.

00:21:30.696 --> 00:21:31.896 A:middle
Well, if I performed a search

00:21:31.896 --> 00:21:33.556 A:middle
like this, I might get similar

00:21:33.556 --> 00:21:35.046 A:middle
objects in a very general sense,

00:21:35.446 --> 00:21:36.406 A:middle
but the way in which those

00:21:36.406 --> 00:21:37.556 A:middle
objects appear and the

00:21:37.556 --> 00:21:39.066 A:middle
relationships between them could

00:21:39.066 --> 00:21:40.026 A:middle
be very different.

00:21:40.966 --> 00:21:43.036 A:middle
As well, I would be limited by

00:21:43.036 --> 00:21:44.586 A:middle
the taxonomy of my classifier.

00:21:45.386 --> 00:21:46.736 A:middle
That is, any object that

00:21:46.736 --> 00:21:48.426 A:middle
appeared in my image that wasn't

00:21:48.426 --> 00:21:49.756 A:middle
in my classification networks

00:21:49.756 --> 00:21:51.686 A:middle
taxonomy couldn't be expressed

00:21:51.786 --> 00:21:52.876 A:middle
in a search like this.

00:21:54.216 --> 00:21:55.486 A:middle
What I really want is a

00:21:55.486 --> 00:21:56.966 A:middle
high-level description of the

00:21:56.996 --> 00:21:58.176 A:middle
objects that appear in the image

00:21:58.386 --> 00:21:59.856 A:middle
that isn't fixated on the exact

00:21:59.856 --> 00:22:01.096 A:middle
pixel values but still cares

00:21:59.856 --> 00:22:01.096 A:middle
pixel values but still cares

00:22:01.096 --> 00:22:01.556 A:middle
about them.

00:22:02.256 --> 00:22:04.006 A:middle
I also want this to apply to any

00:22:04.006 --> 00:22:05.896 A:middle
natural image and not just those

00:22:05.896 --> 00:22:07.266 A:middle
within a specific taxonomy.

00:22:07.746 --> 00:22:09.946 A:middle
As it turns out, this kind of

00:22:09.946 --> 00:22:11.756 A:middle
representation learning is

00:22:11.756 --> 00:22:12.676 A:middle
something that's naturally

00:22:12.676 --> 00:22:14.016 A:middle
engendered in our classification

00:22:14.016 --> 00:22:15.296 A:middle
network as part of its training

00:22:15.296 --> 00:22:15.786 A:middle
process.

00:22:16.786 --> 00:22:18.736 A:middle
The upper layers of the network

00:22:18.976 --> 00:22:20.356 A:middle
contain all of the salient

00:22:20.356 --> 00:22:22.166 A:middle
information necessary to perform

00:22:22.166 --> 00:22:23.686 A:middle
classification while discarding

00:22:23.756 --> 00:22:25.466 A:middle
any redundant or unnecessary

00:22:25.466 --> 00:22:26.726 A:middle
information that doesn't aid it

00:22:26.726 --> 00:22:28.336 A:middle
in that task.

00:22:28.386 --> 00:22:29.596 A:middle
We can make use of these upper

00:22:29.596 --> 00:22:31.136 A:middle
layers then to act as our

00:22:31.136 --> 00:22:32.456 A:middle
feature descriptor, and it's

00:22:32.456 --> 00:22:33.516 A:middle
something we refer to as the

00:22:33.516 --> 00:22:34.156 A:middle
feature print.

00:22:35.196 --> 00:22:36.706 A:middle
Now, the feature print is a

00:22:36.706 --> 00:22:37.946 A:middle
vector that describes the

00:22:37.946 --> 00:22:39.666 A:middle
content of the image that isn't

00:22:39.666 --> 00:22:40.696 A:middle
constrained to a particular

00:22:40.696 --> 00:22:42.086 A:middle
taxonomy, even the one that the

00:22:42.086 --> 00:22:43.146 A:middle
classification network was

00:22:43.146 --> 00:22:43.636 A:middle
trained on.

00:22:43.896 --> 00:22:45.106 A:middle
It simply leverages what the

00:22:45.106 --> 00:22:46.486 A:middle
network has learned about images

00:22:46.486 --> 00:22:47.736 A:middle
during its training process.

00:22:48.816 --> 00:22:50.296 A:middle
If we look at these pairs of

00:22:50.296 --> 00:22:51.696 A:middle
images, we can compare how

00:22:51.696 --> 00:22:52.786 A:middle
similar their feature prints

00:22:52.786 --> 00:22:54.236 A:middle
are, and the smaller the value

00:22:54.236 --> 00:22:55.486 A:middle
is, the more similar the two

00:22:55.486 --> 00:22:57.296 A:middle
images are in a semantic sense.

00:22:58.046 --> 00:22:59.316 A:middle
We can see that even though the

00:22:59.316 --> 00:23:00.556 A:middle
two images of the cats are

00:22:59.316 --> 00:23:00.556 A:middle
two images of the cats are

00:23:00.556 --> 00:23:02.026 A:middle
visually dissimilar, they have a

00:23:02.026 --> 00:23:03.506 A:middle
much more similar feature print

00:23:03.746 --> 00:23:05.526 A:middle
than the visually similar pairs

00:23:05.596 --> 00:23:06.636 A:middle
of different animals.

00:23:07.136 --> 00:23:09.416 A:middle
To make this a little more

00:23:09.416 --> 00:23:10.646 A:middle
concrete, let's go through a

00:23:10.646 --> 00:23:11.686 A:middle
specific example.

00:23:12.396 --> 00:23:13.476 A:middle
Let's say I have the source

00:23:13.476 --> 00:23:15.186 A:middle
image on screen, and I want to

00:23:15.186 --> 00:23:16.706 A:middle
find other semantically similar

00:23:16.706 --> 00:23:17.476 A:middle
images to it.

00:23:18.336 --> 00:23:19.656 A:middle
I'm going to take a library of

00:23:19.656 --> 00:23:21.196 A:middle
images and compute the feature

00:23:21.196 --> 00:23:22.896 A:middle
print for each image and then

00:23:22.896 --> 00:23:24.476 A:middle
retrieve those images with the

00:23:24.476 --> 00:23:26.296 A:middle
most similar feature print to my

00:23:26.296 --> 00:23:26.986 A:middle
source image.

00:23:27.746 --> 00:23:29.136 A:middle
When I do it for this image of

00:23:29.136 --> 00:23:29.926 A:middle
the gentleman in the coffee

00:23:29.926 --> 00:23:31.876 A:middle
shop, I find I get other images

00:23:31.926 --> 00:23:33.176 A:middle
of people in coffee shop and

00:23:33.176 --> 00:23:33.946 A:middle
restaurant settings.

00:23:34.886 --> 00:23:36.266 A:middle
If I focus on a crop of the

00:23:36.266 --> 00:23:38.246 A:middle
newspaper, however, I get other

00:23:38.246 --> 00:23:39.586 A:middle
images of newspapers.

00:23:40.246 --> 00:23:42.156 A:middle
And if I focus on the teapot, I

00:23:42.236 --> 00:23:43.906 A:middle
get other images of teapots.

00:23:45.326 --> 00:23:46.616 A:middle
I'd like to now invite the

00:23:46.616 --> 00:23:48.616 A:middle
Vision Team onstage to help me

00:23:48.616 --> 00:23:50.116 A:middle
with a quick demonstration to

00:23:50.116 --> 00:23:51.546 A:middle
expand a little more on how

00:23:51.546 --> 00:23:52.646 A:middle
Image Similarity works.

00:23:54.516 --> 00:23:58.166 A:middle
[ Applause ]

00:23:58.666 --> 00:23:59.466 A:middle
&gt;&gt; Hello everyone.

00:23:59.886 --> 00:24:00.846 A:middle
My name is Brett, and we have a

00:23:59.886 --> 00:24:00.846 A:middle
My name is Brett, and we have a

00:24:00.846 --> 00:24:02.046 A:middle
really fun way to demonstrate

00:24:02.046 --> 00:24:03.546 A:middle
Image Similarity for you today.

00:24:03.546 --> 00:24:05.286 A:middle
We have very creatively called

00:24:05.286 --> 00:24:06.846 A:middle
it the Image Similarity game.

00:24:07.876 --> 00:24:09.086 A:middle
And here is how you play.

00:24:09.676 --> 00:24:11.066 A:middle
You draw something on a piece of

00:24:11.066 --> 00:24:14.026 A:middle
paper, then ask a few friends to

00:24:14.026 --> 00:24:15.166 A:middle
re-create your original as close

00:24:15.166 --> 00:24:15.726 A:middle
as possible.

00:24:16.296 --> 00:24:17.596 A:middle
So I will start by drawing the

00:24:17.596 --> 00:24:17.976 A:middle
original.

00:24:30.096 --> 00:24:33.496 A:middle
Okay. Tap continue to scan it in

00:24:33.496 --> 00:24:33.976 A:middle
as my original.

00:24:42.046 --> 00:24:42.976 A:middle
And then save.

00:24:44.526 --> 00:24:46.456 A:middle
Now, my team will act as

00:24:46.456 --> 00:24:47.886 A:middle
contestants, and they will draw

00:24:48.156 --> 00:24:48.976 A:middle
this as best as they can.

00:24:54.266 --> 00:24:55.206 A:middle
Now while they're drawing, I

00:24:55.206 --> 00:24:57.316 A:middle
should tell you that this sample

00:24:57.316 --> 00:24:58.476 A:middle
app is currently available to

00:24:58.536 --> 00:25:00.576 A:middle
you now on the developer

00:24:58.536 --> 00:25:00.576 A:middle
you now on the developer

00:25:00.576 --> 00:25:04.356 A:middle
documentation website as sample

00:25:04.356 --> 00:25:05.986 A:middle
code, and also, we are using the

00:25:05.986 --> 00:25:07.586 A:middle
Vision kit document scanner to

00:25:07.586 --> 00:25:08.886 A:middle
scan in our drawings, and you

00:25:08.886 --> 00:25:09.966 A:middle
can learn more about that at our

00:25:09.966 --> 00:25:11.226 A:middle
text recognition session.

00:25:12.706 --> 00:25:15.076 A:middle
Let's them give a few more

00:25:16.636 --> 00:25:16.886 A:middle
seconds.

00:25:16.966 --> 00:25:19.896 A:middle
Five, four, three, okay, I guess

00:25:19.926 --> 00:25:20.186 A:middle
they're done.

00:25:20.266 --> 00:25:22.186 A:middle
Okay. Let's bring them up and

00:25:22.186 --> 00:25:24.266 A:middle
start scanning them in.

00:25:24.996 --> 00:25:26.256 A:middle
Contestant number one.

00:25:31.046 --> 00:25:31.576 A:middle
Pretty good [applause].

00:25:32.756 --> 00:25:33.406 A:middle
That might be a winner.

00:25:33.406 --> 00:25:35.256 A:middle
Let's see contestant number two.

00:25:38.296 --> 00:25:39.786 A:middle
Still pretty good.

00:25:40.096 --> 00:25:40.506 A:middle
Nicely done.

00:25:41.256 --> 00:25:43.256 A:middle
[ Applause ]

00:25:43.496 --> 00:25:44.706 A:middle
Contestant number three please.

00:25:48.056 --> 00:25:50.056 A:middle
[ Laughter and Applause ]

00:25:50.096 --> 00:25:50.976 A:middle
I think that's pretty good.

00:25:51.016 --> 00:25:52.586 A:middle
[ Applause ]

00:25:52.586 --> 00:25:53.696 A:middle
And contestant number four.

00:25:57.046 --> 00:25:58.136 A:middle
Well, I don't know about that,

00:25:58.196 --> 00:26:00.836 A:middle
but we'll see how it goes.

00:25:58.196 --> 00:26:00.836 A:middle
but we'll see how it goes.

00:26:01.151 --> 00:26:03.151 A:middle
[ Applause ]

00:26:03.286 --> 00:26:03.836 A:middle
All right.

00:26:03.836 --> 00:26:07.006 A:middle
So let's save those, and we find

00:26:07.006 --> 00:26:08.286 A:middle
out that the winner is

00:26:08.286 --> 00:26:09.636 A:middle
contestant number one.

00:26:09.636 --> 00:26:10.446 A:middle
Congratulations.

00:26:11.016 --> 00:26:12.736 A:middle
[ Applause ]

00:26:12.736 --> 00:26:14.716 A:middle
Now I can swipe over, and we can

00:26:14.796 --> 00:26:16.076 A:middle
see that the faces are more

00:26:16.076 --> 00:26:17.656 A:middle
semantically similar that way.

00:26:18.336 --> 00:26:19.996 A:middle
They are closer to the original

00:26:20.096 --> 00:26:21.376 A:middle
while the tree is semantically

00:26:21.376 --> 00:26:22.416 A:middle
different, it was much further

00:26:22.416 --> 00:26:22.686 A:middle
away.

00:26:23.336 --> 00:26:24.426 A:middle
And that is the Image Similarity

00:26:24.426 --> 00:26:25.376 A:middle
game, and background check to

00:26:25.376 --> 00:26:25.726 A:middle
Rohan.

00:26:26.516 --> 00:26:31.776 A:middle
[ Applause ]

00:26:32.276 --> 00:26:32.876 A:middle
&gt;&gt; Thanks everyone.

00:26:33.476 --> 00:26:34.736 A:middle
I want to take a quick look at a

00:26:34.736 --> 00:26:35.846 A:middle
snippet from that demo

00:26:35.846 --> 00:26:37.196 A:middle
application to show how we

00:26:37.196 --> 00:26:37.946 A:middle
determined the winning

00:26:37.946 --> 00:26:38.546 A:middle
contestant.

00:26:39.686 --> 00:26:41.906 A:middle
So here I have the portion of

00:26:41.906 --> 00:26:43.506 A:middle
the code that compares each of

00:26:43.506 --> 00:26:45.286 A:middle
the contestant's drawings

00:26:45.396 --> 00:26:46.796 A:middle
feature print to Brett's

00:26:46.796 --> 00:26:47.666 A:middle
drawing's feature print.

00:26:48.326 --> 00:26:49.006 A:middle
Now, I extracted the

00:26:49.006 --> 00:26:50.276 A:middle
contestant's feature print with

00:26:50.276 --> 00:26:51.396 A:middle
a function we have defined in

00:26:51.396 --> 00:26:52.566 A:middle
the application called

00:26:52.566 --> 00:26:54.466 A:middle
featureprintObservationForImage.

00:26:55.186 --> 00:26:56.956 A:middle
Once I have each feature print,

00:26:57.226 --> 00:26:58.666 A:middle
I then need to determine how

00:26:58.666 --> 00:27:00.056 A:middle
similar it was to the original

00:26:58.666 --> 00:27:00.056 A:middle
similar it was to the original

00:27:00.056 --> 00:27:01.846 A:middle
drawing, and I can do that using

00:27:01.846 --> 00:27:03.246 A:middle
computeDistance, which returns

00:27:03.246 --> 00:27:04.576 A:middle
me a floating-point value.

00:27:05.166 --> 00:27:05.826 A:middle
Now, the smaller the

00:27:05.826 --> 00:27:07.206 A:middle
floating-point value, the more

00:27:07.206 --> 00:27:08.646 A:middle
similar the two images are.

00:27:09.236 --> 00:27:10.456 A:middle
And so, once I've determined

00:27:10.456 --> 00:27:11.636 A:middle
this for every contestant, I

00:27:11.636 --> 00:27:13.116 A:middle
simply need to sort them in

00:27:13.116 --> 00:27:14.276 A:middle
order to determine the winner.

00:27:15.336 --> 00:27:17.006 A:middle
Well, this concludes the portion

00:27:17.156 --> 00:27:18.156 A:middle
on Image Similarity.

00:27:18.486 --> 00:27:19.536 A:middle
I'd now like to hand the mic

00:27:19.536 --> 00:27:20.816 A:middle
over to Sergey to talk about

00:27:20.816 --> 00:27:21.866 A:middle
some of the changes coming to

00:27:21.866 --> 00:27:22.836 A:middle
Face Technologies.

00:27:23.516 --> 00:27:28.500 A:middle
[ Applause ]

00:27:33.056 --> 00:27:33.876 A:middle
&gt;&gt; Good morning everybody.

00:27:34.196 --> 00:27:35.226 A:middle
My name is Sergey Kamensky.

00:27:35.226 --> 00:27:36.196 A:middle
I'm a software engineer on the

00:27:36.196 --> 00:27:36.926 A:middle
Vision Framework Team.

00:27:37.356 --> 00:27:38.456 A:middle
I'm excited to share with you

00:27:38.456 --> 00:27:39.786 A:middle
today even more new features

00:27:39.876 --> 00:27:40.926 A:middle
coming to the Framework this

00:27:40.926 --> 00:27:41.096 A:middle
year.

00:27:41.096 --> 00:27:43.286 A:middle
Let's talk about Face Technology

00:27:43.286 --> 00:27:43.606 A:middle
first.

00:27:44.206 --> 00:27:45.956 A:middle
Remember, two years ago when we

00:27:45.956 --> 00:27:47.836 A:middle
introduced Vision Framework, we

00:27:47.836 --> 00:27:49.366 A:middle
also talked about Face Landmark

00:27:49.416 --> 00:27:49.806 A:middle
detector.

00:27:50.266 --> 00:27:51.596 A:middle
This year, we're coming with a

00:27:51.596 --> 00:27:52.796 A:middle
new revision for this algorithm.

00:27:53.176 --> 00:27:54.806 A:middle
So, what are the changes?

00:27:55.916 --> 00:27:57.786 A:middle
Well, first, we now have

00:27:57.926 --> 00:27:59.616 A:middle
76-point cancellation, and this

00:27:59.616 --> 00:28:01.426 A:middle
is versus 65-point cancellation

00:27:59.616 --> 00:28:01.426 A:middle
is versus 65-point cancellation

00:28:01.426 --> 00:28:02.166 A:middle
as we had before.

00:28:02.166 --> 00:28:04.116 A:middle
The 76-point cancellation gives

00:28:04.116 --> 00:28:05.146 A:middle
us a greater density to

00:28:05.146 --> 00:28:06.276 A:middle
represent different face

00:28:06.276 --> 00:28:06.686 A:middle
regions.

00:28:07.556 --> 00:28:09.556 A:middle
Second, we now report confidence

00:28:09.556 --> 00:28:11.306 A:middle
score per landmark point, and

00:28:11.306 --> 00:28:12.886 A:middle
this is versus a single average

00:28:12.886 --> 00:28:14.446 A:middle
confidence score, as we reported

00:28:14.446 --> 00:28:14.836 A:middle
before.

00:28:14.836 --> 00:28:16.646 A:middle
But the biggest improvement

00:28:16.646 --> 00:28:18.126 A:middle
comes in the pupil detection.

00:28:18.706 --> 00:28:19.996 A:middle
As you can see, the image on the

00:28:19.996 --> 00:28:21.296 A:middle
right-hand side has pupils

00:28:21.296 --> 00:28:22.406 A:middle
detected with much better

00:28:22.566 --> 00:28:23.066 A:middle
accuracy.

00:28:23.516 --> 00:28:25.886 A:middle
Let's take a look at the client

00:28:25.886 --> 00:28:26.416 A:middle
code sample.

00:28:27.986 --> 00:28:29.676 A:middle
This code snippet will repeat

00:28:29.676 --> 00:28:31.056 A:middle
throughout the presentation so

00:28:31.056 --> 00:28:32.386 A:middle
the first time we're going to go

00:28:32.386 --> 00:28:33.206 A:middle
line by line.

00:28:33.496 --> 00:28:35.516 A:middle
Also, I use for [inaudible] in

00:28:35.516 --> 00:28:36.246 A:middle
my samples.

00:28:36.246 --> 00:28:37.946 A:middle
If this is just to simplify the

00:28:37.946 --> 00:28:39.376 A:middle
slides, when you develop your

00:28:39.376 --> 00:28:40.736 A:middle
apps, you probably should use

00:28:40.786 --> 00:28:42.276 A:middle
proper error handling to avoid

00:28:42.276 --> 00:28:43.516 A:middle
unwanted boundary conditions.

00:28:44.256 --> 00:28:45.156 A:middle
Let's get back to the sample.

00:28:46.276 --> 00:28:47.276 A:middle
In order to get your facial

00:28:47.276 --> 00:28:48.866 A:middle
landmarks, first you need to

00:28:48.866 --> 00:28:49.416 A:middle
create a

00:28:49.416 --> 00:28:50.716 A:middle
DetectFaceLandmarksRequest.

00:28:51.296 --> 00:28:52.536 A:middle
Then, you need to create

00:28:52.536 --> 00:28:54.036 A:middle
ImageRequestHandler, passing the

00:28:54.036 --> 00:28:56.096 A:middle
image into it the image that

00:28:56.096 --> 00:28:58.346 A:middle
needs to be processed, and then

00:28:58.626 --> 00:28:59.626 A:middle
you need to use that request

00:28:59.626 --> 00:29:00.776 A:middle
handler to process your request.

00:28:59.626 --> 00:29:00.776 A:middle
handler to process your request.

00:29:01.426 --> 00:29:03.136 A:middle
Finally, you need to look at the

00:29:03.136 --> 00:29:03.606 A:middle
results.

00:29:04.386 --> 00:29:05.446 A:middle
The results for everything that

00:29:05.446 --> 00:29:06.736 A:middle
this human face related in

00:29:06.736 --> 00:29:08.156 A:middle
Vision Framework will come in

00:29:08.196 --> 00:29:09.946 A:middle
forms of face observations.

00:29:10.506 --> 00:29:12.006 A:middle
Face observation derives some

00:29:12.006 --> 00:29:13.486 A:middle
detected object observation.

00:29:13.856 --> 00:29:15.006 A:middle
It inherits bounding box

00:29:15.046 --> 00:29:16.396 A:middle
property, and it also adds

00:29:16.396 --> 00:29:17.766 A:middle
several other properties on its

00:29:17.766 --> 00:29:19.886 A:middle
level to describe human face.

00:29:20.766 --> 00:29:21.806 A:middle
This time we'll be interested in

00:29:21.806 --> 00:29:22.636 A:middle
the landmarks property.

00:29:23.136 --> 00:29:24.366 A:middle
The landmarks property is of

00:29:24.426 --> 00:29:25.976 A:middle
FaceLandmarks2D class.

00:29:26.176 --> 00:29:28.166 A:middle
FaceLandmarks2D class consists

00:29:28.416 --> 00:29:29.896 A:middle
of the confidence score.

00:29:30.086 --> 00:29:31.356 A:middle
This is the average single

00:29:31.356 --> 00:29:32.476 A:middle
average confidence score for the

00:29:32.476 --> 00:29:34.716 A:middle
entire set and multiple face

00:29:34.716 --> 00:29:37.296 A:middle
regions where each face region

00:29:37.296 --> 00:29:38.196 A:middle
is represented by

00:29:38.196 --> 00:29:40.176 A:middle
FaceLandmarksRegion2D class.

00:29:40.636 --> 00:29:41.856 A:middle
Let's take a closer look at the

00:29:41.856 --> 00:29:44.536 A:middle
properties of this class.

00:29:44.716 --> 00:29:46.166 A:middle
First is pointCount.

00:29:46.666 --> 00:29:48.156 A:middle
PointCount will tell you how

00:29:48.156 --> 00:29:49.546 A:middle
many points represent a

00:29:49.546 --> 00:29:50.756 A:middle
particular face region.

00:29:50.996 --> 00:29:52.256 A:middle
This property will [inaudible] a

00:29:52.256 --> 00:29:53.616 A:middle
different value depending how

00:29:53.616 --> 00:29:55.366 A:middle
you configure your request, with

00:29:55.366 --> 00:29:56.936 A:middle
65-point cancellation or

00:29:56.936 --> 00:29:58.376 A:middle
76-point cancellation.

00:29:59.646 --> 00:30:01.146 A:middle
The normalizedPoints property

00:29:59.646 --> 00:30:01.146 A:middle
The normalizedPoints property

00:30:01.916 --> 00:30:03.876 A:middle
will represent the actual

00:30:03.876 --> 00:30:05.596 A:middle
landmarks point, and the

00:30:05.596 --> 00:30:07.326 A:middle
precisionEstimatesPerPoint will

00:30:07.326 --> 00:30:08.916 A:middle
represent the actual confidence

00:30:08.916 --> 00:30:10.416 A:middle
score for teach landmark point.

00:30:11.456 --> 00:30:12.386 A:middle
Let's take a look at the codes

00:30:12.386 --> 00:30:12.666 A:middle
needed.

00:30:12.666 --> 00:30:14.496 A:middle
This is the same code snippet as

00:30:14.496 --> 00:30:16.106 A:middle
in the previous slide, but now

00:30:16.106 --> 00:30:17.056 A:middle
we're going to look at it from a

00:30:17.056 --> 00:30:18.196 A:middle
slightly different perspective.

00:30:18.576 --> 00:30:20.266 A:middle
We want to see how revisioning

00:30:20.266 --> 00:30:22.296 A:middle
of the algorithm works in Vision

00:30:22.296 --> 00:30:22.696 A:middle
Framework.

00:30:23.446 --> 00:30:25.056 A:middle
If you take this code snippet

00:30:25.056 --> 00:30:26.276 A:middle
and recompile it with the last

00:30:26.276 --> 00:30:28.076 A:middle
[inaudible], what you will get

00:30:28.076 --> 00:30:30.086 A:middle
is that the request object will

00:30:30.086 --> 00:30:31.806 A:middle
be configured as follows: the

00:30:31.806 --> 00:30:33.196 A:middle
revision property will be set to

00:30:33.196 --> 00:30:34.786 A:middle
revision number 2, and the

00:30:34.786 --> 00:30:36.136 A:middle
cancellation property will be

00:30:36.136 --> 00:30:37.426 A:middle
set to cancellation of 65

00:30:37.426 --> 00:30:37.816 A:middle
points.

00:30:38.396 --> 00:30:39.056 A:middle
Technically, we didn't have

00:30:39.056 --> 00:30:40.286 A:middle
cancellation property last year,

00:30:40.286 --> 00:30:41.456 A:middle
but if we did, we could have set

00:30:41.456 --> 00:30:42.616 A:middle
it to a single value only.

00:30:43.426 --> 00:30:45.726 A:middle
Now, if on the other hand you

00:30:45.726 --> 00:30:47.246 A:middle
take the same code snippet and

00:30:47.246 --> 00:30:49.136 A:middle
recompile it with the current

00:30:49.686 --> 00:30:50.336 A:middle
[inaudible], what you will get

00:30:50.336 --> 00:30:51.856 A:middle
is that the revision property

00:30:51.856 --> 00:30:52.846 A:middle
will be set to revision number

00:30:52.846 --> 00:30:55.056 A:middle
3, and the cancellation property

00:30:55.056 --> 00:30:56.726 A:middle
will be set to cancellation 76

00:30:56.726 --> 00:30:57.076 A:middle
points.

00:30:58.626 --> 00:30:59.706 A:middle
This actually represents the

00:30:59.706 --> 00:31:00.846 A:middle
philosophy of how Vision

00:30:59.706 --> 00:31:00.846 A:middle
philosophy of how Vision

00:31:00.846 --> 00:31:02.606 A:middle
Framework handles revisions of

00:31:02.606 --> 00:31:04.006 A:middle
algorithms by default.

00:31:04.096 --> 00:31:05.766 A:middle
If you don't specify a revision,

00:31:05.766 --> 00:31:07.716 A:middle
what we will do is, we will give

00:31:07.716 --> 00:31:09.366 A:middle
the latest supported by the SDK

00:31:09.366 --> 00:31:11.436 A:middle
your code is compiled and linked

00:31:11.436 --> 00:31:11.916 A:middle
against.

00:31:12.116 --> 00:31:14.066 A:middle
Of course, we'll always

00:31:14.066 --> 00:31:14.856 A:middle
recommend to set those

00:31:14.856 --> 00:31:15.836 A:middle
properties explicitly.

00:31:16.116 --> 00:31:17.106 A:middle
This is just to guarantee

00:31:17.106 --> 00:31:18.296 A:middle
deterministic behavior in the

00:31:18.296 --> 00:31:18.656 A:middle
future.

00:31:19.386 --> 00:31:22.266 A:middle
Let's take a new metric that we

00:31:22.266 --> 00:31:23.556 A:middle
developed this year, Face

00:31:23.556 --> 00:31:24.286 A:middle
Capture Quality.

00:31:24.786 --> 00:31:25.686 A:middle
There are two images on the

00:31:25.686 --> 00:31:25.946 A:middle
screen.

00:31:26.296 --> 00:31:27.416 A:middle
You can clearly see that one

00:31:27.416 --> 00:31:28.536 A:middle
image was captured with better

00:31:28.536 --> 00:31:29.436 A:middle
lighting and focusing

00:31:29.436 --> 00:31:29.996 A:middle
conditions.

00:31:30.846 --> 00:31:32.016 A:middle
We wanted to develop the metric

00:31:32.016 --> 00:31:33.376 A:middle
that looks at the image as a

00:31:33.376 --> 00:31:35.016 A:middle
whole and gives you one score

00:31:35.016 --> 00:31:36.796 A:middle
back saying how bad or good the

00:31:36.796 --> 00:31:37.966 A:middle
capture quality was.

00:31:37.966 --> 00:31:40.366 A:middle
As a result, we came up with a

00:31:40.366 --> 00:31:41.596 A:middle
Face Capture Quality metric.

00:31:42.466 --> 00:31:43.656 A:middle
We trained our models for this

00:31:43.656 --> 00:31:45.026 A:middle
metric in such a way so they

00:31:45.026 --> 00:31:46.876 A:middle
tend to score lower if the image

00:31:46.876 --> 00:31:48.456 A:middle
was captured with low light or

00:31:48.456 --> 00:31:50.206 A:middle
bad focus, or for example, if a

00:31:50.236 --> 00:31:51.936 A:middle
person had negative expressions.

00:31:52.896 --> 00:31:54.316 A:middle
If we run this metric on these

00:31:54.346 --> 00:31:55.626 A:middle
two images, we will get our

00:31:55.626 --> 00:31:56.206 A:middle
scores back.

00:31:57.036 --> 00:31:57.876 A:middle
These are floating-point

00:31:57.876 --> 00:31:58.316 A:middle
numbers.

00:31:58.636 --> 00:31:59.636 A:middle
You can compare them against

00:31:59.636 --> 00:32:00.776 A:middle
each other, and you can say that

00:31:59.636 --> 00:32:00.776 A:middle
each other, and you can say that

00:32:00.776 --> 00:32:02.856 A:middle
the image that scored higher is

00:32:02.856 --> 00:32:04.056 A:middle
the image that was captured with

00:32:04.056 --> 00:32:04.716 A:middle
better quality.

00:32:05.346 --> 00:32:07.466 A:middle
Let's take a look at the code

00:32:07.466 --> 00:32:08.136 A:middle
sample.

00:32:09.606 --> 00:32:10.756 A:middle
This is very similar to what we

00:32:10.756 --> 00:32:11.986 A:middle
saw just a couple of slides ago,

00:32:12.196 --> 00:32:13.416 A:middle
with the differences being in

00:32:13.416 --> 00:32:15.596 A:middle
the request type and the

00:32:15.596 --> 00:32:16.136 A:middle
results.

00:32:16.846 --> 00:32:18.526 A:middle
Since we still with C1 faces,

00:32:18.736 --> 00:32:19.556 A:middle
we're going to get our face

00:32:19.556 --> 00:32:20.826 A:middle
observation back, but now we're

00:32:20.826 --> 00:32:21.686 A:middle
going to look at a different

00:32:21.686 --> 00:32:22.426 A:middle
property of the face

00:32:22.426 --> 00:32:23.926 A:middle
observation, Face Capture

00:32:23.926 --> 00:32:24.626 A:middle
Quality property.

00:32:24.876 --> 00:32:27.546 A:middle
Let's take a look at the broader

00:32:27.546 --> 00:32:27.946 A:middle
example.

00:32:28.616 --> 00:32:29.646 A:middle
Let's say I have a sequence of

00:32:29.646 --> 00:32:30.586 A:middle
images that could have been

00:32:30.586 --> 00:32:31.976 A:middle
obtained by using the burst mode

00:32:31.976 --> 00:32:33.346 A:middle
on the selfie camera or in the

00:32:33.346 --> 00:32:34.316 A:middle
photo burst, for example.

00:32:34.626 --> 00:32:35.776 A:middle
And you will ask yourself a

00:32:35.776 --> 00:32:36.166 A:middle
question.

00:32:36.426 --> 00:32:37.816 A:middle
Which image was captured with

00:32:37.816 --> 00:32:38.566 A:middle
the best quality?

00:32:39.686 --> 00:32:40.966 A:middle
What you can do now, you can run

00:32:40.966 --> 00:32:42.166 A:middle
our algorithm on each image,

00:32:42.506 --> 00:32:45.366 A:middle
assign scores, rank them, and

00:32:45.366 --> 00:32:46.466 A:middle
the image that apps on the most

00:32:46.466 --> 00:32:47.876 A:middle
light is the image that was

00:32:47.876 --> 00:32:50.896 A:middle
captured with the best quality.

00:32:50.966 --> 00:32:52.436 A:middle
Let's try to understand how we

00:32:52.436 --> 00:32:54.026 A:middle
can interpret the results that

00:32:54.026 --> 00:32:55.166 A:middle
are coming from the Face Capture

00:32:55.196 --> 00:32:55.856 A:middle
Quality metric.

00:32:56.586 --> 00:32:58.206 A:middle
I have two sequences of images

00:32:58.356 --> 00:32:58.886 A:middle
on the slide.

00:32:59.426 --> 00:33:00.726 A:middle
Each sequence is of the same

00:32:59.426 --> 00:33:00.726 A:middle
Each sequence is of the same

00:33:00.726 --> 00:33:02.146 A:middle
person, and each sequence is

00:33:02.146 --> 00:33:03.456 A:middle
represented by the images that

00:33:03.456 --> 00:33:05.286 A:middle
scores lowest and the highest in

00:33:05.286 --> 00:33:06.346 A:middle
the sequence with respect to

00:33:06.346 --> 00:33:07.216 A:middle
Face Capture Quality.

00:33:08.116 --> 00:33:09.076 A:middle
What can we say about these

00:33:09.076 --> 00:33:09.496 A:middle
ranges?

00:33:10.816 --> 00:33:12.356 A:middle
Well, there is some overlapping

00:33:12.356 --> 00:33:13.696 A:middle
region, but there are some also

00:33:13.696 --> 00:33:15.006 A:middle
regions that belong to one and

00:33:15.006 --> 00:33:16.336 A:middle
don't belong to the other.

00:33:16.516 --> 00:33:18.216 A:middle
If you had yet another sequence,

00:33:18.536 --> 00:33:19.306 A:middle
it could have happened that

00:33:19.306 --> 00:33:20.536 A:middle
there was no overlapping region

00:33:20.536 --> 00:33:20.926 A:middle
at all.

00:33:21.856 --> 00:33:22.976 A:middle
The point I'm trying to make

00:33:22.976 --> 00:33:24.686 A:middle
here is that the Face Capture

00:33:24.686 --> 00:33:26.146 A:middle
Quality should not be compared

00:33:26.146 --> 00:33:26.906 A:middle
against a threshold.

00:33:28.126 --> 00:33:29.466 A:middle
In this particular example, if I

00:33:29.466 --> 00:33:32.096 A:middle
picked 0.52, I would have missed

00:33:32.326 --> 00:33:33.876 A:middle
all the images on the left, and

00:33:33.876 --> 00:33:36.556 A:middle
I would pretty much can get any

00:33:36.556 --> 00:33:37.406 A:middle
image that's just past the

00:33:37.406 --> 00:33:38.626 A:middle
midpoint on the right.

00:33:39.996 --> 00:33:41.286 A:middle
But then what is Face Capture

00:33:41.346 --> 00:33:41.676 A:middle
Quality?

00:33:42.706 --> 00:33:44.246 A:middle
We define Face Capture Quality

00:33:44.246 --> 00:33:46.286 A:middle
is a comparative or ranking

00:33:46.326 --> 00:33:47.766 A:middle
measure of the same subject.

00:33:48.186 --> 00:33:49.756 A:middle
Now, comparative and same are

00:33:49.756 --> 00:33:51.086 A:middle
the key words in this sentence.

00:33:51.526 --> 00:33:53.216 A:middle
If you're thinking, cool, I have

00:33:53.216 --> 00:33:54.476 A:middle
this great new metric, I'm going

00:33:54.476 --> 00:33:55.606 A:middle
to develop my beauty contest

00:33:55.606 --> 00:33:55.726 A:middle
app.

00:33:56.946 --> 00:33:58.126 A:middle
Probably not a good idea.

00:33:58.606 --> 00:33:59.636 A:middle
In a beauty contest app, you

00:33:59.636 --> 00:34:01.296 A:middle
would have to compare faces of

00:33:59.636 --> 00:34:01.296 A:middle
would have to compare faces of

00:34:01.296 --> 00:34:02.806 A:middle
different people, and that's not

00:34:02.806 --> 00:34:03.996 A:middle
what this metric was developed

00:34:03.996 --> 00:34:04.626 A:middle
and designed for.

00:34:06.266 --> 00:34:07.696 A:middle
And that's Face Technology.

00:34:09.295 --> 00:34:10.076 A:middle
Let's take a look at the new

00:34:10.076 --> 00:34:11.146 A:middle
detectors we're adding this

00:34:11.146 --> 00:34:11.335 A:middle
year.

00:34:12.896 --> 00:34:15.466 A:middle
We're introducing Human Detector

00:34:15.466 --> 00:34:16.726 A:middle
that detects human upper body

00:34:16.726 --> 00:34:18.076 A:middle
that consists of human head and

00:34:18.076 --> 00:34:20.386 A:middle
torso and also a pet detector,

00:34:20.795 --> 00:34:22.326 A:middle
an Animal Detector that detects

00:34:22.696 --> 00:34:23.406 A:middle
cats and dogs.

00:34:23.735 --> 00:34:24.755 A:middle
The Animal Detector gives you

00:34:24.755 --> 00:34:26.166 A:middle
bounding box back, and in

00:34:26.166 --> 00:34:27.286 A:middle
addition to bounding boxes it

00:34:27.286 --> 00:34:28.636 A:middle
gives you also a label saying

00:34:28.866 --> 00:34:30.235 A:middle
which animal was detected.

00:34:31.795 --> 00:34:32.746 A:middle
Let's take a look at the client

00:34:32.746 --> 00:34:33.226 A:middle
code sample.

00:34:35.956 --> 00:34:37.916 A:middle
Two snippets, one for Human

00:34:37.916 --> 00:34:39.366 A:middle
Detector, one for Animal

00:34:39.366 --> 00:34:39.815 A:middle
Detector.

00:34:40.326 --> 00:34:41.386 A:middle
Very similar to what we had

00:34:41.386 --> 00:34:41.835 A:middle
before.

00:34:42.016 --> 00:34:43.646 A:middle
Again, the differences are in

00:34:43.646 --> 00:34:44.596 A:middle
the request types that you

00:34:44.596 --> 00:34:46.386 A:middle
create and in the results.

00:34:47.186 --> 00:34:49.525 A:middle
Now, for Human Detector, all we

00:34:49.826 --> 00:34:51.275 A:middle
care about is the bounding box.

00:34:51.886 --> 00:34:53.166 A:middle
So, we use for that

00:34:53.306 --> 00:34:54.606 A:middle
DetectedObjectObservation.

00:34:55.726 --> 00:34:56.706 A:middle
For the Animal Detector on the

00:34:56.706 --> 00:34:57.706 A:middle
other hand, we also need the

00:34:57.706 --> 00:34:58.986 A:middle
label, so we use

00:34:59.676 --> 00:35:01.206 A:middle
RecognizedObjectObservation that

00:34:59.676 --> 00:35:01.206 A:middle
RecognizedObjectObservation that

00:35:01.206 --> 00:35:02.496 A:middle
derives from detected object

00:35:02.496 --> 00:35:03.076 A:middle
observation.

00:35:03.186 --> 00:35:04.536 A:middle
It inherits bounding box, but it

00:35:04.536 --> 00:35:06.736 A:middle
also adds a label property on

00:35:06.806 --> 00:35:07.296 A:middle
the [inaudible].

00:35:07.806 --> 00:35:10.486 A:middle
And that's new detectors.

00:35:11.246 --> 00:35:12.456 A:middle
Let's take a look at what's new

00:35:12.456 --> 00:35:13.336 A:middle
in tracking this year.

00:35:14.216 --> 00:35:15.126 A:middle
We're coming up with a new

00:35:15.126 --> 00:35:16.556 A:middle
revision for the Tracker.

00:35:16.556 --> 00:35:17.996 A:middle
The changes are, we have

00:35:17.996 --> 00:35:19.516 A:middle
improvements in the bounding

00:35:19.516 --> 00:35:20.396 A:middle
boxes expansion area.

00:35:21.266 --> 00:35:22.386 A:middle
We can now handle better

00:35:22.386 --> 00:35:22.976 A:middle
occlusions.

00:35:23.536 --> 00:35:25.716 A:middle
We are machine learning based

00:35:25.716 --> 00:35:26.176 A:middle
this time.

00:35:26.886 --> 00:35:28.086 A:middle
And we can run with low power

00:35:28.086 --> 00:35:29.046 A:middle
consumption on multiple

00:35:29.046 --> 00:35:29.476 A:middle
[inaudible] devices.

00:35:29.476 --> 00:35:32.516 A:middle
Let's take a look at a sample.

00:35:32.956 --> 00:35:35.056 A:middle
I have a mini video clip where a

00:35:35.056 --> 00:35:36.446 A:middle
man is running in the forest,

00:35:36.606 --> 00:35:38.126 A:middle
and he appears sometimes behind

00:35:38.126 --> 00:35:38.646 A:middle
the trees.

00:35:38.976 --> 00:35:40.126 A:middle
As you can see, the tracker is

00:35:40.126 --> 00:35:41.606 A:middle
able to successfully recapture

00:35:41.606 --> 00:35:42.926 A:middle
the tracked object and keep

00:35:42.926 --> 00:35:43.726 A:middle
going with the tracking

00:35:43.726 --> 00:35:44.186 A:middle
sequence.

00:35:46.016 --> 00:35:47.046 A:middle
[ Applause ]

00:35:47.046 --> 00:35:47.486 A:middle
Thank you.

00:35:48.516 --> 00:35:51.976 A:middle
[ Applause ]

00:35:52.476 --> 00:35:53.756 A:middle
Let's take a look at the client

00:35:53.756 --> 00:35:54.256 A:middle
code sample.

00:35:54.676 --> 00:35:56.046 A:middle
This is exactly the same snippet

00:35:56.046 --> 00:35:56.996 A:middle
that we showed last year.

00:35:57.256 --> 00:35:58.306 A:middle
It represents probably the

00:35:58.306 --> 00:35:59.416 A:middle
simplest tracking sequence you

00:35:59.416 --> 00:35:59.946 A:middle
can imagine.

00:36:00.166 --> 00:36:01.106 A:middle
It tracks your object of

00:36:01.106 --> 00:36:02.336 A:middle
interest for five consecutive

00:36:02.336 --> 00:36:02.786 A:middle
frames.

00:36:03.986 --> 00:36:05.356 A:middle
I want to go line-by-line, but I

00:36:05.356 --> 00:36:06.916 A:middle
want to emphasize two points

00:36:06.916 --> 00:36:07.116 A:middle
here.

00:36:07.486 --> 00:36:09.116 A:middle
First is we use our

00:36:09.116 --> 00:36:10.036 A:middle
SequenceRequestHandler.

00:36:11.066 --> 00:36:12.046 A:middle
That is as opposite to

00:36:12.046 --> 00:36:13.226 A:middle
ImageRequestHandler as we have

00:36:13.226 --> 00:36:14.226 A:middle
used so far throughout the

00:36:14.226 --> 00:36:14.816 A:middle
presentation.

00:36:15.336 --> 00:36:16.616 A:middle
SequenceRequestHandler is used

00:36:16.616 --> 00:36:17.986 A:middle
in Vision when you work with a

00:36:17.986 --> 00:36:19.216 A:middle
sequence of frames and you need

00:36:19.216 --> 00:36:20.426 A:middle
to cache some information from

00:36:20.426 --> 00:36:21.496 A:middle
frame to frame to frame.

00:36:22.826 --> 00:36:24.356 A:middle
Second point is when you

00:36:24.356 --> 00:36:25.106 A:middle
implement your tracking

00:36:25.106 --> 00:36:26.836 A:middle
sequence, you need to get your

00:36:26.836 --> 00:36:28.356 A:middle
results from iteration number n

00:36:28.356 --> 00:36:30.026 A:middle
and feed it as an input to a

00:36:30.026 --> 00:36:31.276 A:middle
duration number n plus 1.

00:36:31.976 --> 00:36:35.216 A:middle
Of course, if you recompiled

00:36:35.216 --> 00:36:36.106 A:middle
this quote with the current

00:36:36.106 --> 00:36:37.646 A:middle
[inaudible] SDK, the revision of

00:36:37.646 --> 00:36:38.846 A:middle
the request will be set to

00:36:38.846 --> 00:36:40.316 A:middle
revision number 2 by default.

00:36:40.496 --> 00:36:41.836 A:middle
But we also recommend to set it

00:36:41.836 --> 00:36:42.426 A:middle
explicitly.

00:36:42.966 --> 00:36:45.106 A:middle
And that's the tracking.

00:36:46.136 --> 00:36:47.376 A:middle
Let's take a look at the news

00:36:47.376 --> 00:36:49.696 A:middle
with respect to Vision and Core

00:36:49.696 --> 00:36:50.396 A:middle
ML integration.

00:36:51.146 --> 00:36:52.766 A:middle
Last year, we presented

00:36:52.766 --> 00:36:53.836 A:middle
integration with Vision and Core

00:36:53.836 --> 00:36:55.496 A:middle
ML, and we showed how you can

00:36:55.496 --> 00:36:57.066 A:middle
run Core ML models through

00:36:57.066 --> 00:36:57.596 A:middle
Vision API.

00:36:57.596 --> 00:36:59.776 A:middle
The advantage of doing that was

00:36:59.776 --> 00:37:01.706 A:middle
that you can use 1 over 5

00:36:59.776 --> 00:37:01.706 A:middle
that you can use 1 over 5

00:37:01.816 --> 00:37:03.236 A:middle
different overloads of the image

00:37:03.236 --> 00:37:04.916 A:middle
request handler to translate the

00:37:04.916 --> 00:37:06.266 A:middle
image that you have in your hand

00:37:06.586 --> 00:37:08.976 A:middle
to the image type, size, and

00:37:08.976 --> 00:37:10.416 A:middle
color scheme that the Core ML

00:37:10.416 --> 00:37:11.266 A:middle
model requires.

00:37:12.246 --> 00:37:13.256 A:middle
We will run the inference for

00:37:13.256 --> 00:37:15.016 A:middle
you, and we'll pack the outputs

00:37:15.016 --> 00:37:16.436 A:middle
or results coming from Core ML

00:37:16.436 --> 00:37:17.886 A:middle
model into Vision observations.

00:37:20.716 --> 00:37:22.426 A:middle
Now, if you have a different

00:37:22.426 --> 00:37:23.696 A:middle
task in mind, for example, if

00:37:23.696 --> 00:37:24.646 A:middle
you want to do image style

00:37:24.646 --> 00:37:26.066 A:middle
transfer, you need to have at

00:37:26.066 --> 00:37:27.416 A:middle
least two images, the image

00:37:27.416 --> 00:37:29.056 A:middle
content and the image style.

00:37:29.256 --> 00:37:30.356 A:middle
You may also need to have some

00:37:30.356 --> 00:37:31.816 A:middle
mixed ratio saying how much of a

00:37:31.926 --> 00:37:33.386 A:middle
style needs to be applied on the

00:37:33.386 --> 00:37:33.826 A:middle
content.

00:37:34.306 --> 00:37:35.506 A:middle
So, I have three parameters now.

00:37:36.826 --> 00:37:37.886 A:middle
Well, this year we're going to

00:37:37.886 --> 00:37:39.666 A:middle
introduce API where we can use

00:37:39.666 --> 00:37:41.716 A:middle
multiple inputs through Vision

00:37:41.896 --> 00:37:43.476 A:middle
to Core ML, and that's including

00:37:43.476 --> 00:37:44.426 A:middle
multi-image inputs.

00:37:44.426 --> 00:37:47.696 A:middle
Also, on the output section,

00:37:48.186 --> 00:37:49.346 A:middle
this sample shows only one

00:37:49.346 --> 00:37:49.686 A:middle
output.

00:37:49.686 --> 00:37:50.616 A:middle
But, for example, if you had

00:37:50.616 --> 00:37:52.116 A:middle
more than one, especially if you

00:37:52.116 --> 00:37:53.376 A:middle
have more than one of the same

00:37:53.376 --> 00:37:55.256 A:middle
type, it's hard to distinguish

00:37:55.256 --> 00:37:56.356 A:middle
them when they come in forms of

00:37:56.356 --> 00:37:57.946 A:middle
observation later on.

00:37:58.416 --> 00:37:59.616 A:middle
So, what we do this year, we

00:37:59.616 --> 00:38:00.646 A:middle
introduce a new field in the

00:37:59.616 --> 00:38:00.646 A:middle
introduce a new field in the

00:38:00.646 --> 00:38:02.616 A:middle
observation that maps exactly to

00:38:02.616 --> 00:38:04.376 A:middle
the name that shows up here in

00:38:04.376 --> 00:38:05.056 A:middle
the output section.

00:38:06.216 --> 00:38:07.696 A:middle
Let's take a look at the inputs

00:38:07.696 --> 00:38:08.316 A:middle
and outputs.

00:38:08.316 --> 00:38:09.316 A:middle
We will use them in the next

00:38:09.316 --> 00:38:09.576 A:middle
slide.

00:38:12.936 --> 00:38:14.136 A:middle
This is the code snippet that

00:38:14.176 --> 00:38:16.816 A:middle
represents how to use Core ML

00:38:16.956 --> 00:38:17.396 A:middle
through Vision.

00:38:18.676 --> 00:38:20.236 A:middle
The highlighted sections show

00:38:20.446 --> 00:38:21.256 A:middle
what's new this year.

00:38:21.716 --> 00:38:22.796 A:middle
Let's keep them for now, and

00:38:22.796 --> 00:38:23.896 A:middle
we'll go over the code, and

00:38:23.896 --> 00:38:25.316 A:middle
we'll return to them later.

00:38:26.156 --> 00:38:27.466 A:middle
In order to run Core ML through

00:38:27.466 --> 00:38:29.446 A:middle
Vision, first you need to log

00:38:29.446 --> 00:38:30.236 A:middle
your Core ML model.

00:38:31.266 --> 00:38:32.926 A:middle
Then, you need to create Vision

00:38:32.926 --> 00:38:34.956 A:middle
CoreMLmodel wrapper around it.

00:38:35.726 --> 00:38:37.476 A:middle
Then, you need to create Vision

00:38:37.476 --> 00:38:39.346 A:middle
CoreMLRequest and pass in that

00:38:39.346 --> 00:38:39.676 A:middle
wrapper.

00:38:41.266 --> 00:38:42.326 A:middle
Then you create

00:38:42.556 --> 00:38:44.136 A:middle
ImageRequestHandler, you process

00:38:44.136 --> 00:38:45.446 A:middle
your request, and you look at

00:38:45.446 --> 00:38:46.086 A:middle
the results.

00:38:47.386 --> 00:38:49.596 A:middle
Now, with the new API that we

00:38:49.596 --> 00:38:51.596 A:middle
added this year, that only image

00:38:51.596 --> 00:38:53.116 A:middle
that you could use last year is

00:38:53.116 --> 00:38:54.986 A:middle
the default or the main image is

00:38:54.986 --> 00:38:56.276 A:middle
the image that is passing to

00:38:56.476 --> 00:38:58.046 A:middle
ImageRequestHandler, but that's

00:38:58.046 --> 00:38:59.526 A:middle
also the image whose name needs

00:38:59.526 --> 00:39:01.126 A:middle
to be assigned to input feature

00:38:59.526 --> 00:39:01.126 A:middle
to be assigned to input feature

00:39:01.506 --> 00:39:03.456 A:middle
name field of the CoreMLModel

00:39:03.456 --> 00:39:03.796 A:middle
wrapper.

00:39:05.026 --> 00:39:06.886 A:middle
All other parameters whether

00:39:06.886 --> 00:39:08.646 A:middle
images or not will have to be

00:39:08.716 --> 00:39:10.406 A:middle
passed through feature provider

00:39:10.446 --> 00:39:11.686 A:middle
property of the CoreMLModel

00:39:11.686 --> 00:39:12.016 A:middle
wrapper.

00:39:12.376 --> 00:39:14.036 A:middle
As you can see, image style and

00:39:14.036 --> 00:39:15.436 A:middle
mixed ratio are passed in that

00:39:15.436 --> 00:39:15.666 A:middle
way.

00:39:16.956 --> 00:39:18.336 A:middle
Finally, when you look at the

00:39:18.336 --> 00:39:19.506 A:middle
results, you can look at the

00:39:19.786 --> 00:39:21.386 A:middle
feature name property of the

00:39:21.386 --> 00:39:22.646 A:middle
observation that comes out, and

00:39:22.646 --> 00:39:24.186 A:middle
you can compare it in this case

00:39:24.186 --> 00:39:25.246 A:middle
against image result.

00:39:25.506 --> 00:39:26.656 A:middle
That's exactly the name that

00:39:26.656 --> 00:39:27.706 A:middle
appears in the output section of

00:39:27.756 --> 00:39:29.066 A:middle
Core ML, and that way you can

00:39:29.096 --> 00:39:30.006 A:middle
process your results

00:39:30.146 --> 00:39:30.666 A:middle
accordingly.

00:39:32.636 --> 00:39:33.626 A:middle
This slide actually concludes

00:39:33.626 --> 00:39:34.666 A:middle
our presentation for today.

00:39:34.966 --> 00:39:36.076 A:middle
For more information you can

00:39:36.076 --> 00:39:37.136 A:middle
refer to the links on the slide.

00:39:37.466 --> 00:39:39.276 A:middle
Thank you, and have a great rest

00:39:39.276 --> 00:39:39.976 A:middle
of your WWDC.

00:39:40.016 --> 00:39:42.000 A:middle
[ Applause ]
