WEBVTT

00:00:00.506 --> 00:00:05.916 A:middle
[ Music ]

00:00:06.416 --> 00:00:08.846 A:middle
&gt;&gt; Hi. I'm Neha Agrawal, and I'm

00:00:08.846 --> 00:00:10.516 A:middle
a software engineer working on

00:00:10.516 --> 00:00:11.426 A:middle
speech recognition.

00:00:12.426 --> 00:00:14.776 A:middle
In 2016, we introduced the

00:00:14.776 --> 00:00:16.436 A:middle
Speech Recognition framework for

00:00:16.686 --> 00:00:18.756 A:middle
developers to solve their speech

00:00:18.756 --> 00:00:19.686 A:middle
recognition needs.

00:00:20.326 --> 00:00:22.096 A:middle
For anyone who is new to this

00:00:22.096 --> 00:00:24.096 A:middle
framework, I highly recommend

00:00:24.316 --> 00:00:25.876 A:middle
watching this Speech Recognition

00:00:25.876 --> 00:00:28.166 A:middle
API session by my colleague

00:00:28.316 --> 00:00:29.246 A:middle
Henry Mason.

00:00:31.066 --> 00:00:32.695 A:middle
In this video, we're going to

00:00:32.695 --> 00:00:35.106 A:middle
discuss exciting new advances in

00:00:35.106 --> 00:00:35.426 A:middle
the APIs.

00:00:35.426 --> 00:00:36.906 A:middle
Let's get started.

00:00:39.216 --> 00:00:40.596 A:middle
Speech recognition is now

00:00:40.596 --> 00:00:42.206 A:middle
supported for macOS.

00:00:43.036 --> 00:00:44.706 A:middle
The support is available for

00:00:44.706 --> 00:00:47.376 A:middle
both AppKit and iPad apps on

00:00:47.376 --> 00:00:47.716 A:middle
Mac.

00:00:48.886 --> 00:00:51.536 A:middle
Just like iOS, over 50 languages

00:00:51.536 --> 00:00:52.266 A:middle
are supported.

00:00:53.816 --> 00:00:55.756 A:middle
You need approval from your

00:00:55.756 --> 00:00:57.856 A:middle
users to access the microphone

00:00:57.976 --> 00:00:59.616 A:middle
and record their speech, and

00:01:00.406 --> 00:01:01.736 A:middle
they also need to have Siri

00:01:01.736 --> 00:01:02.246 A:middle
enabled.

00:01:02.876 --> 00:01:05.596 A:middle
In addition to supporting speech

00:01:05.596 --> 00:01:08.366 A:middle
recognition on macOS, we are now

00:01:08.366 --> 00:01:10.056 A:middle
allowing developers to run

00:01:10.056 --> 00:01:12.026 A:middle
recognition on-device for

00:01:12.026 --> 00:01:13.936 A:middle
privacy sensitive applications.

00:01:14.646 --> 00:01:17.876 A:middle
With on-device support, your

00:01:17.876 --> 00:01:19.746 A:middle
user's data will not be sent to

00:01:19.746 --> 00:01:20.586 A:middle
Apple servers.

00:01:22.096 --> 00:01:24.156 A:middle
Your app no longer needs to rely

00:01:24.446 --> 00:01:27.136 A:middle
on a network connection, and

00:01:27.136 --> 00:01:28.626 A:middle
cellular data will not be

00:01:28.626 --> 00:01:29.196 A:middle
consumed.

00:01:31.216 --> 00:01:33.556 A:middle
However, there are tradeoffs to

00:01:33.556 --> 00:01:34.026 A:middle
consider.

00:01:34.736 --> 00:01:37.426 A:middle
Accuracy is good on-device, but

00:01:37.426 --> 00:01:39.536 A:middle
you may find it is better on

00:01:39.536 --> 00:01:40.996 A:middle
server due to a continuous

00:01:40.996 --> 00:01:41.416 A:middle
learning.

00:01:41.666 --> 00:01:43.896 A:middle
A server-based recognition

00:01:43.896 --> 00:01:46.246 A:middle
support has limits on number of

00:01:46.246 --> 00:01:48.196 A:middle
requests and audio duration.

00:01:48.976 --> 00:01:50.496 A:middle
With on-device recognition,

00:01:50.876 --> 00:01:52.436 A:middle
these limits do not apply.

00:01:53.726 --> 00:01:54.776 A:middle
The number of languages

00:01:54.776 --> 00:01:57.256 A:middle
supported on server are more

00:01:57.256 --> 00:01:58.536 A:middle
than on-device.

00:01:59.846 --> 00:02:02.126 A:middle
Also, if server isn't available,

00:01:59.846 --> 00:02:02.126 A:middle
Also, if server isn't available,

00:02:02.126 --> 00:02:04.426 A:middle
our server mode automatically

00:02:04.426 --> 00:02:06.376 A:middle
falls back on on-device

00:02:06.376 --> 00:02:08.276 A:middle
recognition if it is supported.

00:02:09.156 --> 00:02:11.736 A:middle
All iPhones and iPads with Apple

00:02:12.156 --> 00:02:13.196 A:middle
A9 or later processors are

00:02:13.196 --> 00:02:15.156 A:middle
supported, and all Mac devices

00:02:15.156 --> 00:02:15.746 A:middle
are supported.

00:02:16.776 --> 00:02:18.086 A:middle
There are over 10 languages

00:02:18.086 --> 00:02:19.246 A:middle
supported for on-device

00:02:19.246 --> 00:02:19.856 A:middle
recognition.

00:02:20.426 --> 00:02:23.746 A:middle
Now, let's look at how to enable

00:02:23.976 --> 00:02:25.876 A:middle
on-device recognition in code.

00:02:26.536 --> 00:02:29.016 A:middle
To recognize pre-recorded audio,

00:02:29.386 --> 00:02:30.986 A:middle
we first create an

00:02:30.986 --> 00:02:33.066 A:middle
SFSpeechRecognizer object and

00:02:33.066 --> 00:02:35.096 A:middle
check for availability of speech

00:02:35.096 --> 00:02:36.856 A:middle
recognition on that object.

00:02:39.206 --> 00:02:40.346 A:middle
If speech recognition is

00:02:40.346 --> 00:02:42.516 A:middle
available, we can create a

00:02:42.516 --> 00:02:44.376 A:middle
recognition request with the

00:02:44.376 --> 00:02:46.366 A:middle
audio file URL and start

00:02:46.366 --> 00:02:47.026 A:middle
recognition.

00:02:49.676 --> 00:02:51.896 A:middle
Now, in order to use on-device

00:02:51.896 --> 00:02:53.616 A:middle
recognition, you need to first

00:02:53.616 --> 00:02:55.416 A:middle
check if on-device recognition

00:02:55.416 --> 00:02:57.396 A:middle
is supported and then set

00:02:57.606 --> 00:02:59.446 A:middle
requiresOnDeviceRecognition

00:02:59.446 --> 00:03:01.396 A:middle
property on the request object.

00:02:59.446 --> 00:03:01.396 A:middle
property on the request object.

00:03:03.136 --> 00:03:04.586 A:middle
Now that we have looked at this

00:03:04.586 --> 00:03:06.296 A:middle
in code, let's talk about the

00:03:06.296 --> 00:03:07.006 A:middle
results you get.

00:03:07.466 --> 00:03:11.466 A:middle
Since iOS 10 in speech

00:03:11.466 --> 00:03:13.016 A:middle
recognition results, we have

00:03:13.016 --> 00:03:14.616 A:middle
provided transcriptions,

00:03:14.966 --> 00:03:16.486 A:middle
alternative interpretations,

00:03:16.786 --> 00:03:18.456 A:middle
confidence levels and timing

00:03:18.456 --> 00:03:19.096 A:middle
information.

00:03:20.636 --> 00:03:22.146 A:middle
We're making a few more

00:03:22.146 --> 00:03:23.616 A:middle
additions to the speech

00:03:23.616 --> 00:03:24.556 A:middle
recognition results.

00:03:26.776 --> 00:03:29.186 A:middle
Speaking rate measures how fast

00:03:29.186 --> 00:03:31.016 A:middle
a person speaks in words per

00:03:31.016 --> 00:03:31.396 A:middle
minute.

00:03:33.366 --> 00:03:35.006 A:middle
Average pause duration measures

00:03:35.276 --> 00:03:36.556 A:middle
the average length of pause

00:03:36.556 --> 00:03:37.336 A:middle
between words.

00:03:37.906 --> 00:03:41.086 A:middle
And voice analytics features

00:03:41.306 --> 00:03:43.196 A:middle
include various measures of

00:03:43.196 --> 00:03:44.566 A:middle
vocal characteristics.

00:03:46.116 --> 00:03:47.906 A:middle
Now, voice analytics gives

00:03:47.906 --> 00:03:49.416 A:middle
insight into four features.

00:03:50.186 --> 00:03:52.216 A:middle
Jitter measures how pitch varies

00:03:52.216 --> 00:03:52.726 A:middle
in audio.

00:03:53.446 --> 00:03:55.296 A:middle
With voice analytics, you can

00:03:55.296 --> 00:03:57.066 A:middle
now understand the amount of

00:03:57.066 --> 00:03:59.066 A:middle
jitter in speech expressed as a

00:03:59.066 --> 00:03:59.816 A:middle
percentage.

00:04:01.236 --> 00:04:02.926 A:middle
Shimmer measures how amplitude

00:04:02.926 --> 00:04:04.886 A:middle
varies in audio, and with voice

00:04:04.886 --> 00:04:06.706 A:middle
analytics, you can understand

00:04:06.706 --> 00:04:10.336 A:middle
shimmer in speech expressed in

00:04:11.536 --> 00:04:11.946 A:middle
decibels.

00:04:12.096 --> 00:04:13.376 A:middle
Let's listen to some audio

00:04:13.376 --> 00:04:14.466 A:middle
samples to understand what

00:04:14.466 --> 00:04:16.166 A:middle
speech with high jitter and

00:04:16.166 --> 00:04:17.125 A:middle
shimmer sounds like.

00:04:17.606 --> 00:04:19.526 A:middle
First, let's hear audio with

00:04:19.526 --> 00:04:20.276 A:middle
normal speech.

00:04:20.866 --> 00:04:21.546 A:middle
&gt;&gt; Apple.

00:04:23.276 --> 00:04:24.756 A:middle
&gt;&gt; Now, audio with perturbed

00:04:24.756 --> 00:04:25.236 A:middle
speech.

00:04:25.956 --> 00:04:26.846 A:middle
&gt;&gt; Apple.

00:04:27.926 --> 00:04:29.176 A:middle
&gt;&gt; Next feature is pitch.

00:04:29.966 --> 00:04:31.976 A:middle
Pitch measures the highness and

00:04:31.976 --> 00:04:33.286 A:middle
lowness of the tone.

00:04:33.656 --> 00:04:35.476 A:middle
Often, women and children have

00:04:35.476 --> 00:04:36.266 A:middle
higher pitch.

00:04:37.496 --> 00:04:39.426 A:middle
And voicing is used to identify

00:04:39.776 --> 00:04:41.766 A:middle
voiced regions in speech.

00:04:42.316 --> 00:04:44.956 A:middle
The voice analytics features are

00:04:44.956 --> 00:04:46.516 A:middle
specific to an individual, and

00:04:46.516 --> 00:04:49.036 A:middle
they can vary with time and

00:04:49.036 --> 00:04:49.926 A:middle
circumstances.

00:04:50.636 --> 00:04:52.506 A:middle
For example, if the person is

00:04:52.506 --> 00:04:54.876 A:middle
tired, these features will be

00:04:54.876 --> 00:04:56.616 A:middle
different than when they're not.

00:04:57.466 --> 00:04:59.346 A:middle
Also, depending on who the

00:04:59.346 --> 00:05:00.996 A:middle
person is talking to, these

00:04:59.346 --> 00:05:00.996 A:middle
person is talking to, these

00:05:00.996 --> 00:05:01.976 A:middle
features may vary.

00:05:04.416 --> 00:05:06.556 A:middle
These new results are part of

00:05:06.556 --> 00:05:08.436 A:middle
the SF transcription object and

00:05:08.436 --> 00:05:10.016 A:middle
will be available periodically.

00:05:10.556 --> 00:05:12.776 A:middle
We will have them at the end

00:05:12.986 --> 00:05:14.586 A:middle
when the isFinal flag is sent,

00:05:14.996 --> 00:05:16.246 A:middle
but we could also see them

00:05:16.246 --> 00:05:16.586 A:middle
before.

00:05:17.306 --> 00:05:19.806 A:middle
You can access speakingRate and

00:05:19.806 --> 00:05:21.796 A:middle
averagePauseDuration as shown.

00:05:23.736 --> 00:05:26.566 A:middle
To access voice analytics, you

00:05:26.566 --> 00:05:28.446 A:middle
would have to access the SF

00:05:28.446 --> 00:05:30.346 A:middle
transcription segment object,

00:05:30.586 --> 00:05:32.256 A:middle
and then you can access it as

00:05:32.256 --> 00:05:32.846 A:middle
shown here.

00:05:34.636 --> 00:05:37.006 A:middle
To summarize, we have made three

00:05:37.006 --> 00:05:37.846 A:middle
key advances.

00:05:38.506 --> 00:05:40.326 A:middle
You can now build apps on macOS

00:05:40.656 --> 00:05:42.366 A:middle
using speech recognition APIs.

00:05:43.546 --> 00:05:45.066 A:middle
Speech recognition can be run

00:05:45.066 --> 00:05:47.196 A:middle
on-device in a privacy-friendly

00:05:47.196 --> 00:05:47.606 A:middle
manner.

00:05:48.386 --> 00:05:50.696 A:middle
And you now have access to voice

00:05:50.696 --> 00:05:52.686 A:middle
analytics features for getting

00:05:52.686 --> 00:05:54.076 A:middle
insight into vocal

00:05:54.076 --> 00:05:55.136 A:middle
characteristics.

00:05:57.176 --> 00:05:59.486 A:middle
For more information, check out

00:05:59.486 --> 00:06:01.006 A:middle
the session's web page and

00:05:59.486 --> 00:06:01.006 A:middle
the session's web page and

00:06:01.006 --> 00:06:01.976 A:middle
thanks for watching.
