1
00:00:06,210 --> 00:00:10,660
>> My name is Gokhan Avkarogullari. I'm
with the iPhone GP software group at Apple.

2
00:00:10,660 --> 00:00:20,180
And my colleague Richard Schreyer and I will be talking
about OpenGL ES and the iPhone implementation of OpenGL,

3
00:00:20,180 --> 00:00:24,180
what's new on it and all the new things we added on iOS 4.

4
00:00:24,180 --> 00:00:30,450
So last year at the WWDC we introduced
OpenGLOpenGL ES 2.0 and iPhone 3GS.

5
00:00:30,450 --> 00:00:34,590
Since then, we introduced third-generation
iPhone, third-generation iPod Touch, iPad,

6
00:00:34,590 --> 00:00:40,160
and we're going to very soon release iPhone 4 and iOS 4.

7
00:00:40,160 --> 00:00:43,940
With all this new hardware and software, there
are a lot of new features added to the system.

8
00:00:43,940 --> 00:00:49,650
Today we're going to give you an overview
of the new extensions that we added.

9
00:00:49,650 --> 00:00:56,050
We're going to talk about the retina display, the higher
definition display, the higher resolution, and what it means

10
00:00:56,050 --> 00:00:59,310
from OpenGL's perspective, which is a pixel-based API.

11
00:00:59,310 --> 00:01:05,880
We're going to talk about the impact of high
resolution displays on OpenGL based applications.

12
00:01:05,880 --> 00:01:12,310
And finally, we're going to talk about multitasking,
and how you can make the user experience the greatest

13
00:01:12,310 --> 00:01:17,550
with the changes to your OpenGL API calls.

14
00:01:17,550 --> 00:01:19,970
We're going to talk about new extensions first.

15
00:01:21,010 --> 00:01:29,440
Multisample Framebuffer is an extension being implemented
to help resolve aliasing issues in the rendering operations.

16
00:01:29,440 --> 00:01:31,550
So let's look at what aliasing is.

17
00:01:31,550 --> 00:01:35,340
Here is an example screenshot from
Touch Fighter application.

18
00:01:35,340 --> 00:01:41,550
If you draw your attention to the edges of the wings of the
fighter plane, you can see that there's a staircase pattern,

19
00:01:41,550 --> 00:01:46,740
a jaggedness to it, on all edges on the wings of the plane.

20
00:01:46,740 --> 00:01:52,670
If you do a close-up, you can see in the
zoomed up version, there's a staircase pattern.

21
00:01:52,670 --> 00:01:57,490
This is actually a significant reverse on the
live application, because the pattern changes

22
00:01:57,490 --> 00:02:01,760
from frame to frame, and they look like busy lines.

23
00:02:01,760 --> 00:02:07,390
So multisampling helps us deal with this problem
by smoothing out the edges by rounding it

24
00:02:07,390 --> 00:02:12,400
to a higher resolution buffer and then generating
results from that higher resolution buffer.

25
00:02:12,400 --> 00:02:15,200
So how can we do it?

26
00:02:15,200 --> 00:02:19,920
This is your regular way of creating
framebuffer objects without multisampling.

27
00:02:19,920 --> 00:02:25,330
You would have a framebuffer object that you would use
to display the results of your rendering operation,

28
00:02:25,330 --> 00:02:31,050
and it would have, normally, a color attachment, a
depth attachment, sometimes a stencil attachment.

29
00:02:31,050 --> 00:02:35,790
Which, if you want to do multisampling in your
application, you will need two of these framebuffer objects.

30
00:02:35,790 --> 00:02:40,760
One to display the results of your rendering
operation, and the other one to generate the --

31
00:02:40,760 --> 00:02:46,590
to do the rendering into where you
can get higher resolution images.

32
00:02:46,590 --> 00:02:51,400
On the right is the one that you will use for
displaying the results of your rendering operation.

33
00:02:51,400 --> 00:02:56,890
It only has a single attachment in this case,
a color attachment, and it's not filled in yet.

34
00:02:56,890 --> 00:03:02,310
On the left is the multisample framebuffer object
where the rendering operations initially take place.

35
00:03:02,310 --> 00:03:06,630
You can see that it has a depth attachment and
that takes place on this frame-buffer object,

36
00:03:06,630 --> 00:03:09,740
that's why the other one doesn't have a depth attachment.

37
00:03:09,740 --> 00:03:14,250
And you can see that the buffers in the
multisample framebuffer object are fill.

38
00:03:14,250 --> 00:03:18,520
Basically it has a result of your rendering commands.

39
00:03:18,520 --> 00:03:24,420
So with multisampling, once that is done, and the size
of the buffers are different on the right and left.

40
00:03:24,420 --> 00:03:29,720
So we're assuming here in this example, there's 4X
multisampling, and therefore the buffer is attached

41
00:03:29,720 --> 00:03:36,850
to the multisample framebuffer object is four times the size
of the buffer attached to the display framebuffer object.

42
00:03:36,850 --> 00:03:42,420
So at the end of your rendering operation, you do resolve,
which means that you take the average of the pixel --

43
00:03:42,420 --> 00:03:48,210
of the color values of the samples for each pixel
and generate a single color value for the pixel,

44
00:03:48,210 --> 00:03:54,050
and write it out to the framebuffer object that you're
going to use to display your images on the screen.

45
00:03:54,050 --> 00:03:57,100
So let's look at how you can do it in the code.

46
00:03:57,100 --> 00:04:05,090
Here's a single sampled framebuffer creation. You have
a color buffer, you generate it, bind it to your FBO,

47
00:04:05,090 --> 00:04:10,770
and then you get a storage packing storage for that from the
CALayer, and now finally you attach it

48
00:04:10,770 --> 00:04:13,420
to the color attachment of your framebuffer object.

49
00:04:13,420 --> 00:04:16,300
Similar operations takes place for the depth buffer.

50
00:04:16,300 --> 00:04:22,560
You generate it, bind it, and get storage for it through the
render buffer storage API call, and now finally attach it

51
00:04:22,560 --> 00:04:26,980
to the depth attachment of your framebuffer
object in the single sample case.

52
00:04:26,980 --> 00:04:31,320
Now, as I told you before, you have to create two
framebuffer objects for the multisampling operation.

53
00:04:31,320 --> 00:04:36,390
So let's look at how the framebuffer objects are
created for the multisample framebuffer object.

54
00:04:36,390 --> 00:04:41,480
The difference is mainly in how you
allocate storage for your buffers.

55
00:04:41,480 --> 00:04:47,400
In a multisample framebuffer case, the color buffer storage
comes from the render buffer storage multisample Apple API.

56
00:04:47,400 --> 00:04:49,760
Just like in the depth render buffer case.

57
00:04:49,760 --> 00:04:53,610
The difference between this API
and the previous one is you specify

58
00:04:53,610 --> 00:04:57,670
to OpenGL how many samples there will be in your buffer.

59
00:04:57,670 --> 00:05:00,410
So in this case, in this example, it could be four.

60
00:05:00,410 --> 00:05:02,270
Four samples to use.

61
00:05:02,270 --> 00:05:08,090
And the buffers that are created will take into account
that you are planning to use four samples per pixel,

62
00:05:08,090 --> 00:05:11,840
and they allocate buffers based on that information.

63
00:05:13,180 --> 00:05:17,280
Let's look at how you would normally
do a render in a single sample case.

64
00:05:17,280 --> 00:05:23,280
You would bind your framebuffer object, set your report,
issue your draw calls, and finally bind your color buffer

65
00:05:23,280 --> 00:05:27,710
that you want to display on the
screen, and then present it on the display.

66
00:05:27,710 --> 00:05:32,650
With the multisampling case, you have two framebuffer
objects, and you do the rendering operations

67
00:05:32,650 --> 00:05:35,670
to one and display operations on the other one.

68
00:05:35,670 --> 00:05:39,040
So to do the rendering operation on
the multisample framebuffer object,

69
00:05:39,040 --> 00:05:41,510
you need to bind multisample framebuffer object first,

70
00:05:41,510 --> 00:05:45,720
and set your viewport to your regular
draw operations, just like you did before.

71
00:05:45,720 --> 00:05:51,550
But you need to get the data from the multisample
framebuffer object to the single sample framebuffer object.

72
00:05:51,550 --> 00:05:56,750
Now, for that, we need to set the multisample
framebuffer object as a target of a resolve operation,

73
00:05:56,750 --> 00:06:02,990
as the retarget of the resolve operation, and
the display framebuffer object as the draw target

74
00:06:02,990 --> 00:06:08,660
of that frame resolve operation, and finally you should
resolve the multisample framebuffer Apple API call

75
00:06:08,660 --> 00:06:14,220
to get the contents of the multisample framebuffer object
to the screen size on the display frame buffer object.

76
00:06:14,220 --> 00:06:19,830
And just before, you would attach, you would bind
the color buffer so you can display it on the screen.

77
00:06:19,830 --> 00:06:26,020
So as you can see, the changes to your application
to get multisampling behavior enabled is very simple.

78
00:06:26,020 --> 00:06:30,290
You need to change your initialization code
to generate a multisample framebuffer object,

79
00:06:30,290 --> 00:06:32,730
and you need to change your rendering just a little

80
00:06:32,730 --> 00:06:37,750
so that you can do a final resolve
operation at the end of the rendering loop.

81
00:06:37,750 --> 00:06:42,700
You might be thinking, "What kind of performance
implications that would have on my application?"

82
00:06:42,700 --> 00:06:49,380
And the performance implications are different,
depending on what kind of GP you have on the product.

83
00:06:49,380 --> 00:06:57,530
All iPhone OS devices starting with
iPhone 3GS have PowerVR SGX GP on them.

84
00:06:57,530 --> 00:07:04,650
And PowerVR SGX GP has native hardware restoration support
for multisampling operation It understands the difference

85
00:07:04,650 --> 00:07:09,050
between what a sample is and what a pixel
is, and therefore it grounds your shaders

86
00:07:09,050 --> 00:07:13,210
to generate color value per pixel, not per sample.

87
00:07:13,210 --> 00:07:21,060
But it keeps depth values for each sample, and it also
does depth testing for each sample, not per pixel.

88
00:07:21,060 --> 00:07:25,750
So in the four-sample case, it would do four
depth tests and would generate four depth values.

89
00:07:25,750 --> 00:07:27,180
A single color value.

90
00:07:27,180 --> 00:07:33,170
And depending on if the depth test passes or fails, it
might have the same depth value and same color value

91
00:07:33,170 --> 00:07:39,270
for all four pixels, or it might have different
depth and different color values for all samples.

92
00:07:39,270 --> 00:07:44,730
It might have different depth and color values
for all samples, depending on if the pixel is

93
00:07:44,730 --> 00:07:47,810
on the edge of a polygon or the inside of a polygon.

94
00:07:47,810 --> 00:07:52,400
And after all these values are computed for each
sample and your rendering operation is done,

95
00:07:52,400 --> 00:07:59,100
it takes the averages of the color values and that one
would be your final color value after the resolve operation.

96
00:07:59,100 --> 00:08:04,140
PowerVR MBX Lite is the GP that we use
on all our products before iPhone 3GS.

97
00:08:04,140 --> 00:08:10,420
Unfortunately, it doesn't have native support, native
hardware restoration support, for multisampling.

98
00:08:10,420 --> 00:08:13,510
Therefore, we implemented multisampling
in that case, a super sampling.

99
00:08:13,510 --> 00:08:19,250
Which means that we used high resolution buffers, and since
the MBX Lite doesn't know the difference between the sample

100
00:08:19,250 --> 00:08:24,810
and pixel, it generates the same, it does processing
for each pixel in these high resolution buffer.

101
00:08:24,810 --> 00:08:31,740
So it generates color values for each sample, it generates
depth samples, does the depth testing, for each sample,

102
00:08:31,740 --> 00:08:39,510
and finally that all these values result in a single
value written into your result framebuffer object.

103
00:08:39,510 --> 00:08:46,830
That means that there is more performance impact on the
PowerVR MBX Lite GPs than there is on the PowerVR SGX GPs.

104
00:08:46,830 --> 00:08:51,740
So you might be thinking to yourself, "I could
already have done that, render the texture.

105
00:08:51,740 --> 00:09:00,790
I could have generated an FPL that does have larger color
attachments as textures and read them back and then sample

106
00:09:00,790 --> 00:09:04,310
from them and have the Multisample
behavior on my application already.

107
00:09:04,310 --> 00:09:05,640
So what is the difference?"

108
00:09:05,640 --> 00:09:10,710
The difference is that you're giving us the
information that your intention is to do multisampling.

109
00:09:10,710 --> 00:09:16,100
So we can actually award the readback
and then average operation at the end

110
00:09:16,100 --> 00:09:19,120
of your rendering textures multisampling.

111
00:09:19,120 --> 00:09:26,230
When we generate the values, the high resolution
values, we know that you're going to generate,

112
00:09:26,230 --> 00:09:29,420
you're going to use those values
to create a single sampled version,

113
00:09:29,420 --> 00:09:34,430
so we write out both the high resolution
and the lower resolution one.

114
00:09:34,430 --> 00:09:39,610
And I'm going to talk about another extension that helps
performance with the multisampling next, that's the discard.

115
00:09:39,610 --> 00:09:45,290
With the discard, the difference between using
multisampling and render-to-texture becomes even larger.

116
00:09:45,290 --> 00:09:53,460
But first, let's give a small demo, what kind of quality
impact does multisampling have on your application.

117
00:09:53,460 --> 00:10:01,340
So, in this case, as you can see, there is a plane
in the front, the edges of the wings are really busy,

118
00:10:01,340 --> 00:10:05,250
you can see the patterns changing from frame to frame.

119
00:10:05,250 --> 00:10:10,480
And even if you look at the ones actually, the
farthest away, it's even more visible over there.

120
00:10:10,480 --> 00:10:16,760
And I turn on multisampling, it becomes
significantly better than what it was before.

121
00:10:16,760 --> 00:10:20,450
To have no effect whatsoever you
have to have infinite resolution,

122
00:10:20,450 --> 00:10:23,350
but this one is significantly better
than what we had before.

123
00:10:23,350 --> 00:10:27,090
And if I go back and forth, it will be more visible to you,

124
00:10:27,090 --> 00:10:36,080
the change from non-multisampled
case to the multisampled case.

125
00:10:36,080 --> 00:10:41,860
As I said, multisampling is very
easy to get into your application.

126
00:10:41,860 --> 00:10:46,980
And you should try, experiment, see what kind
of quality difference it makes and what kind

127
00:10:46,980 --> 00:10:49,830
of performance impact it has on your application.

128
00:10:49,830 --> 00:10:55,880
We're going to talk about a second application,
that is a Discard Framebuffer extension.

129
00:10:55,880 --> 00:11:02,250
Discard Framebuffer, that helps the performance, fill rate performance with multisampling.

130
00:11:02,250 --> 00:11:08,270
And even for it helps with the fill
rate performance in the non-multisampled case.

131
00:11:08,270 --> 00:11:12,110
Usually once a frame is rendered, the depth
and stencil values that are associated

132
00:11:12,110 --> 00:11:16,160
with that frame are not needed
for rendering of the next frame.

133
00:11:16,160 --> 00:11:19,590
In a multisampled case, not only the
depth and stencil but the color attachment

134
00:11:19,590 --> 00:11:24,280
of the multisampled framebuffer object
is not needed to render the next frame.

135
00:11:24,280 --> 00:11:28,860
So by using this extension, you can
tell OpenGL that you don't need it,

136
00:11:28,860 --> 00:11:32,690
and we can discard those values
without writing out to those buffers.

137
00:11:32,690 --> 00:11:38,030
Which means that a significant amount of
memory bandwidth can be preserved and not used

138
00:11:38,030 --> 00:11:40,360
for this operation, the writing-out operation.

139
00:11:40,360 --> 00:11:45,480
If your application is fillrate-bound, is
bound by the amount of memory activity,

140
00:11:45,480 --> 00:11:50,250
then this extension will help significantly to
get better performance from your application.

141
00:11:51,310 --> 00:11:53,170
Here's how it conceptually works.

142
00:11:53,170 --> 00:11:57,020
Here's our framebuffer object,
it's a single sampled example.

143
00:11:57,020 --> 00:11:59,500
Our framebuffer has color and depth attachment.

144
00:11:59,500 --> 00:12:01,380
Right now not filled in.

145
00:12:01,380 --> 00:12:06,910
And the GP has done, has finished rendering
and has generated the color and depth values.

146
00:12:06,910 --> 00:12:11,130
Without using the Discard extension
the next step would be the GP writing

147
00:12:11,130 --> 00:12:15,530
out those values to the color and depth attachments.

148
00:12:15,530 --> 00:12:19,670
So what we don't really need for
most of the cases is the depth values

149
00:12:19,670 --> 00:12:22,430
for the rendering of the next time, the next frame.

150
00:12:22,430 --> 00:12:28,060
So if you use Discard, you can avoid writing out
the depth value, and it will never leave the GPU.

151
00:12:28,060 --> 00:12:33,850
And this way, the write-out section, the amount of memory
used for writing out those values will be available

152
00:12:33,850 --> 00:12:39,070
to the application, to read more
data into your rendering operations.

153
00:12:39,070 --> 00:12:41,530
Let's look at the code example.

154
00:12:41,530 --> 00:12:46,950
This is how we normally render in a
single sample case when you're not using Discard.

155
00:12:46,950 --> 00:12:51,510
You would have your framebuffer bound, and you
would send your report, issue rendering commands,

156
00:12:51,510 --> 00:12:54,440
and finally present your color buffer on the screen.

157
00:12:54,440 --> 00:13:00,510
But with Discard all you need to do is define
what attachments you're going to discard,

158
00:13:00,510 --> 00:13:06,170
and through the DiscardFrameBufferEXT call,
specify which of those attachments to discard.

159
00:13:06,170 --> 00:13:11,220
And my memory's possible OpenGL will
avoid writing out to those buffers.

160
00:13:11,220 --> 00:13:15,350
You can imagine that in a multisample case,
you've created buffers, they are four times larger

161
00:13:15,350 --> 00:13:21,350
than the original single sample case, so there's
significantly more memory activity for writing out color

162
00:13:21,350 --> 00:13:23,410
and depth values of the multisample buffer.

163
00:13:23,410 --> 00:13:27,040
So Discard makes a significant difference
in terms of fillrate performance

164
00:13:27,040 --> 00:13:30,140
of your application, especially in the multisampled case.

165
00:13:30,140 --> 00:13:34,930
But even in the non-multisampled case, you will still
be avoiding writing to the depth and stencil buffers

166
00:13:34,930 --> 00:13:37,760
and it will help your fillrate in your application.

167
00:13:37,760 --> 00:13:41,020
That was the Discard FramebufferFramebuffer.

168
00:13:41,020 --> 00:13:45,940
We have another extension that might help
you, that might help on the performance side

169
00:13:45,940 --> 00:13:48,870
of your application, and that's Vertex Array Objects.

170
00:13:48,870 --> 00:13:56,240
If you have been to the morning session about OpenGL, the
objects in general, GL objects, were discussed in detail,

171
00:13:56,240 --> 00:13:58,700
and Vertex Array Objects was one of them.

172
00:13:58,700 --> 00:14:04,690
What Vertex Array Objects do is they encapsulate
all the data for vertex arrays into a single object.

173
00:14:04,690 --> 00:14:10,670
Things like where the offset for your pointer
is, your vertex array for your position or normal

174
00:14:10,670 --> 00:14:16,920
or for text coordinates, what the size of each
element is within those arrays, what the stride is,

175
00:14:16,920 --> 00:14:19,980
which arrays are enabled, if there's an index buffer or not.

176
00:14:19,980 --> 00:14:22,700
They're all encapsulated into a single object.

177
00:14:22,700 --> 00:14:25,560
So when you switch between different Vertex Array Objects,

178
00:14:25,560 --> 00:14:31,170
once you bind all the different information it becomes
immediately available to the GL to take advantage of.

179
00:14:31,170 --> 00:14:32,930
So how does this help you?

180
00:14:32,930 --> 00:14:35,980
First of all, it provides a great convenience.

181
00:14:35,980 --> 00:14:40,340
Once you log in your assets for an
object, you can log the vertex away

182
00:14:40,340 --> 00:14:45,070
and encapsulate the entire state into
a single object at the load time.

183
00:14:45,070 --> 00:14:49,290
And if you're not dynamically updating your
object ever after, it means that you're not going

184
00:14:49,290 --> 00:14:54,600
to issue these commands ever again, except for
binding it and drawing from the Vertex Array Object.

185
00:14:54,600 --> 00:14:57,170
It also allows us to do optimizations.

186
00:14:57,170 --> 00:15:01,410
Since this state is validated only once, at
the load time, we don't have to rebuild it.

187
00:15:01,410 --> 00:15:05,940
We know, unless you update it, we know that
nothing has changed, nothing has to be revalidated.

188
00:15:05,940 --> 00:15:08,710
So it saves you CP time on state validation.

189
00:15:08,710 --> 00:15:13,530
On top of that, the Vertex Array Object gives us
very good information about the vertex layout,

190
00:15:13,530 --> 00:15:17,000
the layout of your data for your vertices for the arrays.

191
00:15:17,000 --> 00:15:21,840
So we can make use of that to find
out how to reorder them if necessary

192
00:15:21,840 --> 00:15:25,640
to get better performance out of the drawing operations.

193
00:15:25,640 --> 00:15:27,830
Let's look at the code example.

194
00:15:29,370 --> 00:15:33,530
This is when you are using a Vertex
Buffer Object, how you would render it.

195
00:15:33,530 --> 00:15:40,290
You would bind your video, and you would set
your pointers for your position, for your normal,

196
00:15:40,290 --> 00:15:46,490
for your texture coordinates, and you would basically
enable the non-default enables, such as the normal rate,

197
00:15:46,490 --> 00:15:54,890
enable the texture coordinate enables, and you will basically do-- you will do the
draw operation by calling DrawArrays or drawElements calls.

198
00:15:54,890 --> 00:16:03,320
And finally, you have to set the state back so it
doesn't negatively impact the next rendering operation.

199
00:16:03,320 --> 00:16:05,290
This has to be done at every draw call.

200
00:16:05,290 --> 00:16:10,300
You have to specify these things when
you're using VBOs at every draw call.

201
00:16:10,300 --> 00:16:15,340
With VAOs, you only need to specify
which VAO you're going to use,

202
00:16:15,340 --> 00:16:18,600
because all that information is
already encapsulated in the VAO object.

203
00:16:18,600 --> 00:16:21,290
So you know about them, you don't have to re-specify them.

204
00:16:21,290 --> 00:16:25,620
And we use that and then draw our arrays based on that VAO.

205
00:16:25,620 --> 00:16:32,880
This is possible because you've done the work to
define the state only once, at one-time setup time.

206
00:16:32,880 --> 00:16:38,650
You basically bind your VAO, you specify where
the pointers and strides and all these things are,

207
00:16:38,650 --> 00:16:46,300
and you specify which states are enabled, and they are
basically written out, captured in one place at one time,

208
00:16:46,300 --> 00:16:51,360
and can be re-used over and over again
for the subsequent drawing calls.

209
00:16:51,360 --> 00:16:52,570
That was Vertex Array Object.

210
00:16:52,570 --> 00:16:59,360
As you can see, it's very easy to take
advantage of, and it gives you performance boost,

211
00:16:59,360 --> 00:17:03,230
it will hopefully make your code less error-prone
because there will be fewer lines of code,

212
00:17:03,230 --> 00:17:09,250
and it will make your code more readable, because
there are also fewer lines of changes in your code.

213
00:17:09,250 --> 00:17:11,780
There are six more extensions I'd like to talk about.

214
00:17:11,780 --> 00:17:16,300
I'll give information on each one and how
you can use them in your applications.

215
00:17:16,300 --> 00:17:22,270
The first one I'd like to talk about is
the APPLE_texture_max_level extension.

216
00:17:22,270 --> 00:17:28,600
This extension allows the application to
specify the maximum (coarsest) mipmap level

217
00:17:28,600 --> 00:17:31,830
that may be selected for texturing operation.

218
00:17:31,830 --> 00:17:35,560
So it helps us to control the filtering
across atlas boundaries.

219
00:17:35,560 --> 00:17:42,270
As texture atlases contain textures for multiple
different objects, as mipmap levels get smaller,

220
00:17:42,270 --> 00:17:47,730
there is some atlas boundary filtering operation
that takes place that generates visual artifacts

221
00:17:47,730 --> 00:17:52,410
So this extension is implemented to solve that problem.

222
00:17:52,410 --> 00:17:58,350
You can enable this extension by doing a text parameter
call with the GL_TEXTURE_MAX_LEVEL_APPLE call,

223
00:17:58,350 --> 00:18:03,280
and you can specify up to which mipmap level
you want to use for the texturing operation.

224
00:18:03,280 --> 00:18:05,120
Let's visualize this.

225
00:18:05,120 --> 00:18:06,910
This is a texture atlas from the Quest game.

226
00:18:06,910 --> 00:18:15,640
You can see that there are textures in one, single texture
object, textures for walls, for stairs, for statues,

227
00:18:15,640 --> 00:18:19,290
and all of them are here in one single texture atlas.

228
00:18:19,290 --> 00:18:26,390
If you look at the mipmap levels and I have
from 256 by 256 to 1 by 1 in this picture,

229
00:18:26,390 --> 00:18:30,770
this is the visualization of the
mipmap levels in terms of pixels.

230
00:18:30,770 --> 00:18:33,330
But let's look at the mipmap levels
in terms of the coordinates.

231
00:18:33,330 --> 00:18:37,410
If you were to use 0 to 1 coordinates
for the entire texture atlas,

232
00:18:37,410 --> 00:18:40,710
the 0 to 1 coordinate on this mipmap
levels will look like this.

233
00:18:40,710 --> 00:18:44,680
At the very top level, it will
be exactly correct and perfect.

234
00:18:44,680 --> 00:18:47,110
It will have all the necessary pixels in it.

235
00:18:47,110 --> 00:18:49,460
This is a 256 by 256.

236
00:18:49,460 --> 00:18:57,600
But at the lowest level, 1 by 1 pixel, 0 to 1 coordinate
scan is only one pixel, and therefore it has only one color.

237
00:18:57,600 --> 00:19:05,030
So if you can imagine that you have the stairs, the walls
and the statues, close up, they will use the first one,

238
00:19:05,030 --> 00:19:12,060
the 256 by 256 one, but farther away, they will use this
one, or something small, something closer in mipmap level

239
00:19:12,060 --> 00:19:15,430
to this one, and they will all look the same color.

240
00:19:15,430 --> 00:19:19,760
So texture_max_level extension
avoids this problem by allowing you

241
00:19:19,760 --> 00:19:24,380
to specify the mipmap levels you
care about and only use those.

242
00:19:24,380 --> 00:19:30,300
So in this example, you can say the max level is 3,
and the texturing hardware will only use the first,

243
00:19:30,300 --> 00:19:35,810
second and third level of mipmap levels for
texturing from the particular texture object.

244
00:19:35,810 --> 00:19:39,360
Let's look at another extension that
modifies the texturing behavior,

245
00:19:39,360 --> 00:19:43,020
that is Apple_shader_texture level of detail extension.

246
00:19:43,020 --> 00:19:47,840
It gives you explicit control over setting the
level of detail for your texturing operations.

247
00:19:47,840 --> 00:19:53,970
So if you'd like to have an object look
sharper, then you can specify a mipmap level

248
00:19:53,970 --> 00:19:56,340
that is finer than the hardware would choose.

249
00:19:56,340 --> 00:19:59,570
You can specify a lower level mipmap with higher precision.

250
00:19:59,570 --> 00:20:07,800
Or if you want to have better fluid performance at the
expense of having fuzzy detection applied to your objects,

251
00:20:07,800 --> 00:20:14,340
you can choose a lower mipmap level, a
coarser mipmap level, through this extension.

252
00:20:14,340 --> 00:20:20,370
So all you need to do is in your
shader, enable the extension

253
00:20:20,370 --> 00:20:23,310
and control the mipmap level through the texture_lod.

254
00:20:23,310 --> 00:20:27,110
There are further APIs in this extension
for controlling the gradients and such.

255
00:20:27,110 --> 00:20:33,070
I'd like you to go through and read the extension
to find out what more you can do with this extension.

256
00:20:33,070 --> 00:20:42,040
And this is a shader-based extension,
it's only available on PowerVR SGX based devices that have PowerVR SGX GPUs on them. Okay.

257
00:20:42,040 --> 00:20:48,410
So we also added the depth texture extension to our
system, so that you can capture the depth information

258
00:20:48,410 --> 00:20:53,740
into the texture and use it for things like
shadow mapping or depth of field effect.

259
00:20:53,740 --> 00:20:57,880
In shadow mapping, you would render your
scene from the perspective of light,

260
00:20:57,880 --> 00:21:01,960
and then you will capture the depth information
from the perspective of light, into a texture.

261
00:21:01,960 --> 00:21:03,640
You will do it for every light.

262
00:21:03,640 --> 00:21:09,150
And then once you're rendering from the perspective
of the camera, you can basically calculate

263
00:21:09,150 --> 00:21:16,120
if that particular pixel sees any of the lights, and if it
sees all of the light, it will be eliminated by all of them.

264
00:21:16,120 --> 00:21:19,000
If it sees some of them, it will
be eliminated by some of them.

265
00:21:19,000 --> 00:21:22,880
If it sees none of them, it will be entirely in a shadow.

266
00:21:22,880 --> 00:21:26,690
So you can use that texture extension to do shadow mapping.

267
00:21:26,690 --> 00:21:31,970
But since you're rendering the scene multiple times, there's
a lot of fillrate this consumes, so you need to be careful

268
00:21:31,970 --> 00:21:37,130
about performance implications of using this technique.

269
00:21:37,130 --> 00:21:40,880
Another example of using that textures
is depth of field effect.

270
00:21:40,880 --> 00:21:45,620
I will show a small demo of that,
so I will talk to that in the demo.

271
00:21:45,620 --> 00:21:50,760
One thing I need to remind you that when you are using a
texture as a depth attachment to your framebuffer objects

272
00:21:50,760 --> 00:21:55,760
to capture the depth information of the
scene, you can only use the NEAREST method.

273
00:21:55,760 --> 00:22:03,380
The reason for that is that doing filtering across different
depth values, just you know, it's incorrect values.

274
00:22:03,380 --> 00:22:08,570
Something close to the river and something
far away in the river, when they're filtered,

275
00:22:08,570 --> 00:22:16,790
the depth values filter generates something in between,
though there's no object in between these two depth values.

276
00:22:16,790 --> 00:22:21,580
Okay. So how can you get that texture
extension into your application.

277
00:22:21,580 --> 00:22:27,880
You just generate your texture as usual, and when you
create your framebuffer object, you attach this texture

278
00:22:27,880 --> 00:22:30,360
to the depth attachment of your framebuffer object.

279
00:22:30,360 --> 00:22:35,850
And in subsequent rendering operations, the depth
information will be captured in this texture,

280
00:22:35,850 --> 00:22:42,760
and then you can use it for texturing later to do whatever
post-processing effects or whatever effects you want to do.

281
00:22:42,760 --> 00:22:50,950
And let's look at the demo, how we can generate a
depth of field effect with the depth_texture extension.

282
00:22:50,950 --> 00:22:55,420
So here again, we are using three
planes, and I'd like to point

283
00:22:55,420 --> 00:22:59,840
out that this is how you would normally
render it without the depth of field effect.

284
00:22:59,840 --> 00:23:05,930
Everything is sharp, in focus, the plane in
the back, the stars, the plane in the front.

285
00:23:05,930 --> 00:23:08,380
So here's the depth, visualization of depth information.

286
00:23:08,380 --> 00:23:13,080
This is the depth texture captured by rendering
to texture and then displaying the texture.

287
00:23:13,080 --> 00:23:15,480
So things that are black are closer to the screen,

288
00:23:15,480 --> 00:23:22,590
things that are white are farther away .And we
re-render the same thing in a blurred version,

289
00:23:22,590 --> 00:23:25,430
at a low resolution and the blurring introduced.

290
00:23:25,430 --> 00:23:32,870
So human eye, when it focus onto something, there's a
range of objects that are in that range that are sharp,

291
00:23:32,870 --> 00:23:39,120
but the things that are closer or things that are farther
away from that focus point on the range are blurrier.

292
00:23:39,120 --> 00:23:46,840
So this texture, this will capture that blurrier part,
and the original scene will capture the sharper part.

293
00:23:46,840 --> 00:23:48,910
You can see that they are quite different.

294
00:23:48,910 --> 00:23:54,040
So the operation between the two is
basically generating a mixture of these two.

295
00:23:54,040 --> 00:24:01,110
So here, I'm visualizing which texture I'm going to
use for my finally rendering depth of field effect.

296
00:24:01,110 --> 00:24:05,500
If the focus is by the near plane, and
this entire range, it will be black,

297
00:24:05,500 --> 00:24:11,140
so it's entirely from the darker, from the sharper image.

298
00:24:11,140 --> 00:24:19,750
And as I get the range closer, smaller, it will
basically start using the values from the blurred image.

299
00:24:19,750 --> 00:24:24,510
So that when I have the range at 0, it means
that everything is blurred, nothing is in focus,

300
00:24:24,510 --> 00:24:29,060
and everything will be used from the blurred image.

301
00:24:29,060 --> 00:24:31,280
So let's look at it visually.

302
00:24:31,280 --> 00:24:34,860
So if I have full range, I end up with the original scene.

303
00:24:34,860 --> 00:24:39,060
Original rendering without the depth of field effect.

304
00:24:39,060 --> 00:24:44,410
Then if I set the range to shorter, you can
see that the stars have become blurrier,

305
00:24:44,410 --> 00:24:47,050
and the two planes in the back are blurry.

306
00:24:47,050 --> 00:24:48,590
Now the entire thing is blurry.

307
00:24:48,590 --> 00:24:54,340
I'll move the range out and it gets
things into focus and it becomes sharper.

308
00:24:54,340 --> 00:25:02,780
Or I can use a short range and move my focal point away and
the things in the front will become blurrier and the stars

309
00:25:02,780 --> 00:25:06,470
and the third plane in the back will become sharper.

310
00:25:07,960 --> 00:25:11,510
As you can see, the things -- the
planes in the front are blurrier.

311
00:25:11,510 --> 00:25:16,580
But if I move my range to capture the entire --
near and far planes, again everything is sharper,

312
00:25:16,580 --> 00:25:23,440
because we have enough range everywhere
to have focal point covering everything.

313
00:25:23,440 --> 00:25:30,730
So that's one of the examples of how
you can use depth_texture extension.

314
00:25:30,730 --> 00:25:34,400
Another shadowing technique that's
very popular is stencil shadow volume.

315
00:25:34,400 --> 00:25:40,210
This extension, the stencil_wrap extension,
helps us to improve performance for that.

316
00:25:40,210 --> 00:25:43,880
With this extension, the value of
the stencil buffer will wrap around,

317
00:25:43,880 --> 00:25:46,000
then the array goes in and out of the shadow volume.

318
00:25:46,000 --> 00:25:52,160
Now, stencil shadow volumes is a large topic, and
it's a very nice way of creating real-time shadows.

319
00:25:52,160 --> 00:25:57,170
We're spending a significant amount of time in the
next session over here in the shaders OpenGLS shading

320
00:25:57,170 --> 00:26:01,580
and advanced rendering session, on
how to generate them, how to use them.

321
00:26:01,580 --> 00:26:07,530
There's really cool demos on visualization
of this technique and implementation of it.

322
00:26:08,770 --> 00:26:12,630
We added two new data types for
texturing operations, float textures,

323
00:26:12,630 --> 00:26:18,030
you can specify your textures to
contain 16 bit or 32 bit floats.

324
00:26:18,030 --> 00:26:26,230
So again, this requires hardware support that is
only available on devices that have PowerVR SGX GPUs.

325
00:26:26,230 --> 00:26:32,910
The float values that you can store in your textures
can be used for visualizing high dynamic range images,

326
00:26:32,910 --> 00:26:38,080
for you can use it for tone mapping
and display high dynamic range images

327
00:26:38,080 --> 00:26:42,120
in all its glory on iOS 4 based devices.

328
00:26:42,120 --> 00:26:47,690
It also can be used for general-purpose
GP operations, for the GPU math.

329
00:26:47,690 --> 00:26:54,070
You can load your data in the float textures and do
FFTs and other signal processing or other applications,

330
00:26:54,070 --> 00:26:57,330
whatever math you want to use with this extension.

331
00:26:57,330 --> 00:27:04,120
And the way you specify a texture to be
float values is basically telling to OpenGL

332
00:27:04,120 --> 00:27:09,220
that its format is GL half float OES or GL float OES.

333
00:27:09,220 --> 00:27:12,020
This is the last extension I'm going to talk about today.

334
00:27:12,020 --> 00:27:15,050
It's the APPLE_rgb_422 extension.

335
00:27:15,050 --> 00:27:19,500
And this extension enables getting video textures onto GPU.

336
00:27:19,500 --> 00:27:25,220
Specifically, interleaved 422 YCbCr type of video.

337
00:27:25,220 --> 00:27:30,890
In this extension, we do not specify the color
format of the video, so it could be based on,

338
00:27:30,890 --> 00:27:34,980
it could be captured from a 601,
standard definition video format,

339
00:27:34,980 --> 00:27:42,300
or it could be coming from HD709 based color
format, or it could be JPEG full-range video.

340
00:27:42,300 --> 00:27:47,610
And, so therefore, since we do not specify it,
we give you the freedom and flexibility

341
00:27:47,610 --> 00:27:51,400
to implement your color space convergent
to your-- in your shader.

342
00:27:51,400 --> 00:28:00,020
With this extension, when you specify YCbYCr, you copy the
values over from Y to the G channel, copy the values from CR

343
00:28:00,020 --> 00:28:06,190
to R channel, and Cb values to the B channel,
and once you do your color space conversion,

344
00:28:06,190 --> 00:28:11,240
you end up with the RGB values that are
coming from originally a video texture.

345
00:28:11,240 --> 00:28:17,790
Then again this extension relies on hardware support,
so it's only available on the devices that have

346
00:28:17,790 --> 00:28:18,320
PowerVR SGX.

347
00:28:18,320 --> 00:28:22,230
So, let's look at how you can use this extension.

348
00:28:22,230 --> 00:28:28,450
This is how you would do texturing operations,
something from a texture in non-RGB 422 case,

349
00:28:28,450 --> 00:28:33,840
you will specify your type of image and then you will
create a sample and texture and sample from that.

350
00:28:33,840 --> 00:28:42,650
With the 422 extension, you need to specify
the format of your texture as RGB_422_APPLE,

351
00:28:42,650 --> 00:28:52,050
and you need to specify the type of it as unsigned short,
either 8 and 8 rev or forward or reverse ordering of Cb and Ys,

352
00:28:52,050 --> 00:28:58,950
and finally your shader, you need to
convert from YCbCr values to the RGB values,

353
00:28:58,950 --> 00:29:05,270
and then you can do whatever effect you want to do, do black and white or just attach your texture

354
00:29:05,270 --> 00:29:09,600
to another object that you want
to do -- you want to use it on.

355
00:29:09,600 --> 00:29:11,770
So these are these other six extensions.

356
00:29:11,770 --> 00:29:21,130
The texture_max_level, the shaded texture level of detail,
depth texturing, stencil wrap, flow texturing and RGB 422,

357
00:29:21,130 --> 00:29:27,850
and with that, I'd like to invite
Richard to talk about retina display,

358
00:29:27,850 --> 00:29:32,250
the impact of high-resolution displays
on performance, and multitasking.

359
00:29:32,250 --> 00:29:33,530
>> Thank you.

360
00:29:37,050 --> 00:29:38,250
So thank you.

361
00:29:38,250 --> 00:29:46,370
So, Gokhan has just given us a description of all
the new features you'll find within OpenGL in iOS 4,

362
00:29:46,370 --> 00:29:52,040
so I'm going to continue the what's new topic, but really
going to focus on what's new in the rest of the platform

363
00:29:52,040 --> 00:29:56,230
around you that impacts you as
an OpenGL application developer.

364
00:29:56,230 --> 00:30:01,370
First and foremost among these is the new
Retina Display you'll find on iPhone 4.

365
00:30:01,370 --> 00:30:03,880
So, you've undoubtedly seen the demo.

366
00:30:03,880 --> 00:30:09,760
The Retina Display gives, is a 640x960 pixel display.

367
00:30:09,760 --> 00:30:15,300
That's in effect four times larger
than we've seen on any previous iPhone.

368
00:30:15,300 --> 00:30:19,800
One of the really big points I want to drive home about
the Retina display is that we're not cramming a whole bunch

369
00:30:19,800 --> 00:30:21,760
of content into the upper left-hand corner.

370
00:30:21,760 --> 00:30:28,150
All of the various views and other widgets remain
physically exactly the same size on the display.

371
00:30:28,150 --> 00:30:32,630
The status bar, the URL bar, are all exactly the same size.

372
00:30:32,630 --> 00:30:36,470
What's changed is the amount of detail
you find within any specific view.

373
00:30:36,470 --> 00:30:43,060
This is equivalently true to the UIKit
content as it is to OpenGL content.

374
00:30:43,060 --> 00:30:49,470
So how do you actually adopt -- really
make the best use of the Retina display?

375
00:30:49,470 --> 00:30:52,120
For OpenGL applications, it requires
a little bit of adoption.

376
00:30:52,120 --> 00:30:54,430
It's not something you get out of the box.

377
00:30:54,430 --> 00:30:56,330
The steps are pretty simple.

378
00:30:56,330 --> 00:30:58,910
Right off the bat, we want to render more pixels.

379
00:30:58,910 --> 00:31:01,040
We need to allocate a larger image.

380
00:31:01,040 --> 00:31:06,890
The second step is, now that you're rendering to a
different size image, we've found that a large number

381
00:31:06,890 --> 00:31:12,450
of applications have, for their own convenience,
hard-coded various pixel dimensions in their applications.

382
00:31:12,450 --> 00:31:14,860
That's something that we'll need to flush out.

383
00:31:14,860 --> 00:31:17,610
And finally, this is where it really gets interesting,

384
00:31:17,610 --> 00:31:21,620
taking advantage of the new display
to load great new artwork.

385
00:31:21,620 --> 00:31:24,420
So, step 1.

386
00:31:24,420 --> 00:31:28,160
Generating high resolution content is
actually done on a view by view basis,

387
00:31:28,160 --> 00:31:33,800
and this is controlled with a new
UIKit API called Content Scale Factor.

388
00:31:33,800 --> 00:31:41,960
So, you can figure your view with the same bounds that
you would always have, in a 320 by 480 coordinate space.

389
00:31:41,960 --> 00:31:48,140
What changes is that you set the content scale factor
to, say, 1 or 2, and that will in turn affect the number

390
00:31:48,140 --> 00:31:53,280
of pixels that are allocated to
back the image behind that view.

391
00:31:53,280 --> 00:31:59,320
For UIKit content, this is generally set on your behalf
to whatever is appropriate for the current device.

392
00:31:59,320 --> 00:32:05,760
Right out of the box, all of your buttons and your text
fields are going to be as sharp as they possibly can be.

393
00:32:05,760 --> 00:32:08,810
But that is not true for OpenGL views.

394
00:32:08,810 --> 00:32:13,020
For OpenGL views, the default value
for content scale factor remains at 1,

395
00:32:13,020 --> 00:32:18,610
and you have to explicitly opt in by setting that otherwise.

396
00:32:18,610 --> 00:32:23,500
Usually, the straightforward thing to do is to query
the scale of the screen that you're running on,

397
00:32:23,500 --> 00:32:31,470
and then set that to the content scale factor .On
an iPhone 3GS, this will be 1, and nothing changes.

398
00:32:31,470 --> 00:32:37,430
On an iPhone 4, this will be 2, and you're effectively
doubling the width and height of your render buffer.

399
00:32:37,430 --> 00:32:42,900
At the time you call render buffer storage, core animation
is going to snapshot both the bounds of your view

400
00:32:42,900 --> 00:32:50,890
and the scale factor, and it will do that to arrive at
the actual width of the image you'll be rendering into.

401
00:32:50,890 --> 00:32:55,410
Knowing what that width and height is
is usually pretty convenient to have,

402
00:32:55,410 --> 00:33:02,620
so you can derive that by doing your own bounds times scale,
or , even easier and more foolproof, is to just go ahead

403
00:33:02,620 --> 00:33:05,780
and ask OpenGL what the allocated width and height are.

404
00:33:05,780 --> 00:33:10,800
So, I just want to -- this is actually a pretty good
idea to just ask OpenGL and take these two values

405
00:33:10,800 --> 00:33:13,060
and stash them away somewhere on the side.

406
00:33:13,060 --> 00:33:17,090
They're really useful to have.

407
00:33:17,090 --> 00:33:19,210
That brings us to step 2, and that is, fixing any place

408
00:33:19,210 --> 00:33:23,260
where you have any hard-coded dimensions
that may no longer be valid.

409
00:33:23,260 --> 00:33:27,410
If you've -- if your application is already
universal, it runs on both iPhone and iPad,

410
00:33:27,410 --> 00:33:30,280
you've probably already done this, and you can move on.

411
00:33:30,280 --> 00:33:34,890
If you haven't done that, you may find that you have
a few of these cases, and I want to point out a couple

412
00:33:34,890 --> 00:33:39,270
of the most common cases you'll find in your application.

413
00:33:39,270 --> 00:33:46,140
First is that while core animation has chosen the size
of your color buffer, the depth buffer is something

414
00:33:46,140 --> 00:33:50,180
that you allocate, and the sizes of
those two resources has to match.

415
00:33:50,180 --> 00:33:55,170
And so, and this is a case where we'll want to
use that saved pixel width and pixel height,

416
00:33:55,170 --> 00:33:58,400
and pass it right on through to render buffer storage.

417
00:33:58,400 --> 00:34:03,800
If you don't do this, you'll find yourself with an
incomplete framebuffer and no drawing will happen.

418
00:34:03,800 --> 00:34:07,020
Another common case is GL Viewport.

419
00:34:07,020 --> 00:34:12,900
Viewport is a function which chooses which subregion of
the view you're rendering into at any given point in time,

420
00:34:12,900 --> 00:34:15,860
and every single application has to set it at least once.

421
00:34:15,860 --> 00:34:18,800
You'll find it somewhere in your source code.

422
00:34:18,800 --> 00:34:23,750
Most applications really don't ever use anything other
than a full screen viewport, so this is another case

423
00:34:23,750 --> 00:34:27,170
where you'll just want to pass pixel
width and pixel height right on through.

424
00:34:27,170 --> 00:34:30,260
Step three is actually where it gets really interesting.

425
00:34:30,260 --> 00:34:35,270
At this point, your application is a basic
correctly adopter of the Retina display.

426
00:34:35,270 --> 00:34:41,150
You've now got much greater detail on your polygon
edges, but there's still more room to improve things

427
00:34:41,150 --> 00:34:43,590
and really take advantage of this display.

428
00:34:43,590 --> 00:34:49,850
And so, you know, this is the right place to, for example,
load higher resolution textures and other artwork.

429
00:34:49,850 --> 00:34:53,660
Again, if your application is universal,
you may already have a library of assets

430
00:34:53,660 --> 00:34:57,820
that are perfectly relevant, that
you can use right away on this.

431
00:34:57,820 --> 00:35:05,300
Usually, the easiest way to do this is take your
existing bitmap textures and just add a new base level,

432
00:35:05,300 --> 00:35:08,400
and leave all the existing artwork in place.

433
00:35:08,400 --> 00:35:12,890
This can really significantly improve
the visual quality of your application.

434
00:35:12,890 --> 00:35:20,040
Just one word of caution here, is that you can do this
on any iPhone OS device, but it's going to be a waste

435
00:35:20,040 --> 00:35:24,820
on the devices that don't have large displays,
and so you actually really want to be selective

436
00:35:24,820 --> 00:35:28,240
about which devices you choose to
load the largest level of detail on.

437
00:35:28,240 --> 00:35:30,100
Otherwise you're just burning memory.

438
00:35:30,100 --> 00:35:35,680
One other word of warning is using UIImage to load textures.

439
00:35:35,680 --> 00:35:41,340
UIImage has a size property which refers to dimensions, the
dimensions of that image, but those dimensions are measured

440
00:35:41,340 --> 00:35:44,700
in device-independent points, not pixels.

441
00:35:44,700 --> 00:35:51,500
So if you have a higher resolution image that's
256 by 256 pixels, the size might only be,

442
00:35:51,500 --> 00:35:59,280
the size in points might only be 128 by 128, so you can't
just take those values and pipe them into GL Tech Image 2D.

443
00:35:59,280 --> 00:36:05,660
So, this is another one of those cases where you'll have to
do your own size by scale, or you can just drop down a level

444
00:36:05,660 --> 00:36:11,450
to CGImageGetWidth, GetHeight, which will give
you the image dimensions straight out in pixels.

445
00:36:11,450 --> 00:36:15,910
If you get caught up by this, you'll probably
see some really really strange effects.

446
00:36:15,910 --> 00:36:20,480
That's really about all there is to say
about making the most of the Retina display.

447
00:36:20,480 --> 00:36:26,280
If -- tomorrow there's going to be a session
that talks about the UIKit changes in detail,

448
00:36:26,280 --> 00:36:33,760
which is where you'll hear all about how UIKits
measurements in points, where OpenGL is a pixel-based API.

449
00:36:33,760 --> 00:36:37,650
So UIKit can do almost everything
for you with no application changes,

450
00:36:37,650 --> 00:36:39,980
where you do need some changes for OpenGL.

451
00:36:39,980 --> 00:36:43,370
But really ,when you get right down
to it, the one line of code change

452
00:36:43,370 --> 00:36:46,230
that really matters is setting content scale factor.

453
00:36:46,230 --> 00:36:54,110
There's one more really interesting topic about the Retina
display, is that you're drawing four times as many pixels.

454
00:36:54,110 --> 00:36:57,130
That can have some pretty significant
performance implications.

455
00:36:57,130 --> 00:37:03,250
This is equivalently true if your application runs on
iPad, or even if your application uses an external display.

456
00:37:03,250 --> 00:37:05,430
TVs can be quite large as well.

457
00:37:05,430 --> 00:37:08,200
So I want to talk a little bit about this too.

458
00:37:08,200 --> 00:37:14,390
So really, the first thing to do here is to roll up
your sleeves and start working through he standard set

459
00:37:14,390 --> 00:37:17,330
of fillrate optimizations and investigations.

460
00:37:17,330 --> 00:37:24,120
You have to think about how many pixels is your
application drawing, in this case, X and Y got a lot bigger.

461
00:37:24,120 --> 00:37:27,750
You also have a lot of control
over how expensive each pixel is.

462
00:37:27,750 --> 00:37:31,190
Properties of mipmaps can significantly
improve GPU efficiency.

463
00:37:31,190 --> 00:37:37,630
You are in direct control over the complexity of your
fragment shaders, operations like Alpha Test and Discard,

464
00:37:37,630 --> 00:37:41,330
also the costs of those add up pretty
quickly with screen size as well.

465
00:37:41,330 --> 00:37:46,170
I'm really going to stop here, and really not
get into the details of performance optimization,

466
00:37:46,170 --> 00:37:49,350
because that's a gigantic subject,
and we're going spend a whole session

467
00:37:49,350 --> 00:37:53,230
on that this afternoon, in OpenGL
ES Tuning and Optimization.

468
00:37:53,230 --> 00:37:57,520
That being said, in our experience, there are a
lot of interesting applications that do have room

469
00:37:57,520 --> 00:38:04,130
for performance applications, and do end up being
satisfied with the performance they get on these devices,

470
00:38:04,130 --> 00:38:08,840
even when running on higher resolutions,
both iPhone 4 and iPad.

471
00:38:08,840 --> 00:38:11,300
But that's not universally true.

472
00:38:11,300 --> 00:38:14,100
There are -- some developers are really aggressive.

473
00:38:14,100 --> 00:38:17,470
They're already using everything
these devices have to offer.

474
00:38:17,470 --> 00:38:23,550
And for these particularly complex applications,
you may find that you've used up all the --

475
00:38:23,550 --> 00:38:26,490
you've optimized everything there is to optimize.

476
00:38:26,490 --> 00:38:32,160
And so there's one more big tool in our
toolbox, and we're actually going to go back

477
00:38:32,160 --> 00:38:35,520
to how many pixels are you actually drawing.

478
00:38:35,520 --> 00:38:40,570
So you don't necessarily need to render
at the size that matches the display.

479
00:38:40,570 --> 00:38:45,750
For example, on iPhone 4, has a 640 by 960 screen.

480
00:38:45,750 --> 00:38:54,050
If you could instead render 720 by 480, that's still a
significant step up in quality when compared to a 3GS,

481
00:38:54,050 --> 00:38:56,860
and on the other hand, you're only
filling half as many pixels

482
00:38:56,860 --> 00:39:00,690
as you would be had you gone all
the way up to match the display.

483
00:39:00,690 --> 00:39:07,340
You're going to find very few other opportunities out there
to find a 2x performance jump in a single line of code.

484
00:39:07,340 --> 00:39:12,020
So if this becomes an option that you
want to pursue, how do you do this?

485
00:39:12,020 --> 00:39:17,810
You could just throw some black bars
around the sides, but not really.

486
00:39:17,810 --> 00:39:21,300
What you really want to do is we want to
scale that, take that lower resolution image

487
00:39:21,300 --> 00:39:24,200
and actually scale it to fill the whole display.

488
00:39:24,200 --> 00:39:27,160
Okay? How do you do that?

489
00:39:27,160 --> 00:39:29,250
Well, you actually don't need to do that at all.

490
00:39:29,250 --> 00:39:32,580
This is something that Core Animation will do for you.

491
00:39:32,580 --> 00:39:37,120
In fact, this is something that Core Animation
will do for you really well and really efficiently.

492
00:39:37,120 --> 00:39:39,460
Much more so than you could do yourself.

493
00:39:39,460 --> 00:39:44,780
In this case, you know, actually, move on to the next slide.

494
00:39:44,780 --> 00:39:50,210
That is, in the end, a really nice tradeoff
between performance and visual quality.

495
00:39:50,210 --> 00:39:53,320
So actually, how do you really make use of this?

496
00:39:53,320 --> 00:39:57,150
Well, the answer is that you literally have to do nothing.

497
00:39:57,150 --> 00:40:01,020
This is how your applications already
work right out of the box.

498
00:40:01,020 --> 00:40:05,770
The API that controls this is, again, Content Scale Factor.

499
00:40:05,770 --> 00:40:11,460
As I said, for compatibility reasons, today your
applications render into a view with a Content Scale Factor

500
00:40:11,460 --> 00:40:19,470
of 1, which means that Core Animation is already taking your
content and scaling it to fit the display, very efficiently.

501
00:40:19,470 --> 00:40:22,690
So right out of the gate, your
application already performs as well

502
00:40:22,690 --> 00:40:25,270
as it always has, and looks as good as it always has.

503
00:40:25,270 --> 00:40:31,070
That's a pretty decent place to start, if you
can change your application such that you can run

504
00:40:31,070 --> 00:40:35,130
at the native resolution of these
devices, well, that's pretty good too.

505
00:40:35,130 --> 00:40:40,610
But we see that there's going to be a fairly large class
of applications that do have performance headroom to, say,

506
00:40:40,610 --> 00:40:45,220
step it up a little bit, but you can't take
a four times jump in the number of pixels.

507
00:40:45,220 --> 00:40:49,180
And so there's some really interesting
middle grounds to think about here.

508
00:40:49,180 --> 00:40:54,090
One of these is to stick with a scale
factor of 1, but adopt anti-aliasing.

509
00:40:54,090 --> 00:40:59,310
We just saw Gokhan discuss our edition of the
Apple Framebuffer Multisampling Extension.

510
00:40:59,310 --> 00:41:02,980
This can also do a really good
job of smoothing polygon edges.

511
00:41:02,980 --> 00:41:07,940
In our experience, many applications that adopt
multisampling end up looking almost as good

512
00:41:07,940 --> 00:41:12,060
as if you were running at native resolution,
but the performance impact can be much,

513
00:41:12,060 --> 00:41:16,630
much less severe than increasing
the number of pixels four times.

514
00:41:16,630 --> 00:41:21,080
So this is a very compelling tradeoff to think about.

515
00:41:21,080 --> 00:41:27,110
Another interesting option is that you don't
necessarily have to pick integer content scale factor.

516
00:41:27,110 --> 00:41:29,330
You can pick something between 1 and 2.

517
00:41:29,330 --> 00:41:36,930
In the example I started from, 720 by 480, to get that
effect, you can set a content scale factor of 1.5,

518
00:41:36,930 --> 00:41:40,230
and that's actually all you really have to do.

519
00:41:40,230 --> 00:41:41,810
I want to half change the subject now.

520
00:41:41,810 --> 00:41:45,370
I want to talk about iPad.

521
00:41:45,370 --> 00:41:53,000
iPad has an even larger display, and so the motives for
wanting to do application optimization are just as true,

522
00:41:53,000 --> 00:41:59,220
and for some applications, the motives for wanting to
render to a smaller render buffer are just as true.

523
00:41:59,220 --> 00:42:06,330
There's just one unfortunate catch, and that is
that our very convenient new API is new to iOS 4.

524
00:42:06,330 --> 00:42:10,560
It's just not there for you to use in iPhone OS 3.2.

525
00:42:10,560 --> 00:42:16,210
Fortunately, there is another way, and that
is using the UIView Transform property,

526
00:42:16,210 --> 00:42:20,530
which has been there since iPhone SDK first shipped.

527
00:42:20,530 --> 00:42:23,490
So, I'm going to put up a snippet of sample code here.

528
00:42:23,490 --> 00:42:27,200
So think back to the beginning of the
presentation, when I said that the size

529
00:42:27,200 --> 00:42:30,660
of your render buffer was your bounds times your scale.

530
00:42:30,660 --> 00:42:33,050
On iPad, the scale is implicitly 1.

531
00:42:33,050 --> 00:42:35,960
There's no API, so we'll call it implicitly 1.

532
00:42:35,960 --> 00:42:42,880
In which case, if we want to render an 800 by 600
image on iPad, you can set the bounds to 800 by 600,

533
00:42:42,880 --> 00:42:48,950
and then you can set on the Transform property,
you can set a scaling transform that will take

534
00:42:48,950 --> 00:42:51,970
that and scale it up to fill the display.

535
00:42:51,970 --> 00:42:55,970
Performance-wise, these two methods are --
actually, both performance and quality-wise,

536
00:42:55,970 --> 00:42:58,130
these two methods are pretty much equivalent.

537
00:42:58,130 --> 00:43:02,670
The advantage of this method is that
you can start using it on iPhone OS 3.2,

538
00:43:02,670 --> 00:43:08,400
whereas ContentScaleFactor for iPhone iOS
4 and later is just more convenient.

539
00:43:08,400 --> 00:43:10,920
That's what I have to say about large display performance.

540
00:43:10,920 --> 00:43:18,950
We're going to talk a lot about this kind of performance
investigation in detail later this afternoon in the Tuning

541
00:43:18,950 --> 00:43:24,670
and Optimization session, and if you just run out of steam
there, then you've got some really fine-grained control

542
00:43:24,670 --> 00:43:32,370
over what resolution you actually render at, which can
significantly reduce the number of pixels you have to fill.

543
00:43:32,370 --> 00:43:36,740
That brings us to the last topic of
the day, and that's multitasking.

544
00:43:36,740 --> 00:43:40,060
So, I want to start this by providing an example.

545
00:43:40,060 --> 00:43:45,050
Say your product is a game, and the user is
playing, and they receive a text message.

546
00:43:45,050 --> 00:43:47,240
They leave your game to go write a response.

547
00:43:47,240 --> 00:43:49,760
Sure, dinner sounds great.

548
00:43:49,760 --> 00:43:51,140
They come right back to your game.

549
00:43:51,140 --> 00:43:54,060
They probably spent 10 seconds outside of it.

550
00:43:54,060 --> 00:43:56,290
So what do they see when they return?

551
00:43:57,690 --> 00:44:01,530
They see that they're going to get to wait.

552
00:44:01,530 --> 00:44:04,020
That's not a very good user experience.

553
00:44:04,020 --> 00:44:10,180
In fact, they end up waiting for longer than they
spent outside of your application in the first place,

554
00:44:10,180 --> 00:44:13,060
that's really not a good user experience.

555
00:44:13,060 --> 00:44:17,860
And so, this is what we're talking about
when we talk about fast app switching.

556
00:44:17,860 --> 00:44:20,740
If you went to the session, you heard
that there are a bunch of other scenarios.

557
00:44:20,740 --> 00:44:26,990
There's voice over IP, location
tracking, let's see, location tracking,

558
00:44:26,990 --> 00:44:29,440
there's finish tasks, there's audio tasks.

559
00:44:29,440 --> 00:44:34,180
All of those are about various modes
of doing work while in the background.

560
00:44:34,180 --> 00:44:36,160
Fast App Switching is different.

561
00:44:36,160 --> 00:44:41,060
The Fast App Switching scenario is an application
that does absolutely nothing in the background.

562
00:44:41,060 --> 00:44:43,860
It's completely silent, completely idle.

563
00:44:43,860 --> 00:44:49,670
It's there simply to lie in wait, so that it can leap back
into action the instant that the user relaunches that app.

564
00:44:49,670 --> 00:44:54,470
And in fact, for OpenGL, that is the only mode.

565
00:44:54,470 --> 00:44:59,510
GPU access is exclusively for the foreground
application to ensure responsiveness.

566
00:44:59,510 --> 00:45:05,750
So while you can use some of these other scenarios, you can
create a finish task to do CPU processing in the background,

567
00:45:05,750 --> 00:45:10,410
you do not have access to the GP one in the background.

568
00:45:10,410 --> 00:45:14,710
There's one really important point I want to make,
is that if you think back to that progress bar,

569
00:45:14,710 --> 00:45:19,710
a lot of what that application was probably
doing was loading things like OpenGL textures.

570
00:45:19,710 --> 00:45:21,910
That tends to be pretty time consuming.

571
00:45:21,910 --> 00:45:27,180
So one thing you don't have to do is
de-allocate all of your OpenGL resources.

572
00:45:27,180 --> 00:45:31,090
You can leave all of your textures and all of
your buffer objects and everything else in place.

573
00:45:31,090 --> 00:45:36,070
You just have to go hands off and not touch
them for awhile, but they can stay there.

574
00:45:36,070 --> 00:45:40,630
This means that when a user does bring your application
back to the foreground, all o those really expensive

575
00:45:40,630 --> 00:45:45,120
to load resources are already there, ready to go right away.

576
00:45:45,120 --> 00:45:48,340
Keeping all that stuff in the background
does have some implications on memory usage,

577
00:45:48,340 --> 00:45:52,070
and that leads to a really interesting
tradeoff that you should think about carefully.

578
00:45:52,070 --> 00:45:58,680
It is generally a really good idea to reduce your
application's memory usage when you run in the background.

579
00:45:58,680 --> 00:46:02,580
For example, the system memory
as a whole is a shared resource.

580
00:46:02,580 --> 00:46:06,750
If the application in the foreground needs more
memory, the system will go find applications

581
00:46:06,750 --> 00:46:09,610
in the background to terminate to make room for it.

582
00:46:09,610 --> 00:46:13,100
That list is ordered by who's using the most memory.

583
00:46:13,100 --> 00:46:14,970
Guess who's going to be on top?

584
00:46:14,970 --> 00:46:17,780
So, you have a really compelling,
even perfectly selfish reason,

585
00:46:17,780 --> 00:46:20,560
to want to reduce your memory use as much as possible.

586
00:46:20,560 --> 00:46:26,570
Because that means that your process is probably going to be
more likely to be there to come back to in the first place.

587
00:46:26,570 --> 00:46:32,240
On the other hand, if you're making the resume
operation slow by spending a whole bunch

588
00:46:32,240 --> 00:46:35,120
of time loading resources, we've
kind of defeated the purpose.

589
00:46:35,120 --> 00:46:37,520
There's really a balancing act to be made here.

590
00:46:37,520 --> 00:46:44,280
So the way you should think about it is look at
your application on a resource-by-resource basis,

591
00:46:44,280 --> 00:46:48,640
and think about what you really need to
pick up right where the user left off.

592
00:46:48,640 --> 00:46:52,250
And also think about how expensive is this to re-create.

593
00:46:52,250 --> 00:46:56,440
If you've got your standard textures, those
are probably pretty expensive to re-create.

594
00:46:56,440 --> 00:46:58,130
You want to keep those around.

595
00:46:58,130 --> 00:47:01,270
On the other hand, there are some
that are really cheap to re-create.

596
00:47:01,270 --> 00:47:03,390
Think about your color and depth buffers.

597
00:47:03,390 --> 00:47:08,280
There's no drawing in the background, so they're
really just sitting there, not doing anything.

598
00:47:08,280 --> 00:47:12,360
Re-creating them is not like a texture,
where you have to go to the file system

599
00:47:12,360 --> 00:47:15,610
and load data and decompress it and so on.

600
00:47:15,610 --> 00:47:19,230
Reallocating your color and depth buffers
is just conjuring up empty memory.

601
00:47:19,230 --> 00:47:21,080
It's really fast.

602
00:47:21,080 --> 00:47:24,910
Also think about cases where you actually have idle
resources that aren't actually needed for the current scene.

603
00:47:24,910 --> 00:47:30,540
You know, if you've got a bunch of GL textures that
are around because you needed them in the past,

604
00:47:30,540 --> 00:47:34,950
and you're keeping them around pre-emptively because you
might need them in the future, this is a really good time

605
00:47:34,950 --> 00:47:38,200
to clear out all the idle textures in that
cache and leave all the active ones in place.

606
00:47:38,200 --> 00:47:42,310
A little bit more about the mechanics of it.

607
00:47:42,310 --> 00:47:44,920
How do you actually enter the background and come back?

608
00:47:44,920 --> 00:47:50,450
Your application is going to receive
a data enter background notification.

609
00:47:50,450 --> 00:47:53,790
When this happens, we have to stop our usage of the GPU.

610
00:47:53,790 --> 00:47:58,460
Specifically, your access to the GPU ends
as soon as this notification returns.

611
00:47:58,460 --> 00:48:03,030
So you have to be done before you return from this function.

612
00:48:03,030 --> 00:48:06,330
The second is that you want to save application state.

613
00:48:06,330 --> 00:48:11,330
In this case, if you're writing a painting application
with OpenGL, that might involve using read pixels

614
00:48:11,330 --> 00:48:15,660
to pull back what the user has painted
and save it to the file system.

615
00:48:15,660 --> 00:48:19,370
And this is really important, because
your application may be terminated

616
00:48:19,370 --> 00:48:23,000
in the background to make memory, to free up memory.

617
00:48:23,000 --> 00:48:27,750
And then finally, here's our example of
releasing memory for this application.

618
00:48:27,750 --> 00:48:30,000
You know, we say we're going to go release our framebuffers,

619
00:48:30,000 --> 00:48:34,140
because we can re-create them really
fast, without slowing the user down.

620
00:48:34,140 --> 00:48:38,850
On the other side, when your application
wants to enter the foreground,

621
00:48:38,850 --> 00:48:42,690
you'll receive an applicationWillEnterForeground notification.

622
00:48:42,690 --> 00:48:47,860
We'll spend a tiny fraction of a second allocating
our framebuffer, and then we're ready to go.

623
00:48:47,860 --> 00:48:49,780
This is exactly where we want to be.

624
00:48:49,780 --> 00:48:50,800
So that's great.

625
00:48:50,800 --> 00:48:54,880
Except that there's one other case here that
you really have to think about very carefully.

626
00:48:54,880 --> 00:49:00,490
And this is-- and this is, you might receive
applicationDidFinishLaunching instead.

627
00:49:00,490 --> 00:49:03,320
If your process was terminated while in the background,

628
00:49:03,320 --> 00:49:07,370
then you have lost all of your GL
resources, as well as everything else.

629
00:49:07,370 --> 00:49:10,500
And you now have to reload them,
and that's going to take time.

630
00:49:10,500 --> 00:49:14,420
The more interesting part here
is restoring that saved state,

631
00:49:14,420 --> 00:49:18,460
restoring that state that we saved
when entering the background.

632
00:49:18,460 --> 00:49:22,550
Because when the user enters -- the user doesn't know

633
00:49:22,550 --> 00:49:26,910
when an application is terminated
in the background to free up memory.

634
00:49:26,910 --> 00:49:30,750
It's a completely invisible implementation detail to them.

635
00:49:30,750 --> 00:49:37,980
So to you, it's effectively unpredictable which one of these
paths your application will take, to reenter the foreground.

636
00:49:37,980 --> 00:49:42,780
And so, if in one of these cases you put them
right back in their game, and everything's golden,

637
00:49:42,780 --> 00:49:50,390
whereas in the other case, you make them page through a
parade of logos and select a menu and click Load Game,

638
00:49:50,390 --> 00:49:52,670
the user is effectively going to see random behavior here.

639
00:49:52,670 --> 00:49:58,700
They won't know which case to expect when they
press on your icon, and that's fairly disconcerting.

640
00:49:58,700 --> 00:50:04,270
And so it's really critical here that
regardless of which path your application takes,

641
00:50:04,270 --> 00:50:09,830
you put them back in exactly the same
place, say, reloading their game.

642
00:50:09,830 --> 00:50:13,380
Ideally, the user just can't tell the
difference between these two cases.

643
00:50:13,380 --> 00:50:16,110
Practically speaking, there will
be a performance difference.

644
00:50:16,110 --> 00:50:20,000
The whole point of Fast App Switching is to keep
those resources around, and if they're not around,

645
00:50:20,000 --> 00:50:22,220
you're going to have to spend time to load them.

646
00:50:22,220 --> 00:50:26,760
But for the best-behaved applications, the
application's performance will be the only difference

647
00:50:26,760 --> 00:50:28,680
in behavior the user can see.

648
00:50:28,680 --> 00:50:30,580
That's Fast App Switching.

649
00:50:30,580 --> 00:50:35,290
You want to free up as much memory as possible, but not
to the extent that you're going to slow down Resume.

650
00:50:35,290 --> 00:50:39,890
And then you also need to think about doing a really
good job of saving and restoring your GL state.

651
00:50:39,890 --> 00:50:48,070
Which could include actually the contents of your
OpenGL resources, if you're modifying those on the fly.

652
00:50:48,070 --> 00:50:51,140
So that actually brings us to the
end of today's presentation.

653
00:50:51,140 --> 00:50:56,380
To just give you a quick recap of where we've
been, we've talked about some new OpenGL extensions

654
00:50:56,380 --> 00:51:01,170
to improve the visual quality of your application:
multisample, float texture and depth texture.

655
00:51:01,170 --> 00:51:06,600
We have new features to improve the performance of your
application: Vertex Array Objects and Discard Framebuffer.

656
00:51:06,600 --> 00:51:09,250
Discard and Multisample go together particularly well.

657
00:51:09,250 --> 00:51:12,960
We talked about how to adopt the Retina display.

658
00:51:12,960 --> 00:51:15,390
Ideally, that's one line of code.

659
00:51:15,390 --> 00:51:18,690
We talked about resolution selection for large displays.

660
00:51:18,690 --> 00:51:24,290
This is really your big hammer to solve
fillrate issues if you have no other option.

661
00:51:24,290 --> 00:51:29,710
And finally we talked about multitasking, where the key
is always think about the phrase Fast App Switching.

662
00:51:29,710 --> 00:51:31,760
We have a number of related sessions.

663
00:51:31,760 --> 00:51:37,160
Coming up later this afternoon is OpenGL ES Tuning and
Optimization, where we'll go through the process of how

664
00:51:37,160 --> 00:51:42,050
to actually look at fillrate performance in your
application, as well as introduce a new developer tool

665
00:51:42,050 --> 00:51:46,490
that can really help you understand
what your application is really doing.

666
00:51:46,490 --> 00:51:49,800
Shading and Advanced Rendering is more of
an applied session, where we're going to go

667
00:51:49,800 --> 00:51:52,370
through some really classic graphics
algorithms and then talk

668
00:51:52,370 --> 00:51:57,740
about how we practically applied
them to the graphics in Quest.

669
00:51:57,740 --> 00:52:05,230
OpenGL Essential Design Practices happened earlier today,
and this is a really great talk which goes into the subject

670
00:52:05,230 --> 00:52:12,450
of general OpenGL design practices, where you want to use
this kind of object, learning about modern API changes.

671
00:52:12,450 --> 00:52:18,100
This kind of stuff is equally applicable
to both desktop and embedded OpenGL.

672
00:52:18,100 --> 00:52:23,730
And then finally, there's a couple sessions that
talk about multitasking and the Retina display,

673
00:52:23,730 --> 00:52:27,130
as they apply to the whole platform, not just OpenGL.

674
00:52:27,130 --> 00:52:31,610
You can contact Alan Schaffer directly, he's
our Game and Graphics Technologies Evangelist,

675
00:52:31,610 --> 00:52:38,160
and we also have a great collection of written
documentation in the OpenGL ES programming guide for iPhone.

676
00:52:38,160 --> 00:52:42,630
So, with that, I hope this talk was useful to
you today, and I hope to see you at the labs.

677
00:52:42,630 --> 00:52:42,880
Thank you.

