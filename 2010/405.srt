1
00:00:07,330 --> 00:00:12,230
>> Kevin Calhoun: Hello, and welcome to
Session 405, "Discovering AVFoundation."

2
00:00:12,230 --> 00:00:18,430
Now and for the next sixty minutes you
will join me on a voyage of discovery.

3
00:00:18,430 --> 00:00:22,460
My name is Kevin Calhoun of Apple's
Media Systems Engineering Group,

4
00:00:22,460 --> 00:00:28,420
and we'll be discovering the vastly
expanded AVFoundation framework in iOS 4.

5
00:00:28,420 --> 00:00:33,860
We'll talk about why you would want to use this
framework and for what purpose; we'll talk in some depth

6
00:00:33,860 --> 00:00:40,730
about concepts underlying the use of Time Media that
inform the design of the APIs we'll be discussing

7
00:00:40,730 --> 00:00:46,310
in which you'll want to be familiar with to aid in your
adoption of the APIs and; of course, we'll talk specifically

8
00:00:46,310 --> 00:00:50,460
about tasks that you can accomplish with this API set.

9
00:00:50,460 --> 00:00:54,320
Now, where are we going in our
voyage of discovery this afternoon?

10
00:00:54,320 --> 00:00:58,320
We are going beneath the blue line.

11
00:00:58,320 --> 00:01:07,400
Underneath Media Player, underneath UIKit where
the pressures are and the media takes time,

12
00:01:07,400 --> 00:01:10,350
we are at the level of AVFoundation,
the Objective C framework

13
00:01:10,350 --> 00:01:14,170
which gives you a great degree of control over Time Media.

14
00:01:14,170 --> 00:01:20,290
We sit on top of frameworks that are familiar to you such
as Core Animation and Core Audio and also a framework

15
00:01:20,290 --> 00:01:23,890
which is newly public in iOS 4, Core Media.

16
00:01:23,890 --> 00:01:30,190
So that's where we're talking about in the
system today underneath the level of UI.

17
00:01:30,190 --> 00:01:36,070
Now there have been technologies on iPhone in the
past dating back to the original release and enhanced

18
00:01:36,070 --> 00:01:42,650
through iPhone OS 3.0 and even after that
are useful for Time Media operations.

19
00:01:42,650 --> 00:01:49,320
There are several very easy ways that you can use Time
Media in frameworks that sit atop the level of AVFoundation.

20
00:01:49,320 --> 00:01:55,720
For example, for playback the MediaPlayer
framework offers MPMoviePlayerController

21
00:01:55,720 --> 00:02:02,270
and MPMoviePlayerViewController well integrated
with UIKit successfully used by nearly every app

22
00:02:02,270 --> 00:02:05,530
on the platform that currently plays Time Media.

23
00:02:05,530 --> 00:02:13,240
In addition starting with iPhone OS 3.0, the browser
has offered support for the HTML5 video and audio tags.

24
00:02:13,240 --> 00:02:19,650
So if you have web-based content that you want to
integrate Time Media with, that's a great solution for you.

25
00:02:19,650 --> 00:02:24,310
Now I mentioned that AVFoundation
is vastly expanded in iOS 4.

26
00:02:24,310 --> 00:02:27,020
It initially was shipped with iPhone OS 3.0

27
00:02:27,020 --> 00:02:33,020
and in that version it offered a class called
AVAudioPlayer, which is useful for playing audio files.

28
00:02:33,020 --> 00:02:37,130
So, those are very easy ways to play
Time Media and may still be appropriate

29
00:02:37,130 --> 00:02:41,730
for your apps even now with the release of iOS 4.

30
00:02:41,730 --> 00:02:46,220
Similarly for Capture there are
solutions already extant on the platform.

31
00:02:46,220 --> 00:02:52,430
UIImagePickerController in UIKit supports
video capture as well as still image capture

32
00:02:52,430 --> 00:02:59,150
and AVFoundation starting iPhone S3.0 supports
AudioRecorder for recording audio files.

33
00:02:59,150 --> 00:03:06,640
So that's great stuff to be aware of and may be appropriate
for your app even after we survey AVFoundation together.

34
00:03:06,640 --> 00:03:14,200
So, the question is, why would you use AVFoundation and
iOS 4 if these other great technologies are available?

35
00:03:14,200 --> 00:03:19,510
The basic answer is we give you a much
larger measure of control over Time Media

36
00:03:19,510 --> 00:03:23,470
in AVFoundation and a lot more features as well.

37
00:03:23,470 --> 00:03:30,140
In particular if you need to inspect the contents of
a Time Media resource, you can get deep information.

38
00:03:30,140 --> 00:03:36,670
You can glean what media is available, what
metadata is present in a Time Media resource.

39
00:03:36,670 --> 00:03:43,460
If you need to play Time Media in ways that are more
sophisticated than you can with the other frameworks,

40
00:03:43,460 --> 00:03:47,260
in particular, if you want to implement
a totally custom user interface

41
00:03:47,260 --> 00:03:51,640
to control play back, you can do that with this API set.

42
00:03:51,640 --> 00:03:58,000
In addition if you wish to pull together media from
multiple sources such as iMovie does pull some video

43
00:03:58,000 --> 00:04:02,600
from this resource, some audio from
another resource, arrange it temporary,

44
00:04:02,600 --> 00:04:06,860
perform editing operations, this is the API set for you.

45
00:04:06,860 --> 00:04:14,390
If you want to take existing media resources either simple
ones such as simple Time Media files or complex resources

46
00:04:14,390 --> 00:04:18,450
such as compositions that you've edited
together and re-encode them in order

47
00:04:18,450 --> 00:04:22,420
to create new Time Media resources,
you can do that through this API set.

48
00:04:22,420 --> 00:04:28,150
And, finally, if you want full
control over the input devices

49
00:04:28,150 --> 00:04:32,930
that are present including the camera,
this API set has the features for you.

50
00:04:32,930 --> 00:04:36,850
So these are the five areas of functionality that
we're going to be talking about in this session

51
00:04:36,850 --> 00:04:39,890
and the two subsequent sessions so keep these five in mind.

52
00:04:39,890 --> 00:04:44,010
There's a bargain we're going to strike.

53
00:04:44,010 --> 00:04:51,820
We give you in this framework this vastly greater degree
of control over Time Media, but as you accept that power,

54
00:04:51,820 --> 00:04:59,090
that ability to control Time Media, you also accept
the responsibility for handling Time Media in ways

55
00:04:59,090 --> 00:05:01,940
that are appropriate for this type of content.

56
00:05:01,940 --> 00:05:09,330
So it's important to be aware of the challenges of
working with Time Media as you adopt these APIs.

57
00:05:09,330 --> 00:05:13,890
The most important point to make about Time
Media is an obvious one that it's intended

58
00:05:13,890 --> 00:05:18,190
to be processed, intended to be consumed incrementally.

59
00:05:18,190 --> 00:05:23,020
You see a sequence of video frames
or hear a sequence of audio samples.

60
00:05:23,020 --> 00:05:30,680
Time Media takes time and that point is fundamental to the
design of the APIs we're going to be talking about today.

61
00:05:30,680 --> 00:05:38,330
It will be necessary for you to code some forbearance
into your apps because Time Media takes time to process,

62
00:05:38,330 --> 00:05:45,580
not just to play but also to perform other operations
as well even operations as simple as inspection.

63
00:05:45,580 --> 00:05:52,580
You might not be surprised to learn that because Time Media
is intended to be represented over a period of time some

64
00:05:52,580 --> 00:05:59,540
of the formats in which it's delivered are not optimized to
provide summary information about the Time Media resource

65
00:05:59,540 --> 00:06:06,770
as a whole and as a result in order to get information about
these resources, for example, just asking a simple question

66
00:06:06,770 --> 00:06:11,000
like ("what's the duration of a resource?")
you may require a good deal of work

67
00:06:11,000 --> 00:06:14,420
to be done on your behalf to deliver the answer.

68
00:06:14,420 --> 00:06:22,990
So, Time Media takes time, but it doesn't monopolize
the device while it's performing an operation.

69
00:06:22,990 --> 00:06:28,310
You wish your apps to remain responsive to
your end user while these things are going on.

70
00:06:28,310 --> 00:06:35,170
So it's possible for the user to turn his or her attention
to some other task or for the device to handle an event

71
00:06:35,170 --> 00:06:41,950
such as an incoming phone call that changes the
circumstances of operation that you are enjoying at the time

72
00:06:41,950 --> 00:06:46,920
that you kicked off the operation that you're currently
performing, playback or re-encoding, for example.

73
00:06:46,920 --> 00:06:53,890
So it's necessary for the APIs to give you the
opportunity to respond to these changes in circumstance.

74
00:06:53,890 --> 00:06:59,380
Finally the last important point to make about
Time Media is that the variety of formats,

75
00:06:59,380 --> 00:07:07,160
the variety of delivery protocols is great and if
you wish to handle Time Media resources uniformly,

76
00:07:07,160 --> 00:07:12,850
it may be necessary for app to do a little bit of
additional work to take some extra steps in order

77
00:07:12,850 --> 00:07:17,110
to provide a uniform processing path
for the operations you have in mind.

78
00:07:17,110 --> 00:07:22,420
We'll get back to details about this when we
discuss playback a little bit later in the hour.

79
00:07:22,420 --> 00:07:24,100
Okay, but that's what to keep in mind.

80
00:07:24,100 --> 00:07:28,830
The five areas of functionality we offer in
AVFoundation and the challenges associated

81
00:07:28,830 --> 00:07:32,910
with Time Media that inform the API design.

82
00:07:32,910 --> 00:07:37,540
Let me give you an overview first
before we go into detail of the classes

83
00:07:37,540 --> 00:07:41,250
that we are making available to
you in AVFoundation in iOS 4.

84
00:07:41,250 --> 00:07:47,190
I mention AVFoundation first shift and
iPhone OS 3.0 and with that version

85
00:07:47,190 --> 00:07:53,030
of the framework there were audio-related classes
very useful, you'll still want to use these today.

86
00:07:53,030 --> 00:07:55,190
AVAudioSession is present.

87
00:07:55,190 --> 00:08:01,710
This is the class that you use in order to inform the
underlying audio system on the platform of the type

88
00:08:01,710 --> 00:08:04,110
of audio processing that you're performing.

89
00:08:04,110 --> 00:08:10,840
If you tell the audio subsystem what you're doing,
then it can arbitrate resources appropriately

90
00:08:10,840 --> 00:08:14,490
for what your app is doing and what
else is going on in the system.

91
00:08:14,490 --> 00:08:18,860
So, you're going to use this class in
connection with AVFoundation even in 4.0.

92
00:08:18,860 --> 00:08:27,910
Other classes in the audio category present in AVFoundation
mentioned earlier AVAudioPlayer and AVAudioRecorder,

93
00:08:27,910 --> 00:08:33,970
but now the expansion, the annex, the
annex, which is quite large in iOS 4

94
00:08:33,970 --> 00:08:38,300
in AVFoundation the five areas of
functionality we mentioned earlier.

95
00:08:38,300 --> 00:08:40,320
First, inspection.

96
00:08:40,320 --> 00:08:47,910
In order to provide uniform inspection of Time Media
resources, we offer a model object known as AVAsset,

97
00:08:47,910 --> 00:08:53,150
Audio Visual Asset, that allows you to get
information about the contents of a resource.

98
00:08:53,150 --> 00:09:00,270
Assets can contain multiple streams of media
each of which is represented by an AVAssetTrack.

99
00:09:00,270 --> 00:09:07,270
They also can contain collections of metadata that we
make available to you as a collection of AVMetadataItems;

100
00:09:07,270 --> 00:09:13,560
these are the basic inspection classes, but remember
the challenge I mentioned earlier it can take time

101
00:09:13,560 --> 00:09:18,840
to provide you with information about a resource
and so these classes implement a protocol known

102
00:09:18,840 --> 00:09:24,480
as AVAsynchronousKeyValueLoading that
extends key value coding to allow you

103
00:09:24,480 --> 00:09:28,960
to request the value of a property to be loaded on demand.

104
00:09:28,960 --> 00:09:31,930
We'll talk in great detail about how that works shortly.

105
00:09:31,930 --> 00:09:38,780
In addition, we allow you to represent resources as
still images, thumbnails, other types of visual previews,

106
00:09:38,780 --> 00:09:43,040
you would create those thumbnails and other still
image previews by means of AVAssetImageGenerator.

107
00:09:43,040 --> 00:09:49,190
Second area of functionality I mentioned is playback.

108
00:09:49,190 --> 00:09:53,410
Obviously to play Time Media, you
require a class, a controller class,

109
00:09:53,410 --> 00:09:57,500
that has basic play and pause types of methods.

110
00:09:57,500 --> 00:10:01,570
That class and this framework is
known as AVPlayer, Audiovisual Player.

111
00:10:01,570 --> 00:10:05,460
A strangely apt name if I do say so myself.

112
00:10:05,460 --> 00:10:08,880
AVPlayer plays AVPlayer items.

113
00:10:08,880 --> 00:10:13,780
Note that an asset is merely representation
of the contents of a Time Media resource,

114
00:10:13,780 --> 00:10:17,440
but does not itself carry presentation state.

115
00:10:17,440 --> 00:10:22,780
The presentation state for an asset is carried by
AVPlayerItem and similarly the presentation state

116
00:10:22,780 --> 00:10:26,200
of any asset track is carried by AVPlayerItemTrack.

117
00:10:26,200 --> 00:10:33,970
Now, at this level we do not have UI affordances, no views,
but it would be kind of pointless for us to allow you

118
00:10:33,970 --> 00:10:40,600
to play a video with no way to display it to the end
user so what we do have is a subclass of CA Layer,

119
00:10:40,600 --> 00:10:47,760
Core Animation Layer, known as AVPlayerLayer, which is
capable of displaying the visual output of a player.

120
00:10:47,760 --> 00:10:53,610
Core Animation, of course, is not only useful
for visual display, it's also useful for timing,

121
00:10:53,610 --> 00:11:00,600
for animations that are timed and so we also offer in our
little bag of tricks another subclass of CA Layer known

122
00:11:00,600 --> 00:11:07,680
as AVSynchronizedLayer, which is capable of
synchronizing a layer sub tree with a playback of an item.

123
00:11:07,680 --> 00:11:09,630
Very useful for synchronization.

124
00:11:09,630 --> 00:11:14,910
The third area of functionality, editing.

125
00:11:14,910 --> 00:11:20,810
I mentioned earlier that we have this
wonderful asset model that we use uniformly.

126
00:11:20,810 --> 00:11:26,820
It would be very nice to be able to extend that
asset model in order to describe the composition

127
00:11:26,820 --> 00:11:30,720
of media from multiple sources, multiple URLs.

128
00:11:30,720 --> 00:11:36,510
If I can do that with an asset, like any other asset
I would like that composition to have multiple tracks,

129
00:11:36,510 --> 00:11:42,000
perhaps multiple video tracks, multiple audio
tracks, and I would call those AVCompositionTracks

130
00:11:42,000 --> 00:11:48,800
and they would extend the track model by allowing me
to describe the sequence of media from multiple sources

131
00:11:48,800 --> 00:11:55,570
that a particular track can display, but it's not
sufficient just to be able to describe these compositions.

132
00:11:55,570 --> 00:11:57,720
I want to be able to create them.

133
00:11:57,720 --> 00:12:03,750
So, it would be nice to have mutable subclasses if
AVComposition and AVCompositionTrack that have methods

134
00:12:03,750 --> 00:12:06,930
for the insertion of media and other editing operations

135
00:12:06,930 --> 00:12:10,750
and that should give me enough to
be able to do temporal composition.

136
00:12:10,750 --> 00:12:15,850
It isn't quite enough to describe how
to display a temporal composition so to

137
00:12:15,850 --> 00:12:18,370
that we'll add a couple of additional classes.

138
00:12:18,370 --> 00:12:23,880
If I have multiple audio sources in my composition,
I want to describe the way they are mixed together.

139
00:12:23,880 --> 00:12:34,640
I might want to set their volumes relative to each
other or I might want to control the ramping of audio

140
00:12:34,640 --> 00:12:37,770
from one source down while another ramps up.

141
00:12:37,770 --> 00:12:42,450
The ability to describe that could be
available in a class known as AVAudioMix.

142
00:12:42,450 --> 00:12:47,480
Similarly, we can offer a means to describe
how video should be composed together.

143
00:12:47,480 --> 00:12:50,390
What's the front to back ordering of my video sources?

144
00:12:50,390 --> 00:12:52,560
What's the opacity of each of the layers?

145
00:12:52,560 --> 00:12:56,510
Perhaps I can ramp the opacity,
fade one down and another one up.

146
00:12:56,510 --> 00:12:57,990
That's AVVideoComposition.

147
00:12:57,990 --> 00:13:04,800
So, I have the ability to pull media
together from multiple sources and edit it.

148
00:13:04,800 --> 00:13:11,770
I would like to create new assets from existing ones
either complex asset like a composition or a simple one.

149
00:13:11,770 --> 00:13:18,790
Start with an asset, create an object known as an
AVExportSession that manages the process of export,

150
00:13:18,790 --> 00:13:24,750
which is going to take time so this is a controller
class that I can kick off describing the type of export

151
00:13:24,750 --> 00:13:30,240
that I want to perform by means of one or
another preset that's available in the framework.

152
00:13:30,240 --> 00:13:35,380
Once I have the export session configured,
I can run it, create the new asset,

153
00:13:35,380 --> 00:13:42,580
have it written out to an output URL,
and be told when the job is done.

154
00:13:42,580 --> 00:13:49,900
Last area of functionality in our high-level tour of
AVFoundation is pertaining to capture, input devices.

155
00:13:49,900 --> 00:13:55,330
I want to be able to survey the input devices
that are available in my current context.

156
00:13:55,330 --> 00:13:59,260
For that purpose, we can offer the AVCaptureDevice class.

157
00:13:59,260 --> 00:14:05,820
You can enumerate the capture devices that are available
by type, the audio devices, the cameras and so forth,

158
00:14:05,820 --> 00:14:08,870
and you can find out what features they offer as well

159
00:14:08,870 --> 00:14:13,410
to determine what features you
should make available in your app.

160
00:14:13,410 --> 00:14:19,450
Once you've chosen a capture device to use, you would
like to set up a capture session with the inputs

161
00:14:19,450 --> 00:14:24,070
to the capture sessions where is the media coming
from and to specify the outputs of the session,

162
00:14:24,070 --> 00:14:30,010
where is the media going, in order for you to process
media coming off of the input devices and it's helpful

163
00:14:30,010 --> 00:14:36,010
to have the option either to process the media in
your application, maybe you actually want to examine

164
00:14:36,010 --> 00:14:44,120
and process video frames you should be able to do that with
or without the option of recording that media to a file.

165
00:14:44,120 --> 00:14:49,280
Finally, in order to allow your end user to know
where the camera is pointing in your application,

166
00:14:49,280 --> 00:14:55,580
it would be helpful to have yet another subclass of CA
Layer, AVCaptureVideoPreviewLayer we might choose to call it

167
00:14:55,580 --> 00:15:03,130
if we're long winded, that allows you to display to your
user a preview of what the camera is currently looking at.

168
00:15:03,130 --> 00:15:04,410
So, there you go.

169
00:15:04,410 --> 00:15:06,430
We've just designed the AVFoundation Framework.

170
00:15:06,430 --> 00:15:08,510
Thank you for coming.

171
00:15:08,510 --> 00:15:15,040
[ Applause ]

172
00:15:15,040 --> 00:15:17,920
I will be splitting my proceeds
from this job with you all equally.

173
00:15:17,920 --> 00:15:19,610
Sign up in the lobby.

174
00:15:19,610 --> 00:15:27,380
All right so one thing I need to mention before we go into
greater detail about the use of these APIs I want to point

175
00:15:27,380 --> 00:15:31,980
out the fundamental framework that we
depend on in AVFoundation, Core Media.

176
00:15:31,980 --> 00:15:38,590
Down at this very deep level the pressures are so
great that times are represented as a rational value.

177
00:15:38,590 --> 00:15:40,700
That's pressure.

178
00:15:40,700 --> 00:15:46,010
The Core Media Framework that underlies
AVFoundation defines a number of primitives

179
00:15:46,010 --> 00:15:50,250
that you'll find as you survey the AVFoundation APIs.

180
00:15:50,250 --> 00:15:56,000
Essentially anything that starts, any type that starts
with a CM is defined in the Core Media Framework.

181
00:15:56,000 --> 00:15:59,460
The one that I want to point out to
you now is a representation of time,

182
00:15:59,460 --> 00:16:02,900
which I mentioned is a rational value known as CMTime.

183
00:16:02,900 --> 00:16:10,040
Have a look at the header file in CoreMediaCMTime.H
to survey the means for creating CM Times

184
00:16:10,040 --> 00:16:15,350
for performing arithmetic operations on
them, for comparing them and so forth.

185
00:16:15,350 --> 00:16:21,790
Similarly, CMTimeRange is another data structure
in Core Media that you'll want to be familiar with.

186
00:16:21,790 --> 00:16:26,020
The speakers who follow me this afternoon will
be highlighting more of the details of Core Media

187
00:16:26,020 --> 00:16:28,880
that you'll need to become familiar
with as you adopt the API set.

188
00:16:28,880 --> 00:16:30,650
That's a good start.

189
00:16:30,650 --> 00:16:36,460
So let's rise up from that very great depth so
that we can all breathe more easily, me at least,

190
00:16:36,460 --> 00:16:45,770
and talk in detail for the remainder of our hour
about two of the areas of functionality that we offer.

191
00:16:45,770 --> 00:16:51,980
We'll talk about inspection in AVFoundation,
how you find out about Time Media resources,

192
00:16:51,980 --> 00:16:56,710
and we'll talk about playback, how you play them and
that will carry us through to the end of the hour

193
00:16:56,710 --> 00:17:02,360
and the remaining three areas of functionality will
be covered over the remainder of the afternoon.

194
00:17:02,360 --> 00:17:04,220
So, how do you inspect?

195
00:17:04,220 --> 00:17:06,710
Well, obviously you want one of these AVAsset things.

196
00:17:06,710 --> 00:17:08,260
That's the model object.

197
00:17:08,260 --> 00:17:16,870
An AVAsset is the model for time-based resources that we use
uniformly that provides information about assets as a whole.

198
00:17:16,870 --> 00:17:19,210
What's the asset's duration?

199
00:17:19,210 --> 00:17:25,150
Also, you can provide presentation hints though
it doesn't carry presentation state itself,

200
00:17:25,150 --> 00:17:30,940
that's the job of APPlayerItem, AVAsset does
carry information about the way an asset likes

201
00:17:30,940 --> 00:17:34,200
to be displayed, for example, what's its natural size?

202
00:17:34,200 --> 00:17:44,930
Note that an asset is not constrained in any way in the
number and type of sequences of media that it can present.

203
00:17:44,930 --> 00:17:51,090
It can present one or more streams of audio, one
or more streams of video, and we design it this way

204
00:17:51,090 --> 00:17:58,360
so that we can apply this uniform model to any number, any
number of the variety of media formats that we support;

205
00:17:58,360 --> 00:18:02,950
the audio only ones, the video only ones and so forth.

206
00:18:02,950 --> 00:18:07,940
Here on the slide are some examples of formats
that we can represent by AVAsset and in addition,

207
00:18:07,940 --> 00:18:13,660
a couple of other objects available in the
OS that can work together with AVAsset.

208
00:18:13,660 --> 00:18:17,900
Objects from the MediaPlayer framework, for
example, and from the AssetsLibrary.framework.

209
00:18:17,900 --> 00:18:22,300
I'll give you details about that a little later.

210
00:18:22,300 --> 00:18:25,530
Now, if AVAsset can contain multiple sequences

211
00:18:25,530 --> 00:18:32,330
of media data how we represent them
each one is an instance of AVAssetTrack.

212
00:18:32,330 --> 00:18:35,540
Each track represents a sequence of uniform type.

213
00:18:35,540 --> 00:18:42,140
A track will be all video or all audio, for example,
and each track not only will have its own uniform type,

214
00:18:42,140 --> 00:18:46,100
it will also have its own set of format descriptions.

215
00:18:46,100 --> 00:18:52,800
The format descriptions tell you about the encoding of the
media; is it H264 video, for example, or something else.

216
00:18:52,800 --> 00:18:57,560
It's possible for there to be more than one
format description represented in a single track

217
00:18:57,560 --> 00:19:01,060
so the coding does not need to be
uniform across the whole thing.

218
00:19:01,060 --> 00:19:07,600
A track also has a timeline expressed in
terms of the timeline of its parent asset.

219
00:19:07,600 --> 00:19:13,160
A track doesn't need to start at time zero if its
parent asset nor does it need to play all the way

220
00:19:13,160 --> 00:19:16,930
through to the duration of its parent
asset, it can occupy any segment

221
00:19:16,930 --> 00:19:20,750
of the parent asset that's convenient for offering.

222
00:19:20,750 --> 00:19:24,700
Other information about tracks available via AVAssetTrack.

223
00:19:24,700 --> 00:19:32,210
Now a typical asset will contain a single audio track
and video track that are synchronized with each other,

224
00:19:32,210 --> 00:19:34,640
but as mentioned before, there's really no constraint.

225
00:19:34,640 --> 00:19:36,570
Any number of tracks is possible.

226
00:19:36,570 --> 00:19:42,420
Some of you are going to immediately rush out and author
assets with say seventeen closed-captioned tracks just

227
00:19:42,420 --> 00:19:47,800
to prove a point, and I say go
ahead; the model will support it.

228
00:19:47,800 --> 00:19:54,520
What use you might put it to other than
as a conversation piece to be discussed.

229
00:19:54,520 --> 00:20:01,680
Now, let's go through some specific workflow examples,
some code, some pseudocode that you might wish to write

230
00:20:01,680 --> 00:20:07,310
to inspect assets and here in the next several
slides I intend to give you the basic flavor

231
00:20:07,310 --> 00:20:09,620
of what it's like to work with this framework.

232
00:20:09,620 --> 00:20:14,390
Remember I mentioned that you need to
code some forbearance into your apps.

233
00:20:14,390 --> 00:20:17,300
Why? Because Time Media takes time.

234
00:20:17,300 --> 00:20:19,500
That's what I want you to take away from this session.

235
00:20:19,500 --> 00:20:24,390
I'll give you a very concrete example
of what that means in just a minute.

236
00:20:24,390 --> 00:20:34,010
To inspect an asset I will start by initializing an
AVURLAsset object, a concrete subclass of AVAsset

237
00:20:34,010 --> 00:20:42,770
that presents the Time Media model for any asset that can be
reference by URL, but note just because I have that instance

238
00:20:42,770 --> 00:20:49,020
of AVURLAsset in hand does not mean that
any work has been done on my behalf.

239
00:20:49,020 --> 00:20:57,930
Remember Time Media takes time, which has a corollary
applied to AVFoundation that initialization of an object

240
00:20:57,930 --> 00:21:05,310
in this framework does not guarantee suitability
or readiness for any particular purpose.

241
00:21:05,310 --> 00:21:11,560
Specifically once you have initialized
an AVURLAsset what is it ready to do?

242
00:21:11,560 --> 00:21:14,380
The answer is nothing yet.

243
00:21:14,380 --> 00:21:18,100
We have not examined the resource at all.

244
00:21:21,770 --> 00:21:27,140
We have not even attempted to find the host.

245
00:21:27,140 --> 00:21:33,240
Initialization of an AVURLAsset
from any URL will always succeed.

246
00:21:33,240 --> 00:21:38,820
So how do you find out what you need to know,
how you get us to do some work on your behalf?

247
00:21:38,820 --> 00:21:45,100
You use the AVAsynchronousKeyValueLoading protocol that
I mentioned earlier in order to tell the framework,

248
00:21:45,100 --> 00:21:52,420
in order to tell the asset, which values for its keys, its
properties that you wish to have loaded on your behalf.

249
00:21:52,420 --> 00:21:55,120
This protocol has two methods.

250
00:21:55,120 --> 00:22:01,240
First of all in order to find out whether any
particular value for a key such as duration

251
00:22:01,240 --> 00:22:07,440
or tracks is already available use
the method, statusOfValueForKeyError

252
00:22:07,440 --> 00:22:14,170
and this method will tell you whether the information
you seek has already been loaded on your behalf.

253
00:22:14,170 --> 00:22:19,420
At initialization times since we've done no
work, the status for virtually all the keys

254
00:22:19,420 --> 00:22:23,240
of AVAsset and AVAssetTrack will be unknown.

255
00:22:23,240 --> 00:22:30,060
You have to request the loading of a particular key in
order for the status to change to loading and subsequently

256
00:22:30,060 --> 00:22:32,750
to arrive at one of the three terminal statuses.

257
00:22:32,750 --> 00:22:39,100
Ideally, will arrive at the status loaded, which tells you
that, okay, now you can call the getter and get the value

258
00:22:39,100 --> 00:22:45,070
that you wish, the array of tracks, the
CM time for the duration, et cetera,

259
00:22:45,070 --> 00:22:47,780
but it's also possible for loading to fail.

260
00:22:47,780 --> 00:22:53,510
Remember we're doing nothing to vet the
URL when you initialize the AVURLAsset.

261
00:22:53,510 --> 00:22:58,690
It's only after you've requested some loading
that we may arrive at the decision that, well,

262
00:22:58,690 --> 00:23:04,250
the URL you reference is not a Time Media resource
at all or, oh, by the way the network's down.

263
00:23:04,250 --> 00:23:08,950
That failure will occur as a result of a request to load.

264
00:23:08,950 --> 00:23:16,890
All right so with no more suspense how do you request
the loading of a value, a key, on one of these classes?

265
00:23:16,890 --> 00:23:21,100
A value for a property, a declared
property in these classes.

266
00:23:21,100 --> 00:23:23,420
Use the other method in the protocol.

267
00:23:23,420 --> 00:23:29,490
Load values, plural, asynchronously, adverb,
and you have no idea how hard it was to work

268
00:23:29,490 --> 00:23:34,610
in an adverb into an API name, four keys.

269
00:23:34,610 --> 00:23:41,410
The idea here is that you decide what collection of values
you require for the operation that you're performing.

270
00:23:41,410 --> 00:23:46,450
Put them all together into an array, all of the
strings representing the names of the keys you want

271
00:23:46,450 --> 00:23:51,640
and present them all at once to the asset
via this method and then the asset will

272
00:23:51,640 --> 00:23:55,920
in turn do the work that's necessary
in order to do the loading.

273
00:23:55,920 --> 00:24:02,360
When all of the keys in the collection have
reached a terminal status, the block that you pass

274
00:24:02,360 --> 00:24:07,500
to load values asynchronously for keys will be
invoked and at that point you can test their status

275
00:24:07,500 --> 00:24:10,120
and then move on to do the appropriate thing.

276
00:24:10,120 --> 00:24:14,450
What does it look like in a code example?

277
00:24:14,450 --> 00:24:16,660
Here's how you put it all together.

278
00:24:16,660 --> 00:24:21,760
The first thing you do with a URL is to
initialize an instance of AVURLAsset,

279
00:24:21,760 --> 00:24:23,820
which at that point is not ready to tell you anything.

280
00:24:23,820 --> 00:24:25,510
It needs to do work.

281
00:24:25,510 --> 00:24:31,600
Then you would say here's the array of keys that
I require to be loaded, their values to be loaded,

282
00:24:31,600 --> 00:24:34,390
in order to perform the operation I'm interested in.

283
00:24:34,390 --> 00:24:39,830
In this particular case, I'm going to prepare an
asset for playback and what I need to load in order

284
00:24:39,830 --> 00:24:43,170
to play something back is its array of tracks.

285
00:24:43,170 --> 00:24:49,540
So, I'm going to tell the asset please load your tracks key
by invoking LoadValuesAsynchronouslyForKeys with the array

286
00:24:49,540 --> 00:24:55,230
that in this case contains just the one
key, and I'll supply a block that I wish

287
00:24:55,230 --> 00:24:57,570
to be called the net loading is complete.

288
00:24:57,570 --> 00:25:02,310
You can tell this is a block because
it starts with the funny hat.

289
00:25:02,310 --> 00:25:08,500
This particular block that you pass to this method
takes no parameters so the code that's executed

290
00:25:08,500 --> 00:25:11,830
when loading completes is all inside the braces there.

291
00:25:11,830 --> 00:25:17,140
The first thing that it does is that it checks
the status for the key I wish to have loaded;

292
00:25:17,140 --> 00:25:22,100
and according to the terminal state that was reached
if it's loaded, I want to update my user interface

293
00:25:22,100 --> 00:25:28,380
with this tracks information; if it failed to
load, I wish to report the error to the end user;

294
00:25:28,380 --> 00:25:34,560
or if I've canceled the loading of the values for
key zone and asset, I want to do some bookkeeping.

295
00:25:34,560 --> 00:25:40,940
So, that's basically what it would look like and this is
the way that you prepare assets for operations in your app,

296
00:25:40,940 --> 00:25:47,260
this is the means by which you code the
forbearance for the time that Time Media takes.

297
00:25:47,260 --> 00:25:48,540
So, let me review this really quickly.

298
00:25:48,540 --> 00:25:53,210
How do you inspect and load assets
in order to prepare them for use?

299
00:25:53,210 --> 00:25:56,820
You have information you want to find out.

300
00:25:56,820 --> 00:25:58,670
You know that it may take time.

301
00:25:58,670 --> 00:26:00,290
I'll give you a concrete example.

302
00:26:00,290 --> 00:26:06,730
Something as innocuous as an MP3 file can take an
enormous time in order to deliver just a very simple piece

303
00:26:06,730 --> 00:26:11,890
of information about the duration of
an MP3 file may require the parsing

304
00:26:11,890 --> 00:26:16,270
of every single audio packet in
the file in order to calculate.

305
00:26:16,270 --> 00:26:22,510
It's not necessarily the case, in other words, that MP3
files contain summary information about their contents.

306
00:26:22,510 --> 00:26:27,100
You do not want your user to have
to wait while that work goes on.

307
00:26:27,100 --> 00:26:35,130
You can ask for the work to be done synchronously,
but there are significant downsides to doing that.

308
00:26:35,130 --> 00:26:42,190
First of all, you risk having your app become unresponsive
to the user, which of course, is a total no-no.

309
00:26:42,190 --> 00:26:48,770
Users expect apps to respond to their control, but
there's an even worse consequence that's possible

310
00:26:48,770 --> 00:26:53,970
if you request these pieces of
information to be loaded synchronously.

311
00:26:53,970 --> 00:27:02,060
There's a watch dog on iOS 4 that watches interaction
with the Media Services available on the platform.

312
00:27:02,060 --> 00:27:09,060
If any one of the clients of Media Services
request an operation to be performed synchronously

313
00:27:09,060 --> 00:27:15,180
and that operation takes longer than a timeout value
that that watchdog manages, the watchdog will come along

314
00:27:15,180 --> 00:27:22,620
and kill the application that took all that time and this
may also have the side effect of causing media services

315
00:27:22,620 --> 00:27:27,810
that are in use by all of the entities
on the system to be reset.

316
00:27:27,810 --> 00:27:30,770
Don't let this happen to your app.

317
00:27:30,770 --> 00:27:32,260
What will this result in?

318
00:27:32,260 --> 00:27:37,300
Well, fewer stars in the app store that's for sure.

319
00:27:37,300 --> 00:27:39,560
How do you avoid this calamity?

320
00:27:39,560 --> 00:27:45,730
Come to www.C210 meet me in Presidio, and I will
tell you all about how to use the AV, oh, wait,

321
00:27:45,730 --> 00:27:48,590
I'm having one of those time shift things, right?

322
00:27:48,590 --> 00:27:57,630
Sorry. Use the protocol just described, load the
values for T's asynchronously and you will be good.

323
00:27:57,630 --> 00:28:00,440
Now, this duration thing, this troublesome duration thing.

324
00:28:00,440 --> 00:28:05,740
I told you how expensive it is sometimes
to calculate the duration of an MP3 file.

325
00:28:05,740 --> 00:28:07,510
Even if you do it right and you load

326
00:28:07,510 --> 00:28:13,600
that value asynchronously you're sitting there saying I
don't really need you to do all of that work on my behalf,

327
00:28:13,600 --> 00:28:19,670
I just want to play the darned thing, I don't need
you to find out the duration exactly in advance.

328
00:28:19,670 --> 00:28:21,140
Well, yes, that's actually true.

329
00:28:21,140 --> 00:28:28,580
It turns out that for duration, which is a special value
defined by AVAsset, it's usually sufficient particularly

330
00:28:28,580 --> 00:28:32,530
in playback scenarios to use an estimated value.

331
00:28:32,530 --> 00:28:37,610
You don't need to know exactly how long something
takes unless you're trying to coordinate something else

332
00:28:37,610 --> 00:28:41,140
with its playback, which is not typically the case.

333
00:28:41,140 --> 00:28:50,990
So, by default the behavior of AVURLAsset is to
provide enough accuracy for a playback scenario.

334
00:28:50,990 --> 00:28:58,180
Note that if the underlying format that stores the media
offers summary information about the timing and duration

335
00:28:58,180 --> 00:29:02,810
of the resource, the information that we
provide you will be completely accurate.

336
00:29:02,810 --> 00:29:08,310
For example, if the file is a QuickTime movie
file or an MPEG4 file, those things do contain

337
00:29:08,310 --> 00:29:13,730
that summary information, and we will give it to
you and you can find out if any particular instance

338
00:29:13,730 --> 00:29:20,820
of AVURLAsset provides precise duration
and timing by examining its eponymous key,

339
00:29:22,030 --> 00:29:28,820
but if you require precise duration and timing from every
asset that you're working with, if you're doing something

340
00:29:28,820 --> 00:29:35,960
that requires that degree of precision, you can request it
at initialization time by setting in the options dictionary

341
00:29:35,960 --> 00:29:42,750
when you initialize the AVURLAsset the key
AVURLAssetPreferPreciseDurationAndTimingKey

342
00:29:42,750 --> 00:29:48,040
and we will give you an instance of AVURLAsset
that will be accurate regardless of cost.

343
00:29:48,040 --> 00:29:55,950
Okay. So that is the fundamental interaction
that you will have with this framework,

344
00:29:55,950 --> 00:30:00,450
and you'll note as we discuss these classes
this afternoon that there are similar stages

345
00:30:00,450 --> 00:30:04,200
of operation with each of the chief classes.

346
00:30:04,200 --> 00:30:11,120
You initialize something, you prepare it for the purpose
that you want to use it for, you observe its status in order

347
00:30:11,120 --> 00:30:15,230
to determine whether it's ready
for that purpose then you move on.

348
00:30:15,230 --> 00:30:20,550
Playback not surprisingly is very
similar in behavior to inspection.

349
00:30:24,530 --> 00:30:29,100
I mentioned earlier that the chief class
for controlling playback is AVPlayer.

350
00:30:29,100 --> 00:30:35,600
It has the methods on it for controlling rate and
so forth that you would expect in such a class,

351
00:30:35,600 --> 00:30:41,870
but beyond control it's extremely rich in
the facilities that it provides to allow you

352
00:30:41,870 --> 00:30:45,450
to observe the presentation state, the playback state,

353
00:30:45,450 --> 00:30:50,110
as it changes so that you can synchronize
a UI to playback state, for example.

354
00:30:50,110 --> 00:30:57,180
I mentioned that the AVPlayer plays items, it has a
property known as its current item so you can find

355
00:30:57,180 --> 00:31:02,390
out what it's playing at any given
time, and the AVPlayer item

356
00:31:02,390 --> 00:31:07,670
as I mentioned earlier confers
presentation state upon an asset.

357
00:31:07,670 --> 00:31:15,130
It describes how an asset should be presented so it's
possible to play an asset with more than one player item.

358
00:31:15,130 --> 00:31:20,790
For example, in one context you may wish to play a
particular time segment of an asset with one instance

359
00:31:20,790 --> 00:31:26,770
of AVPlayer item and in another context play a
different AVPlayer item associated with the same asset

360
00:31:26,770 --> 00:31:29,180
to play a different time range of interest.

361
00:31:29,180 --> 00:31:37,550
You can initialize an AVPlayer item with an
existing asset that you have or directly from a URL.

362
00:31:37,550 --> 00:31:42,080
If you initialize it with a URL, the
AVPlayer item will prepare the instance

363
00:31:42,080 --> 00:31:45,870
of AVAsset for you that you can use for inspection.

364
00:31:47,480 --> 00:31:53,960
AVPlayer item is also the class that you use in
order to control time as playback progresses.

365
00:31:53,960 --> 00:31:59,110
This is what you'd use to seek, for example, or to step.

366
00:31:59,110 --> 00:32:04,650
In addition, AVPlayerItems have one or more
AVPlayerItem tracks that correspond with the tracks

367
00:32:04,650 --> 00:32:08,530
of the asset you're playing and those
have presentation state as well.

368
00:32:08,530 --> 00:32:13,950
In particular, whether a track
is enabled for playback or not.

369
00:32:13,950 --> 00:32:18,270
So, having given you the overview
of the player related classes,

370
00:32:18,270 --> 00:32:24,650
let's have a look at how you would code
up preparation for playback in your app.

371
00:32:24,650 --> 00:32:29,670
One of the challenges I mentioned right at
the start of our talk is that it's difficult

372
00:32:29,670 --> 00:32:35,620
or it may require a little extra work in
order for you to treat assets uniformly

373
00:32:35,620 --> 00:32:39,640
because of differences particularly in delivery protocol.

374
00:32:39,640 --> 00:32:44,950
There are two main classes of assets that
you need to be aware of for playback.

375
00:32:44,950 --> 00:32:47,610
The first one is a file-based asset.

376
00:32:47,610 --> 00:32:53,750
Essentially an asset that we have random access to
in order to read information out of its container.

377
00:32:53,750 --> 00:32:56,330
The second one is a stream-based asset.

378
00:32:56,330 --> 00:33:00,420
We have a lesser degree of control over
what we can read out of that asset.

379
00:33:00,420 --> 00:33:04,290
It's essentially being beamed to us by a server.

380
00:33:04,290 --> 00:33:08,980
There's a little bit of a difference in the way
that you set up playback of an asset depending

381
00:33:08,980 --> 00:33:12,170
on whether it's filed based or stream based.

382
00:33:12,170 --> 00:33:15,420
So, let's talk about the workflows for each and then talk

383
00:33:15,420 --> 00:33:18,840
about what you would do if you
don't know what type you have.

384
00:33:18,840 --> 00:33:25,880
To start with if you have a file-based asset, in other
words, something like a video from the camera role

385
00:33:25,880 --> 00:33:32,080
that was shot with the camera or an item from the iPod
library that you can access by the media library framework

386
00:33:32,080 --> 00:33:36,980
or even a file that resides in a remote HTTP server.

387
00:33:36,980 --> 00:33:41,730
Here's the workflow that you would use to
playback any one of those file-based assets.

388
00:33:41,730 --> 00:33:47,770
As I mentioned earlier, initialize an instance
of AVURLAsset with the asset of interest

389
00:33:47,770 --> 00:33:53,750
and then it is your responsibility in preparing
that asset for playback to load its tracks.

390
00:33:53,750 --> 00:33:56,500
The player is going to want to know
what media in there so go ahead

391
00:33:56,500 --> 00:34:03,310
and do the job using the AVAsychronousKeyValueLoading
protocol to load the tracks of that asset.

392
00:34:03,310 --> 00:34:12,780
If that succeeds, then you can go on to initialize an
AVPlayerItem with that asset and remember our correlary

393
00:34:12,780 --> 00:34:19,230
to our basic tenet, that Time Media takes time, that
initialization of an object does not guarantee readiness

394
00:34:19,230 --> 00:34:24,410
or suitability for any particular purpose,
I've just initialized an AVPlayerItem

395
00:34:24,410 --> 00:34:29,050
but when initialization completes,
it is not yet ready to play.

396
00:34:29,050 --> 00:34:37,330
What you'll want to do once you've created an AVPlayerItem
is observe its status key by a key value observing.

397
00:34:37,330 --> 00:34:44,590
Observing a status key you can be informed
of when the PlayerItem becomes ready to play

398
00:34:44,590 --> 00:34:52,010
and you initiate the process by which it becomes ready
to play by associating the AVPlayerItem with an AVPlayer.

399
00:34:52,010 --> 00:34:56,790
In this example, I'm initializing the
AVPlayer with the AVPlayerItem I created.

400
00:34:56,790 --> 00:35:01,390
That kicks off the process to prepare all of the
chutes and ladders to get the thing ready to play

401
00:35:01,390 --> 00:35:10,450
and via key value observing you'll soon discover that the
status of the player item has changed to ready to play.

402
00:35:10,450 --> 00:35:16,470
When that occurs, then it's possible for you to
survey the presentation state of the player item

403
00:35:16,470 --> 00:35:23,320
where tracks are enabled, for example, to choose a track
in this particular case to be disabled for playback,

404
00:35:23,320 --> 00:35:30,260
as an example, other customization of presentation state is
also possible, of course, but once you've prepared the item

405
00:35:30,260 --> 00:35:37,300
for playback with whatever customization that you
desire, then go ahead and tell the AVPlayer to play

406
00:35:37,300 --> 00:35:44,060
and that's essentially the workflow that
you follow to play a file-based asset.

407
00:35:46,310 --> 00:35:51,740
The second example as I mentioned
earlier would be for stream-based assets.

408
00:35:51,740 --> 00:35:57,700
As you know, iOS 4 supports the HTTP live stream protocol

409
00:35:57,700 --> 00:36:01,950
and HTTP live streams are essentially
a playback only technology.

410
00:36:01,950 --> 00:36:10,210
It is not possible for you to create an AVURLAsset
from an HTPP live stream URL from scratch.

411
00:36:10,210 --> 00:36:14,990
What you need to do if you fall into this category
you have an HTTP live stream that you wish

412
00:36:14,990 --> 00:36:22,350
to play is go directly to the player related classes.

413
00:36:22,350 --> 00:36:32,720
Start with an AVPlayerItem, initialize it with the URL for
your HTTP live stream, do not spill water on the hardware,

414
00:36:32,720 --> 00:36:38,840
you just weren't aware of the safety tips
you're going to receive at this session.

415
00:36:38,840 --> 00:36:42,000
I'm glad you're enjoying them.

416
00:36:42,000 --> 00:36:47,310
Associate the player item with an AVPlayer in order to
start the process of making that PlayerItem ready to play.

417
00:36:47,310 --> 00:36:49,780
Then a little magic happens.

418
00:36:49,780 --> 00:36:57,220
Once that PlayerItem becomes ready to play, the AVPlayerItem
will create on your behalf an AVAsset that you can use

419
00:36:57,220 --> 00:37:02,410
to inspect the contexts of that HTTP live stream
so you can find out what tracks are present.

420
00:37:02,410 --> 00:37:09,800
For example, and if necessary, you can customize the
presentation state as well, but presuming that you just want

421
00:37:09,800 --> 00:37:14,990
to play it then, of course, you can move on once
it's ready to play to tell the AVPlayer to play.

422
00:37:14,990 --> 00:37:25,280
As a side note, it is possible for you to take a shortcut
for HTTP live streams and simply initialize an instance

423
00:37:25,280 --> 00:37:31,360
of AVPlayer with a URL that you wish to play and the
AVPlayer will create on your behalf the AVPlayerItem

424
00:37:31,360 --> 00:37:33,780
and the whole chain of events will be kicked off for you.

425
00:37:33,780 --> 00:37:37,450
So, if you know that you're playing HTTP live streams,

426
00:37:37,450 --> 00:37:42,300
you can take this shortcut, but
here's how we put it all together.

427
00:37:42,300 --> 00:37:48,680
If you don't know in advance the type of resource
that you wish to play could be a file-based asset,

428
00:37:48,680 --> 00:37:52,920
it could be a stream-based one, here's what we recommend.

429
00:37:52,920 --> 00:37:58,800
Essentially, you have to concatenate the two workflows, and
we recommend that you start with the file-based workflow.

430
00:37:58,800 --> 00:38:01,830
Try the URL as a file-based asset.

431
00:38:01,830 --> 00:38:09,920
Create an AVURLAsset from that URL with that URL,
attempt to load its tracks key as described previously.

432
00:38:09,920 --> 00:38:14,160
If that succeeds, move on with
the file-based playback scenario.

433
00:38:14,160 --> 00:38:23,180
If it fails, it's possible that
URL is to a valid HTTP live stream.

434
00:38:23,180 --> 00:38:28,430
So, try it then by initializing an AVPlayerItem with the URL

435
00:38:28,430 --> 00:38:32,350
and move on with the stream-based
workflow as mentioned earlier.

436
00:38:32,350 --> 00:38:42,280
The two code paths converge, and you can treat them
uniformly once the Player Item becomes ready to play.

437
00:38:42,280 --> 00:38:47,270
All right so now you're preparing items for
playback, you've told the AVPlayerItem to play,

438
00:38:47,270 --> 00:38:51,790
how does your app stay in sync with time and control time?

439
00:38:51,790 --> 00:38:58,040
First of all AVPlayerItem as I mentioned earlier
is the class that provides control over time.

440
00:38:58,040 --> 00:38:59,620
Could I have a mic down for a second, please?

441
00:38:59,620 --> 00:39:06,370
I have a little housekeeping to take care of.

442
00:39:06,370 --> 00:39:08,750
Are we ready?

443
00:39:09,970 --> 00:39:11,660
Now? No. Okay.

444
00:39:11,660 --> 00:39:12,510
Sorry.

445
00:39:12,510 --> 00:39:16,770
[ Coughing ]

446
00:39:16,770 --> 00:39:21,030
[ Laughter ]

447
00:39:21,030 --> 00:39:23,110
>> Kevin Calhoun: Okay, thank you.

448
00:39:23,110 --> 00:39:29,350
That impromptu performance was
rehearsed endlessly for weeks at a time.

449
00:39:29,350 --> 00:39:33,200
[Laughter] Until it was perfected in San
Francisco, California, I'll skip that.

450
00:39:33,200 --> 00:39:34,920
So, control over time.

451
00:39:34,920 --> 00:39:41,560
Seek the time is the method that you use to move
in time within the time range of an AVPlayerItem.

452
00:39:41,560 --> 00:39:45,550
You should note that seek to time
is not necessarily precise.

453
00:39:45,550 --> 00:39:48,370
One of the things that you'll note
in the design of this framework is

454
00:39:48,370 --> 00:39:53,920
that we place a very high value
on responsiveness to the end user.

455
00:39:53,920 --> 00:40:01,370
So, seeking to time can be an extremely expensive
operation if you wish it to be for it to be precise.

456
00:40:01,370 --> 00:40:08,900
It can require to go to any specific time the decoding
of an arbitrary long sequence of dependent video frames.

457
00:40:08,900 --> 00:40:13,480
You don't necessarily need that work
to be done arriving at a time nearer

458
00:40:13,480 --> 00:40:18,160
to the time you wish is usually sufficient
and that's a behavior seek to time.

459
00:40:18,160 --> 00:40:24,100
It will give you good responsiveness and good
enough results for typical playback scenarios.

460
00:40:24,100 --> 00:40:30,170
However, if you need more precise control over
time as you seek around in an AVPlayerItem,

461
00:40:30,170 --> 00:40:37,600
you can use the variant method seek to time tolerance
before, tolerance after and these tolerances allow you

462
00:40:37,600 --> 00:40:43,430
to essentially to define the time range
within which you'll be satisfied for the time

463
00:40:43,430 --> 00:40:46,160
to arrive at when they seek operation is complete.

464
00:40:46,160 --> 00:40:51,430
You can set these tolerances to zero to
arrive at precisely the time that you desire,

465
00:40:51,430 --> 00:40:57,100
but you should note as I mentioned just earlier
this operation can be expensive and, in fact,

466
00:40:57,100 --> 00:41:02,770
it can be detrimental to the responsiveness of your
application to the end user so use with caution.

467
00:41:02,770 --> 00:41:07,260
Where does the media come from?

468
00:41:07,260 --> 00:41:10,000
The question I know we all ask each other.

469
00:41:10,000 --> 00:41:15,600
You can play file-based assets from
the camera roll as I mentioned earlier.

470
00:41:15,600 --> 00:41:16,700
How do you do that?

471
00:41:16,700 --> 00:41:21,700
The framework that you want to become familiar with in
order to play video that you can shoot with the camera

472
00:41:21,700 --> 00:41:25,810
on the device is the AssetsLibrary.framework.

473
00:41:25,810 --> 00:41:31,290
That framework has facilities that allow you to
survey the groups of assets that are available,

474
00:41:31,290 --> 00:41:37,000
essentially the camera rolls that have been
recorded, and within each group it allows you

475
00:41:37,000 --> 00:41:39,860
to enumerate the assets that are present.

476
00:41:39,860 --> 00:41:47,110
You can filter them by type such as just showing you the
videos, but once you have a specific instance of ALAsset

477
00:41:47,110 --> 00:41:53,710
from the AssetsLibrary.framework that you wish
to play, you obtain the URL from that ALAsset

478
00:41:53,710 --> 00:41:57,700
by asking it we missed this particular code
point up here so I'll tell you what it is.

479
00:41:57,700 --> 00:42:03,080
Use the method ALAsset default representation
to get us to fault representation

480
00:42:03,080 --> 00:42:07,820
and ask of its default representation for its URL.

481
00:42:07,820 --> 00:42:12,880
Once you have that URL in hand, you can
initialize an AVURLAsset and proceed

482
00:42:12,880 --> 00:42:17,220
with the file-based playback workflow as described earlier.

483
00:42:17,220 --> 00:42:24,980
Similarly, if you wish to play media from the iPod library,
the MediaPlayer framework has the facilities for you

484
00:42:24,980 --> 00:42:30,570
to allow you to query for any particular
piece of media of interest.

485
00:42:30,570 --> 00:42:38,150
Essentially you create an instance of MPMediaQuery and
resolve it against the MediaLibrary and it will give you one

486
00:42:38,150 --> 00:42:41,880
or more MPMediaItems that satisfy your query.

487
00:42:41,880 --> 00:42:46,870
Once you have the MPMediaItem in hand
that you wish to play via AVPlayer,

488
00:42:46,870 --> 00:42:53,750
you obtain its URL by requesting
its MPMediaItemPropertyAssetURL.

489
00:42:53,750 --> 00:42:57,510
That's the URL that you would use
to initialize an AVURLAsset

490
00:42:57,510 --> 00:43:01,750
and then you can proceed again with
the file-based playback workflow.

491
00:43:01,750 --> 00:43:09,230
So, now you have a source of media you know how to set up
playback and get it going, even the seek around and time,

492
00:43:09,230 --> 00:43:13,620
how do you keep your app in sync
with playback as it proceeds?

493
00:43:13,620 --> 00:43:18,400
Well, let's talk about the things that you can
observe and respond to while playback occurs.

494
00:43:18,400 --> 00:43:22,670
First of all you can track presentation state.

495
00:43:22,670 --> 00:43:26,950
A prime example here, for example, is the rate of playback.

496
00:43:26,950 --> 00:43:34,680
As it changes, you can use key-value observation in
order for your app to respond to changes to properties

497
00:43:34,680 --> 00:43:38,810
of playback both in AVPlayer and in AVPlayerItem.

498
00:43:38,810 --> 00:43:41,210
So key-value observing is your friend.

499
00:43:41,210 --> 00:43:48,580
Register for the observation of these keys and you'll
be able to discover not only when changes occur that you

500
00:43:48,580 --> 00:43:56,160
as the application initiates perhaps in response to
user input, but also and equally important changes

501
00:43:56,160 --> 00:44:02,080
that are initiated underneath you
by the framework or by the system.

502
00:44:02,080 --> 00:44:09,270
Well, what kinds of changes can occur in playback not
initiated by you the application in response to your user?

503
00:44:09,270 --> 00:44:16,820
For example, if you are playing visual media and
the user multitasks, which is out of your app,

504
00:44:16,820 --> 00:44:24,610
you'll observe a change in the rate because that playback
item, that AVPlayerItem will automatically be paused

505
00:44:24,610 --> 00:44:29,080
and you would observe that by key-value
observation of the rate key.

506
00:44:29,080 --> 00:44:36,090
Also, if you're playing remote media, you can observe
changes in the AVPlayerItemsProperties loaded time ranges

507
00:44:36,090 --> 00:44:44,450
and seekable time ranges, which will tell you what portions
of the timeline of the AVPlayerItem are currently available.

508
00:44:44,450 --> 00:44:50,120
Similarly if you are playing a HTTP live
stream that has alternate encodings,

509
00:44:50,120 --> 00:44:57,440
higher data rates for greater network bandwidth, lower
data rates for conditions that are not quite as promising,

510
00:44:57,440 --> 00:45:05,630
you can actually observe when the HTTP live stream
changes from one of the alternate encodings to another

511
00:45:05,630 --> 00:45:10,690
by observing the tracks key of AVPlayerItem
and as a switch occurs, you can see, ah-ha,

512
00:45:10,690 --> 00:45:14,660
now this AVPlayerItem is playing this encoding.

513
00:45:15,840 --> 00:45:21,730
One last thing you'll want to observe on AVPlayer and
AVPlayerItem already recommended as you set things

514
00:45:21,730 --> 00:45:25,130
up you wish to be observing their status keys.

515
00:45:25,130 --> 00:45:26,230
Are they ready to play?

516
00:45:26,230 --> 00:45:31,020
That's the first thing you'll need to know before you
kickoff playback, but it's possible for these objects

517
00:45:31,020 --> 00:45:38,130
to arrive at other interesting statuses as
well, in particular, if you or your neighbor

518
00:45:38,130 --> 00:45:44,300
or the guy three rows behind you writes one of those
applications that requests information synchronously

519
00:45:44,300 --> 00:45:52,780
of an AVAsset, that takes longer than the time out value and
his app is killed and media services are reset for everyone

520
00:45:52,780 --> 00:45:59,700
on the system after we have a moment to hang our heads
in shame, the first thing that you have to realize is

521
00:45:59,700 --> 00:46:05,350
that every other client of media services on
the system will be responsible for setting

522
00:46:05,350 --> 00:46:08,760
up their media operations all over again.

523
00:46:08,760 --> 00:46:10,180
How do you know that you have to do this?

524
00:46:10,180 --> 00:46:17,580
If someone has caused this calamity to occur, you
observe the status key on AVPlayer and AVPlayerItem.

525
00:46:17,580 --> 00:46:24,790
The status can change to a failure status and the
error key of either of those classes can report

526
00:46:24,790 --> 00:46:31,760
that media services were reset via the error code
of the NSError that's available from the error key.

527
00:46:31,760 --> 00:46:36,530
When you see that that has occurred, you know
oh, no, someone has done the wrong thing,

528
00:46:36,530 --> 00:46:43,460
media services have been reset, I need to create
new instances of my playback objects and so forth,

529
00:46:43,460 --> 00:46:47,610
put them into the state that I was
in before and then I can proceed.

530
00:46:51,040 --> 00:46:56,450
Now, if you are observing changes that you
initiate and you are also observing changes

531
00:46:56,450 --> 00:47:03,100
that the framework initiates underneath you, well, there
needs to be some way of serializing your registration

532
00:47:03,100 --> 00:47:08,740
and unregistration of interest and
notification with notifications in flight.

533
00:47:08,740 --> 00:47:13,830
We don't wish for you to have to deal with any
possible race conditions if you're trying to disengage

534
00:47:13,830 --> 00:47:18,560
from key-value observation while
there's a notification about to arrive.

535
00:47:18,560 --> 00:47:24,860
So, in order to avoid that, our recommendation for
iOS 4, for your key-value observation of AVPlayer

536
00:47:24,860 --> 00:47:31,310
and the other player related classes, is to register
for a key-value observation and unregister from it

537
00:47:31,310 --> 00:47:36,410
on the main thread and that will guarantee
a sensible serialization of registration

538
00:47:36,410 --> 00:47:39,930
and unregistration with notifications in flight.

539
00:47:39,930 --> 00:47:44,830
Now, note that we still very highly
value responsiveness to the end user.

540
00:47:44,830 --> 00:47:49,820
We're not actually going to perform any of the work
associated with these state changes on the main thread,

541
00:47:49,820 --> 00:47:53,460
it's only the notifications of those
changes that we deliver on that thread

542
00:47:53,460 --> 00:47:56,560
and that we recommend that you register and unregister on.

543
00:47:56,560 --> 00:48:04,550
You can also track the readiness
for visual display of a visual item.

544
00:48:04,550 --> 00:48:11,020
Perhaps you have an AVPlayer playing audio visual
content, and you want to know when the AVPlayer layer

545
00:48:11,020 --> 00:48:17,410
that you have setup to display the visual output of
the player is ready for display in your layer tree.

546
00:48:17,410 --> 00:48:23,510
You can observe the ready for display key
on AVPlayerLayer and when that becomes yes,

547
00:48:23,510 --> 00:48:28,910
you know that you can insert the AVPlayerLayer into
your layer tree and it has something ready to draw.

548
00:48:28,910 --> 00:48:35,260
This is particularly useful if you want to code a
Core Animation transition from the tree in a state

549
00:48:35,260 --> 00:48:39,700
in which it lacks the AVPlayerLayer to
display the visual output to one that does,

550
00:48:39,700 --> 00:48:45,020
and you can do quite a few interesting effects with this.

551
00:48:45,020 --> 00:48:48,650
You can also track the progression of time.

552
00:48:48,650 --> 00:48:53,870
Now because time progresses incrementally during playback,

553
00:48:53,870 --> 00:48:57,250
it's not something that you can
observe by a key value observing.

554
00:48:57,250 --> 00:48:58,700
It doesn't work for that model.

555
00:48:58,700 --> 00:49:03,540
So, we have a different model available to
you for tracking the progression of time.

556
00:49:03,540 --> 00:49:11,700
AVPlayer offers you the option of creating one or more
periodic time observers and you get your hands on one

557
00:49:11,700 --> 00:49:16,490
of these by using the
AVPlayerMethodAddPeriodicTimeObserverForInterval,

558
00:49:16,490 --> 00:49:23,440
you supply an interval at which you want to be invoked
as time progresses and the block that you supply

559
00:49:23,440 --> 00:49:28,250
to this method will be called at that interval
and that should allow you, for example,

560
00:49:28,250 --> 00:49:32,750
to keep a UI that's tracking the
current time in sync with playback.

561
00:49:32,750 --> 00:49:36,990
The block will also be called when time
jumps also when playback starts or stops.

562
00:49:36,990 --> 00:49:45,670
So, you'll have full information, full disclosure about the
progression of time if it's moving smoothly or if it jumps.

563
00:49:45,670 --> 00:49:51,420
Also if you're trying to perform some manual programmatic
synchronization of something going on in your app

564
00:49:51,420 --> 00:49:56,360
with playback, we offer a boundary time observer.

565
00:49:56,360 --> 00:50:04,290
You create one of these on AVPlayer, give it a list of
times of interest, an array of CMTimes stored in NS Values,

566
00:50:04,290 --> 00:50:10,380
and when any one of those times is
traversed, when it's crossed during playback,

567
00:50:10,380 --> 00:50:15,260
the block that you supply will be evoked
and you can respond appropriately according

568
00:50:15,260 --> 00:50:19,350
to the current time that has been reached at that point.

569
00:50:19,350 --> 00:50:25,970
Now note that if you do a lot of expensive operations
in one of these blocks in response to either a periodic

570
00:50:25,970 --> 00:50:32,360
or a boundary time observer, we don't guarantee
delivery of all of the invocations of the block.

571
00:50:32,360 --> 00:50:37,740
It's up to you, of course, to code your apps to fit in the
operations that you perform in connection with playback

572
00:50:37,740 --> 00:50:43,080
so that you don't swamp the CPU; that you can co-exist with
the Time Media operation that's going on at the same time.

573
00:50:43,080 --> 00:50:49,600
Finally, the last thing that you
can track as playback progresses is

574
00:50:49,600 --> 00:50:53,820
when an AVPlayerItem reaches its end time and stops.

575
00:50:53,820 --> 00:51:00,660
We offer a good old NS notification for that purpose
known as AVPlayerItemDidPlayToEndTimeNotification.

576
00:51:00,660 --> 00:51:08,730
You can listen for this, it will fire when the
AVPlayerItem has played all the way through to the end.

577
00:51:08,730 --> 00:51:11,090
All right so that's basically the playback classes.

578
00:51:11,090 --> 00:51:17,500
What you need to do in order to initiate playback, how you
can get media from the various sources available to you

579
00:51:17,500 --> 00:51:21,520
in order to play it and how you can
keep in sync with playback as it occurs.

580
00:51:21,520 --> 00:51:27,470
A couple of best practices to cover before we move on.

581
00:51:27,470 --> 00:51:33,200
How do you become a good citizen of the platform
now that you are taking control over Time Media?

582
00:51:33,200 --> 00:51:38,990
Well, use the AVAsynchronousKeyValueLoadingProtocol
described earlier, that's number one.

583
00:51:38,990 --> 00:51:44,700
Also, tell the audio subsystem the type of
audio processing that you're performing.

584
00:51:44,700 --> 00:51:51,510
To do that use AVAudioSession in AVFoundation, set the
category of audio processing that you are performing.

585
00:51:51,510 --> 00:51:55,040
If you're playing, tell it that your category is playback.

586
00:51:55,040 --> 00:52:01,410
This allows the audio subsystem on the device to
arbitrate resources, audio-related resources properly

587
00:52:01,410 --> 00:52:05,560
for the various applications that
are trying to make use of them.

588
00:52:05,560 --> 00:52:07,700
There's more that you can do with AVAudioSession.

589
00:52:07,700 --> 00:52:10,470
For example, you can use it to become aware of interruptions

590
00:52:10,470 --> 00:52:14,370
that may arise during playback
or other Time Media operations.

591
00:52:14,370 --> 00:52:22,970
More details about AVAudioSession are available in the core
audio sessions that I'll point to you in just a few minutes.

592
00:52:22,970 --> 00:52:26,700
Special multi-tasking note.

593
00:52:26,700 --> 00:52:31,960
You are already aware from other sessions that
you can create your application and register it

594
00:52:31,960 --> 00:52:37,380
to get processing time in the background
and even to play audio in the background.

595
00:52:37,380 --> 00:52:42,490
What I need to make clear to you is the specific
behavior that occurs when you're playing visual media

596
00:52:42,490 --> 00:52:47,000
and the user switches you from
the foreground to the background.

597
00:52:47,000 --> 00:52:51,680
When that happens with no intervention
necessary on your part,

598
00:52:51,680 --> 00:52:57,020
the playback of a visual item will
automatically be paused and its display

599
00:52:57,020 --> 00:53:03,410
in the layer tree will automatically be disengaged
and you need do nothing in order to accomplish this.

600
00:53:03,410 --> 00:53:08,070
That's they standard user interface for what should
happen when visual items are playing on the platform

601
00:53:08,070 --> 00:53:10,770
and the app is switched to the background.

602
00:53:10,770 --> 00:53:16,220
Now, if you setup your application to get processing
time in the background and play audio, you can,

603
00:53:16,220 --> 00:53:19,730
if it's appropriate for your content
and the workflow of your app,

604
00:53:19,730 --> 00:53:26,800
to continue playing the audio portion
of the AVPlayerItem in the background.

605
00:53:26,800 --> 00:53:32,540
One other note about multi-tasking referring
back to the earlier functionality of inspection,

606
00:53:32,540 --> 00:53:37,720
the loading the values of an asset in
order to inspect information about it,

607
00:53:37,720 --> 00:53:44,700
any loading that you have initiated will continue
to progress even if your app is in the background

608
00:53:44,700 --> 00:53:51,140
and if the loading has completed while your app is in the
background and you've set up your app to get processing time

609
00:53:51,140 --> 00:53:56,420
in the background, you will be notified at the completion
of that loading while you're still in the background

610
00:53:56,420 --> 00:54:03,650
so you can set yourself up on the basis of what an asset
contains even while you await return to the foreground.

611
00:54:03,650 --> 00:54:09,050
If your application is not setup to get processing time in
the background, then you'll be notified of the completion

612
00:54:09,050 --> 00:54:13,330
of any loading that has occurred in the
interim when you return to the foreground.

613
00:54:14,690 --> 00:54:19,930
Okay. Let's review what we've covered
during these 55 or so minutes so far.

614
00:54:19,930 --> 00:54:29,270
I've given you an overview of the AVFoundation framework
in iOS 4; told you the ways in which we've expanded it;

615
00:54:29,270 --> 00:54:32,960
and covered the main areas of functionality that it offers.

616
00:54:32,960 --> 00:54:39,790
I have also given you the flavor of the API and what
you need to do in your apps, to as I said earlier,

617
00:54:39,790 --> 00:54:47,510
code a little forbearance into your applications because
the processes that you apply to Time Media will take time.

618
00:54:47,510 --> 00:54:51,640
You want your apps to remain responsive
and good citizens of the platform.

619
00:54:51,640 --> 00:54:58,720
I've told you in detail how to use the
inspection-related classes; AVAsset, AVAssetTrack,

620
00:54:58,720 --> 00:55:02,570
and how to use the playback-related
classes AVPlayer, et cetera.

621
00:55:02,570 --> 00:55:09,210
Remember that an AVFoundation because most of the
operations that occur occur asynchronously together

622
00:55:09,210 --> 00:55:16,630
with other things going on in the platform we're making
full use of programming paradigms that permit this level

623
00:55:16,630 --> 00:55:20,080
of asynchronicity, this level of cooperation.

624
00:55:20,080 --> 00:55:27,050
In particular, we're extending key-value coding with
something we're calling asynchronous key-value loading

625
00:55:27,050 --> 00:55:32,470
in AVFoundation so that you can request
specifically the information you require

626
00:55:32,470 --> 00:55:39,240
and the framework can tailor the work that it does on
your behalf specifically to those things that you need,

627
00:55:39,240 --> 00:55:44,560
notify you of when the information is available
and allow you to proceed on with your tasks all

628
00:55:44,560 --> 00:55:47,640
without reducing responsiveness to the end user.

629
00:55:47,640 --> 00:55:53,480
Remember, even simple questions can take time to answer.

630
00:55:53,480 --> 00:56:00,400
We're using other very typical paradigms available in
objective C 2.0, our classes have declared properties,

631
00:56:00,400 --> 00:56:05,260
you call their getters after checking whether the
information is available in order to obtain values

632
00:56:05,260 --> 00:56:15,220
of interest to you, you use key value observing to note
changes that occur in state, you use blocks as call backs.

633
00:56:15,220 --> 00:56:18,140
Lots of information about blocks
available at the conference.

634
00:56:18,140 --> 00:56:23,320
I'll give you a couple of other sessions that you
can go to to learn more about block-base programming.

635
00:56:23,320 --> 00:56:28,960
In the context of AVFoundation, a block is simply
a piece of code that you wish to have evoked

636
00:56:28,960 --> 00:56:35,660
when a certain operation is complete or a certain state
is reached and the block is evoked at some time later

637
00:56:35,660 --> 00:56:41,250
when that occurs or perhaps in line if that state has
already been reached, and then that block is the code

638
00:56:41,250 --> 00:56:46,830
that you would supply that tells you what to do as
a result of the operation that you have completed.

639
00:56:46,830 --> 00:56:50,110
So, where else can you go for more information?

640
00:56:50,110 --> 00:56:54,710
Eryk Vershen is your Evangelist, your Media
Technologies Evangelist you can contact.

641
00:56:54,710 --> 00:56:59,620
You heard him talk about HTTP live streaming and
know that he's well informed on these topics.

642
00:56:59,620 --> 00:57:06,430
Documentation for the AVFoundation Framework is available
together with the other iPhone documentation for iOS 4.

643
00:57:06,430 --> 00:57:12,620
And, as usual, the Apple Developer Forums are
excellent sources of information and contacts.

644
00:57:12,620 --> 00:57:20,790
Other sessions for you to attend not just the sessions
that follow this one about editing and use of the camera.

645
00:57:20,790 --> 00:57:22,880
Also I mentioned a core audio session.

646
00:57:22,880 --> 00:57:28,420
Let's see tomorrow in Mission 10:15AM,
Fundamentals of Digital Audio.

647
00:57:28,420 --> 00:57:33,320
There are other audio-related sessions that you may wish to
attend to learn more about audio processing on the platform.

648
00:57:33,320 --> 00:57:40,470
There will be a repeat of this session hopefully
delivered with expanded lung power on Thursday.

649
00:57:40,470 --> 00:57:45,410
If you have a colleague who wished to attend this
session but needed to go to one of the others,

650
00:57:45,410 --> 00:57:49,600
you can tell him exactly what will be said there and,
in fact, you can let him in on all the jokes as well.

651
00:57:49,600 --> 00:57:51,650
I'll come up with new ones by then.

652
00:57:51,650 --> 00:57:57,410
A couple of sessions about block-based programming
are available to you as well to learn more

653
00:57:57,410 --> 00:58:01,030
about structuring your application around the use of blocks.

654
00:58:01,030 --> 00:58:02,390
Very powerful technology there.

655
00:58:02,390 --> 00:58:05,090
I recommend that you become familiar with it.

656
00:58:05,090 --> 00:58:11,140
So, to summarize, Time Media takes time.

657
00:58:11,140 --> 00:58:17,350
Initialization of an object does not guarantee
suitability or fitness for any particular purpose.

658
00:58:17,350 --> 00:58:25,000
Observe the status of those things that you're interested in
and as the status becomes ready, then you can move forward

659
00:58:25,000 --> 00:58:28,310
with the operation that you wish to undertake.

660
00:58:28,310 --> 00:58:31,950
So by following these best practices,
your apps can stay current

661
00:58:31,950 --> 00:58:35,930
and have the full media processing
power of the platform available to them.

662
00:58:35,930 --> 00:58:40,830
All of the things you've seen in the demos
in the last day and some including an iMovie

663
00:58:40,830 --> 00:58:46,810
and other applications you've seen demoed, you can
do in your applications as well with these APIs.

664
00:58:46,810 --> 00:58:51,490
Make sure your apps stay responsive
by using the asynchronous facilities.

665
00:58:51,490 --> 00:58:54,290
Stay in time by tracking time as it progresses

666
00:58:54,290 --> 00:59:00,210
and above all make sure your app stays
alive, don't let the watch dog get you.

667
00:59:00,210 --> 00:59:05,420
And with that please stay tuned for the remainder of
the AV's Foundation sessions later this afternoon.

668
00:59:05,420 --> 00:59:06,390
Thank you very much.

669
00:59:06,390 --> 00:59:08,390
[ Applause ]

