1
00:00:06,370 --> 00:00:09,290
>> Good morning.

2
00:00:09,290 --> 00:00:18,190
My name is William Stewart and I manage the Core Audio
group and we are doing three sessions this morning

3
00:00:18,190 --> 00:00:21,640
in this room on audio, primarily on iPhone.

4
00:00:21,640 --> 00:00:31,880
We're also covering some general audio topics that
are relevant for the desktop as well as iPhone.

5
00:00:31,880 --> 00:00:40,960
The set of frameworks and services that we provide are
quite extensive and cover a number of different areas

6
00:00:40,960 --> 00:00:43,850
and we'll be going through some of these in the talk.

7
00:00:43,850 --> 00:00:56,570
The Media Player API is just a general kind of remote access
for the iPod application and the media library and the iPod.

8
00:00:56,570 --> 00:01:05,580
And the iPod itself uses the same sets of APIs and
frameworks that we're discussing with you today.

9
00:01:05,580 --> 00:01:11,120
So it's all, you know, what Apple uses itself
to implement its features is, of course,

10
00:01:11,120 --> 00:01:14,200
the same things that you get to use as developers.

11
00:01:14,200 --> 00:01:19,910
OpenAL is an industry standard, I
guess you'd call it, for doing games

12
00:01:19,910 --> 00:01:24,300
and we support that on the platform for game audio.

13
00:01:24,300 --> 00:01:34,800
AV Foundation made its debut last year with some
very simple AV audio player and recorder objects

14
00:01:34,800 --> 00:01:39,560
and this year it's become quite an extensive
framework with a collection of video.

15
00:01:39,560 --> 00:01:48,740
And there's a whole series of sessions on the new classes in
AV Foundation but there is some specific audio functionality

16
00:01:48,740 --> 00:01:53,430
in this framework as well and Allan
will be going through that in a moment.

17
00:01:53,430 --> 00:01:57,920
And all of these technologies are really built
on a collection of services that are rendered

18
00:01:57,920 --> 00:02:08,890
through Audio Toolbox and that's really the primary
services that my team delivers to the platform.

19
00:02:08,890 --> 00:02:15,610
That includes services for reading and writing
audio files, for converting data formats,

20
00:02:15,610 --> 00:02:23,300
for using audio units that provide
processing, mixing, all kinds of things --

21
00:02:23,300 --> 00:02:28,420
basically the collection of tools
that you need in order to do audio.

22
00:02:28,420 --> 00:02:30,340
So that's a very general introduction.

23
00:02:30,340 --> 00:02:34,830
There will be some more data overviews
through the sessions this morning.

24
00:02:34,830 --> 00:02:38,760
The sessions that we have is this one and one of the things

25
00:02:38,760 --> 00:02:44,350
that will be covered here is how your
application integrates with the rest of iPhone OS.

26
00:02:44,350 --> 00:02:53,400
So it's primarily audio session and managing the
resources of the platform and how you can best use them.

27
00:02:53,400 --> 00:02:58,580
What we thought we'd do in the second
session is take a step back from talking --

28
00:02:58,580 --> 00:03:04,280
rather than sort of focusing specifically on APIs, we
thought we'd rather take it from a different angle.

29
00:03:04,280 --> 00:03:08,470
And so what we're doing in that session is
looking at: what are the fundamentals of audio?

30
00:03:08,470 --> 00:03:15,690
If you're talking about digital audio, not just on
our platform but on any platform, what does that mean?

31
00:03:15,690 --> 00:03:17,780
What is Linear PCM?

32
00:03:17,780 --> 00:03:19,220
What is AAC?

33
00:03:19,220 --> 00:03:20,510
How are these things different?

34
00:03:20,510 --> 00:03:22,150
How do we express them in our APIs?

35
00:03:22,150 --> 00:03:27,030
But really more like: what is the
fundamental features of these things?

36
00:03:27,030 --> 00:03:37,660
And our APIs are very fundamentally shaped by what audio
looks like and as a data format in some of the constraints

37
00:03:37,660 --> 00:03:45,130
in terms of time and resolution and everything that
we deal with by dealing with this media format.

38
00:03:45,130 --> 00:03:49,350
And then the last session, Audio Development for iPhone OS,

39
00:03:49,350 --> 00:03:53,170
is really taking a more detailed
look at how to use audio units.

40
00:03:53,170 --> 00:03:57,140
So what do audio units look like for your application?

41
00:03:57,140 --> 00:04:00,590
How do you interact with them?

42
00:04:00,590 --> 00:04:06,740
And we're also taking a little bit of a forward-looking
stance at that and looking at some of the general ways

43
00:04:06,740 --> 00:04:12,920
that you can deal with more complicated
processing demands with IU graphs and so forth.

44
00:04:12,920 --> 00:04:15,470
So that's enough of me talking.

45
00:04:15,470 --> 00:04:20,900
I'll get Allan to come up and he'll
begin his discussion on AV Foundation.

46
00:04:20,900 --> 00:04:23,980
Thank you.

47
00:04:23,980 --> 00:04:25,180
[ Applause ]

48
00:04:25,180 --> 00:04:25,580
>> Allan Schaffer: Great.

49
00:04:25,580 --> 00:04:28,520
So thank you, Bill and good morning everyone.

50
00:04:28,520 --> 00:04:34,040
The AV Foundation has a number of high-level classes
that you can use for audio playback and recording.

51
00:04:34,040 --> 00:04:37,010
And this is where we're going to spend
most of the time in this session.

52
00:04:37,010 --> 00:04:43,320
So I'm going to be talking about the audio player, which
lets you play back audio from a file or from data in memory.

53
00:04:43,320 --> 00:04:50,270
I'll talk about the recorder, which lets you record audio,
capture from the microphone and record that to a file.

54
00:04:50,270 --> 00:04:52,300
And then I'll talk about the audio session, which,

55
00:04:52,300 --> 00:04:58,210
as Bill said is going to let you manage the
audio behavior of your application on the device.

56
00:04:58,210 --> 00:05:00,650
There's a fourth class that actually
I'm not going to be covering

57
00:05:00,650 --> 00:05:03,070
in this session but it's worth taking a look at as well.

58
00:05:03,070 --> 00:05:09,950
We covered it yesterday in the AV Foundation
sessions for video and that is the new stream player.

59
00:05:09,950 --> 00:05:15,880
Now, a lot of the new functionality in AV
Foundation has been geared towards a lot

60
00:05:15,880 --> 00:05:20,250
of very expressive video functionality.

61
00:05:20,250 --> 00:05:25,000
And so that class is part of all of
that but it can also be used for audio,

62
00:05:25,000 --> 00:05:30,200
to either play audio from a local
file or to stream it over a network.

63
00:05:30,200 --> 00:05:32,350
So I'm just going to jump straight in.

64
00:05:32,350 --> 00:05:34,090
Let's talk about the AVAudioPlayer.

65
00:05:34,090 --> 00:05:38,690
And this is really a very simple
class for you to use to play a sound.

66
00:05:38,690 --> 00:05:44,290
It supports a variety of different file formats and the
ones that are supported by the audio file service is API.

67
00:05:44,290 --> 00:05:49,610
So that's things like caf files, m4as, mp3s, and so on.

68
00:05:49,610 --> 00:05:54,060
And the class provides a number of
just basic playback operations --

69
00:05:54,060 --> 00:05:58,960
to play a sound, stop, pause, move
the playhead around, and so on.

70
00:05:58,960 --> 00:06:03,730
And with this object, if you want to play multiple
sounds simultaneously, you can do that as well --

71
00:06:03,730 --> 00:06:10,400
what you do is just create multiple instances of the
object and have each one controlling the different sounds.

72
00:06:10,400 --> 00:06:14,930
There's also a number of properties that I'll go
through in just a moment but things like volume control,

73
00:06:14,930 --> 00:06:22,680
you can enable metering, you can have the sound be looping
as it's played back, and the few new features in iOS 4 --

74
00:06:22,680 --> 00:06:26,560
the object now supports stereo panning from left to right.

75
00:06:26,560 --> 00:06:32,010
It also synchronizes playback if you have
multiple instances playing simultaneously.

76
00:06:32,010 --> 00:06:35,030
So right away, I'll just jump into some API here.

77
00:06:35,030 --> 00:06:43,370
To get started with this class to instantiate an object,
you just -- we'll call init with contents of URL.

78
00:06:43,370 --> 00:06:52,400
The URL needs to be a local file that's in the sandbox
for your application or you can create it from an NSData.

79
00:06:52,400 --> 00:06:58,970
And one quick side note before I go on: All of the code
snippets that are on the slides in this talk are available

80
00:06:58,970 --> 00:07:02,490
on the attendee website so you don't
need to worry about writing them down.

81
00:07:02,490 --> 00:07:08,160
You can just go and download them right after
the talk or go ahead and do it right now.

82
00:07:08,160 --> 00:07:14,090
Now, there's a number of properties on the
AVAudioPlayer that let you control playback.

83
00:07:14,090 --> 00:07:19,300
And you can either set these up before you begin playing or
with a number of them, actually, you can just change them

84
00:07:19,300 --> 00:07:23,710
as the player is playing a sound,
so you can change the volume --

85
00:07:23,710 --> 00:07:27,060
here I'm setting it to 100% of the current output volume.

86
00:07:27,060 --> 00:07:31,180
You can change the panning -- here I
have it set all the way to the left.

87
00:07:31,180 --> 00:07:34,930
If I had it to 1.0, then it would
be all the way to the right.

88
00:07:34,930 --> 00:07:38,860
The number of loops here is something also you can control.

89
00:07:38,860 --> 00:07:45,930
So 0 means no loops, -1 means loop indefinitely,
or you can have a specific number of times

90
00:07:45,930 --> 00:07:51,520
that the audio will loop back after
it's played through once.

91
00:07:51,520 --> 00:07:56,670
You can have direct control over the playhead
as well with the current time property.

92
00:07:56,670 --> 00:08:01,830
And so if you want to implement something where
you are scrubbing around in an audio file,

93
00:08:01,830 --> 00:08:04,530
you would just be changing the value of this property.

94
00:08:04,530 --> 00:08:08,500
Or if you want to reset the playhead
to the beginning, you set it to zero.

95
00:08:08,500 --> 00:08:11,780
And there's a delegate as well
that we use for notifications.

96
00:08:11,780 --> 00:08:18,130
And then other properties, some that I mentioned --
so to be able to enable metering of the playback,

97
00:08:18,130 --> 00:08:23,090
you can find out of the duration of the audio
file that you're playing, the number of channels,

98
00:08:23,090 --> 00:08:27,080
and the state of the player as it's playing your sounds.

99
00:08:27,080 --> 00:08:34,130
Now, the playback controls here are really
simple, there's just these four controls

100
00:08:34,130 --> 00:08:37,040
that you'll be using all the time with this object.

101
00:08:37,040 --> 00:08:40,590
PrepareToPlay is actually probably
one of the more important ones.

102
00:08:40,590 --> 00:08:46,260
This is going to get the player ready to play
a sound for you with absolutely minimal lag.

103
00:08:46,260 --> 00:08:50,810
So what this will do is allocate
the buffers that the player is going

104
00:08:50,810 --> 00:08:54,290
to use internally and prime those buffers with your data.

105
00:08:54,290 --> 00:08:59,920
And that way, when you go to invoke the play
method, it can happen nearly instantaneously.

106
00:08:59,920 --> 00:09:03,870
So the play method just starts playing your sound now.

107
00:09:03,870 --> 00:09:12,830
And so -- if you later on you pause or stop the sound, the
play method will resume from the point that it leaves off.

108
00:09:12,830 --> 00:09:18,620
And that's maybe an important note to make is that if
you had expected, after you stopped playing a sound,

109
00:09:18,620 --> 00:09:22,490
for the playhead to go back to the
beginning, that actually isn't the behavior.

110
00:09:22,490 --> 00:09:25,860
The playhead stays where it was, just like a tape deck.

111
00:09:25,860 --> 00:09:31,060
And so if you want to go back to the beginning,
you would reset the current time to zero.

112
00:09:31,060 --> 00:09:37,980
Pause is going to pause the playback but with
Pause, the player stays ready to resume again;

113
00:09:37,980 --> 00:09:44,010
the cues and the buffers are still
going to be allocated and ready to go.

114
00:09:44,010 --> 00:09:47,530
And that's the difference between Pause and Stop.

115
00:09:47,530 --> 00:09:52,580
With Stop, the cues are disposed
of and the buffer is disposed of.

116
00:09:52,580 --> 00:09:58,900
So if you want to restart playing after you have
stopped, after you've invoked the stop method,

117
00:09:58,900 --> 00:10:05,280
you would probably call prepareToPlay
and then later on call play.

118
00:10:05,280 --> 00:10:09,100
Another element of the player class
are some delegate methods.

119
00:10:09,100 --> 00:10:12,750
So these will be invoked when certain events happen.

120
00:10:12,750 --> 00:10:18,580
And probably the one that's very important for
you to implement is when the player is finished

121
00:10:18,580 --> 00:10:22,770
and that's called audioPlayerDidFinishPlayingsuccessfully.

122
00:10:22,770 --> 00:10:28,840
And in that method you might clean up, you might change the
state of your interface, and take care of other things just

123
00:10:28,840 --> 00:10:32,880
to indicate to the user, "Okay,
the sound is no longer playing."

124
00:10:32,880 --> 00:10:35,860
Then there's a number of other
delegates that you can implement.

125
00:10:35,860 --> 00:10:42,780
If there's a decoding error in the file that you
played back or if interruptions began or ended --

126
00:10:42,780 --> 00:10:45,810
interruptions are things like a
phone call came in for example,

127
00:10:45,810 --> 00:10:52,080
and I'll talk about interruptions a
lot more towards the end of the talk.

128
00:10:52,080 --> 00:10:55,520
So let's just put this together and look at playing a sound.

129
00:10:55,520 --> 00:11:02,930
So here in this method, I'm being passed in the
URL of a local file; I create my player object;

130
00:11:02,930 --> 00:11:11,860
I next set the passing appointer to that URL; I set up
a delegate for any of the notifications that I may want

131
00:11:11,860 --> 00:11:19,230
to have fired later; prepare the
player for playback and hit Play.

132
00:11:19,230 --> 00:11:23,740
So all of this is really -- this is just to show that
this is a very simple class but a great way for you

133
00:11:23,740 --> 00:11:27,320
to get started with audio playback in your app.

134
00:11:27,320 --> 00:11:28,800
So that's the player.

135
00:11:28,800 --> 00:11:32,690
Let me now go just about as quickly through the recorder.

136
00:11:32,690 --> 00:11:40,260
Another simple class -- this lets you record audio to a
file and its behavior actually is that it will either record

137
00:11:40,260 --> 00:11:46,710
until you stop it by calling it stop method or you
can set it up to record for a specific duration.

138
00:11:46,710 --> 00:11:50,020
And it supports a variety of different encoding formats,

139
00:11:50,020 --> 00:11:55,110
I've listed a bunch here -- AAC,
ALAC, Linear PCM, and so on.

140
00:11:55,110 --> 00:12:01,270
AAC is interesting though because we have hardware
support for the AAC encoder on certain platforms --

141
00:12:01,270 --> 00:12:07,040
the second generation and third generation iPod
touch, we have hardware support for it on the iPad,

142
00:12:07,040 --> 00:12:09,690
the iPhone 3GS, and of course on the iPhone 4.

143
00:12:09,690 --> 00:12:16,940
Now with the AVAudioRecorder, the API here is really
just the mirror image of what you saw with the player,

144
00:12:16,940 --> 00:12:19,880
so I won't go through it in quite as much detail.

145
00:12:19,880 --> 00:12:24,160
You initialize the recorder with a URL to a local file.

146
00:12:24,160 --> 00:12:28,250
One difference, though, is this next
parameter: the settings dictionary.

147
00:12:28,250 --> 00:12:32,220
I'll cover that on the next slide
so I'll come right back to that.

148
00:12:32,220 --> 00:12:37,200
There's a number of recording controls that
you can manage, so prepareToRecord, Record,

149
00:12:37,200 --> 00:12:44,300
or Record for a particular duration, then Pause and Stop,
and sort of your predictable properties that you can get

150
00:12:44,300 --> 00:12:47,500
about the state of the object and
the state of the recording.

151
00:12:47,500 --> 00:12:51,070
Now about that the settings dictionary.

152
00:12:51,070 --> 00:12:58,700
So when you're recording, you need to specify exactly what
format you want to record into, what sample rate to use,

153
00:12:58,700 --> 00:13:03,670
the number of channels, and then perhaps
format specific settings as well,

154
00:13:03,670 --> 00:13:08,170
like for Linear PCM you'd specify
the bit depth, the endian-ness.

155
00:13:08,170 --> 00:13:13,090
For certain encoded formats, you might
specify the quality or the bit rate and so on.

156
00:13:13,090 --> 00:13:15,120
So let's take a look at that.

157
00:13:15,120 --> 00:13:18,320
Now this looks like a lot of code but
really, it's actually very simple.

158
00:13:18,320 --> 00:13:25,670
All I'm doing here is setting up a dictionary containing
key value pairs for each of those encoding settings.

159
00:13:25,670 --> 00:13:35,390
So on the left here, I'm setting my format to AAC,
the rate to 44100, the number of channels to two,

160
00:13:35,390 --> 00:13:40,130
the bit rate to 128K, and the audio quality to the maximum.

161
00:13:40,130 --> 00:13:48,630
So all of that is just being packed into an array and I
give that array now and create a dictionary from that.

162
00:13:48,630 --> 00:13:53,940
And then that dictionary is what I pass
when I initialize the audio recorder.

163
00:13:53,940 --> 00:13:55,590
And so now it's all ready to go.

164
00:13:55,590 --> 00:14:06,010
I can call its methods to prepareToRecord and start
recording or record for a particular amount of time.

165
00:14:06,010 --> 00:14:10,510
Okay, so I know that was pretty quick through
those objects but they're really just very simple.

166
00:14:10,510 --> 00:14:17,580
The thing is, though, that they're very feature-rich so
they do suit a lot of the basic needs of audio developers

167
00:14:17,580 --> 00:14:21,010
who just want to play and record some sounds.

168
00:14:21,010 --> 00:14:24,490
If you saw the Quest demo on Monday
or yesterday, for example,

169
00:14:24,490 --> 00:14:30,360
we're using this API to play all
of the game background soundtrack.

170
00:14:30,360 --> 00:14:34,400
And so, you know, it's just a very,
very capable API for doing that.

171
00:14:34,400 --> 00:14:40,390
And really, it's recommended as the
starting point for you in most cases.

172
00:14:40,390 --> 00:14:49,250
You know, it's the good starting point unless your
requirements go into more complicated uses for audio.

173
00:14:49,250 --> 00:14:56,350
So if you need access to audio samples, for example for
processing, then you might use the audio unit's API,

174
00:14:56,350 --> 00:15:00,040
which Murray will be covering in the third session.

175
00:15:00,040 --> 00:15:06,410
If you need to do spatial 3D positioning of audio
sources in a game, then OpenAL is perfect for that.

176
00:15:06,410 --> 00:15:09,720
And so you would probably choose that API in that instance.

177
00:15:09,720 --> 00:15:15,180
And if you need to do network streaming, then
you might use the new AV Player or you might go

178
00:15:15,180 --> 00:15:18,330
into the audio file streaming services API.

179
00:15:18,330 --> 00:15:24,480
Okay, where I want to go next, though,
is into audio session management.

180
00:15:24,480 --> 00:15:30,340
And this is really an important topic for developers
to all get absolutely right in their applications

181
00:15:30,340 --> 00:15:32,700
and that's why we put so much focus on it.

182
00:15:32,700 --> 00:15:37,550
And really, I'm going to dedicate
the rest of this talk to this topic.

183
00:15:37,550 --> 00:15:44,900
So the idea here is that this is how you can manage
the behavior of the sounds in your application

184
00:15:44,900 --> 00:15:51,740
and make them behave according to both the expectations of
the user for the kind of application that you're writing

185
00:15:51,740 --> 00:15:58,600
and to be consistent with either built-in applications
or other applications of the same time of app as yours.

186
00:15:58,600 --> 00:16:06,420
What you're going to do with this API is to categorize
your application into one of six possible categories

187
00:16:06,420 --> 00:16:13,010
and then your app's audio is going to follow the
behaviors that are defined for that category.

188
00:16:13,010 --> 00:16:19,810
Then this is also the API that will let you
manage some of the shared resources on the device

189
00:16:19,810 --> 00:16:22,610
and do things like mix with background audio.

190
00:16:22,610 --> 00:16:28,400
It's how you'll interact with interruptions if
they occur, say again, if a phone call comes in

191
00:16:28,400 --> 00:16:36,110
and it's how you can handle changes in the routing if
the user were to, say, plug in or unplug a headset.

192
00:16:36,110 --> 00:16:41,760
Now there's actually two APIs here
that are relevant to what we're going

193
00:16:41,760 --> 00:16:45,580
to be talking about: a high-level API and a low-level API.

194
00:16:45,580 --> 00:16:49,090
So the high-level API is the AV audio session class.

195
00:16:49,090 --> 00:16:55,150
It's an Objective-C class, part
of the AV Foundation framework.

196
00:16:55,150 --> 00:16:59,400
And really, it wraps up all of the
most commonly used functionality

197
00:16:59,400 --> 00:17:01,990
that you need to manage with the audio session.

198
00:17:01,990 --> 00:17:09,600
Then there's also a lower-level API called Audio
Session Services and that's part of the Audio Toolbox.

199
00:17:09,600 --> 00:17:13,200
And that's really all of the implementation
that we expose to you.

200
00:17:13,200 --> 00:17:17,390
C-based, a lower level and has a bit more functionality.

201
00:17:17,390 --> 00:17:21,360
But what's interesting to note is that it
is possible and quite okay for you to mix

202
00:17:21,360 --> 00:17:24,630
and match between the high-level and low-level APIs.

203
00:17:24,630 --> 00:17:30,830
In fact, what's quite typical is you might set up your audio
session using the high-level API and then maybe just drop

204
00:17:30,830 --> 00:17:37,420
into the low-level API to set some overrides or
other things that aren't exposed at the high level.

205
00:17:37,420 --> 00:17:42,410
So there's five basic tasks that we're going to go
through for the remainder of the session here to talk

206
00:17:42,410 --> 00:17:49,460
about with AVAudioSession: We're going to set up the
session and configure its delegate; we will choose --

207
00:17:49,460 --> 00:17:54,780
very carefully choose and set -- and audio session
category; we'll go active and we'll talk about that;

208
00:17:54,780 --> 00:17:59,350
and the things that are new there in relation to iOS 4;

209
00:17:59,350 --> 00:18:04,190
then I'll talk about how we handle
interruptions and handle route changes.

210
00:18:04,190 --> 00:18:07,860
So first setting up the session -- very easy.

211
00:18:07,860 --> 00:18:12,660
The audio session instance is just a
singleton object for your application.

212
00:18:12,660 --> 00:18:15,440
So you just retrieve a handle to that.

213
00:18:15,440 --> 00:18:20,070
You'll set up a delegate for any notifications
that might occur, like an interruption.

214
00:18:20,070 --> 00:18:24,790
And this is the place where you might
request certain preferred hardware settings.

215
00:18:24,790 --> 00:18:31,450
Just for example, in this case I'm
requesting a sample rate of 44100.

216
00:18:31,450 --> 00:18:37,610
But now the second part is really -- this is
the most important part of using this API.

217
00:18:37,610 --> 00:18:43,400
And it's not a line of code, it's a choice
that you have to make for your application.

218
00:18:43,400 --> 00:18:48,720
You will choose instead a category based
on the role that audio plays in your app

219
00:18:48,720 --> 00:18:51,820
and the kind of application that you're writing.

220
00:18:51,820 --> 00:18:57,510
And there's six possible categories: playback;
record; play and record; audio processing;

221
00:18:57,510 --> 00:19:00,110
and then two more -- ambient and solo ambient.

222
00:19:00,110 --> 00:19:02,450
So let me show you how these differ.

223
00:19:02,450 --> 00:19:07,090
So I have the categories listed here on the left.

224
00:19:07,090 --> 00:19:11,950
Their intended usage will be a column that we
build out as I talk about this and then a number

225
00:19:11,950 --> 00:19:14,830
of the behaviors are listed in the table as well.

226
00:19:14,830 --> 00:19:18,890
And it's things like whether each
category obeys the ringer switch, meaning,

227
00:19:18,890 --> 00:19:24,050
is your audio silenced if your user
flips the ringer switch to silent?

228
00:19:24,050 --> 00:19:29,240
Is your audio silenced if the user locks their screen?

229
00:19:29,240 --> 00:19:37,150
Does this category allow your sounds
to be mixed with others?

230
00:19:37,150 --> 00:19:41,190
Does it use input or does it use output?

231
00:19:41,190 --> 00:19:48,090
And is it allowed to play in the background if your
application is transitioned into the background?

232
00:19:48,090 --> 00:19:49,320
So let's have a look.

233
00:19:49,320 --> 00:19:51,120
Okay, first with playback.

234
00:19:51,120 --> 00:19:59,900
This is probably the most straightforward of all of the
categories because it's intended for such an exact purpose.

235
00:19:59,900 --> 00:20:07,340
This is the category that you should choose if your
application is an audio player or a video player.

236
00:20:07,340 --> 00:20:15,410
So if that's the primary purpose of your application is to
output media, then you would choose the playback category.

237
00:20:15,410 --> 00:20:18,750
And you can see that the behaviors here, in a way,

238
00:20:18,750 --> 00:20:24,770
are very similar to what you see with
the iPod application on the device.

239
00:20:24,770 --> 00:20:30,930
This does not -- applications that are using this category
are not affected by the state of the ringer switch;

240
00:20:30,930 --> 00:20:35,300
they can continue to play if the user locks the screen.

241
00:20:35,300 --> 00:20:41,130
Now MixWithOthers is not enabled
by default by saying optional here,

242
00:20:41,130 --> 00:20:45,730
it is possible here for you to optionally turn it on.

243
00:20:45,730 --> 00:20:47,620
This is an output category.

244
00:20:47,620 --> 00:20:49,300
And the last one.

245
00:20:49,300 --> 00:20:56,230
With this category, it will allow your application to
continue to play audio through a background transition

246
00:20:56,230 --> 00:21:03,520
if you have set up the audio key for the
UI background modes in your info plist.

247
00:21:03,520 --> 00:21:06,010
Now Record is very similar.

248
00:21:06,010 --> 00:21:10,020
But of course, its intended usage is for audio recorders,

249
00:21:10,020 --> 00:21:13,610
applications that are doing voice
capture, that sort of thing.

250
00:21:13,610 --> 00:21:18,110
The behaviors, though, are right
across the board mostly the same.

251
00:21:18,110 --> 00:21:22,860
Obviously this is an input category
that uses input instead of output.

252
00:21:22,860 --> 00:21:29,330
But it will also survive through a transition
in the background if you have set that key.

253
00:21:29,330 --> 00:21:30,350
Play and record.

254
00:21:30,350 --> 00:21:33,690
Well, this one is essentially combining those first two.

255
00:21:33,690 --> 00:21:40,340
So this is intended for applications that are
doing voice over IP or voice chat types of apps.

256
00:21:40,340 --> 00:21:44,760
You can optionally mix with others,
just like with the playback category.

257
00:21:44,760 --> 00:21:48,270
But with this one, this is using
input and output simultaneously

258
00:21:48,270 --> 00:21:53,270
or enabling you to do that, so both of those are shown.

259
00:21:53,270 --> 00:21:58,000
And if you choose this category also, your
application can go into the background.

260
00:21:58,000 --> 00:22:02,000
One important difference with this category,
though, compared to what we've seen, say,

261
00:22:02,000 --> 00:22:05,730
with the playback category is the default audio route.

262
00:22:05,730 --> 00:22:10,830
So with the playback category, by default
your output will go through the speaker.

263
00:22:10,830 --> 00:22:15,260
With the play and record category, your
output will go on a phone to the receiver,

264
00:22:15,260 --> 00:22:19,620
which is the speaker you hold up to
your ear when you're on the phone.

265
00:22:19,620 --> 00:22:22,780
Then the audio processing category.

266
00:22:22,780 --> 00:22:31,070
All right, so this one is used for offline conversion
of audio file formats, offline processing of audio data,

267
00:22:31,070 --> 00:22:36,080
and you can see that actually many
of the behaviors are similar

268
00:22:36,080 --> 00:22:39,830
but it's not using either input or output for any sounds.

269
00:22:39,830 --> 00:22:42,850
All it's doing is doing this processing in memory.

270
00:22:42,850 --> 00:22:48,920
Now, one thing about -- a special note about this,
about how I say that it's allowed in the background.

271
00:22:48,920 --> 00:22:56,280
So yes, if you set up your application to use this
category, the processing can continue in the background.

272
00:22:56,280 --> 00:23:03,540
But unlike the previous three, just setting this category
alone does not enable your application to transition

273
00:23:03,540 --> 00:23:09,280
into the background and keep running; you would have to
use one of the other ways of going into the background.

274
00:23:09,280 --> 00:23:13,630
So for example, maybe you would just be
asking for extra time to do processing

275
00:23:13,630 --> 00:23:16,720
and that's how you would transition into the background.

276
00:23:16,720 --> 00:23:23,370
Now, these next two are very similar to each other,
so I'm just going to put them up simultaneously

277
00:23:23,370 --> 00:23:27,390
so you can see the difference: ambient and solo ambient.

278
00:23:27,390 --> 00:23:33,840
But the purpose of these two categories is very
different from the previous four that we talked about.

279
00:23:33,840 --> 00:23:40,950
The top four are really intended for applications
whose main purpose is very audio-centric, right?

280
00:23:40,950 --> 00:23:41,860
A playback app.

281
00:23:41,860 --> 00:23:49,700
You know, an audio player, a voice recorder, a VoIP app or
something doing audio conversion, so very audio-centric kind

282
00:23:49,700 --> 00:23:53,950
of purpose of the way that application uses audio.

283
00:23:53,950 --> 00:23:58,260
These other two are really intended
for much broader purpose kinds of apps.

284
00:23:58,260 --> 00:24:04,590
So games and productivity apps or utility apps
would probably choose these latter two categories

285
00:24:04,590 --> 00:24:11,200
and the reason is because of the behaviors that
they enforce are consistent with the behaviors

286
00:24:11,200 --> 00:24:14,910
that users expect for that kind of application.

287
00:24:14,910 --> 00:24:19,640
So what you can see is, well, these
both obey the ringer switch.

288
00:24:19,640 --> 00:24:23,760
That means the user is playing your
game or using your to-do list app

289
00:24:23,760 --> 00:24:27,740
or whatever your app happens to
be that's using in this category.

290
00:24:27,740 --> 00:24:30,460
If they're doing that and they hit the ringer switch, well,

291
00:24:30,460 --> 00:24:33,830
the audio will be silenced, which
is exactly what they expect.

292
00:24:33,830 --> 00:24:41,900
And again, it's because the purpose -- the usage of audio
-- is not critical to the purpose of the application.

293
00:24:41,900 --> 00:24:46,340
It's perfectly expected that you can play a game
with the sound turned off, or many games at least.

294
00:24:46,340 --> 00:24:53,830
It's expected that you can use your
productivity apps like Mail or Safari and so on

295
00:24:53,830 --> 00:24:56,460
and have the sound turned off in those as well.

296
00:24:56,460 --> 00:25:04,250
They will both obey the ScreenLock as well, that means
that audio playback will stop if the user locks his screen.

297
00:25:04,250 --> 00:25:06,200
I'm going to jump over.

298
00:25:06,200 --> 00:25:12,470
They both are output categories and neither
of these enable your application to transition

299
00:25:12,470 --> 00:25:15,570
and continue to play audio in the background.

300
00:25:15,570 --> 00:25:19,330
But the difference between them
actually is this MixWithOthers parameter.

301
00:25:19,330 --> 00:25:26,230
And so I'm going to be talking about that in a
little more detail, really, just coming up next.

302
00:25:26,230 --> 00:25:33,340
But it has to do with whether your application
needs access to a hardware codec or not.

303
00:25:33,340 --> 00:25:40,120
With MixWithOthers with ambient that means you would
use that for applications that don't require access

304
00:25:40,120 --> 00:25:43,070
to a hardware codec, don't need to use it.

305
00:25:43,070 --> 00:25:43,440
Excuse me.

306
00:25:43,440 --> 00:25:48,090
With solo ambient, that's the category
you would choose if your, you know,

307
00:25:48,090 --> 00:25:53,160
game or productivity app does require
access to a hardware codec for decoding.

308
00:25:53,160 --> 00:25:56,540
Okay, so here is where we are setting category.

309
00:25:56,540 --> 00:26:01,830
So we've gone through the table now, we've made
our choice for the specific kind of application

310
00:26:01,830 --> 00:26:04,600
or applications that you guys are writing.

311
00:26:04,600 --> 00:26:08,390
And in this case, we're choosing the ambient category.

312
00:26:08,390 --> 00:26:13,020
We go on to the next part here
and set our session to be active.

313
00:26:13,020 --> 00:26:18,610
So to do that, all we do is call set
active, tell it yes, we are now active.

314
00:26:18,610 --> 00:26:24,430
And once we're active, now we can play sounds or
record sounds if we have chosen the record category

315
00:26:24,430 --> 00:26:28,660
and go on to set up our audio APIs,
handle interruptions, and so on.

316
00:26:28,660 --> 00:26:37,120
Like we have now asserted that we want to make
use of the audio functionality on the device.

317
00:26:37,120 --> 00:26:41,770
But let me go back now and talk about going active.

318
00:26:41,770 --> 00:26:48,260
So it would be typical for most applications
to just go active when they start up

319
00:26:48,260 --> 00:26:51,760
and stay that way for the remainder of the app.

320
00:26:51,760 --> 00:26:58,230
But there's a few classes of applications that should not
do that and it's because of the interaction that they have

321
00:26:58,230 --> 00:27:02,540
with other audio that might be playing in the background.

322
00:27:02,540 --> 00:27:08,520
So with a voice recorder application, for
example, or a VoIP app or a Turn-by-Turn app --

323
00:27:08,520 --> 00:27:15,940
well, all of those should have different behaviors related
to music that might be playing on the iPod when they start

324
00:27:15,940 --> 00:27:22,520
up or there may be an email notification
sound and so on that might happen.

325
00:27:22,520 --> 00:27:28,960
So with those kinds of applications, you want to
be a little more clever about when you go active.

326
00:27:28,960 --> 00:27:32,400
And the story is that you should
only go active in those kinds

327
00:27:32,400 --> 00:27:36,540
of applications while you're actually doing those things.

328
00:27:36,540 --> 00:27:41,420
So in a recorder app, you would only
go active once you actually start

329
00:27:41,420 --> 00:27:43,970
to record, not just when the application starts up.

330
00:27:43,970 --> 00:27:48,400
And you would go inactive as soon as you're done recording.

331
00:27:48,400 --> 00:27:54,550
On a VoIP app, you would only go active while you're
on the call and then go inactive when you're done.

332
00:27:54,550 --> 00:27:59,560
And in a Turn-by-Turn app, well, there's some
specific behavior that we would want there,

333
00:27:59,560 --> 00:28:06,470
which is that let's say the user had been playing music
in the background when they ran your app and now it's time

334
00:28:06,470 --> 00:28:08,910
for you to announce the next turn, "turn left."

335
00:28:08,910 --> 00:28:14,330
What we want to happen there is for the iPod music
to be ducked -- for it to lower its volume --

336
00:28:14,330 --> 00:28:20,260
you make your announcement and then you go
inactive to bring the iPod music back up again.

337
00:28:20,260 --> 00:28:26,530
The same would be true if it was audio playing from
a third-party application in the background as well.

338
00:28:26,530 --> 00:28:33,080
So let me just show you in specific
how you might go about that.

339
00:28:33,080 --> 00:28:36,620
I want to focus on the VoIP app and the Turn-by-Turn app.

340
00:28:36,620 --> 00:28:41,060
So again I said, with the VoIP app, you would
go active when it's time to start the call --

341
00:28:41,060 --> 00:28:46,000
and this is going interrupt sounds that
might be playing in the background --

342
00:28:46,000 --> 00:28:49,250
and then go inactive when the call is over.

343
00:28:49,250 --> 00:28:58,250
And what we can do, there's actually a new
method in iOS 4.0 called setActivewithFlags.

344
00:28:58,250 --> 00:29:04,930
And what that can do is if you set that flag to
notify others on deactivation, then that other,

345
00:29:04,930 --> 00:29:10,090
the background process that was playing
audio can be notified when the call is over.

346
00:29:10,090 --> 00:29:13,530
And it will be told, "Ah, okay, the call is done.

347
00:29:13,530 --> 00:29:18,790
You can go and resume your audio now,"
with Turn-by-Turn navigation types of apps.

348
00:29:18,790 --> 00:29:22,120
As I said, there's a couple of things you want to do here

349
00:29:22,120 --> 00:29:27,890
so that the other audio gets ducked while you
make your turn announcement and then comes back.

350
00:29:27,890 --> 00:29:29,450
And this is in three steps here.

351
00:29:29,450 --> 00:29:32,090
So this is the first step, the setup.

352
00:29:32,090 --> 00:29:38,320
The first thing that we're going to do, of course, is just
choose the right category to put our application into.

353
00:29:38,320 --> 00:29:43,890
So the main purpose of a Turn-by-Turn app
would be to announce these instructions.

354
00:29:43,890 --> 00:29:50,110
So we would need that to happen regardless of the state
of the ringer switch, the screen locking and so on.

355
00:29:50,110 --> 00:29:53,360
So it will choose the playback category.

356
00:29:53,360 --> 00:29:56,860
Now next, though, we're going to
set an override on that category

357
00:29:56,860 --> 00:29:59,830
and this is the part where I said was optional before.

358
00:29:59,830 --> 00:30:06,640
We're going to set this override to enable our
category -- override our category -- to MixWithOthers.

359
00:30:06,640 --> 00:30:12,770
So "others" being, say, the background music
coming off of another app or maybe from the iPod.

360
00:30:12,770 --> 00:30:19,680
And then third in this step is that we're
going to enable other mixable audio --

361
00:30:19,680 --> 00:30:24,210
or say that the other mixable audio
should duck when we go active.

362
00:30:24,210 --> 00:30:29,380
And so that's what's going to lower its volume when we go
active and then when we go inactive, it will come back.

363
00:30:29,380 --> 00:30:31,780
So here are those two parts.

364
00:30:31,780 --> 00:30:37,450
So when it's time now for us to make the
Turn-by-Turn announcement, we'll go active,

365
00:30:37,450 --> 00:30:42,940
we have some audio player ready to go with the
sound of that announcement, and we play that.

366
00:30:42,940 --> 00:30:47,880
And then whenever that is done, when
it's finished making the announcement --

367
00:30:47,880 --> 00:30:50,090
for example, if we were using the AVAudioPlayer,

368
00:30:50,090 --> 00:30:54,980
we could do that through the delegate
-- that's when we'll go inactive again.

369
00:30:54,980 --> 00:31:02,100
But now I've been talking about mixing with background
audio so let me go into a little more detail about that.

370
00:31:02,100 --> 00:31:05,570
And there's really a convergence
of a few different topics here.

371
00:31:05,570 --> 00:31:12,680
So your application might be playing a variety of
sounds, it might be taking advantage of a hardware codec,

372
00:31:12,680 --> 00:31:17,560
it may be using a software codec, or just
playing straight through with the mixer.

373
00:31:17,560 --> 00:31:22,500
Another app might be running in the
background and also playing sounds.

374
00:31:22,500 --> 00:31:25,440
So it might be the iPod application is running

375
00:31:25,440 --> 00:31:29,800
or it could be a third-party application
now that's running in the background.

376
00:31:29,800 --> 00:31:33,890
And so we have to sort of arbitrate:
Well, what's the user going to hear?

377
00:31:33,890 --> 00:31:38,830
And it's going to depend on things that
are going on in both of the applications.

378
00:31:38,830 --> 00:31:42,890
It will depend on what category
both of the applications have set.

379
00:31:42,890 --> 00:31:51,200
And related to that, it will depend on whether either
one of those have enabled MixingWithOthers if they happen

380
00:31:51,200 --> 00:31:53,930
to choose the playback or play and record category.

381
00:31:53,930 --> 00:32:02,260
So all of this is ending up to define something that I call
"mixable" or "nonmixable" as a state of your application.

382
00:32:02,260 --> 00:32:10,270
If your application -- now by default, the only mixable
kind of application is one that chooses the ambient category

383
00:32:10,270 --> 00:32:17,790
but if you choose the playback or play and record and
override it to MixWithOthers, then those become mixable.

384
00:32:17,790 --> 00:32:21,640
Otherwise, everything but ambient would be nonmixable.

385
00:32:21,640 --> 00:32:26,430
So let me show you this just in a
picture what's going to happen here.

386
00:32:26,430 --> 00:32:31,200
So let's say that this is your app, the
foreground app, and you're playing in the AAC file

387
00:32:31,200 --> 00:32:34,420
or some of you might be playing an mp3 file.

388
00:32:34,420 --> 00:32:38,480
But just for the sake of discussion, I'm going to use AAC --

389
00:32:38,480 --> 00:32:43,810
but just bear in mind that the same
things would apply if that were the case.

390
00:32:43,810 --> 00:32:51,090
Now, if you had put your application into a category that
is nonmixable, then you will be able to take advantage

391
00:32:51,090 --> 00:32:56,300
of the hardware codec to playback that compressed track.

392
00:32:56,300 --> 00:33:01,530
It will go into the mixer and out to the playback hardware.

393
00:33:01,530 --> 00:33:09,600
Now, if you had chosen a mixable
category, then what will happen is

394
00:33:09,600 --> 00:33:13,470
that actually your sound is going to be decoded in software.

395
00:33:13,470 --> 00:33:20,730
So we have mp3 and AAC and a number of other
software decoders and those will just run on the CPU

396
00:33:20,730 --> 00:33:25,880
to code your audio, have that go in through
the mixer and out to the playback hardware.

397
00:33:25,880 --> 00:33:31,640
But okay, so this is the basics and you guys
are probably already familiar with this part

398
00:33:31,640 --> 00:33:34,800
but now what happens if there's a background application?

399
00:33:34,800 --> 00:33:38,220
The thing is, it's the exact same story.

400
00:33:38,220 --> 00:33:45,860
So a background application -- let's say that it's
playing a music soundtrack of its own, it has an mp3 file.

401
00:33:45,860 --> 00:33:52,570
If it chooses a nonmixable category, then it will
be able to take advantage of the hardware codec

402
00:33:52,570 --> 00:33:54,820
and its sounds will play through the mixer,

403
00:33:54,820 --> 00:34:00,620
they will be mixed with your music
soundtrack, and out to the playback hardware.

404
00:34:00,620 --> 00:34:02,990
The same thing again.

405
00:34:02,990 --> 00:34:09,630
If they choose a mixable category -- if this is the case --
and you're mixable, too, then both of these will be decoded

406
00:34:09,630 --> 00:34:13,340
in software and play out through the playback hardware.

407
00:34:13,340 --> 00:34:21,940
But there's one case that you need to be thinking
about: What if you've chosen a nonmixable category

408
00:34:21,940 --> 00:34:28,120
for your application, meaning you're asking to use the
hardware codec and there's something else that's going

409
00:34:28,120 --> 00:34:30,960
to be running the background, maybe
when your application started,

410
00:34:30,960 --> 00:34:33,960
there was already something there,
what's going to happen with it?

411
00:34:33,960 --> 00:34:38,890
Well, the result will depend on the
category that the background app chose.

412
00:34:38,890 --> 00:34:43,090
So if they chose a mixable category,
then they'll get a software codec

413
00:34:43,090 --> 00:34:46,330
and both of these are going to be mixed together.

414
00:34:46,330 --> 00:34:51,910
So even though you have chosen a nonmixable
category for your app, since they have decided

415
00:34:51,910 --> 00:34:55,380
to choose a mixable category, essentially
they're playing nice.

416
00:34:55,380 --> 00:34:58,700
They're saying, "Okay, I can pretty much mix with anything."

417
00:34:58,700 --> 00:35:04,650
And so both of these sounds will be heard; they'll be
mixed in the CPU and sent out to the playback hardware.

418
00:35:04,650 --> 00:35:11,170
But if they've chosen a nonmixable category also,
then the sounds from the background app are going

419
00:35:11,170 --> 00:35:14,820
to be silenced when your application goes active.

420
00:35:14,820 --> 00:35:20,560
Those are the different cases now
for mixing with background audio.

421
00:35:20,560 --> 00:35:30,210
Now, what's interesting, though, is that there's actually
a way for you to detect in advance what might happen

422
00:35:30,210 --> 00:35:37,020
and therefore for you to decide maybe of a different
category, depending on whether something is already there.

423
00:35:37,020 --> 00:35:41,070
And you may have seen this in some apps that are
doing something like this: they'll say, "Hey,

424
00:35:41,070 --> 00:35:46,970
do you want to play the game sounds, like, do you want
the game music soundtrack to play in the background

425
00:35:46,970 --> 00:35:53,850
or do you want the iPod or a third-party
app soundtrack to play in the background?"

426
00:35:53,850 --> 00:35:56,230
And it depends, the user either says yes or no.

427
00:35:56,230 --> 00:36:00,080
If they say yes, well, then a couple of things happen.

428
00:36:00,080 --> 00:36:07,110
If they say yes, this is a game, so we would usually
either choose the ambient or solo ambient categories.

429
00:36:07,110 --> 00:36:12,530
So in this case, they say yes, it means,
"Okay, then that means they want my game music

430
00:36:12,530 --> 00:36:18,880
and my game music is an mp3 file or an AAC
file, so it's best to use the hardware codec.

431
00:36:18,880 --> 00:36:21,080
So I want the hardware codec."

432
00:36:21,080 --> 00:36:24,590
So I'll use solo ambient and I'll play my game soundtrack.

433
00:36:24,590 --> 00:36:29,780
And if the user says no, well, then I'm
not going to use the hardware codec.

434
00:36:29,780 --> 00:36:36,010
Maybe all I'm going to do is play sort of
the incidental sounds in my game, you know,

435
00:36:36,010 --> 00:36:39,130
the bullet sounds or just a momentary sounds.

436
00:36:39,130 --> 00:36:44,970
But maybe those are something that would be okay to
decode in software or they may be Linear PCM, WAV,

437
00:36:44,970 --> 00:36:49,080
or AAIF files that can just be mixed directly.

438
00:36:49,080 --> 00:36:53,090
So in that case, I'll choose ambient,
saying, "Hey, I'm fine to mix with others

439
00:36:53,090 --> 00:36:55,240
and there's something else playing in the background.

440
00:36:55,240 --> 00:36:59,330
So I won't play my soundtrack, I'll
just play my incidental sounds."

441
00:36:59,330 --> 00:37:02,210
So all of this is fine.

442
00:37:02,210 --> 00:37:10,680
This is a perfectly good way -- logic -- to use for defining
your app but one part that just might be not be necessary is

443
00:37:10,680 --> 00:37:18,390
to leave this choice as something that the user has to
figure out when they first start the application up.

444
00:37:18,390 --> 00:37:21,490
Instead, you can just detect this programmatically.

445
00:37:21,490 --> 00:37:26,440
So there's an audio session property
called OtherAudioIsPlaying.

446
00:37:26,440 --> 00:37:28,330
And it will come back you know, 1 or 0.

447
00:37:28,330 --> 00:37:34,700
And you can use this to decide whether or not
you play your game music soundtrack and really,

448
00:37:34,700 --> 00:37:42,280
you'll use this to decide what category to use,
whether or not to enable MixingWithOthers or not.

449
00:37:42,280 --> 00:37:45,910
And so I show you here just how
you can get at this information.

450
00:37:45,910 --> 00:37:52,380
So AudioSessionGetProperty, I pass that
token above and I get back the result.

451
00:37:52,380 --> 00:38:00,770
Now one change that's important to note in iOS 4 is
the behavior where your application may be suspended.

452
00:38:00,770 --> 00:38:07,440
So prior to iOS 4, it would be very typical to
just put this in the beginning of your application.

453
00:38:07,440 --> 00:38:10,550
Maybe application did finish launching,
you would check this,

454
00:38:10,550 --> 00:38:15,520
you'd have a value that was valid for
the entire run of your application.

455
00:38:15,520 --> 00:38:21,090
But now something that might happen with, say,
a game is that user might come into your game,

456
00:38:21,090 --> 00:38:23,070
start it up, and then realize, "Oh, you know what?

457
00:38:23,070 --> 00:38:28,140
I want to listen to my first person shooter
soundtrack instead of the game sounds."

458
00:38:28,140 --> 00:38:34,870
And they suspend your app, they go over the iPod, they start
up their playlist, and then they come back into your game.

459
00:38:34,870 --> 00:38:37,820
So you don't want to have already
made your decision about this,

460
00:38:37,820 --> 00:38:42,790
you want to recheck it every time your
application comes back from being suspended.

461
00:38:42,790 --> 00:38:46,900
So check again in applicationDidBecomeActive.

462
00:38:48,300 --> 00:38:53,570
Okay. One more thing, we've been talking
a lot about the behavior about mixing

463
00:38:53,570 --> 00:38:56,690
and how to detect that when your session goes active.

464
00:38:56,690 --> 00:39:02,930
We talked about the behavior with, like, recorders and
VoIP apps and Turn-by-Turn apps, when they go active.

465
00:39:02,930 --> 00:39:07,270
There's one more thing about going
active in a tip or a change that occurred

466
00:39:07,270 --> 00:39:09,530
that I want to just bring to your attention.

467
00:39:09,530 --> 00:39:13,210
It's a behavior change with the MPMoviePlayerController.

468
00:39:13,210 --> 00:39:18,870
So some of you guys might be using this to
playback video; very simple class for that.

469
00:39:18,870 --> 00:39:22,910
But its behavior has changed in
relation to your audio session category.

470
00:39:22,910 --> 00:39:30,250
So prior to iPhone iOS 3.2, the movie player had
its own session -- playback was its category.

471
00:39:30,250 --> 00:39:34,950
And so that would interrupt your session
potentially, it might silence other audio

472
00:39:34,950 --> 00:39:39,160
because that playback by default is a nonmixable category.

473
00:39:39,160 --> 00:39:42,380
And so this could have a number of effects.

474
00:39:42,380 --> 00:39:50,310
Well, now in iPhone iOS 3.2 and above -- so iPad
and then all the devices that support iOS 4 --

475
00:39:50,310 --> 00:39:56,640
the movie player controller now uses your audio
session; it just inherits whatever setting you made.

476
00:39:56,640 --> 00:39:58,500
And so this is actually nice.

477
00:39:58,500 --> 00:40:05,040
It's something now where the movie player's
behavior is now made consistent with your app.

478
00:40:05,040 --> 00:40:10,480
But of course, you have to be just aware of this
change in case you were relying on the movie player

479
00:40:10,480 --> 00:40:16,920
to silence other sounds or do something that was
kind of a side effect just of you playing the video.

480
00:40:16,920 --> 00:40:22,820
Now, if you want to go with the default
behavior -- sorry the new behavior --

481
00:40:22,820 --> 00:40:25,830
then you do nothing, it's just the behavior's changed.

482
00:40:25,830 --> 00:40:30,710
But if you want to revert back to the
way it was prior to iPhone iOS 3.2,

483
00:40:30,710 --> 00:40:35,450
there's a property on the movie player
object if you set that to false.

484
00:40:35,450 --> 00:40:42,860
Use application audio session, you set that to false, then
the movie player will go back to choosing its own category.

485
00:40:42,860 --> 00:40:45,270
But all right.

486
00:40:45,270 --> 00:40:50,400
So folks, we have gone through
three of the five basic tasks.

487
00:40:50,400 --> 00:40:56,190
We've done setting the session in delegate, we talked
about making the right choices as far as your category,

488
00:40:56,190 --> 00:41:01,210
and then going active and all of the
effects that going active can have.

489
00:41:01,210 --> 00:41:03,880
So two more things to talk about and
that's interruptions and route changes.

490
00:41:03,880 --> 00:41:06,750
So let's go into interruptions.

491
00:41:06,750 --> 00:41:14,620
The thing to understand here is that your
application's audio might be interrupted at any time.

492
00:41:14,620 --> 00:41:21,350
So it could be interrupted by a phone call, a
clock alarm, if you're running in the background,

493
00:41:21,350 --> 00:41:24,800
it could be interrupted by a foreground application.

494
00:41:24,800 --> 00:41:32,330
So what will happen if you are interrupted is that your
session is just made inactive and whatever you were doing

495
00:41:32,330 --> 00:41:38,390
with audio is stopped -- if you were playing, it's no
longer playing; if you were recording, then that is stopped.

496
00:41:38,390 --> 00:41:45,640
And it just -- this just happens to you -- it's not a
request that this is about to happen, it's just done.

497
00:41:45,640 --> 00:41:50,470
So what you can do -- there's certain steps,
though, that you can take in reaction to that.

498
00:41:50,470 --> 00:41:54,310
Of course, you might update your user
interface to reflect that this has happened.

499
00:41:54,310 --> 00:42:02,190
But more than anything else, you're interested
in what happens after the interruption has ended.

500
00:42:02,190 --> 00:42:06,570
So if the user has declined the phone
call and comes back into your app,

501
00:42:06,570 --> 00:42:10,290
you're interested in getting your audio restarted again.

502
00:42:10,290 --> 00:42:14,300
And there's a number of other cases as well
where, you know, you just need to be --

503
00:42:14,300 --> 00:42:21,110
all that you're really wanting to do is get
back up and running after the interruption.

504
00:42:21,110 --> 00:42:25,250
So there's a few different things that you need to do.

505
00:42:25,250 --> 00:42:32,070
So the first is let's say that we've set up our audio
session and we've implemented these two delegate methods.

506
00:42:32,070 --> 00:42:39,450
Begin interruption and then this one is new
in iOS 4, it's endInterruptionWithFlags.

507
00:42:39,450 --> 00:42:43,100
The old delegate is still available
as well, just endInterruption.

508
00:42:43,100 --> 00:42:45,570
But this lets you get a little bit more fine-grain control.

509
00:42:45,570 --> 00:42:47,620
I'll get to it in a second.

510
00:42:47,620 --> 00:42:51,310
So when the interruption begins, that
means the phone call has come in.

511
00:42:51,310 --> 00:42:55,010
You know, as I say, the playback has
stopped, you are already inactive

512
00:42:55,010 --> 00:43:00,850
and so what you really should just do is change
the state of your user interface to reflect that.

513
00:43:00,850 --> 00:43:03,250
If you're a game, you would probably
go to your pause screen.

514
00:43:03,250 --> 00:43:08,930
If you're an audio player, well, you would
change your playback icon from whatever it was

515
00:43:08,930 --> 00:43:11,820
to something to say, "Okay, restart again."

516
00:43:11,820 --> 00:43:18,410
Let's say that the user declines the phone
call and is now back in your application.

517
00:43:18,410 --> 00:43:24,250
Well, now the interruption has ended and there's this
flag that can be passed into you that will tell you,

518
00:43:24,250 --> 00:43:31,670
based on various characteristics of the interruption
whether or not your session should resume, okay?

519
00:43:31,670 --> 00:43:35,870
Whether you should start playback,
restart playback, or restart recording.

520
00:43:35,870 --> 00:43:42,310
And assuming that that's the case, meaning if you're past
the flag -- AVAudioSessionInterruptionFlag should resume --

521
00:43:42,310 --> 00:43:45,300
then now you can just fire everything back up.

522
00:43:45,300 --> 00:43:51,550
So you will set your session as active again, you
can update your user interface, and resume playback.

523
00:43:51,550 --> 00:43:57,350
So that actually, though, is just the general case.

524
00:43:57,350 --> 00:44:02,900
There's actually a few instances depending on if
you might be using one of the lower-level audio APIs

525
00:44:02,900 --> 00:44:09,910
from the Audio Toolbox or using
OpenAL that you need to take care of.

526
00:44:09,910 --> 00:44:13,140
So let's start with OpenAL.

527
00:44:13,140 --> 00:44:23,220
Now OpenAL has this concept of a context that is analogous
to the position of the listener in the OpenAL's world.

528
00:44:23,220 --> 00:44:30,740
And the context does not survive through an interruption,
does not stay current through an interruption.

529
00:44:30,740 --> 00:44:36,190
So what you need to actually do is when the
interruption begins, if you are using OpenAL,

530
00:44:36,190 --> 00:44:39,450
then at that point you will want to invalidate the context.

531
00:44:39,450 --> 00:44:45,710
So you do that by calling alcMakeContextCurrent and
just passing it nil -- that invalidates the context.

532
00:44:45,710 --> 00:44:52,530
Then when interruption is over, you can end that
interruption and pass it -- well, excuse me --

533
00:44:52,530 --> 00:44:59,940
you implement the endInterruptionWithFlags delegate method,
you check to see if the flag says that you should resume,

534
00:44:59,940 --> 00:45:07,980
and if so, then you can set yourself as active and now
is when you make your OpenAL context current again.

535
00:45:07,980 --> 00:45:16,400
Another case that you have to take a few extra steps is if
you're using the AudioQueue API and an interruption occurs.

536
00:45:16,400 --> 00:45:22,590
So if you're using this API, well, then first of
all, when the interruption happens you probably want

537
00:45:22,590 --> 00:45:27,410
to save your playback head or the recording position just

538
00:45:27,410 --> 00:45:32,470
in case your application actually
gets quit or suspended for later.

539
00:45:32,470 --> 00:45:41,180
But now the decision about how you restart will depend
on whether your application is using a hardware codec

540
00:45:41,180 --> 00:45:45,610
or a software codec for whatever it's
doing -- its playback or recording.

541
00:45:45,610 --> 00:45:52,340
If you're using a hardware codec, then that
attachment cannot survive through the interruption.

542
00:45:52,340 --> 00:45:58,580
So you will dispose of the currently playing
AudioQueue when the interruption begins

543
00:45:58,580 --> 00:46:01,480
and then when the interruption is
over, you will create it back again

544
00:46:01,480 --> 00:46:04,590
and start it again with a new queue whenever you're ready.

545
00:46:04,590 --> 00:46:12,650
Now if you're using a software codec, then it's simpler,
there's no need to dispose and restart -- excuse me --

546
00:46:12,650 --> 00:46:21,070
dispose and recreate the queue, all you have to do
is restart it, for example here with AudioQueueStart.

547
00:46:21,070 --> 00:46:25,010
This actually, I just -- one side note that I put up here.

548
00:46:25,010 --> 00:46:27,410
This can actually get a little intricate though.

549
00:46:27,410 --> 00:46:33,350
So we wrote a technical Q&A that has some snippets of
code that you can take a look at and it's up there,

550
00:46:33,350 --> 00:46:36,480
number 1558, up on the developer website.

551
00:46:36,480 --> 00:46:42,180
And the last topic here is routing.

552
00:46:42,180 --> 00:46:48,860
So the behavior that users expect in
terms of the audio system routing is

553
00:46:48,860 --> 00:46:55,550
that whatever gesture they have made most
recently is taken as an expression of their intent

554
00:46:55,550 --> 00:47:02,710
for where the audio should be routed or saying it
way more simply, "whatever happened last, wins."

555
00:47:02,710 --> 00:47:08,530
So if the user plugs in the headset, then we
take that as an expression of their intent --

556
00:47:08,530 --> 00:47:12,760
that they want the audio to now
be routed out through the headset.

557
00:47:12,760 --> 00:47:16,820
Or if they were using the microphone, they
plugged in a headset with a microphone,

558
00:47:16,820 --> 00:47:21,580
then they want the audio input to be
taken from the headset microphone.

559
00:47:21,580 --> 00:47:27,620
And now usually along with that, there's some behavior
as far as whether the audio should continue or not.

560
00:47:27,620 --> 00:47:33,180
When they plug in a headset, we want the
audio to continue to go -- if it was output --

561
00:47:33,180 --> 00:47:35,760
to just keep playing without pausing at all.

562
00:47:35,760 --> 00:47:42,370
But when they unplug the headset, okay,
that's also taken as an intentional gesture

563
00:47:42,370 --> 00:47:45,380
from the user to change the routing on the device.

564
00:47:45,380 --> 00:47:53,030
We'll route back to whenever it was previously, probably the
speaker and in that case generally we want audio playback

565
00:47:53,030 --> 00:47:58,150
to pause so that they unplug their headset, it
doesn't just start blaring out through the speaker

566
00:47:58,150 --> 00:48:01,330
at them before they have a chance to take care of it.

567
00:48:01,330 --> 00:48:08,530
But okay, so these are the behaviors that users expect
and so there are ways for you to respond to route changes

568
00:48:08,530 --> 00:48:11,320
that will let you implement this in your app.

569
00:48:11,320 --> 00:48:17,510
So really there's just three topics I'm going to mention
here: so it's possible for you to query the current route;

570
00:48:17,510 --> 00:48:24,130
it's possible for you to listen for changes and then in
response to those changes, you might either keep playing

571
00:48:24,130 --> 00:48:30,380
or stop playing as I just said; and it's
also possible in limited cases for you

572
00:48:30,380 --> 00:48:35,670
to redirect where the output is currently going.

573
00:48:35,670 --> 00:48:37,880
So first of all, just getting the current route.

574
00:48:37,880 --> 00:48:43,510
So this is just another case where
you can call AudioSessionGetProperty,

575
00:48:43,510 --> 00:48:47,530
pass it this token that I have
at the top of the audio route.

576
00:48:47,530 --> 00:48:52,450
So this is going to give you back CFStringRef,
just with the name of the current route.

577
00:48:52,450 --> 00:48:56,470
And so you can see here that I'm outputting that to the log.

578
00:48:56,470 --> 00:49:04,160
But it will just tell you, "Okay, the current route is
the speaker, the headphone, the receiver," and so on.

579
00:49:04,160 --> 00:49:10,610
Now, more importantly, though, is that you probably
actually would rather be listening for changes to the route

580
00:49:10,610 --> 00:49:14,980
than to just know where it is now and output
that to the log because you want to react

581
00:49:14,980 --> 00:49:17,270
to those changes accordingly in your app.

582
00:49:17,270 --> 00:49:22,460
And what you do here is -- now this is down in
that lower level API, as I had mentioned earlier --

583
00:49:22,460 --> 00:49:29,190
you're going to set up a C-based callback
using AudioSessionAddPropertyListener.

584
00:49:29,190 --> 00:49:33,430
And basically you will be registering
for notifications of a route change.

585
00:49:33,430 --> 00:49:38,580
And you pass it this token,
AudioSessionProperty_AudioRouteChange.

586
00:49:38,580 --> 00:49:44,160
And then your callback is going to be told the reason
why the route changed, like the user unplugged something

587
00:49:44,160 --> 00:49:49,790
or plugged something in and you'll also be
informed what the route had been previously.

588
00:49:49,790 --> 00:49:54,410
And of course, through what I'd just shown you,
you can also find out what the route is now.

589
00:49:54,410 --> 00:49:56,800
So here's the code, just to get this started.

590
00:49:56,800 --> 00:49:58,910
We're setting up a property listener here.

591
00:49:58,910 --> 00:50:03,810
AudioSessionAddPropertyListener with the
audio route change, we set up our C function

592
00:50:03,810 --> 00:50:07,740
and you can optionally pass it some data as well.

593
00:50:07,740 --> 00:50:12,150
Now what that C function is going to look
like, this is what you would have written.

594
00:50:12,150 --> 00:50:18,050
The final parameter to this function
is this void* with some data.

595
00:50:18,050 --> 00:50:22,090
And that data can be cast to a CFDictionaryRef.

596
00:50:22,090 --> 00:50:25,570
And inside that dictionary is where
you're going find the information

597
00:50:25,570 --> 00:50:28,740
about what the old route was and the reason why it changed.

598
00:50:28,740 --> 00:50:31,170
So that's all I'm doing here, getting those values out.

599
00:50:31,170 --> 00:50:36,040
And then you can act accordingly in your app.

600
00:50:37,150 --> 00:50:42,500
Now, the third one that I mentioned
is that in certain limited cases,

601
00:50:42,500 --> 00:50:47,270
it is also possible for you to redirect the output.

602
00:50:47,270 --> 00:50:52,310
Now we actually limit this in most
cases because for most of the instances

603
00:50:52,310 --> 00:50:58,280
where the output should be rerouted,
we want to leave that up to the user.

604
00:50:58,280 --> 00:51:00,920
If the user has plugged in or unplugged the headset,

605
00:51:00,920 --> 00:51:05,930
then we take as an intentional gesture
from the user to change the route.

606
00:51:05,930 --> 00:51:11,960
And we really don't allow for third-party
apps to interfere with that.

607
00:51:11,960 --> 00:51:19,160
But there's this one case where it may be necessary for
an application to change the route itself and that's

608
00:51:19,160 --> 00:51:22,330
if you're using the category play and record.

609
00:51:22,330 --> 00:51:27,340
Now you might remember from earlier in the talk how I
mentioned that play and record by default will output

610
00:51:27,340 --> 00:51:31,540
to the receiver, that speaker you hold
up to your ear when you're on the phone.

611
00:51:31,540 --> 00:51:39,110
Well, let's say that you're writing a VoIP app and so you're
going to be implementing something where normally that's

612
00:51:39,110 --> 00:51:43,590
where the output would go but you
also want the option of rerouting

613
00:51:43,590 --> 00:51:47,020
out to the main speaker for like a speakerphone mode.

614
00:51:47,020 --> 00:51:53,690
So that's really what this allows you to do: You can set
a property to override the audio route and in this case,

615
00:51:53,690 --> 00:51:59,050
if you were in the play and record category
and you were already outputting the receiver,

616
00:51:59,050 --> 00:52:03,180
you could redirect that out to the main speaker.

617
00:52:03,180 --> 00:52:10,400
But okay, folks, so actually that takes us
through our five topics on the agenda here.

618
00:52:10,400 --> 00:52:15,170
As I said, we set up the session, we chose
a category, and choose that very wisely

619
00:52:15,170 --> 00:52:20,370
for the role that audio plays in your application.

620
00:52:20,370 --> 00:52:24,750
We made the session active and saw all the
effects that that might have with relation

621
00:52:24,750 --> 00:52:29,540
to background audio and mixing and so on.

622
00:52:29,540 --> 00:52:34,310
Then I talked a little bit about handling
interruptions and route changes here in the end.

623
00:52:34,310 --> 00:52:39,590
I just wanted to mention a couple -- as Bill had said,
some sessions coming up so the next session is going to be

624
00:52:39,590 --> 00:52:42,650
about fundamentals about digital audio,
it'll be right here in this room.

625
00:52:42,650 --> 00:52:48,700
And the third session in this room as well is
going to go deep into the use of audio units,

626
00:52:48,700 --> 00:52:54,460
which are awesome for writing applications
that need to do more intense audio processing.

627
00:52:54,460 --> 00:53:01,810
Here is actually my contact information: I'm Allan Schaffer,
so my email address or you can contact Eryk Vershen

628
00:53:01,810 --> 00:53:06,180
who is our media technologies evangelist if you
want to get in touch with us after the show.

629
00:53:06,180 --> 00:53:12,170
And a couple more notes: The audio session programming
guide has a lot of great information that goes

630
00:53:12,170 --> 00:53:17,460
into a little more detail than what I just
spoke about, so be sure to check that out

631
00:53:17,460 --> 00:53:21,760
and really make the right category choice for your app.

632
00:53:21,760 --> 00:53:24,000
And then finally, we have the Apple Developer Forum.

633
00:53:24,000 --> 00:53:30,370
So if you have questions about audio and want to talk
about just among yourselves, among other developers

634
00:53:30,370 --> 00:53:33,070
and along with us, check out the dev forums.

635
00:53:33,070 --> 00:53:34,670
So thank you very much.

636
00:53:34,670 --> 00:53:38,510
[ Applause ]

